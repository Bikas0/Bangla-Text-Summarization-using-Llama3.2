{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0zkwkcxFN9Kk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cdbd5d0-3091-41ce-8180-5893352f8f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb -q\n",
        "!pip install rouge-score bert_score evaluate\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ],
      "metadata": {
        "id": "B5qmtslo0sSi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I696-YVe0sWB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import wandb\n",
        "from datasets import DatasetDict\n",
        "wandb.init(mode=\"disabled\")\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Specify the dataset name\n",
        "dataset_name = \"csebuetnlp/xlsum\"\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(dataset_name,'bengali')\n",
        "\n",
        "# Display the structure of the dataset\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NzMFPD_rzO6L",
        "outputId": "e6dc844d-ec67-466c-d687-3f26eb174436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each split to a DataFrame\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['test'])\n",
        "validation_df = pd.DataFrame(dataset['validation'])\n",
        "\n",
        "# # Display a preview of each DataFrame\n",
        "# print(\"Train DataFrame:\\n\", train_df.head())\n",
        "# print(\"\\nTest DataFrame:\\n\", test_df.head())\n",
        "# print(\"\\nValidation DataFrame:\\n\", validation_df.head())"
      ],
      "metadata": {
        "id": "RebXbjQvzO9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[[\"text\",\"summary\"]]\n",
        "test_df = test_df[[\"text\",\"summary\"]]\n",
        "validation_df = validation_df[[\"text\",\"summary\"]]\n",
        "train_df"
      ],
      "metadata": {
        "id": "-z63GFqfzPBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def replace_strings(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           u\"\\u00C0-\\u017F\"          #latin\n",
        "                           u\"\\u2000-\\u206F\"          #generalPunctuations\n",
        "\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
        "    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n",
        "\n",
        "    text=emoji_pattern.sub(r'', text)\n",
        "    text=english_pattern.sub(r'', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_punctuations(my_str):\n",
        "    # define punctuation\n",
        "    punctuations = '''````¬£|¬¢|√ë+-*/=EROero‡ß≥‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ012‚Äì34567‚Ä¢89‡•§!()-[]{};:'\"‚Äú\\‚Äô,<>./?@#$%^&*_~‚Äò‚Äî‡••‚Äù‚Ä∞ü§£‚öΩÔ∏è‚úåÔøΩÔø∞‡ß∑Ôø∞'''\n",
        "\n",
        "    no_punct = \"\"\n",
        "    for char in my_str:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "\n",
        "    # display the unpunctuated string\n",
        "    return no_punct\n",
        "\n",
        "def preprocessing(text):\n",
        "    out=remove_punctuations(replace_strings(text))\n",
        "    return out"
      ],
      "metadata": {
        "id": "s-oZgjehzoE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'] = train_df.text.apply(lambda x: preprocessing(str(x)))\n",
        "train_df['summary'] = train_df.summary.apply(lambda x: preprocessing(str(x)))\n",
        "\n",
        "test_df['text'] = test_df.text.apply(lambda x: preprocessing(str(x)))\n",
        "test_df['summary'] = test_df.summary.apply(lambda x: preprocessing(str(x)))\n",
        "\n",
        "validation_df['text'] = validation_df.text.apply(lambda x: preprocessing(str(x)))\n",
        "validation_df['summary'] =validation_df.summary.apply(lambda x: preprocessing(str(x)))"
      ],
      "metadata": {
        "id": "qXaDOalNzoHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 =pd.read_excel('/content/stopwords_bangla.xlsx')\n",
        "stop = data1['words'].tolist()"
      ],
      "metadata": {
        "id": "v_YGvThXzoJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwordRemoval(text):\n",
        "    x=str(text)\n",
        "    l=x.split()\n",
        "\n",
        "    stm=[elem for elem in l if elem not in stop]\n",
        "\n",
        "    out=' '.join(stm)\n",
        "\n",
        "    return str(out)"
      ],
      "metadata": {
        "id": "p4zzbbYczoMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'] = train_df.text.apply(lambda x: stopwordRemoval(str(x)))\n",
        "train_df['summary'] = train_df.summary.apply(lambda x: stopwordRemoval(str(x)))\n",
        "\n",
        "test_df['text'] = test_df.text.apply(lambda x: stopwordRemoval(str(x)))\n",
        "test_df['summary'] = test_df.summary.apply(lambda x: stopwordRemoval(str(x)))\n",
        "\n",
        "validation_df['text'] = validation_df.text.apply(lambda x: stopwordRemoval(str(x)))\n",
        "validation_df['summary'] = validation_df.summary.apply(lambda x: stopwordRemoval(str(x)))"
      ],
      "metadata": {
        "id": "bTuYbVARzoQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_df[~train_df['summary'].apply(lambda x: len(x.split()) < 6)]\n",
        "test = test_df[~test_df['summary'].apply(lambda x: len(x.split()) < 6)]\n",
        "validation = validation_df[~validation_df['summary'].apply(lambda x: len(x.split()) < 6)]"
      ],
      "metadata": {
        "id": "8G_9XqmazPE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "-7GYG65H0Ull"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.rename(columns={'text': 'article'}, inplace=True)\n",
        "test.rename(columns={'text': 'article'}, inplace=True)\n",
        "validation.rename(columns={'text': 'article'}, inplace=True)"
      ],
      "metadata": {
        "id": "oIfJHgYs3atO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.shape\n",
        "train = train[:300]\n",
        "test = test[:50]\n",
        "validation = validation[:90]"
      ],
      "metadata": {
        "id": "gaQ9pR6jsrHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "wROS3l5m4XwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06Q4XhFyG9wp"
      },
      "outputs": [],
      "source": [
        "train = Dataset.from_dict(train)\n",
        "validation = Dataset.from_dict(validation)\n",
        "test = Dataset.from_dict(test)\n",
        "\n",
        "# Create DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'validation': validation,\n",
        "    'test': test,\n",
        "    'train': train\n",
        "})\n",
        "\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    # model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        "    token = \"\",\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "alpaca_prompt = \"\"\"\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    # Retrieve question and answer from the example\n",
        "    instruction = \"Please provide a summary of the following article\"\n",
        "    question = example[\"article\"]\n",
        "    answer = example[\"summary\"]\n",
        "\n",
        "    # Check the structure and content of the example\n",
        "    # print(f\"Question: {question}\")\n",
        "    # print(f\"Answer: {answer}\")\n",
        "\n",
        "    # Construct the formatted prompt text\n",
        "    prompt_text = alpaca_prompt.format(instruction, question, answer) + EOS_TOKEN\n",
        "\n",
        "    # Return the formatted prompt text as a dictionary\n",
        "    return {\"text\": prompt_text}\n",
        "\n",
        "# Assuming 'dataset' is your dataset object\n",
        "dataset = dataset.map(formatting_prompts_func)\n",
        "\n",
        "# Now check the dataset and ensure that it has been transformed correctly\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    eval_dataset = dataset[\"validation\"],\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 3,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 5,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        evaluation_strategy='epoch',\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"output3\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzjSOSiYF3Kq"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amG-yRpzUplh"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/output1/checkpoint-80\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Please provide a summary of the following article\", # instruction\n",
        "        \"‡¶Ö‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßç‡¶∞‡ßü‡ßá‡¶° ‡¶´‡ßã‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶Æ‡¶æ‡¶§‡¶æ‡¶∞‡¶æ ‡¶®‡¶ø‡¶ú ‡¶°‡¶ø‡¶≠‡¶æ‡¶á‡¶∏‡ßá ‡¶Ö‡¶™‡¶æ‡¶∞‡ßá‡¶ü‡¶ø‡¶Ç ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶ì‡¶è‡¶∏ ‡¶≤‡¶≤‡¶ø‡¶™‡¶™‡ßá‡¶∞ ‡¶™‡ßÅ‡¶∞‡¶æ‡¶®‡ßã ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡¶∞‡¶£ ‡¶ï‡¶ø‡¶ü‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶á\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2048)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "# Calculating METEOR (requires nltk)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Calculating ROUGE scores\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "\n",
        "# Load your model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"/content/output1/checkpoint-80\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)  # Enable faster inference\n"
      ],
      "metadata": {
        "id": "yllaPKR8fPg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import nltk\n",
        "nltk.download('wordnet')  # Required for METEOR\n",
        "\n",
        "# Define inputs and outputs\n",
        "references = [\"‡¶ï‡¶æ‡¶†‡¶¨‡¶æ‡¶¶‡¶æ‡¶Æ ‡¶ñ‡¶æ‡¶® ‡¶≠‡ßÅ‡¶Å‡ßú‡¶ø ‡¶ï‡¶Æ‡¶æ‡¶®\"]\n",
        "predictions = [generated_text]\n",
        "\n",
        "# BLEU Score\n",
        "bleu_scores = [sentence_bleu([ref], pred) for ref, pred in zip(references, predictions)]\n",
        "avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "# ROUGE Score - removing stemmer for Bengali\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
        "rouge_scores = [scorer.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "avg_rouge_scores = {k: sum([score[k].fmeasure for score in rouge_scores]) / len(rouge_scores) for k in rouge_scores[0].keys()}\n",
        "\n",
        "# Tokenize Bengali Text for METEOR\n",
        "tokenized_references = [[ref.split()] for ref in references]  # Tokenize each reference sentence\n",
        "tokenized_predictions = [pred.split() for pred in predictions]\n",
        "\n",
        "# METEOR Score\n",
        "meteor_scores = [meteor_score(ref, pred) for ref, pred in zip(tokenized_references, tokenized_predictions)]\n",
        "avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
        "\n",
        "# BERTScore (Change 'en' to 'bn' for Bengali if supported)\n",
        "P, R, F1 = bert_score(predictions, references, lang=\"bn\")\n",
        "avg_bert_score = F1.mean().item()\n",
        "\n",
        "# Print results\n",
        "print(\"Average BLEU Score:\", avg_bleu_score)\n",
        "print(\"Average ROUGE Scores:\", avg_rouge_scores)\n",
        "print(\"Average METEOR Score:\", avg_meteor_score)\n",
        "print(\"Average BERTScore F1:\", avg_bert_score)\n"
      ],
      "metadata": {
        "id": "rjInlvoqgnVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"test\"][0]"
      ],
      "metadata": {
        "id": "JF6HpfRHe_4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset  # Only load_dataset is imported from datasets\n",
        "from evaluate import load # load instead of load_metric is imported from evaluate\n",
        "\n",
        "# Your code remains the same...\n",
        "rouge_metric = load('rouge') # load instead of load_metric is used here"
      ],
      "metadata": {
        "id": "8Herrpv_jO4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tqdm\n",
        "from tqdm import tqdm # Import tqdm"
      ],
      "metadata": {
        "id": "oFoD1f2ykmMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]"
      ],
      "metadata": {
        "id": "7FcM-KSKlTyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                               batch_size=16,\n",
        "                               column_text=\"article\",\n",
        "                               column_summary=\"highlights\"):\n",
        "\n",
        "    # Define the device (CPU or GPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device here\n",
        "\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        # Change max_length to a higher value or use max_new_tokens\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                         length_penalty=0.8, num_beams=1, # Set num_beams to 1\n",
        "                         max_new_tokens=128)\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the  token, and add the decoded texts with the references to the metric.\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
        "\n",
        "\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score\n",
        "\n",
        "rouge_metric = load('rouge')\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "score = calculate_metric_on_test_ds(\n",
        "    test,\n",
        "    rouge_metric,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch_size = 4,\n",
        "    column_text = 'article',\n",
        "    column_summary= 'summary'\n",
        ")\n",
        "# Access the fmeasure directly from the score dictionary\n",
        "rouge_dict = dict((rn, score[rn]) for rn in rouge_names) # Change this line to access the fmeasure directly\n",
        "pd.DataFrame(rouge_dict, index = [f'pegasus'] )"
      ],
      "metadata": {
        "id": "e9Y8yJwEnmQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWC_-ClFdada"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY7_pJQOL4vC"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Please provide a detailed answer to the following question\", # instruction\n",
        "        \"‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶™‡¶∞‡¶ø‡¶∑‡¶¶‡ßá‡¶∞ ‡¶∏‡¶≠‡¶æ ‡¶ï‡ßã‡¶•‡¶æ‡ßü ‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶ø‡¶§ ‡¶π‡ßü?\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 2048, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufyC8oRpL_iC"
      },
      "source": [
        "You can also use Hugging Face's AutoModelForPeftCausalLM. Only use this if you do not have unsloth installed. It can be hopelessly slow, since 4bit model downloading is not supported, and Unsloth's inference is 2x faster."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}