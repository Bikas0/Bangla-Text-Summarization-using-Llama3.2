{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 40.241448692152915,
  "eval_steps": 500,
  "global_step": 20000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002012072434607646,
      "grad_norm": 0.3592970073223114,
      "learning_rate": 4e-05,
      "loss": 1.1404,
      "step": 1
    },
    {
      "epoch": 0.004024144869215292,
      "grad_norm": 0.35322245955467224,
      "learning_rate": 8e-05,
      "loss": 1.1162,
      "step": 2
    },
    {
      "epoch": 0.006036217303822937,
      "grad_norm": 0.38041993975639343,
      "learning_rate": 0.00012,
      "loss": 1.1778,
      "step": 3
    },
    {
      "epoch": 0.008048289738430584,
      "grad_norm": 0.3622376024723053,
      "learning_rate": 0.00016,
      "loss": 1.1586,
      "step": 4
    },
    {
      "epoch": 0.01006036217303823,
      "grad_norm": 0.3219459354877472,
      "learning_rate": 0.0002,
      "loss": 1.0904,
      "step": 5
    },
    {
      "epoch": 0.012072434607645875,
      "grad_norm": 0.3063453137874603,
      "learning_rate": 0.00019999597545024652,
      "loss": 1.0914,
      "step": 6
    },
    {
      "epoch": 0.014084507042253521,
      "grad_norm": 0.3730325698852539,
      "learning_rate": 0.000199991950900493,
      "loss": 1.0699,
      "step": 7
    },
    {
      "epoch": 0.01609657947686117,
      "grad_norm": 0.3381705582141876,
      "learning_rate": 0.00019998792635073952,
      "loss": 1.0777,
      "step": 8
    },
    {
      "epoch": 0.018108651911468814,
      "grad_norm": 0.2758950889110565,
      "learning_rate": 0.00019998390180098603,
      "loss": 1.0558,
      "step": 9
    },
    {
      "epoch": 0.02012072434607646,
      "grad_norm": 0.27777737379074097,
      "learning_rate": 0.00019997987725123252,
      "loss": 1.0552,
      "step": 10
    },
    {
      "epoch": 0.022132796780684104,
      "grad_norm": 0.25962239503860474,
      "learning_rate": 0.00019997585270147903,
      "loss": 1.0032,
      "step": 11
    },
    {
      "epoch": 0.02414486921529175,
      "grad_norm": 0.3075472414493561,
      "learning_rate": 0.00019997182815172551,
      "loss": 0.9904,
      "step": 12
    },
    {
      "epoch": 0.026156941649899398,
      "grad_norm": 0.26472148299217224,
      "learning_rate": 0.00019996780360197205,
      "loss": 1.0162,
      "step": 13
    },
    {
      "epoch": 0.028169014084507043,
      "grad_norm": 0.23052650690078735,
      "learning_rate": 0.00019996377905221854,
      "loss": 0.9935,
      "step": 14
    },
    {
      "epoch": 0.030181086519114688,
      "grad_norm": 0.21523545682430267,
      "learning_rate": 0.00019995975450246505,
      "loss": 0.9591,
      "step": 15
    },
    {
      "epoch": 0.03219315895372234,
      "grad_norm": 0.19451825320720673,
      "learning_rate": 0.00019995572995271154,
      "loss": 0.9457,
      "step": 16
    },
    {
      "epoch": 0.03420523138832998,
      "grad_norm": 0.21367216110229492,
      "learning_rate": 0.00019995170540295805,
      "loss": 1.0298,
      "step": 17
    },
    {
      "epoch": 0.03621730382293763,
      "grad_norm": 0.2106979787349701,
      "learning_rate": 0.00019994768085320456,
      "loss": 0.9827,
      "step": 18
    },
    {
      "epoch": 0.03822937625754527,
      "grad_norm": 0.18794304132461548,
      "learning_rate": 0.00019994365630345107,
      "loss": 0.9313,
      "step": 19
    },
    {
      "epoch": 0.04024144869215292,
      "grad_norm": 0.19104403257369995,
      "learning_rate": 0.00019993963175369756,
      "loss": 0.9478,
      "step": 20
    },
    {
      "epoch": 0.04225352112676056,
      "grad_norm": 0.18822218477725983,
      "learning_rate": 0.00019993560720394407,
      "loss": 0.9501,
      "step": 21
    },
    {
      "epoch": 0.04426559356136821,
      "grad_norm": 0.19012857973575592,
      "learning_rate": 0.00019993158265419055,
      "loss": 0.9551,
      "step": 22
    },
    {
      "epoch": 0.04627766599597585,
      "grad_norm": 0.18311160802841187,
      "learning_rate": 0.0001999275581044371,
      "loss": 0.926,
      "step": 23
    },
    {
      "epoch": 0.0482897384305835,
      "grad_norm": 0.23411378264427185,
      "learning_rate": 0.00019992353355468358,
      "loss": 0.9305,
      "step": 24
    },
    {
      "epoch": 0.05030181086519115,
      "grad_norm": 0.1914064884185791,
      "learning_rate": 0.0001999195090049301,
      "loss": 0.9358,
      "step": 25
    },
    {
      "epoch": 0.052313883299798795,
      "grad_norm": 0.18441425263881683,
      "learning_rate": 0.00019991548445517658,
      "loss": 0.9074,
      "step": 26
    },
    {
      "epoch": 0.05432595573440644,
      "grad_norm": 0.18069277703762054,
      "learning_rate": 0.0001999114599054231,
      "loss": 0.9371,
      "step": 27
    },
    {
      "epoch": 0.056338028169014086,
      "grad_norm": 0.17367291450500488,
      "learning_rate": 0.0001999074353556696,
      "loss": 0.9109,
      "step": 28
    },
    {
      "epoch": 0.05835010060362173,
      "grad_norm": 0.18528318405151367,
      "learning_rate": 0.0001999034108059161,
      "loss": 0.9326,
      "step": 29
    },
    {
      "epoch": 0.060362173038229376,
      "grad_norm": 0.17921629548072815,
      "learning_rate": 0.0001998993862561626,
      "loss": 0.9024,
      "step": 30
    },
    {
      "epoch": 0.06237424547283702,
      "grad_norm": 0.17852111160755157,
      "learning_rate": 0.0001998953617064091,
      "loss": 0.9016,
      "step": 31
    },
    {
      "epoch": 0.06438631790744467,
      "grad_norm": 0.18328867852687836,
      "learning_rate": 0.0001998913371566556,
      "loss": 0.8763,
      "step": 32
    },
    {
      "epoch": 0.06639839034205232,
      "grad_norm": 0.18954968452453613,
      "learning_rate": 0.00019988731260690213,
      "loss": 0.9291,
      "step": 33
    },
    {
      "epoch": 0.06841046277665996,
      "grad_norm": 0.17676837742328644,
      "learning_rate": 0.00019988328805714862,
      "loss": 0.9491,
      "step": 34
    },
    {
      "epoch": 0.07042253521126761,
      "grad_norm": 0.1839909553527832,
      "learning_rate": 0.00019987926350739513,
      "loss": 0.9633,
      "step": 35
    },
    {
      "epoch": 0.07243460764587525,
      "grad_norm": 0.17566779255867004,
      "learning_rate": 0.00019987523895764162,
      "loss": 0.8901,
      "step": 36
    },
    {
      "epoch": 0.0744466800804829,
      "grad_norm": 0.1882728487253189,
      "learning_rate": 0.00019987121440788813,
      "loss": 0.9071,
      "step": 37
    },
    {
      "epoch": 0.07645875251509054,
      "grad_norm": 0.18943190574645996,
      "learning_rate": 0.00019986718985813464,
      "loss": 0.9047,
      "step": 38
    },
    {
      "epoch": 0.07847082494969819,
      "grad_norm": 0.19589433073997498,
      "learning_rate": 0.00019986316530838113,
      "loss": 0.9129,
      "step": 39
    },
    {
      "epoch": 0.08048289738430583,
      "grad_norm": 0.18922263383865356,
      "learning_rate": 0.00019985914075862764,
      "loss": 0.9063,
      "step": 40
    },
    {
      "epoch": 0.08249496981891348,
      "grad_norm": 0.18758566677570343,
      "learning_rate": 0.00019985511620887415,
      "loss": 0.8803,
      "step": 41
    },
    {
      "epoch": 0.08450704225352113,
      "grad_norm": 0.1876031905412674,
      "learning_rate": 0.00019985109165912064,
      "loss": 0.9233,
      "step": 42
    },
    {
      "epoch": 0.08651911468812877,
      "grad_norm": 0.1912473738193512,
      "learning_rate": 0.00019984706710936715,
      "loss": 0.9439,
      "step": 43
    },
    {
      "epoch": 0.08853118712273642,
      "grad_norm": 0.1948671191930771,
      "learning_rate": 0.00019984304255961366,
      "loss": 0.9103,
      "step": 44
    },
    {
      "epoch": 0.09054325955734406,
      "grad_norm": 0.18167667090892792,
      "learning_rate": 0.00019983901800986015,
      "loss": 0.8782,
      "step": 45
    },
    {
      "epoch": 0.0925553319919517,
      "grad_norm": 0.18685849010944366,
      "learning_rate": 0.00019983499346010666,
      "loss": 0.9306,
      "step": 46
    },
    {
      "epoch": 0.09456740442655935,
      "grad_norm": 0.18111549317836761,
      "learning_rate": 0.00019983096891035314,
      "loss": 0.8696,
      "step": 47
    },
    {
      "epoch": 0.096579476861167,
      "grad_norm": 0.2049015313386917,
      "learning_rate": 0.00019982694436059968,
      "loss": 0.909,
      "step": 48
    },
    {
      "epoch": 0.09859154929577464,
      "grad_norm": 0.180277481675148,
      "learning_rate": 0.00019982291981084617,
      "loss": 0.8717,
      "step": 49
    },
    {
      "epoch": 0.1006036217303823,
      "grad_norm": 0.18026374280452728,
      "learning_rate": 0.00019981889526109268,
      "loss": 0.8932,
      "step": 50
    },
    {
      "epoch": 0.10261569416498995,
      "grad_norm": 0.18579944968223572,
      "learning_rate": 0.00019981487071133916,
      "loss": 0.8922,
      "step": 51
    },
    {
      "epoch": 0.10462776659959759,
      "grad_norm": 0.18443752825260162,
      "learning_rate": 0.00019981084616158568,
      "loss": 0.8886,
      "step": 52
    },
    {
      "epoch": 0.10663983903420524,
      "grad_norm": 0.18890155851840973,
      "learning_rate": 0.0001998068216118322,
      "loss": 0.875,
      "step": 53
    },
    {
      "epoch": 0.10865191146881288,
      "grad_norm": 0.1908087283372879,
      "learning_rate": 0.0001998027970620787,
      "loss": 0.8968,
      "step": 54
    },
    {
      "epoch": 0.11066398390342053,
      "grad_norm": 0.17529773712158203,
      "learning_rate": 0.0001997987725123252,
      "loss": 0.8602,
      "step": 55
    },
    {
      "epoch": 0.11267605633802817,
      "grad_norm": 0.1899130791425705,
      "learning_rate": 0.0001997947479625717,
      "loss": 0.9149,
      "step": 56
    },
    {
      "epoch": 0.11468812877263582,
      "grad_norm": 0.1788182556629181,
      "learning_rate": 0.00019979072341281818,
      "loss": 0.9201,
      "step": 57
    },
    {
      "epoch": 0.11670020120724346,
      "grad_norm": 0.19750864803791046,
      "learning_rate": 0.00019978669886306472,
      "loss": 0.8682,
      "step": 58
    },
    {
      "epoch": 0.11871227364185111,
      "grad_norm": 0.18664686381816864,
      "learning_rate": 0.0001997826743133112,
      "loss": 0.8851,
      "step": 59
    },
    {
      "epoch": 0.12072434607645875,
      "grad_norm": 0.20584599673748016,
      "learning_rate": 0.00019977864976355772,
      "loss": 0.8996,
      "step": 60
    },
    {
      "epoch": 0.1227364185110664,
      "grad_norm": 0.18857166171073914,
      "learning_rate": 0.0001997746252138042,
      "loss": 0.8999,
      "step": 61
    },
    {
      "epoch": 0.12474849094567404,
      "grad_norm": 0.2012850046157837,
      "learning_rate": 0.00019977060066405072,
      "loss": 0.8907,
      "step": 62
    },
    {
      "epoch": 0.1267605633802817,
      "grad_norm": 0.19432254135608673,
      "learning_rate": 0.00019976657611429723,
      "loss": 0.8828,
      "step": 63
    },
    {
      "epoch": 0.12877263581488935,
      "grad_norm": 0.18406537175178528,
      "learning_rate": 0.00019976255156454374,
      "loss": 0.8783,
      "step": 64
    },
    {
      "epoch": 0.13078470824949698,
      "grad_norm": 0.1907995194196701,
      "learning_rate": 0.00019975852701479023,
      "loss": 0.8931,
      "step": 65
    },
    {
      "epoch": 0.13279678068410464,
      "grad_norm": 0.1925678551197052,
      "learning_rate": 0.00019975450246503674,
      "loss": 0.8796,
      "step": 66
    },
    {
      "epoch": 0.13480885311871227,
      "grad_norm": 0.20473621785640717,
      "learning_rate": 0.00019975047791528322,
      "loss": 0.9364,
      "step": 67
    },
    {
      "epoch": 0.13682092555331993,
      "grad_norm": 0.19179785251617432,
      "learning_rate": 0.00019974645336552976,
      "loss": 0.8708,
      "step": 68
    },
    {
      "epoch": 0.13883299798792756,
      "grad_norm": 0.1910959631204605,
      "learning_rate": 0.00019974242881577625,
      "loss": 0.8735,
      "step": 69
    },
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 0.18460755050182343,
      "learning_rate": 0.00019973840426602276,
      "loss": 0.9019,
      "step": 70
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.19343465566635132,
      "learning_rate": 0.00019973437971626925,
      "loss": 0.8856,
      "step": 71
    },
    {
      "epoch": 0.1448692152917505,
      "grad_norm": 0.19092684984207153,
      "learning_rate": 0.00019973035516651576,
      "loss": 0.8601,
      "step": 72
    },
    {
      "epoch": 0.14688128772635814,
      "grad_norm": 0.19624558091163635,
      "learning_rate": 0.00019972633061676227,
      "loss": 0.8755,
      "step": 73
    },
    {
      "epoch": 0.1488933601609658,
      "grad_norm": 0.1975712776184082,
      "learning_rate": 0.00019972230606700876,
      "loss": 0.8644,
      "step": 74
    },
    {
      "epoch": 0.15090543259557343,
      "grad_norm": 0.19293682277202606,
      "learning_rate": 0.00019971828151725527,
      "loss": 0.8665,
      "step": 75
    },
    {
      "epoch": 0.1529175050301811,
      "grad_norm": 0.20011775195598602,
      "learning_rate": 0.00019971425696750178,
      "loss": 0.8585,
      "step": 76
    },
    {
      "epoch": 0.15492957746478872,
      "grad_norm": 0.19549405574798584,
      "learning_rate": 0.00019971023241774827,
      "loss": 0.8747,
      "step": 77
    },
    {
      "epoch": 0.15694164989939638,
      "grad_norm": 0.18755458295345306,
      "learning_rate": 0.00019970620786799478,
      "loss": 0.8287,
      "step": 78
    },
    {
      "epoch": 0.158953722334004,
      "grad_norm": 0.19893276691436768,
      "learning_rate": 0.0001997021833182413,
      "loss": 0.8451,
      "step": 79
    },
    {
      "epoch": 0.16096579476861167,
      "grad_norm": 0.19470801949501038,
      "learning_rate": 0.00019969815876848778,
      "loss": 0.9123,
      "step": 80
    },
    {
      "epoch": 0.16297786720321933,
      "grad_norm": 0.18407076597213745,
      "learning_rate": 0.0001996941342187343,
      "loss": 0.8478,
      "step": 81
    },
    {
      "epoch": 0.16498993963782696,
      "grad_norm": 0.18501164019107819,
      "learning_rate": 0.00019969010966898077,
      "loss": 0.8375,
      "step": 82
    },
    {
      "epoch": 0.16700201207243462,
      "grad_norm": 0.20076759159564972,
      "learning_rate": 0.0001996860851192273,
      "loss": 0.8275,
      "step": 83
    },
    {
      "epoch": 0.16901408450704225,
      "grad_norm": 0.18843623995780945,
      "learning_rate": 0.0001996820605694738,
      "loss": 0.8914,
      "step": 84
    },
    {
      "epoch": 0.1710261569416499,
      "grad_norm": 0.1884811818599701,
      "learning_rate": 0.0001996780360197203,
      "loss": 0.8535,
      "step": 85
    },
    {
      "epoch": 0.17303822937625754,
      "grad_norm": 0.20549069344997406,
      "learning_rate": 0.0001996740114699668,
      "loss": 0.8832,
      "step": 86
    },
    {
      "epoch": 0.1750503018108652,
      "grad_norm": 0.23831482231616974,
      "learning_rate": 0.0001996699869202133,
      "loss": 0.8669,
      "step": 87
    },
    {
      "epoch": 0.17706237424547283,
      "grad_norm": 0.19938699901103973,
      "learning_rate": 0.00019966596237045982,
      "loss": 0.8301,
      "step": 88
    },
    {
      "epoch": 0.1790744466800805,
      "grad_norm": 0.19146068394184113,
      "learning_rate": 0.00019966193782070633,
      "loss": 0.8406,
      "step": 89
    },
    {
      "epoch": 0.18108651911468812,
      "grad_norm": 0.21146000921726227,
      "learning_rate": 0.00019965791327095282,
      "loss": 0.8517,
      "step": 90
    },
    {
      "epoch": 0.18309859154929578,
      "grad_norm": 0.20206010341644287,
      "learning_rate": 0.00019965388872119933,
      "loss": 0.8404,
      "step": 91
    },
    {
      "epoch": 0.1851106639839034,
      "grad_norm": 0.20892788469791412,
      "learning_rate": 0.0001996498641714458,
      "loss": 0.8813,
      "step": 92
    },
    {
      "epoch": 0.18712273641851107,
      "grad_norm": 0.20156921446323395,
      "learning_rate": 0.00019964583962169235,
      "loss": 0.8764,
      "step": 93
    },
    {
      "epoch": 0.1891348088531187,
      "grad_norm": 0.19878286123275757,
      "learning_rate": 0.00019964181507193884,
      "loss": 0.8614,
      "step": 94
    },
    {
      "epoch": 0.19114688128772636,
      "grad_norm": 0.21149598062038422,
      "learning_rate": 0.00019963779052218535,
      "loss": 0.8495,
      "step": 95
    },
    {
      "epoch": 0.193158953722334,
      "grad_norm": 0.19808919727802277,
      "learning_rate": 0.00019963376597243184,
      "loss": 0.8903,
      "step": 96
    },
    {
      "epoch": 0.19517102615694165,
      "grad_norm": 0.20713049173355103,
      "learning_rate": 0.00019962974142267835,
      "loss": 0.8821,
      "step": 97
    },
    {
      "epoch": 0.19718309859154928,
      "grad_norm": 0.2020808607339859,
      "learning_rate": 0.00019962571687292486,
      "loss": 0.8646,
      "step": 98
    },
    {
      "epoch": 0.19919517102615694,
      "grad_norm": 0.20864011347293854,
      "learning_rate": 0.00019962169232317137,
      "loss": 0.8627,
      "step": 99
    },
    {
      "epoch": 0.2012072434607646,
      "grad_norm": 0.19495107233524323,
      "learning_rate": 0.00019961766777341786,
      "loss": 0.8179,
      "step": 100
    },
    {
      "epoch": 0.20321931589537223,
      "grad_norm": 0.19543775916099548,
      "learning_rate": 0.00019961364322366437,
      "loss": 0.8189,
      "step": 101
    },
    {
      "epoch": 0.2052313883299799,
      "grad_norm": 0.2077588140964508,
      "learning_rate": 0.00019960961867391085,
      "loss": 0.8741,
      "step": 102
    },
    {
      "epoch": 0.20724346076458752,
      "grad_norm": 0.19997112452983856,
      "learning_rate": 0.0001996055941241574,
      "loss": 0.8833,
      "step": 103
    },
    {
      "epoch": 0.20925553319919518,
      "grad_norm": 0.19277673959732056,
      "learning_rate": 0.00019960156957440388,
      "loss": 0.8487,
      "step": 104
    },
    {
      "epoch": 0.2112676056338028,
      "grad_norm": 0.20130972564220428,
      "learning_rate": 0.0001995975450246504,
      "loss": 0.8402,
      "step": 105
    },
    {
      "epoch": 0.21327967806841047,
      "grad_norm": 0.22019734978675842,
      "learning_rate": 0.00019959352047489688,
      "loss": 0.8751,
      "step": 106
    },
    {
      "epoch": 0.2152917505030181,
      "grad_norm": 0.1881532222032547,
      "learning_rate": 0.0001995894959251434,
      "loss": 0.864,
      "step": 107
    },
    {
      "epoch": 0.21730382293762576,
      "grad_norm": 0.19837062060832977,
      "learning_rate": 0.0001995854713753899,
      "loss": 0.849,
      "step": 108
    },
    {
      "epoch": 0.2193158953722334,
      "grad_norm": 0.19891542196273804,
      "learning_rate": 0.00019958144682563639,
      "loss": 0.8362,
      "step": 109
    },
    {
      "epoch": 0.22132796780684105,
      "grad_norm": 0.18832191824913025,
      "learning_rate": 0.0001995774222758829,
      "loss": 0.8165,
      "step": 110
    },
    {
      "epoch": 0.22334004024144868,
      "grad_norm": 0.182406947016716,
      "learning_rate": 0.00019957339772612938,
      "loss": 0.8393,
      "step": 111
    },
    {
      "epoch": 0.22535211267605634,
      "grad_norm": 0.18925000727176666,
      "learning_rate": 0.0001995693731763759,
      "loss": 0.8405,
      "step": 112
    },
    {
      "epoch": 0.22736418511066397,
      "grad_norm": 0.20123854279518127,
      "learning_rate": 0.0001995653486266224,
      "loss": 0.859,
      "step": 113
    },
    {
      "epoch": 0.22937625754527163,
      "grad_norm": 0.2009832113981247,
      "learning_rate": 0.00019956132407686892,
      "loss": 0.8683,
      "step": 114
    },
    {
      "epoch": 0.23138832997987926,
      "grad_norm": 0.1882718801498413,
      "learning_rate": 0.0001995572995271154,
      "loss": 0.83,
      "step": 115
    },
    {
      "epoch": 0.23340040241448692,
      "grad_norm": 0.1933763325214386,
      "learning_rate": 0.00019955327497736192,
      "loss": 0.8434,
      "step": 116
    },
    {
      "epoch": 0.23541247484909456,
      "grad_norm": 0.21327821910381317,
      "learning_rate": 0.0001995492504276084,
      "loss": 0.8563,
      "step": 117
    },
    {
      "epoch": 0.23742454728370221,
      "grad_norm": 0.19349703192710876,
      "learning_rate": 0.00019954522587785494,
      "loss": 0.8213,
      "step": 118
    },
    {
      "epoch": 0.23943661971830985,
      "grad_norm": 0.18578100204467773,
      "learning_rate": 0.00019954120132810143,
      "loss": 0.8,
      "step": 119
    },
    {
      "epoch": 0.2414486921529175,
      "grad_norm": 0.1941307783126831,
      "learning_rate": 0.00019953717677834794,
      "loss": 0.8127,
      "step": 120
    },
    {
      "epoch": 0.24346076458752516,
      "grad_norm": 0.19577060639858246,
      "learning_rate": 0.00019953315222859442,
      "loss": 0.8484,
      "step": 121
    },
    {
      "epoch": 0.2454728370221328,
      "grad_norm": 0.2020144760608673,
      "learning_rate": 0.00019952912767884094,
      "loss": 0.8613,
      "step": 122
    },
    {
      "epoch": 0.24748490945674045,
      "grad_norm": 0.1880103200674057,
      "learning_rate": 0.00019952510312908745,
      "loss": 0.8624,
      "step": 123
    },
    {
      "epoch": 0.24949698189134809,
      "grad_norm": 0.20033690333366394,
      "learning_rate": 0.00019952107857933396,
      "loss": 0.8767,
      "step": 124
    },
    {
      "epoch": 0.2515090543259557,
      "grad_norm": 0.18609407544136047,
      "learning_rate": 0.00019951705402958045,
      "loss": 0.8271,
      "step": 125
    },
    {
      "epoch": 0.2535211267605634,
      "grad_norm": 0.18860207498073578,
      "learning_rate": 0.00019951302947982696,
      "loss": 0.8178,
      "step": 126
    },
    {
      "epoch": 0.25553319919517103,
      "grad_norm": 0.20004241168498993,
      "learning_rate": 0.00019950900493007344,
      "loss": 0.824,
      "step": 127
    },
    {
      "epoch": 0.2575452716297787,
      "grad_norm": 0.2188415676355362,
      "learning_rate": 0.00019950498038031996,
      "loss": 0.8936,
      "step": 128
    },
    {
      "epoch": 0.2595573440643863,
      "grad_norm": 0.18831130862236023,
      "learning_rate": 0.00019950095583056647,
      "loss": 0.8273,
      "step": 129
    },
    {
      "epoch": 0.26156941649899396,
      "grad_norm": 0.1915147304534912,
      "learning_rate": 0.00019949693128081298,
      "loss": 0.8451,
      "step": 130
    },
    {
      "epoch": 0.2635814889336016,
      "grad_norm": 0.20897985994815826,
      "learning_rate": 0.00019949290673105946,
      "loss": 0.8886,
      "step": 131
    },
    {
      "epoch": 0.2655935613682093,
      "grad_norm": 0.20555637776851654,
      "learning_rate": 0.00019948888218130598,
      "loss": 0.8636,
      "step": 132
    },
    {
      "epoch": 0.2676056338028169,
      "grad_norm": 0.2089225798845291,
      "learning_rate": 0.00019948485763155246,
      "loss": 0.8196,
      "step": 133
    },
    {
      "epoch": 0.26961770623742454,
      "grad_norm": 0.18874627351760864,
      "learning_rate": 0.000199480833081799,
      "loss": 0.8231,
      "step": 134
    },
    {
      "epoch": 0.2716297786720322,
      "grad_norm": 0.19505508244037628,
      "learning_rate": 0.0001994768085320455,
      "loss": 0.8519,
      "step": 135
    },
    {
      "epoch": 0.27364185110663986,
      "grad_norm": 0.19684909284114838,
      "learning_rate": 0.000199472783982292,
      "loss": 0.8143,
      "step": 136
    },
    {
      "epoch": 0.27565392354124746,
      "grad_norm": 0.1987629383802414,
      "learning_rate": 0.00019946875943253848,
      "loss": 0.8378,
      "step": 137
    },
    {
      "epoch": 0.2776659959758551,
      "grad_norm": 0.19563283026218414,
      "learning_rate": 0.000199464734882785,
      "loss": 0.82,
      "step": 138
    },
    {
      "epoch": 0.2796780684104628,
      "grad_norm": 0.19654332101345062,
      "learning_rate": 0.0001994607103330315,
      "loss": 0.8298,
      "step": 139
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 0.195412740111351,
      "learning_rate": 0.00019945668578327802,
      "loss": 0.8617,
      "step": 140
    },
    {
      "epoch": 0.2837022132796781,
      "grad_norm": 0.1914370059967041,
      "learning_rate": 0.0001994526612335245,
      "loss": 0.8315,
      "step": 141
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.199712336063385,
      "learning_rate": 0.00019944863668377102,
      "loss": 0.8229,
      "step": 142
    },
    {
      "epoch": 0.28772635814889336,
      "grad_norm": 0.18904425203800201,
      "learning_rate": 0.0001994446121340175,
      "loss": 0.8663,
      "step": 143
    },
    {
      "epoch": 0.289738430583501,
      "grad_norm": 0.19368506968021393,
      "learning_rate": 0.00019944058758426402,
      "loss": 0.886,
      "step": 144
    },
    {
      "epoch": 0.2917505030181087,
      "grad_norm": 0.19549432396888733,
      "learning_rate": 0.00019943656303451053,
      "loss": 0.8315,
      "step": 145
    },
    {
      "epoch": 0.2937625754527163,
      "grad_norm": 0.19159910082817078,
      "learning_rate": 0.000199432538484757,
      "loss": 0.8488,
      "step": 146
    },
    {
      "epoch": 0.29577464788732394,
      "grad_norm": 0.19186140596866608,
      "learning_rate": 0.00019942851393500352,
      "loss": 0.7898,
      "step": 147
    },
    {
      "epoch": 0.2977867203219316,
      "grad_norm": 0.20097804069519043,
      "learning_rate": 0.00019942448938525004,
      "loss": 0.8333,
      "step": 148
    },
    {
      "epoch": 0.29979879275653926,
      "grad_norm": 0.20294663310050964,
      "learning_rate": 0.00019942046483549655,
      "loss": 0.8126,
      "step": 149
    },
    {
      "epoch": 0.30181086519114686,
      "grad_norm": 0.20830601453781128,
      "learning_rate": 0.00019941644028574303,
      "loss": 0.8627,
      "step": 150
    },
    {
      "epoch": 0.3038229376257545,
      "grad_norm": 0.194064199924469,
      "learning_rate": 0.00019941241573598955,
      "loss": 0.8506,
      "step": 151
    },
    {
      "epoch": 0.3058350100603622,
      "grad_norm": 0.19117200374603271,
      "learning_rate": 0.00019940839118623603,
      "loss": 0.8358,
      "step": 152
    },
    {
      "epoch": 0.30784708249496984,
      "grad_norm": 0.18461540341377258,
      "learning_rate": 0.00019940436663648254,
      "loss": 0.8361,
      "step": 153
    },
    {
      "epoch": 0.30985915492957744,
      "grad_norm": 0.20459674298763275,
      "learning_rate": 0.00019940034208672906,
      "loss": 0.8293,
      "step": 154
    },
    {
      "epoch": 0.3118712273641851,
      "grad_norm": 0.1936265230178833,
      "learning_rate": 0.00019939631753697557,
      "loss": 0.8349,
      "step": 155
    },
    {
      "epoch": 0.31388329979879276,
      "grad_norm": 0.20260265469551086,
      "learning_rate": 0.00019939229298722205,
      "loss": 0.8402,
      "step": 156
    },
    {
      "epoch": 0.3158953722334004,
      "grad_norm": 0.2114637792110443,
      "learning_rate": 0.00019938826843746857,
      "loss": 0.8553,
      "step": 157
    },
    {
      "epoch": 0.317907444668008,
      "grad_norm": 0.20784983038902283,
      "learning_rate": 0.00019938424388771505,
      "loss": 0.8372,
      "step": 158
    },
    {
      "epoch": 0.3199195171026157,
      "grad_norm": 0.19796009361743927,
      "learning_rate": 0.0001993802193379616,
      "loss": 0.8495,
      "step": 159
    },
    {
      "epoch": 0.32193158953722334,
      "grad_norm": 0.20606544613838196,
      "learning_rate": 0.00019937619478820808,
      "loss": 0.8846,
      "step": 160
    },
    {
      "epoch": 0.323943661971831,
      "grad_norm": 0.1908896267414093,
      "learning_rate": 0.0001993721702384546,
      "loss": 0.8354,
      "step": 161
    },
    {
      "epoch": 0.32595573440643866,
      "grad_norm": 0.1866932362318039,
      "learning_rate": 0.00019936814568870107,
      "loss": 0.8189,
      "step": 162
    },
    {
      "epoch": 0.32796780684104626,
      "grad_norm": 0.19398744404315948,
      "learning_rate": 0.00019936412113894758,
      "loss": 0.8818,
      "step": 163
    },
    {
      "epoch": 0.3299798792756539,
      "grad_norm": 0.19489195942878723,
      "learning_rate": 0.0001993600965891941,
      "loss": 0.7989,
      "step": 164
    },
    {
      "epoch": 0.3319919517102616,
      "grad_norm": 0.18967723846435547,
      "learning_rate": 0.0001993560720394406,
      "loss": 0.8219,
      "step": 165
    },
    {
      "epoch": 0.33400402414486924,
      "grad_norm": 0.19411438703536987,
      "learning_rate": 0.0001993520474896871,
      "loss": 0.8242,
      "step": 166
    },
    {
      "epoch": 0.33601609657947684,
      "grad_norm": 0.19936823844909668,
      "learning_rate": 0.0001993480229399336,
      "loss": 0.7827,
      "step": 167
    },
    {
      "epoch": 0.3380281690140845,
      "grad_norm": 0.21218110620975494,
      "learning_rate": 0.0001993439983901801,
      "loss": 0.8528,
      "step": 168
    },
    {
      "epoch": 0.34004024144869216,
      "grad_norm": 0.2124064415693283,
      "learning_rate": 0.00019933997384042663,
      "loss": 0.8632,
      "step": 169
    },
    {
      "epoch": 0.3420523138832998,
      "grad_norm": 0.2027176469564438,
      "learning_rate": 0.00019933594929067312,
      "loss": 0.801,
      "step": 170
    },
    {
      "epoch": 0.3440643863179074,
      "grad_norm": 0.19850051403045654,
      "learning_rate": 0.00019933192474091963,
      "loss": 0.8606,
      "step": 171
    },
    {
      "epoch": 0.3460764587525151,
      "grad_norm": 0.19964805245399475,
      "learning_rate": 0.0001993279001911661,
      "loss": 0.8513,
      "step": 172
    },
    {
      "epoch": 0.34808853118712274,
      "grad_norm": 0.21254149079322815,
      "learning_rate": 0.00019932387564141263,
      "loss": 0.9055,
      "step": 173
    },
    {
      "epoch": 0.3501006036217304,
      "grad_norm": 0.19459998607635498,
      "learning_rate": 0.00019931985109165914,
      "loss": 0.8288,
      "step": 174
    },
    {
      "epoch": 0.352112676056338,
      "grad_norm": 0.19016312062740326,
      "learning_rate": 0.00019931582654190565,
      "loss": 0.7759,
      "step": 175
    },
    {
      "epoch": 0.35412474849094566,
      "grad_norm": 0.20554998517036438,
      "learning_rate": 0.00019931180199215213,
      "loss": 0.7994,
      "step": 176
    },
    {
      "epoch": 0.3561368209255533,
      "grad_norm": 0.2079089730978012,
      "learning_rate": 0.00019930777744239865,
      "loss": 0.8759,
      "step": 177
    },
    {
      "epoch": 0.358148893360161,
      "grad_norm": 0.20498615503311157,
      "learning_rate": 0.00019930375289264513,
      "loss": 0.8254,
      "step": 178
    },
    {
      "epoch": 0.36016096579476864,
      "grad_norm": 0.21735982596874237,
      "learning_rate": 0.00019929972834289164,
      "loss": 0.8446,
      "step": 179
    },
    {
      "epoch": 0.36217303822937624,
      "grad_norm": 0.18807707726955414,
      "learning_rate": 0.00019929570379313816,
      "loss": 0.8186,
      "step": 180
    },
    {
      "epoch": 0.3641851106639839,
      "grad_norm": 0.20500755310058594,
      "learning_rate": 0.00019929167924338464,
      "loss": 0.8356,
      "step": 181
    },
    {
      "epoch": 0.36619718309859156,
      "grad_norm": 0.2055683434009552,
      "learning_rate": 0.00019928765469363115,
      "loss": 0.8204,
      "step": 182
    },
    {
      "epoch": 0.3682092555331992,
      "grad_norm": 0.19351178407669067,
      "learning_rate": 0.00019928363014387767,
      "loss": 0.825,
      "step": 183
    },
    {
      "epoch": 0.3702213279678068,
      "grad_norm": 0.194583922624588,
      "learning_rate": 0.00019927960559412418,
      "loss": 0.8327,
      "step": 184
    },
    {
      "epoch": 0.3722334004024145,
      "grad_norm": 0.20618923008441925,
      "learning_rate": 0.00019927558104437066,
      "loss": 0.8626,
      "step": 185
    },
    {
      "epoch": 0.37424547283702214,
      "grad_norm": 0.19402191042900085,
      "learning_rate": 0.00019927155649461718,
      "loss": 0.8057,
      "step": 186
    },
    {
      "epoch": 0.3762575452716298,
      "grad_norm": 0.20112617313861847,
      "learning_rate": 0.00019926753194486366,
      "loss": 0.8335,
      "step": 187
    },
    {
      "epoch": 0.3782696177062374,
      "grad_norm": 0.2123679667711258,
      "learning_rate": 0.00019926350739511017,
      "loss": 0.8375,
      "step": 188
    },
    {
      "epoch": 0.38028169014084506,
      "grad_norm": 0.19677738845348358,
      "learning_rate": 0.00019925948284535669,
      "loss": 0.8362,
      "step": 189
    },
    {
      "epoch": 0.3822937625754527,
      "grad_norm": 0.20116494596004486,
      "learning_rate": 0.0001992554582956032,
      "loss": 0.8225,
      "step": 190
    },
    {
      "epoch": 0.3843058350100604,
      "grad_norm": 0.1986096352338791,
      "learning_rate": 0.00019925143374584968,
      "loss": 0.8059,
      "step": 191
    },
    {
      "epoch": 0.386317907444668,
      "grad_norm": 0.19059471786022186,
      "learning_rate": 0.0001992474091960962,
      "loss": 0.8224,
      "step": 192
    },
    {
      "epoch": 0.38832997987927564,
      "grad_norm": 0.1956692636013031,
      "learning_rate": 0.00019924338464634268,
      "loss": 0.804,
      "step": 193
    },
    {
      "epoch": 0.3903420523138833,
      "grad_norm": 0.2016148716211319,
      "learning_rate": 0.00019923936009658922,
      "loss": 0.8586,
      "step": 194
    },
    {
      "epoch": 0.39235412474849096,
      "grad_norm": 0.1996690332889557,
      "learning_rate": 0.0001992353355468357,
      "loss": 0.8118,
      "step": 195
    },
    {
      "epoch": 0.39436619718309857,
      "grad_norm": 0.193998321890831,
      "learning_rate": 0.00019923131099708222,
      "loss": 0.8284,
      "step": 196
    },
    {
      "epoch": 0.3963782696177062,
      "grad_norm": 0.18818378448486328,
      "learning_rate": 0.0001992272864473287,
      "loss": 0.8676,
      "step": 197
    },
    {
      "epoch": 0.3983903420523139,
      "grad_norm": 0.19346383213996887,
      "learning_rate": 0.00019922326189757521,
      "loss": 0.8283,
      "step": 198
    },
    {
      "epoch": 0.40040241448692154,
      "grad_norm": 0.19678300619125366,
      "learning_rate": 0.00019921923734782173,
      "loss": 0.8072,
      "step": 199
    },
    {
      "epoch": 0.4024144869215292,
      "grad_norm": 0.19104543328285217,
      "learning_rate": 0.00019921521279806824,
      "loss": 0.8123,
      "step": 200
    },
    {
      "epoch": 0.4044265593561368,
      "grad_norm": 0.1930556297302246,
      "learning_rate": 0.00019921118824831472,
      "loss": 0.8558,
      "step": 201
    },
    {
      "epoch": 0.40643863179074446,
      "grad_norm": 0.20166060328483582,
      "learning_rate": 0.00019920716369856124,
      "loss": 0.8022,
      "step": 202
    },
    {
      "epoch": 0.4084507042253521,
      "grad_norm": 0.20910708606243134,
      "learning_rate": 0.00019920313914880772,
      "loss": 0.8582,
      "step": 203
    },
    {
      "epoch": 0.4104627766599598,
      "grad_norm": 0.1898733228445053,
      "learning_rate": 0.00019919911459905426,
      "loss": 0.8441,
      "step": 204
    },
    {
      "epoch": 0.4124748490945674,
      "grad_norm": 0.20081889629364014,
      "learning_rate": 0.00019919509004930075,
      "loss": 0.8384,
      "step": 205
    },
    {
      "epoch": 0.41448692152917505,
      "grad_norm": 0.2062528282403946,
      "learning_rate": 0.00019919106549954726,
      "loss": 0.8703,
      "step": 206
    },
    {
      "epoch": 0.4164989939637827,
      "grad_norm": 0.22452092170715332,
      "learning_rate": 0.00019918704094979374,
      "loss": 0.8213,
      "step": 207
    },
    {
      "epoch": 0.41851106639839036,
      "grad_norm": 0.19865426421165466,
      "learning_rate": 0.00019918301640004025,
      "loss": 0.8389,
      "step": 208
    },
    {
      "epoch": 0.42052313883299797,
      "grad_norm": 0.20899096131324768,
      "learning_rate": 0.00019917899185028677,
      "loss": 0.8152,
      "step": 209
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 0.19223721325397491,
      "learning_rate": 0.00019917496730053328,
      "loss": 0.802,
      "step": 210
    },
    {
      "epoch": 0.4245472837022133,
      "grad_norm": 0.19974149763584137,
      "learning_rate": 0.00019917094275077976,
      "loss": 0.8219,
      "step": 211
    },
    {
      "epoch": 0.42655935613682094,
      "grad_norm": 0.2074500322341919,
      "learning_rate": 0.00019916691820102628,
      "loss": 0.7988,
      "step": 212
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.20133256912231445,
      "learning_rate": 0.00019916289365127276,
      "loss": 0.7919,
      "step": 213
    },
    {
      "epoch": 0.4305835010060362,
      "grad_norm": 0.203199565410614,
      "learning_rate": 0.00019915886910151927,
      "loss": 0.8165,
      "step": 214
    },
    {
      "epoch": 0.43259557344064387,
      "grad_norm": 0.22091370820999146,
      "learning_rate": 0.00019915484455176579,
      "loss": 0.8746,
      "step": 215
    },
    {
      "epoch": 0.4346076458752515,
      "grad_norm": 0.18685661256313324,
      "learning_rate": 0.00019915082000201227,
      "loss": 0.8029,
      "step": 216
    },
    {
      "epoch": 0.43661971830985913,
      "grad_norm": 0.19565345346927643,
      "learning_rate": 0.00019914679545225878,
      "loss": 0.8357,
      "step": 217
    },
    {
      "epoch": 0.4386317907444668,
      "grad_norm": 0.20325933396816254,
      "learning_rate": 0.0001991427709025053,
      "loss": 0.8484,
      "step": 218
    },
    {
      "epoch": 0.44064386317907445,
      "grad_norm": 0.20397326350212097,
      "learning_rate": 0.0001991387463527518,
      "loss": 0.8414,
      "step": 219
    },
    {
      "epoch": 0.4426559356136821,
      "grad_norm": 0.20668001472949982,
      "learning_rate": 0.0001991347218029983,
      "loss": 0.8128,
      "step": 220
    },
    {
      "epoch": 0.44466800804828976,
      "grad_norm": 0.195664182305336,
      "learning_rate": 0.0001991306972532448,
      "loss": 0.8495,
      "step": 221
    },
    {
      "epoch": 0.44668008048289737,
      "grad_norm": 0.1920742690563202,
      "learning_rate": 0.0001991266727034913,
      "loss": 0.8238,
      "step": 222
    },
    {
      "epoch": 0.448692152917505,
      "grad_norm": 0.2044118344783783,
      "learning_rate": 0.0001991226481537378,
      "loss": 0.8028,
      "step": 223
    },
    {
      "epoch": 0.4507042253521127,
      "grad_norm": 0.18644216656684875,
      "learning_rate": 0.00019911862360398431,
      "loss": 0.7763,
      "step": 224
    },
    {
      "epoch": 0.45271629778672035,
      "grad_norm": 0.2171124815940857,
      "learning_rate": 0.00019911459905423083,
      "loss": 0.7978,
      "step": 225
    },
    {
      "epoch": 0.45472837022132795,
      "grad_norm": 0.20547756552696228,
      "learning_rate": 0.0001991105745044773,
      "loss": 0.8052,
      "step": 226
    },
    {
      "epoch": 0.4567404426559356,
      "grad_norm": 0.19495469331741333,
      "learning_rate": 0.00019910654995472382,
      "loss": 0.8124,
      "step": 227
    },
    {
      "epoch": 0.45875251509054327,
      "grad_norm": 0.18754079937934875,
      "learning_rate": 0.0001991025254049703,
      "loss": 0.8163,
      "step": 228
    },
    {
      "epoch": 0.4607645875251509,
      "grad_norm": 0.19530025124549866,
      "learning_rate": 0.00019909850085521685,
      "loss": 0.8282,
      "step": 229
    },
    {
      "epoch": 0.46277665995975853,
      "grad_norm": 0.19719532132148743,
      "learning_rate": 0.00019909447630546333,
      "loss": 0.8648,
      "step": 230
    },
    {
      "epoch": 0.4647887323943662,
      "grad_norm": 0.19687660038471222,
      "learning_rate": 0.00019909045175570985,
      "loss": 0.8036,
      "step": 231
    },
    {
      "epoch": 0.46680080482897385,
      "grad_norm": 0.2051772177219391,
      "learning_rate": 0.00019908642720595633,
      "loss": 0.8257,
      "step": 232
    },
    {
      "epoch": 0.4688128772635815,
      "grad_norm": 0.21039119362831116,
      "learning_rate": 0.00019908240265620284,
      "loss": 0.8661,
      "step": 233
    },
    {
      "epoch": 0.4708249496981891,
      "grad_norm": 0.21405771374702454,
      "learning_rate": 0.00019907837810644936,
      "loss": 0.8155,
      "step": 234
    },
    {
      "epoch": 0.47283702213279677,
      "grad_norm": 0.22653789818286896,
      "learning_rate": 0.00019907435355669587,
      "loss": 0.8138,
      "step": 235
    },
    {
      "epoch": 0.47484909456740443,
      "grad_norm": 0.21049822866916656,
      "learning_rate": 0.00019907032900694235,
      "loss": 0.8331,
      "step": 236
    },
    {
      "epoch": 0.4768611670020121,
      "grad_norm": 0.20439951121807098,
      "learning_rate": 0.00019906630445718887,
      "loss": 0.8035,
      "step": 237
    },
    {
      "epoch": 0.4788732394366197,
      "grad_norm": 0.20112159848213196,
      "learning_rate": 0.00019906227990743535,
      "loss": 0.8232,
      "step": 238
    },
    {
      "epoch": 0.48088531187122735,
      "grad_norm": 0.2124919891357422,
      "learning_rate": 0.0001990582553576819,
      "loss": 0.8373,
      "step": 239
    },
    {
      "epoch": 0.482897384305835,
      "grad_norm": 0.20499911904335022,
      "learning_rate": 0.00019905423080792837,
      "loss": 0.8488,
      "step": 240
    },
    {
      "epoch": 0.48490945674044267,
      "grad_norm": 0.21153973042964935,
      "learning_rate": 0.0001990502062581749,
      "loss": 0.8452,
      "step": 241
    },
    {
      "epoch": 0.4869215291750503,
      "grad_norm": 0.20281265676021576,
      "learning_rate": 0.00019904618170842137,
      "loss": 0.7963,
      "step": 242
    },
    {
      "epoch": 0.48893360160965793,
      "grad_norm": 0.20250217616558075,
      "learning_rate": 0.00019904215715866788,
      "loss": 0.7791,
      "step": 243
    },
    {
      "epoch": 0.4909456740442656,
      "grad_norm": 0.19519659876823425,
      "learning_rate": 0.0001990381326089144,
      "loss": 0.761,
      "step": 244
    },
    {
      "epoch": 0.49295774647887325,
      "grad_norm": 0.21399495005607605,
      "learning_rate": 0.0001990341080591609,
      "loss": 0.8537,
      "step": 245
    },
    {
      "epoch": 0.4949698189134809,
      "grad_norm": 0.19952630996704102,
      "learning_rate": 0.0001990300835094074,
      "loss": 0.8099,
      "step": 246
    },
    {
      "epoch": 0.4969818913480885,
      "grad_norm": 0.202809676527977,
      "learning_rate": 0.0001990260589596539,
      "loss": 0.8382,
      "step": 247
    },
    {
      "epoch": 0.49899396378269617,
      "grad_norm": 0.1948891431093216,
      "learning_rate": 0.0001990220344099004,
      "loss": 0.7985,
      "step": 248
    },
    {
      "epoch": 0.5010060362173038,
      "grad_norm": 0.18884161114692688,
      "learning_rate": 0.0001990180098601469,
      "loss": 0.7969,
      "step": 249
    },
    {
      "epoch": 0.5030181086519114,
      "grad_norm": 0.20079824328422546,
      "learning_rate": 0.00019901398531039342,
      "loss": 0.8078,
      "step": 250
    },
    {
      "epoch": 0.5050301810865191,
      "grad_norm": 0.21258966624736786,
      "learning_rate": 0.0001990099607606399,
      "loss": 0.7841,
      "step": 251
    },
    {
      "epoch": 0.5070422535211268,
      "grad_norm": 0.1893557608127594,
      "learning_rate": 0.0001990059362108864,
      "loss": 0.8315,
      "step": 252
    },
    {
      "epoch": 0.5090543259557344,
      "grad_norm": 0.2002660185098648,
      "learning_rate": 0.0001990019116611329,
      "loss": 0.8348,
      "step": 253
    },
    {
      "epoch": 0.5110663983903421,
      "grad_norm": 0.20545832812786102,
      "learning_rate": 0.00019899788711137944,
      "loss": 0.8348,
      "step": 254
    },
    {
      "epoch": 0.5130784708249497,
      "grad_norm": 0.20271722972393036,
      "learning_rate": 0.00019899386256162592,
      "loss": 0.7994,
      "step": 255
    },
    {
      "epoch": 0.5150905432595574,
      "grad_norm": 0.20336204767227173,
      "learning_rate": 0.00019898983801187243,
      "loss": 0.7826,
      "step": 256
    },
    {
      "epoch": 0.5171026156941649,
      "grad_norm": 0.1980779469013214,
      "learning_rate": 0.00019898581346211892,
      "loss": 0.7936,
      "step": 257
    },
    {
      "epoch": 0.5191146881287726,
      "grad_norm": 0.22841547429561615,
      "learning_rate": 0.00019898178891236543,
      "loss": 0.8685,
      "step": 258
    },
    {
      "epoch": 0.5211267605633803,
      "grad_norm": 0.21340353786945343,
      "learning_rate": 0.00019897776436261194,
      "loss": 0.8163,
      "step": 259
    },
    {
      "epoch": 0.5231388329979879,
      "grad_norm": 0.19939735531806946,
      "learning_rate": 0.00019897373981285846,
      "loss": 0.8234,
      "step": 260
    },
    {
      "epoch": 0.5251509054325956,
      "grad_norm": 0.2068372666835785,
      "learning_rate": 0.00019896971526310494,
      "loss": 0.801,
      "step": 261
    },
    {
      "epoch": 0.5271629778672032,
      "grad_norm": 0.20252451300621033,
      "learning_rate": 0.00019896569071335145,
      "loss": 0.8253,
      "step": 262
    },
    {
      "epoch": 0.5291750503018109,
      "grad_norm": 0.22156307101249695,
      "learning_rate": 0.00019896166616359794,
      "loss": 0.81,
      "step": 263
    },
    {
      "epoch": 0.5311871227364185,
      "grad_norm": 0.21007882058620453,
      "learning_rate": 0.00019895764161384448,
      "loss": 0.8234,
      "step": 264
    },
    {
      "epoch": 0.5331991951710262,
      "grad_norm": 0.19245769083499908,
      "learning_rate": 0.00019895361706409096,
      "loss": 0.819,
      "step": 265
    },
    {
      "epoch": 0.5352112676056338,
      "grad_norm": 0.20157989859580994,
      "learning_rate": 0.00019894959251433748,
      "loss": 0.7977,
      "step": 266
    },
    {
      "epoch": 0.5372233400402414,
      "grad_norm": 0.20540215075016022,
      "learning_rate": 0.00019894556796458396,
      "loss": 0.821,
      "step": 267
    },
    {
      "epoch": 0.5392354124748491,
      "grad_norm": 0.20393359661102295,
      "learning_rate": 0.00019894154341483047,
      "loss": 0.8309,
      "step": 268
    },
    {
      "epoch": 0.5412474849094567,
      "grad_norm": 0.19422046840190887,
      "learning_rate": 0.00019893751886507699,
      "loss": 0.8075,
      "step": 269
    },
    {
      "epoch": 0.5432595573440644,
      "grad_norm": 0.20622295141220093,
      "learning_rate": 0.0001989334943153235,
      "loss": 0.7865,
      "step": 270
    },
    {
      "epoch": 0.545271629778672,
      "grad_norm": 0.2021787315607071,
      "learning_rate": 0.00019892946976556998,
      "loss": 0.8255,
      "step": 271
    },
    {
      "epoch": 0.5472837022132797,
      "grad_norm": 0.21491855382919312,
      "learning_rate": 0.0001989254452158165,
      "loss": 0.8361,
      "step": 272
    },
    {
      "epoch": 0.5492957746478874,
      "grad_norm": 0.2077280730009079,
      "learning_rate": 0.00019892142066606298,
      "loss": 0.837,
      "step": 273
    },
    {
      "epoch": 0.5513078470824949,
      "grad_norm": 0.20300926268100739,
      "learning_rate": 0.00019891739611630952,
      "loss": 0.8147,
      "step": 274
    },
    {
      "epoch": 0.5533199195171026,
      "grad_norm": 0.2048151195049286,
      "learning_rate": 0.000198913371566556,
      "loss": 0.767,
      "step": 275
    },
    {
      "epoch": 0.5553319919517102,
      "grad_norm": 0.20834337174892426,
      "learning_rate": 0.00019890934701680252,
      "loss": 0.8405,
      "step": 276
    },
    {
      "epoch": 0.5573440643863179,
      "grad_norm": 0.19528985023498535,
      "learning_rate": 0.000198905322467049,
      "loss": 0.8099,
      "step": 277
    },
    {
      "epoch": 0.5593561368209256,
      "grad_norm": 0.20563116669654846,
      "learning_rate": 0.00019890129791729551,
      "loss": 0.8201,
      "step": 278
    },
    {
      "epoch": 0.5613682092555332,
      "grad_norm": 0.1949790120124817,
      "learning_rate": 0.00019889727336754203,
      "loss": 0.8267,
      "step": 279
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 0.22644399106502533,
      "learning_rate": 0.0001988932488177885,
      "loss": 0.8278,
      "step": 280
    },
    {
      "epoch": 0.5653923541247485,
      "grad_norm": 0.20286881923675537,
      "learning_rate": 0.00019888922426803502,
      "loss": 0.7773,
      "step": 281
    },
    {
      "epoch": 0.5674044265593562,
      "grad_norm": 0.2066720873117447,
      "learning_rate": 0.00019888519971828154,
      "loss": 0.8507,
      "step": 282
    },
    {
      "epoch": 0.5694164989939637,
      "grad_norm": 0.20087675750255585,
      "learning_rate": 0.00019888117516852802,
      "loss": 0.7984,
      "step": 283
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.2102128565311432,
      "learning_rate": 0.00019887715061877453,
      "loss": 0.8084,
      "step": 284
    },
    {
      "epoch": 0.5734406438631791,
      "grad_norm": 0.1975802183151245,
      "learning_rate": 0.00019887312606902105,
      "loss": 0.8274,
      "step": 285
    },
    {
      "epoch": 0.5754527162977867,
      "grad_norm": 0.20236757397651672,
      "learning_rate": 0.00019886910151926753,
      "loss": 0.8454,
      "step": 286
    },
    {
      "epoch": 0.5774647887323944,
      "grad_norm": 0.19113342463970184,
      "learning_rate": 0.00019886507696951404,
      "loss": 0.7751,
      "step": 287
    },
    {
      "epoch": 0.579476861167002,
      "grad_norm": 0.2072780281305313,
      "learning_rate": 0.00019886105241976053,
      "loss": 0.7763,
      "step": 288
    },
    {
      "epoch": 0.5814889336016097,
      "grad_norm": 0.1979612559080124,
      "learning_rate": 0.00019885702787000707,
      "loss": 0.8047,
      "step": 289
    },
    {
      "epoch": 0.5835010060362174,
      "grad_norm": 0.20402362942695618,
      "learning_rate": 0.00019885300332025355,
      "loss": 0.8156,
      "step": 290
    },
    {
      "epoch": 0.5855130784708249,
      "grad_norm": 0.20844891667366028,
      "learning_rate": 0.00019884897877050006,
      "loss": 0.8156,
      "step": 291
    },
    {
      "epoch": 0.5875251509054326,
      "grad_norm": 0.20669080317020416,
      "learning_rate": 0.00019884495422074655,
      "loss": 0.7934,
      "step": 292
    },
    {
      "epoch": 0.5895372233400402,
      "grad_norm": 0.19781960546970367,
      "learning_rate": 0.00019884092967099306,
      "loss": 0.8134,
      "step": 293
    },
    {
      "epoch": 0.5915492957746479,
      "grad_norm": 0.19691617786884308,
      "learning_rate": 0.00019883690512123957,
      "loss": 0.8127,
      "step": 294
    },
    {
      "epoch": 0.5935613682092555,
      "grad_norm": 0.19208107888698578,
      "learning_rate": 0.00019883288057148609,
      "loss": 0.8324,
      "step": 295
    },
    {
      "epoch": 0.5955734406438632,
      "grad_norm": 0.1899215579032898,
      "learning_rate": 0.00019882885602173257,
      "loss": 0.7813,
      "step": 296
    },
    {
      "epoch": 0.5975855130784709,
      "grad_norm": 0.20017743110656738,
      "learning_rate": 0.00019882483147197908,
      "loss": 0.8247,
      "step": 297
    },
    {
      "epoch": 0.5995975855130785,
      "grad_norm": 0.19748613238334656,
      "learning_rate": 0.00019882080692222557,
      "loss": 0.8019,
      "step": 298
    },
    {
      "epoch": 0.6016096579476862,
      "grad_norm": 0.2028214931488037,
      "learning_rate": 0.0001988167823724721,
      "loss": 0.8459,
      "step": 299
    },
    {
      "epoch": 0.6036217303822937,
      "grad_norm": 0.21693335473537445,
      "learning_rate": 0.0001988127578227186,
      "loss": 0.832,
      "step": 300
    },
    {
      "epoch": 0.6056338028169014,
      "grad_norm": 0.18855153024196625,
      "learning_rate": 0.0001988087332729651,
      "loss": 0.8085,
      "step": 301
    },
    {
      "epoch": 0.607645875251509,
      "grad_norm": 0.19582916796207428,
      "learning_rate": 0.0001988047087232116,
      "loss": 0.8056,
      "step": 302
    },
    {
      "epoch": 0.6096579476861167,
      "grad_norm": 0.21092489361763,
      "learning_rate": 0.0001988006841734581,
      "loss": 0.8096,
      "step": 303
    },
    {
      "epoch": 0.6116700201207244,
      "grad_norm": 0.19717219471931458,
      "learning_rate": 0.00019879665962370461,
      "loss": 0.7926,
      "step": 304
    },
    {
      "epoch": 0.613682092555332,
      "grad_norm": 0.1908944547176361,
      "learning_rate": 0.00019879263507395113,
      "loss": 0.7824,
      "step": 305
    },
    {
      "epoch": 0.6156941649899397,
      "grad_norm": 0.1977272927761078,
      "learning_rate": 0.0001987886105241976,
      "loss": 0.803,
      "step": 306
    },
    {
      "epoch": 0.6177062374245473,
      "grad_norm": 0.19733884930610657,
      "learning_rate": 0.00019878458597444412,
      "loss": 0.8009,
      "step": 307
    },
    {
      "epoch": 0.6197183098591549,
      "grad_norm": 0.19303973019123077,
      "learning_rate": 0.0001987805614246906,
      "loss": 0.7826,
      "step": 308
    },
    {
      "epoch": 0.6217303822937625,
      "grad_norm": 0.19605237245559692,
      "learning_rate": 0.00019877653687493715,
      "loss": 0.8036,
      "step": 309
    },
    {
      "epoch": 0.6237424547283702,
      "grad_norm": 0.19542069733142853,
      "learning_rate": 0.00019877251232518363,
      "loss": 0.8063,
      "step": 310
    },
    {
      "epoch": 0.6257545271629779,
      "grad_norm": 0.20244619250297546,
      "learning_rate": 0.00019876848777543015,
      "loss": 0.8182,
      "step": 311
    },
    {
      "epoch": 0.6277665995975855,
      "grad_norm": 0.19812864065170288,
      "learning_rate": 0.00019876446322567663,
      "loss": 0.7689,
      "step": 312
    },
    {
      "epoch": 0.6297786720321932,
      "grad_norm": 0.19319409132003784,
      "learning_rate": 0.00019876043867592314,
      "loss": 0.828,
      "step": 313
    },
    {
      "epoch": 0.6317907444668008,
      "grad_norm": 0.19366447627544403,
      "learning_rate": 0.00019875641412616966,
      "loss": 0.8289,
      "step": 314
    },
    {
      "epoch": 0.6338028169014085,
      "grad_norm": 0.20197951793670654,
      "learning_rate": 0.00019875238957641614,
      "loss": 0.8241,
      "step": 315
    },
    {
      "epoch": 0.635814889336016,
      "grad_norm": 0.19048883020877838,
      "learning_rate": 0.00019874836502666265,
      "loss": 0.7943,
      "step": 316
    },
    {
      "epoch": 0.6378269617706237,
      "grad_norm": 0.20159044861793518,
      "learning_rate": 0.00019874434047690916,
      "loss": 0.8376,
      "step": 317
    },
    {
      "epoch": 0.6398390342052314,
      "grad_norm": 0.18766562640666962,
      "learning_rate": 0.00019874031592715565,
      "loss": 0.799,
      "step": 318
    },
    {
      "epoch": 0.641851106639839,
      "grad_norm": 0.192321315407753,
      "learning_rate": 0.00019873629137740216,
      "loss": 0.7841,
      "step": 319
    },
    {
      "epoch": 0.6438631790744467,
      "grad_norm": 0.1952517330646515,
      "learning_rate": 0.00019873226682764867,
      "loss": 0.8016,
      "step": 320
    },
    {
      "epoch": 0.6458752515090543,
      "grad_norm": 0.2132752239704132,
      "learning_rate": 0.00019872824227789516,
      "loss": 0.8429,
      "step": 321
    },
    {
      "epoch": 0.647887323943662,
      "grad_norm": 0.206093430519104,
      "learning_rate": 0.00019872421772814167,
      "loss": 0.8277,
      "step": 322
    },
    {
      "epoch": 0.6498993963782697,
      "grad_norm": 0.22014914453029633,
      "learning_rate": 0.00019872019317838816,
      "loss": 0.814,
      "step": 323
    },
    {
      "epoch": 0.6519114688128773,
      "grad_norm": 0.21208643913269043,
      "learning_rate": 0.0001987161686286347,
      "loss": 0.7657,
      "step": 324
    },
    {
      "epoch": 0.6539235412474849,
      "grad_norm": 0.20969656109809875,
      "learning_rate": 0.00019871214407888118,
      "loss": 0.8147,
      "step": 325
    },
    {
      "epoch": 0.6559356136820925,
      "grad_norm": 0.19786354899406433,
      "learning_rate": 0.0001987081195291277,
      "loss": 0.821,
      "step": 326
    },
    {
      "epoch": 0.6579476861167002,
      "grad_norm": 0.19243012368679047,
      "learning_rate": 0.00019870409497937418,
      "loss": 0.8042,
      "step": 327
    },
    {
      "epoch": 0.6599597585513078,
      "grad_norm": 0.19702884554862976,
      "learning_rate": 0.0001987000704296207,
      "loss": 0.8176,
      "step": 328
    },
    {
      "epoch": 0.6619718309859155,
      "grad_norm": 0.20455193519592285,
      "learning_rate": 0.0001986960458798672,
      "loss": 0.8553,
      "step": 329
    },
    {
      "epoch": 0.6639839034205232,
      "grad_norm": 0.19002719223499298,
      "learning_rate": 0.00019869202133011372,
      "loss": 0.7932,
      "step": 330
    },
    {
      "epoch": 0.6659959758551308,
      "grad_norm": 0.1948806792497635,
      "learning_rate": 0.0001986879967803602,
      "loss": 0.7977,
      "step": 331
    },
    {
      "epoch": 0.6680080482897385,
      "grad_norm": 0.21396185457706451,
      "learning_rate": 0.0001986839722306067,
      "loss": 0.8316,
      "step": 332
    },
    {
      "epoch": 0.670020120724346,
      "grad_norm": 0.210575670003891,
      "learning_rate": 0.0001986799476808532,
      "loss": 0.7907,
      "step": 333
    },
    {
      "epoch": 0.6720321931589537,
      "grad_norm": 0.20781706273555756,
      "learning_rate": 0.00019867592313109974,
      "loss": 0.8036,
      "step": 334
    },
    {
      "epoch": 0.6740442655935613,
      "grad_norm": 0.20771706104278564,
      "learning_rate": 0.00019867189858134622,
      "loss": 0.8072,
      "step": 335
    },
    {
      "epoch": 0.676056338028169,
      "grad_norm": 0.21493276953697205,
      "learning_rate": 0.00019866787403159273,
      "loss": 0.7931,
      "step": 336
    },
    {
      "epoch": 0.6780684104627767,
      "grad_norm": 0.1992768496274948,
      "learning_rate": 0.00019866384948183922,
      "loss": 0.8023,
      "step": 337
    },
    {
      "epoch": 0.6800804828973843,
      "grad_norm": 0.20377369225025177,
      "learning_rate": 0.00019865982493208573,
      "loss": 0.8171,
      "step": 338
    },
    {
      "epoch": 0.682092555331992,
      "grad_norm": 0.19157814979553223,
      "learning_rate": 0.00019865580038233224,
      "loss": 0.7774,
      "step": 339
    },
    {
      "epoch": 0.6841046277665996,
      "grad_norm": 0.2095114290714264,
      "learning_rate": 0.00019865177583257876,
      "loss": 0.7844,
      "step": 340
    },
    {
      "epoch": 0.6861167002012073,
      "grad_norm": 0.19992177188396454,
      "learning_rate": 0.00019864775128282524,
      "loss": 0.8144,
      "step": 341
    },
    {
      "epoch": 0.6881287726358148,
      "grad_norm": 0.20412935316562653,
      "learning_rate": 0.00019864372673307175,
      "loss": 0.8436,
      "step": 342
    },
    {
      "epoch": 0.6901408450704225,
      "grad_norm": 0.2125723659992218,
      "learning_rate": 0.00019863970218331824,
      "loss": 0.8015,
      "step": 343
    },
    {
      "epoch": 0.6921529175050302,
      "grad_norm": 0.19545668363571167,
      "learning_rate": 0.00019863567763356478,
      "loss": 0.7779,
      "step": 344
    },
    {
      "epoch": 0.6941649899396378,
      "grad_norm": 0.23155014216899872,
      "learning_rate": 0.00019863165308381126,
      "loss": 0.8102,
      "step": 345
    },
    {
      "epoch": 0.6961770623742455,
      "grad_norm": 0.21022705733776093,
      "learning_rate": 0.00019862762853405778,
      "loss": 0.86,
      "step": 346
    },
    {
      "epoch": 0.6981891348088531,
      "grad_norm": 0.223914235830307,
      "learning_rate": 0.00019862360398430426,
      "loss": 0.7966,
      "step": 347
    },
    {
      "epoch": 0.7002012072434608,
      "grad_norm": 0.20731118321418762,
      "learning_rate": 0.00019861957943455077,
      "loss": 0.8334,
      "step": 348
    },
    {
      "epoch": 0.7022132796780685,
      "grad_norm": 0.2066212147474289,
      "learning_rate": 0.00019861555488479728,
      "loss": 0.7712,
      "step": 349
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 0.23683327436447144,
      "learning_rate": 0.00019861153033504377,
      "loss": 0.8113,
      "step": 350
    },
    {
      "epoch": 0.7062374245472837,
      "grad_norm": 0.19279888272285461,
      "learning_rate": 0.00019860750578529028,
      "loss": 0.7988,
      "step": 351
    },
    {
      "epoch": 0.7082494969818913,
      "grad_norm": 0.2055479735136032,
      "learning_rate": 0.0001986034812355368,
      "loss": 0.7904,
      "step": 352
    },
    {
      "epoch": 0.710261569416499,
      "grad_norm": 0.20029111206531525,
      "learning_rate": 0.00019859945668578328,
      "loss": 0.8089,
      "step": 353
    },
    {
      "epoch": 0.7122736418511066,
      "grad_norm": 0.20764042437076569,
      "learning_rate": 0.0001985954321360298,
      "loss": 0.8166,
      "step": 354
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.20977604389190674,
      "learning_rate": 0.0001985914075862763,
      "loss": 0.8078,
      "step": 355
    },
    {
      "epoch": 0.716297786720322,
      "grad_norm": 0.21095453202724457,
      "learning_rate": 0.0001985873830365228,
      "loss": 0.8158,
      "step": 356
    },
    {
      "epoch": 0.7183098591549296,
      "grad_norm": 0.2183140218257904,
      "learning_rate": 0.0001985833584867693,
      "loss": 0.8452,
      "step": 357
    },
    {
      "epoch": 0.7203219315895373,
      "grad_norm": 0.19798772037029266,
      "learning_rate": 0.00019857933393701579,
      "loss": 0.7872,
      "step": 358
    },
    {
      "epoch": 0.7223340040241448,
      "grad_norm": 0.19906772673130035,
      "learning_rate": 0.00019857530938726233,
      "loss": 0.8083,
      "step": 359
    },
    {
      "epoch": 0.7243460764587525,
      "grad_norm": 0.20957590639591217,
      "learning_rate": 0.0001985712848375088,
      "loss": 0.7969,
      "step": 360
    },
    {
      "epoch": 0.7263581488933601,
      "grad_norm": 0.20033341646194458,
      "learning_rate": 0.00019856726028775532,
      "loss": 0.8113,
      "step": 361
    },
    {
      "epoch": 0.7283702213279678,
      "grad_norm": 0.19958390295505524,
      "learning_rate": 0.0001985632357380018,
      "loss": 0.869,
      "step": 362
    },
    {
      "epoch": 0.7303822937625755,
      "grad_norm": 0.1938924789428711,
      "learning_rate": 0.00019855921118824832,
      "loss": 0.771,
      "step": 363
    },
    {
      "epoch": 0.7323943661971831,
      "grad_norm": 0.19525937736034393,
      "learning_rate": 0.00019855518663849483,
      "loss": 0.837,
      "step": 364
    },
    {
      "epoch": 0.7344064386317908,
      "grad_norm": 0.19609768688678741,
      "learning_rate": 0.00019855116208874134,
      "loss": 0.804,
      "step": 365
    },
    {
      "epoch": 0.7364185110663984,
      "grad_norm": 0.19486847519874573,
      "learning_rate": 0.00019854713753898783,
      "loss": 0.7914,
      "step": 366
    },
    {
      "epoch": 0.738430583501006,
      "grad_norm": 0.19562844932079315,
      "learning_rate": 0.00019854311298923434,
      "loss": 0.8256,
      "step": 367
    },
    {
      "epoch": 0.7404426559356136,
      "grad_norm": 0.20826765894889832,
      "learning_rate": 0.00019853908843948083,
      "loss": 0.7931,
      "step": 368
    },
    {
      "epoch": 0.7424547283702213,
      "grad_norm": 0.20954741537570953,
      "learning_rate": 0.00019853506388972734,
      "loss": 0.8222,
      "step": 369
    },
    {
      "epoch": 0.744466800804829,
      "grad_norm": 0.2070826292037964,
      "learning_rate": 0.00019853103933997385,
      "loss": 0.786,
      "step": 370
    },
    {
      "epoch": 0.7464788732394366,
      "grad_norm": 0.20295239984989166,
      "learning_rate": 0.00019852701479022036,
      "loss": 0.7971,
      "step": 371
    },
    {
      "epoch": 0.7484909456740443,
      "grad_norm": 0.19547344744205475,
      "learning_rate": 0.00019852299024046685,
      "loss": 0.769,
      "step": 372
    },
    {
      "epoch": 0.7505030181086519,
      "grad_norm": 0.21596519649028778,
      "learning_rate": 0.00019851896569071336,
      "loss": 0.8059,
      "step": 373
    },
    {
      "epoch": 0.7525150905432596,
      "grad_norm": 0.19265590608119965,
      "learning_rate": 0.00019851494114095985,
      "loss": 0.7965,
      "step": 374
    },
    {
      "epoch": 0.7545271629778671,
      "grad_norm": 0.19614769518375397,
      "learning_rate": 0.00019851091659120639,
      "loss": 0.7977,
      "step": 375
    },
    {
      "epoch": 0.7565392354124748,
      "grad_norm": 0.19799940288066864,
      "learning_rate": 0.00019850689204145287,
      "loss": 0.7375,
      "step": 376
    },
    {
      "epoch": 0.7585513078470825,
      "grad_norm": 0.20514816045761108,
      "learning_rate": 0.00019850286749169938,
      "loss": 0.8061,
      "step": 377
    },
    {
      "epoch": 0.7605633802816901,
      "grad_norm": 0.19866710901260376,
      "learning_rate": 0.00019849884294194587,
      "loss": 0.7165,
      "step": 378
    },
    {
      "epoch": 0.7625754527162978,
      "grad_norm": 0.20150555670261383,
      "learning_rate": 0.00019849481839219238,
      "loss": 0.773,
      "step": 379
    },
    {
      "epoch": 0.7645875251509054,
      "grad_norm": 0.19503594934940338,
      "learning_rate": 0.0001984907938424389,
      "loss": 0.7949,
      "step": 380
    },
    {
      "epoch": 0.7665995975855131,
      "grad_norm": 0.19540619850158691,
      "learning_rate": 0.0001984867692926854,
      "loss": 0.8267,
      "step": 381
    },
    {
      "epoch": 0.7686116700201208,
      "grad_norm": 0.20034219324588776,
      "learning_rate": 0.0001984827447429319,
      "loss": 0.8429,
      "step": 382
    },
    {
      "epoch": 0.7706237424547284,
      "grad_norm": 0.2035241574048996,
      "learning_rate": 0.0001984787201931784,
      "loss": 0.8239,
      "step": 383
    },
    {
      "epoch": 0.772635814889336,
      "grad_norm": 0.19254262745380402,
      "learning_rate": 0.0001984746956434249,
      "loss": 0.7899,
      "step": 384
    },
    {
      "epoch": 0.7746478873239436,
      "grad_norm": 0.2074495106935501,
      "learning_rate": 0.0001984706710936714,
      "loss": 0.7731,
      "step": 385
    },
    {
      "epoch": 0.7766599597585513,
      "grad_norm": 0.20163767039775848,
      "learning_rate": 0.0001984666465439179,
      "loss": 0.8353,
      "step": 386
    },
    {
      "epoch": 0.778672032193159,
      "grad_norm": 0.194522887468338,
      "learning_rate": 0.00019846262199416442,
      "loss": 0.7328,
      "step": 387
    },
    {
      "epoch": 0.7806841046277666,
      "grad_norm": 0.21481181681156158,
      "learning_rate": 0.0001984585974444109,
      "loss": 0.8176,
      "step": 388
    },
    {
      "epoch": 0.7826961770623743,
      "grad_norm": 0.19848136603832245,
      "learning_rate": 0.00019845457289465742,
      "loss": 0.7786,
      "step": 389
    },
    {
      "epoch": 0.7847082494969819,
      "grad_norm": 0.22258491814136505,
      "learning_rate": 0.00019845054834490393,
      "loss": 0.8356,
      "step": 390
    },
    {
      "epoch": 0.7867203219315896,
      "grad_norm": 0.20296074450016022,
      "learning_rate": 0.00019844652379515042,
      "loss": 0.7829,
      "step": 391
    },
    {
      "epoch": 0.7887323943661971,
      "grad_norm": 0.20547260344028473,
      "learning_rate": 0.00019844249924539693,
      "loss": 0.7873,
      "step": 392
    },
    {
      "epoch": 0.7907444668008048,
      "grad_norm": 0.18946105241775513,
      "learning_rate": 0.00019843847469564342,
      "loss": 0.7986,
      "step": 393
    },
    {
      "epoch": 0.7927565392354124,
      "grad_norm": 0.20242540538311005,
      "learning_rate": 0.00019843445014588993,
      "loss": 0.8222,
      "step": 394
    },
    {
      "epoch": 0.7947686116700201,
      "grad_norm": 0.18907484412193298,
      "learning_rate": 0.00019843042559613644,
      "loss": 0.7455,
      "step": 395
    },
    {
      "epoch": 0.7967806841046278,
      "grad_norm": 0.2080616056919098,
      "learning_rate": 0.00019842640104638295,
      "loss": 0.7703,
      "step": 396
    },
    {
      "epoch": 0.7987927565392354,
      "grad_norm": 0.19110247492790222,
      "learning_rate": 0.00019842237649662944,
      "loss": 0.7763,
      "step": 397
    },
    {
      "epoch": 0.8008048289738431,
      "grad_norm": 0.2044421136379242,
      "learning_rate": 0.00019841835194687595,
      "loss": 0.8021,
      "step": 398
    },
    {
      "epoch": 0.8028169014084507,
      "grad_norm": 0.20055633783340454,
      "learning_rate": 0.00019841432739712243,
      "loss": 0.7579,
      "step": 399
    },
    {
      "epoch": 0.8048289738430584,
      "grad_norm": 0.21318891644477844,
      "learning_rate": 0.00019841030284736897,
      "loss": 0.7589,
      "step": 400
    },
    {
      "epoch": 0.806841046277666,
      "grad_norm": 0.21294273436069489,
      "learning_rate": 0.00019840627829761546,
      "loss": 0.8528,
      "step": 401
    },
    {
      "epoch": 0.8088531187122736,
      "grad_norm": 0.21352574229240417,
      "learning_rate": 0.00019840225374786197,
      "loss": 0.7873,
      "step": 402
    },
    {
      "epoch": 0.8108651911468813,
      "grad_norm": 0.2092064470052719,
      "learning_rate": 0.00019839822919810846,
      "loss": 0.7931,
      "step": 403
    },
    {
      "epoch": 0.8128772635814889,
      "grad_norm": 0.2191779911518097,
      "learning_rate": 0.00019839420464835497,
      "loss": 0.7921,
      "step": 404
    },
    {
      "epoch": 0.8148893360160966,
      "grad_norm": 0.21014201641082764,
      "learning_rate": 0.00019839018009860148,
      "loss": 0.7696,
      "step": 405
    },
    {
      "epoch": 0.8169014084507042,
      "grad_norm": 0.20797152817249298,
      "learning_rate": 0.000198386155548848,
      "loss": 0.8201,
      "step": 406
    },
    {
      "epoch": 0.8189134808853119,
      "grad_norm": 0.20371633768081665,
      "learning_rate": 0.00019838213099909448,
      "loss": 0.8279,
      "step": 407
    },
    {
      "epoch": 0.8209255533199196,
      "grad_norm": 0.20619964599609375,
      "learning_rate": 0.000198378106449341,
      "loss": 0.8104,
      "step": 408
    },
    {
      "epoch": 0.8229376257545271,
      "grad_norm": 0.21135213971138,
      "learning_rate": 0.00019837408189958748,
      "loss": 0.7809,
      "step": 409
    },
    {
      "epoch": 0.8249496981891348,
      "grad_norm": 0.23074480891227722,
      "learning_rate": 0.00019837005734983402,
      "loss": 0.7595,
      "step": 410
    },
    {
      "epoch": 0.8269617706237424,
      "grad_norm": 0.20379240810871124,
      "learning_rate": 0.0001983660328000805,
      "loss": 0.7701,
      "step": 411
    },
    {
      "epoch": 0.8289738430583501,
      "grad_norm": 0.2038278728723526,
      "learning_rate": 0.000198362008250327,
      "loss": 0.792,
      "step": 412
    },
    {
      "epoch": 0.8309859154929577,
      "grad_norm": 0.2202434539794922,
      "learning_rate": 0.0001983579837005735,
      "loss": 0.7981,
      "step": 413
    },
    {
      "epoch": 0.8329979879275654,
      "grad_norm": 0.20519916713237762,
      "learning_rate": 0.00019835395915082,
      "loss": 0.7519,
      "step": 414
    },
    {
      "epoch": 0.8350100603621731,
      "grad_norm": 0.2033589780330658,
      "learning_rate": 0.00019834993460106652,
      "loss": 0.7926,
      "step": 415
    },
    {
      "epoch": 0.8370221327967807,
      "grad_norm": 0.19605132937431335,
      "learning_rate": 0.00019834591005131303,
      "loss": 0.7898,
      "step": 416
    },
    {
      "epoch": 0.8390342052313883,
      "grad_norm": 0.19818177819252014,
      "learning_rate": 0.00019834188550155952,
      "loss": 0.8076,
      "step": 417
    },
    {
      "epoch": 0.8410462776659959,
      "grad_norm": 0.1942414492368698,
      "learning_rate": 0.00019833786095180603,
      "loss": 0.8075,
      "step": 418
    },
    {
      "epoch": 0.8430583501006036,
      "grad_norm": 0.20127610862255096,
      "learning_rate": 0.00019833383640205252,
      "loss": 0.8238,
      "step": 419
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 0.20354793965816498,
      "learning_rate": 0.00019832981185229903,
      "loss": 0.8267,
      "step": 420
    },
    {
      "epoch": 0.8470824949698189,
      "grad_norm": 0.20411019027233124,
      "learning_rate": 0.00019832578730254554,
      "loss": 0.7819,
      "step": 421
    },
    {
      "epoch": 0.8490945674044266,
      "grad_norm": 0.19886890053749084,
      "learning_rate": 0.00019832176275279203,
      "loss": 0.7964,
      "step": 422
    },
    {
      "epoch": 0.8511066398390342,
      "grad_norm": 0.20057538151741028,
      "learning_rate": 0.00019831773820303854,
      "loss": 0.8129,
      "step": 423
    },
    {
      "epoch": 0.8531187122736419,
      "grad_norm": 0.21487173438072205,
      "learning_rate": 0.00019831371365328505,
      "loss": 0.8313,
      "step": 424
    },
    {
      "epoch": 0.8551307847082495,
      "grad_norm": 0.21333584189414978,
      "learning_rate": 0.00019830968910353156,
      "loss": 0.7903,
      "step": 425
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.20483766496181488,
      "learning_rate": 0.00019830566455377805,
      "loss": 0.8129,
      "step": 426
    },
    {
      "epoch": 0.8591549295774648,
      "grad_norm": 0.20165608823299408,
      "learning_rate": 0.00019830164000402456,
      "loss": 0.8203,
      "step": 427
    },
    {
      "epoch": 0.8611670020120724,
      "grad_norm": 0.19338840246200562,
      "learning_rate": 0.00019829761545427105,
      "loss": 0.7876,
      "step": 428
    },
    {
      "epoch": 0.8631790744466801,
      "grad_norm": 0.18933360278606415,
      "learning_rate": 0.00019829359090451756,
      "loss": 0.7985,
      "step": 429
    },
    {
      "epoch": 0.8651911468812877,
      "grad_norm": 0.19900748133659363,
      "learning_rate": 0.00019828956635476407,
      "loss": 0.7982,
      "step": 430
    },
    {
      "epoch": 0.8672032193158954,
      "grad_norm": 0.21454469859600067,
      "learning_rate": 0.00019828554180501058,
      "loss": 0.7831,
      "step": 431
    },
    {
      "epoch": 0.869215291750503,
      "grad_norm": 0.21569159626960754,
      "learning_rate": 0.00019828151725525707,
      "loss": 0.7893,
      "step": 432
    },
    {
      "epoch": 0.8712273641851107,
      "grad_norm": 0.23185937106609344,
      "learning_rate": 0.00019827749270550358,
      "loss": 0.8157,
      "step": 433
    },
    {
      "epoch": 0.8732394366197183,
      "grad_norm": 0.21677014231681824,
      "learning_rate": 0.00019827346815575006,
      "loss": 0.7779,
      "step": 434
    },
    {
      "epoch": 0.8752515090543259,
      "grad_norm": 0.20661532878875732,
      "learning_rate": 0.0001982694436059966,
      "loss": 0.7858,
      "step": 435
    },
    {
      "epoch": 0.8772635814889336,
      "grad_norm": 0.20747685432434082,
      "learning_rate": 0.0001982654190562431,
      "loss": 0.7908,
      "step": 436
    },
    {
      "epoch": 0.8792756539235412,
      "grad_norm": 0.20743614435195923,
      "learning_rate": 0.0001982613945064896,
      "loss": 0.7995,
      "step": 437
    },
    {
      "epoch": 0.8812877263581489,
      "grad_norm": 0.19341936707496643,
      "learning_rate": 0.00019825736995673609,
      "loss": 0.8042,
      "step": 438
    },
    {
      "epoch": 0.8832997987927566,
      "grad_norm": 0.21230310201644897,
      "learning_rate": 0.0001982533454069826,
      "loss": 0.7962,
      "step": 439
    },
    {
      "epoch": 0.8853118712273642,
      "grad_norm": 0.20421820878982544,
      "learning_rate": 0.0001982493208572291,
      "loss": 0.7849,
      "step": 440
    },
    {
      "epoch": 0.8873239436619719,
      "grad_norm": 0.19868838787078857,
      "learning_rate": 0.00019824529630747562,
      "loss": 0.7727,
      "step": 441
    },
    {
      "epoch": 0.8893360160965795,
      "grad_norm": 0.1935422718524933,
      "learning_rate": 0.0001982412717577221,
      "loss": 0.7881,
      "step": 442
    },
    {
      "epoch": 0.8913480885311871,
      "grad_norm": 0.20676660537719727,
      "learning_rate": 0.00019823724720796862,
      "loss": 0.7865,
      "step": 443
    },
    {
      "epoch": 0.8933601609657947,
      "grad_norm": 0.1980438232421875,
      "learning_rate": 0.0001982332226582151,
      "loss": 0.7852,
      "step": 444
    },
    {
      "epoch": 0.8953722334004024,
      "grad_norm": 0.22613348066806793,
      "learning_rate": 0.00019822919810846164,
      "loss": 0.7991,
      "step": 445
    },
    {
      "epoch": 0.89738430583501,
      "grad_norm": 0.20115093886852264,
      "learning_rate": 0.00019822517355870813,
      "loss": 0.7789,
      "step": 446
    },
    {
      "epoch": 0.8993963782696177,
      "grad_norm": 0.19311347603797913,
      "learning_rate": 0.00019822114900895464,
      "loss": 0.7802,
      "step": 447
    },
    {
      "epoch": 0.9014084507042254,
      "grad_norm": 0.21307793259620667,
      "learning_rate": 0.00019821712445920113,
      "loss": 0.8108,
      "step": 448
    },
    {
      "epoch": 0.903420523138833,
      "grad_norm": 0.20473702251911163,
      "learning_rate": 0.00019821309990944764,
      "loss": 0.8118,
      "step": 449
    },
    {
      "epoch": 0.9054325955734407,
      "grad_norm": 0.21147622168064117,
      "learning_rate": 0.00019820907535969415,
      "loss": 0.8205,
      "step": 450
    },
    {
      "epoch": 0.9074446680080482,
      "grad_norm": 0.20015272498130798,
      "learning_rate": 0.00019820505080994066,
      "loss": 0.7842,
      "step": 451
    },
    {
      "epoch": 0.9094567404426559,
      "grad_norm": 0.20949867367744446,
      "learning_rate": 0.00019820102626018715,
      "loss": 0.8156,
      "step": 452
    },
    {
      "epoch": 0.9114688128772636,
      "grad_norm": 0.20011985301971436,
      "learning_rate": 0.00019819700171043366,
      "loss": 0.8046,
      "step": 453
    },
    {
      "epoch": 0.9134808853118712,
      "grad_norm": 0.20144768059253693,
      "learning_rate": 0.00019819297716068015,
      "loss": 0.7685,
      "step": 454
    },
    {
      "epoch": 0.9154929577464789,
      "grad_norm": 0.20337723195552826,
      "learning_rate": 0.00019818895261092666,
      "loss": 0.7904,
      "step": 455
    },
    {
      "epoch": 0.9175050301810865,
      "grad_norm": 0.19667595624923706,
      "learning_rate": 0.00019818492806117317,
      "loss": 0.7699,
      "step": 456
    },
    {
      "epoch": 0.9195171026156942,
      "grad_norm": 0.20368890464305878,
      "learning_rate": 0.00019818090351141966,
      "loss": 0.7785,
      "step": 457
    },
    {
      "epoch": 0.9215291750503019,
      "grad_norm": 0.21752700209617615,
      "learning_rate": 0.00019817687896166617,
      "loss": 0.7764,
      "step": 458
    },
    {
      "epoch": 0.9235412474849095,
      "grad_norm": 0.2019173800945282,
      "learning_rate": 0.00019817285441191268,
      "loss": 0.7916,
      "step": 459
    },
    {
      "epoch": 0.9255533199195171,
      "grad_norm": 0.19994226098060608,
      "learning_rate": 0.0001981688298621592,
      "loss": 0.7412,
      "step": 460
    },
    {
      "epoch": 0.9275653923541247,
      "grad_norm": 0.20383350551128387,
      "learning_rate": 0.00019816480531240568,
      "loss": 0.8342,
      "step": 461
    },
    {
      "epoch": 0.9295774647887324,
      "grad_norm": 0.21645185351371765,
      "learning_rate": 0.0001981607807626522,
      "loss": 0.794,
      "step": 462
    },
    {
      "epoch": 0.93158953722334,
      "grad_norm": 0.19939763844013214,
      "learning_rate": 0.00019815675621289867,
      "loss": 0.7878,
      "step": 463
    },
    {
      "epoch": 0.9336016096579477,
      "grad_norm": 0.189823180437088,
      "learning_rate": 0.0001981527316631452,
      "loss": 0.7764,
      "step": 464
    },
    {
      "epoch": 0.9356136820925554,
      "grad_norm": 0.2028392106294632,
      "learning_rate": 0.0001981487071133917,
      "loss": 0.8095,
      "step": 465
    },
    {
      "epoch": 0.937625754527163,
      "grad_norm": 0.19852226972579956,
      "learning_rate": 0.0001981446825636382,
      "loss": 0.8009,
      "step": 466
    },
    {
      "epoch": 0.9396378269617707,
      "grad_norm": 0.1951942890882492,
      "learning_rate": 0.0001981406580138847,
      "loss": 0.792,
      "step": 467
    },
    {
      "epoch": 0.9416498993963782,
      "grad_norm": 0.20978567004203796,
      "learning_rate": 0.0001981366334641312,
      "loss": 0.8368,
      "step": 468
    },
    {
      "epoch": 0.9436619718309859,
      "grad_norm": 0.24318905174732208,
      "learning_rate": 0.0001981326089143777,
      "loss": 0.7796,
      "step": 469
    },
    {
      "epoch": 0.9456740442655935,
      "grad_norm": 0.2076885998249054,
      "learning_rate": 0.00019812858436462423,
      "loss": 0.7858,
      "step": 470
    },
    {
      "epoch": 0.9476861167002012,
      "grad_norm": 0.1888105273246765,
      "learning_rate": 0.00019812455981487072,
      "loss": 0.7476,
      "step": 471
    },
    {
      "epoch": 0.9496981891348089,
      "grad_norm": 0.18520104885101318,
      "learning_rate": 0.00019812053526511723,
      "loss": 0.7466,
      "step": 472
    },
    {
      "epoch": 0.9517102615694165,
      "grad_norm": 0.19737914204597473,
      "learning_rate": 0.00019811651071536372,
      "loss": 0.8397,
      "step": 473
    },
    {
      "epoch": 0.9537223340040242,
      "grad_norm": 0.20051196217536926,
      "learning_rate": 0.00019811248616561023,
      "loss": 0.8063,
      "step": 474
    },
    {
      "epoch": 0.9557344064386318,
      "grad_norm": 0.22926431894302368,
      "learning_rate": 0.00019810846161585674,
      "loss": 0.8694,
      "step": 475
    },
    {
      "epoch": 0.9577464788732394,
      "grad_norm": 0.19041648507118225,
      "learning_rate": 0.00019810443706610325,
      "loss": 0.7787,
      "step": 476
    },
    {
      "epoch": 0.959758551307847,
      "grad_norm": 0.20132404565811157,
      "learning_rate": 0.00019810041251634974,
      "loss": 0.7652,
      "step": 477
    },
    {
      "epoch": 0.9617706237424547,
      "grad_norm": 0.20492024719715118,
      "learning_rate": 0.00019809638796659625,
      "loss": 0.7818,
      "step": 478
    },
    {
      "epoch": 0.9637826961770624,
      "grad_norm": 0.19986727833747864,
      "learning_rate": 0.00019809236341684273,
      "loss": 0.798,
      "step": 479
    },
    {
      "epoch": 0.96579476861167,
      "grad_norm": 0.19483987987041473,
      "learning_rate": 0.00019808833886708927,
      "loss": 0.7466,
      "step": 480
    },
    {
      "epoch": 0.9678068410462777,
      "grad_norm": 0.19705362617969513,
      "learning_rate": 0.00019808431431733576,
      "loss": 0.742,
      "step": 481
    },
    {
      "epoch": 0.9698189134808853,
      "grad_norm": 0.20203617215156555,
      "learning_rate": 0.00019808028976758227,
      "loss": 0.8035,
      "step": 482
    },
    {
      "epoch": 0.971830985915493,
      "grad_norm": 0.19486942887306213,
      "learning_rate": 0.00019807626521782876,
      "loss": 0.7715,
      "step": 483
    },
    {
      "epoch": 0.9738430583501007,
      "grad_norm": 0.1861552894115448,
      "learning_rate": 0.00019807224066807527,
      "loss": 0.7681,
      "step": 484
    },
    {
      "epoch": 0.9758551307847082,
      "grad_norm": 0.20767946541309357,
      "learning_rate": 0.00019806821611832178,
      "loss": 0.7741,
      "step": 485
    },
    {
      "epoch": 0.9778672032193159,
      "grad_norm": 0.1928442269563675,
      "learning_rate": 0.0001980641915685683,
      "loss": 0.7914,
      "step": 486
    },
    {
      "epoch": 0.9798792756539235,
      "grad_norm": 0.21359173953533173,
      "learning_rate": 0.00019806016701881478,
      "loss": 0.8154,
      "step": 487
    },
    {
      "epoch": 0.9818913480885312,
      "grad_norm": 0.1996477097272873,
      "learning_rate": 0.0001980561424690613,
      "loss": 0.8045,
      "step": 488
    },
    {
      "epoch": 0.9839034205231388,
      "grad_norm": 0.21233682334423065,
      "learning_rate": 0.00019805211791930778,
      "loss": 0.7722,
      "step": 489
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 0.20202818512916565,
      "learning_rate": 0.0001980480933695543,
      "loss": 0.7606,
      "step": 490
    },
    {
      "epoch": 0.9879275653923542,
      "grad_norm": 0.19190384447574615,
      "learning_rate": 0.0001980440688198008,
      "loss": 0.7931,
      "step": 491
    },
    {
      "epoch": 0.9899396378269618,
      "grad_norm": 0.22422270476818085,
      "learning_rate": 0.00019804004427004729,
      "loss": 0.8075,
      "step": 492
    },
    {
      "epoch": 0.9919517102615694,
      "grad_norm": 0.1968851536512375,
      "learning_rate": 0.0001980360197202938,
      "loss": 0.7827,
      "step": 493
    },
    {
      "epoch": 0.993963782696177,
      "grad_norm": 0.195201575756073,
      "learning_rate": 0.0001980319951705403,
      "loss": 0.7631,
      "step": 494
    },
    {
      "epoch": 0.9959758551307847,
      "grad_norm": 0.19191665947437286,
      "learning_rate": 0.00019802797062078682,
      "loss": 0.7777,
      "step": 495
    },
    {
      "epoch": 0.9979879275653923,
      "grad_norm": 0.2088109850883484,
      "learning_rate": 0.0001980239460710333,
      "loss": 0.777,
      "step": 496
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2070178985595703,
      "learning_rate": 0.00019801992152127982,
      "loss": 0.7734,
      "step": 497
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.7799280285835266,
      "eval_runtime": 49.86,
      "eval_samples_per_second": 19.896,
      "eval_steps_per_second": 2.487,
      "step": 497
    },
    {
      "epoch": 1.0020120724346075,
      "grad_norm": 0.19207195937633514,
      "learning_rate": 0.0001980158969715263,
      "loss": 0.7649,
      "step": 498
    },
    {
      "epoch": 1.0040241448692153,
      "grad_norm": 0.20183704793453217,
      "learning_rate": 0.00019801187242177282,
      "loss": 0.7743,
      "step": 499
    },
    {
      "epoch": 1.0060362173038229,
      "grad_norm": 0.18969404697418213,
      "learning_rate": 0.00019800784787201933,
      "loss": 0.7953,
      "step": 500
    },
    {
      "epoch": 1.0080482897384306,
      "grad_norm": 0.19781547784805298,
      "learning_rate": 0.00019800382332226584,
      "loss": 0.7754,
      "step": 501
    },
    {
      "epoch": 1.0100603621730382,
      "grad_norm": 0.20688296854496002,
      "learning_rate": 0.00019799979877251233,
      "loss": 0.7976,
      "step": 502
    },
    {
      "epoch": 1.012072434607646,
      "grad_norm": 0.18593880534172058,
      "learning_rate": 0.00019799577422275884,
      "loss": 0.7586,
      "step": 503
    },
    {
      "epoch": 1.0140845070422535,
      "grad_norm": 0.2028883546590805,
      "learning_rate": 0.00019799174967300532,
      "loss": 0.7713,
      "step": 504
    },
    {
      "epoch": 1.0160965794768613,
      "grad_norm": 0.21714359521865845,
      "learning_rate": 0.00019798772512325186,
      "loss": 0.7923,
      "step": 505
    },
    {
      "epoch": 1.0181086519114688,
      "grad_norm": 0.1955491304397583,
      "learning_rate": 0.00019798370057349835,
      "loss": 0.7386,
      "step": 506
    },
    {
      "epoch": 1.0201207243460764,
      "grad_norm": 0.20278988778591156,
      "learning_rate": 0.00019797967602374486,
      "loss": 0.758,
      "step": 507
    },
    {
      "epoch": 1.0221327967806841,
      "grad_norm": 0.20188376307487488,
      "learning_rate": 0.00019797565147399134,
      "loss": 0.7909,
      "step": 508
    },
    {
      "epoch": 1.0241448692152917,
      "grad_norm": 0.21330103278160095,
      "learning_rate": 0.00019797162692423786,
      "loss": 0.7942,
      "step": 509
    },
    {
      "epoch": 1.0261569416498995,
      "grad_norm": 0.21316145360469818,
      "learning_rate": 0.00019796760237448437,
      "loss": 0.767,
      "step": 510
    },
    {
      "epoch": 1.028169014084507,
      "grad_norm": 0.2026079148054123,
      "learning_rate": 0.00019796357782473088,
      "loss": 0.7583,
      "step": 511
    },
    {
      "epoch": 1.0301810865191148,
      "grad_norm": 0.20556364953517914,
      "learning_rate": 0.00019795955327497737,
      "loss": 0.7624,
      "step": 512
    },
    {
      "epoch": 1.0321931589537223,
      "grad_norm": 0.21641995012760162,
      "learning_rate": 0.00019795552872522388,
      "loss": 0.7957,
      "step": 513
    },
    {
      "epoch": 1.0342052313883299,
      "grad_norm": 0.20833377540111542,
      "learning_rate": 0.00019795150417547036,
      "loss": 0.7475,
      "step": 514
    },
    {
      "epoch": 1.0362173038229376,
      "grad_norm": 0.19111819565296173,
      "learning_rate": 0.0001979474796257169,
      "loss": 0.7399,
      "step": 515
    },
    {
      "epoch": 1.0382293762575452,
      "grad_norm": 0.21051698923110962,
      "learning_rate": 0.0001979434550759634,
      "loss": 0.7532,
      "step": 516
    },
    {
      "epoch": 1.040241448692153,
      "grad_norm": 0.20051497220993042,
      "learning_rate": 0.0001979394305262099,
      "loss": 0.774,
      "step": 517
    },
    {
      "epoch": 1.0422535211267605,
      "grad_norm": 0.21594685316085815,
      "learning_rate": 0.00019793540597645639,
      "loss": 0.7803,
      "step": 518
    },
    {
      "epoch": 1.0442655935613683,
      "grad_norm": 0.2104272097349167,
      "learning_rate": 0.0001979313814267029,
      "loss": 0.7773,
      "step": 519
    },
    {
      "epoch": 1.0462776659959758,
      "grad_norm": 0.21347059309482574,
      "learning_rate": 0.0001979273568769494,
      "loss": 0.8242,
      "step": 520
    },
    {
      "epoch": 1.0482897384305836,
      "grad_norm": 0.21672774851322174,
      "learning_rate": 0.00019792333232719592,
      "loss": 0.7623,
      "step": 521
    },
    {
      "epoch": 1.0503018108651911,
      "grad_norm": 0.23623782396316528,
      "learning_rate": 0.0001979193077774424,
      "loss": 0.734,
      "step": 522
    },
    {
      "epoch": 1.0523138832997987,
      "grad_norm": 0.2030126005411148,
      "learning_rate": 0.00019791528322768892,
      "loss": 0.775,
      "step": 523
    },
    {
      "epoch": 1.0543259557344065,
      "grad_norm": 0.21680450439453125,
      "learning_rate": 0.0001979112586779354,
      "loss": 0.749,
      "step": 524
    },
    {
      "epoch": 1.056338028169014,
      "grad_norm": 0.2219875007867813,
      "learning_rate": 0.00019790723412818192,
      "loss": 0.7448,
      "step": 525
    },
    {
      "epoch": 1.0583501006036218,
      "grad_norm": 0.2089332491159439,
      "learning_rate": 0.00019790320957842843,
      "loss": 0.7684,
      "step": 526
    },
    {
      "epoch": 1.0603621730382293,
      "grad_norm": 0.20282088220119476,
      "learning_rate": 0.00019789918502867491,
      "loss": 0.7483,
      "step": 527
    },
    {
      "epoch": 1.062374245472837,
      "grad_norm": 0.21097545325756073,
      "learning_rate": 0.00019789516047892143,
      "loss": 0.7398,
      "step": 528
    },
    {
      "epoch": 1.0643863179074446,
      "grad_norm": 0.20223158597946167,
      "learning_rate": 0.00019789113592916794,
      "loss": 0.7507,
      "step": 529
    },
    {
      "epoch": 1.0663983903420524,
      "grad_norm": 0.20194655656814575,
      "learning_rate": 0.00019788711137941445,
      "loss": 0.777,
      "step": 530
    },
    {
      "epoch": 1.06841046277666,
      "grad_norm": 0.19441770017147064,
      "learning_rate": 0.00019788308682966094,
      "loss": 0.7629,
      "step": 531
    },
    {
      "epoch": 1.0704225352112675,
      "grad_norm": 0.20259110629558563,
      "learning_rate": 0.00019787906227990745,
      "loss": 0.7423,
      "step": 532
    },
    {
      "epoch": 1.0724346076458753,
      "grad_norm": 0.21383869647979736,
      "learning_rate": 0.00019787503773015393,
      "loss": 0.7659,
      "step": 533
    },
    {
      "epoch": 1.0744466800804828,
      "grad_norm": 0.20197829604148865,
      "learning_rate": 0.00019787101318040045,
      "loss": 0.7879,
      "step": 534
    },
    {
      "epoch": 1.0764587525150906,
      "grad_norm": 0.2151019126176834,
      "learning_rate": 0.00019786698863064696,
      "loss": 0.7884,
      "step": 535
    },
    {
      "epoch": 1.0784708249496981,
      "grad_norm": 0.2094992846250534,
      "learning_rate": 0.00019786296408089347,
      "loss": 0.7817,
      "step": 536
    },
    {
      "epoch": 1.080482897384306,
      "grad_norm": 0.23365266621112823,
      "learning_rate": 0.00019785893953113996,
      "loss": 0.7832,
      "step": 537
    },
    {
      "epoch": 1.0824949698189135,
      "grad_norm": 0.2062283456325531,
      "learning_rate": 0.00019785491498138647,
      "loss": 0.7448,
      "step": 538
    },
    {
      "epoch": 1.084507042253521,
      "grad_norm": 0.20538176596164703,
      "learning_rate": 0.00019785089043163295,
      "loss": 0.7624,
      "step": 539
    },
    {
      "epoch": 1.0865191146881288,
      "grad_norm": 0.21230073273181915,
      "learning_rate": 0.0001978468658818795,
      "loss": 0.7549,
      "step": 540
    },
    {
      "epoch": 1.0885311871227363,
      "grad_norm": 0.21453803777694702,
      "learning_rate": 0.00019784284133212598,
      "loss": 0.8158,
      "step": 541
    },
    {
      "epoch": 1.090543259557344,
      "grad_norm": 0.21376293897628784,
      "learning_rate": 0.0001978388167823725,
      "loss": 0.7829,
      "step": 542
    },
    {
      "epoch": 1.0925553319919517,
      "grad_norm": 0.212716743350029,
      "learning_rate": 0.00019783479223261897,
      "loss": 0.7394,
      "step": 543
    },
    {
      "epoch": 1.0945674044265594,
      "grad_norm": 0.20181281864643097,
      "learning_rate": 0.0001978307676828655,
      "loss": 0.7556,
      "step": 544
    },
    {
      "epoch": 1.096579476861167,
      "grad_norm": 0.21952462196350098,
      "learning_rate": 0.000197826743133112,
      "loss": 0.7829,
      "step": 545
    },
    {
      "epoch": 1.0985915492957747,
      "grad_norm": 0.19502227008342743,
      "learning_rate": 0.0001978227185833585,
      "loss": 0.7635,
      "step": 546
    },
    {
      "epoch": 1.1006036217303823,
      "grad_norm": 0.1956176608800888,
      "learning_rate": 0.000197818694033605,
      "loss": 0.7936,
      "step": 547
    },
    {
      "epoch": 1.10261569416499,
      "grad_norm": 0.20732462406158447,
      "learning_rate": 0.0001978146694838515,
      "loss": 0.7338,
      "step": 548
    },
    {
      "epoch": 1.1046277665995976,
      "grad_norm": 0.21103350818157196,
      "learning_rate": 0.000197810644934098,
      "loss": 0.717,
      "step": 549
    },
    {
      "epoch": 1.1066398390342052,
      "grad_norm": 0.2171984612941742,
      "learning_rate": 0.00019780662038434453,
      "loss": 0.7699,
      "step": 550
    },
    {
      "epoch": 1.108651911468813,
      "grad_norm": 0.21167023479938507,
      "learning_rate": 0.00019780259583459102,
      "loss": 0.7603,
      "step": 551
    },
    {
      "epoch": 1.1106639839034205,
      "grad_norm": 0.22335940599441528,
      "learning_rate": 0.00019779857128483753,
      "loss": 0.782,
      "step": 552
    },
    {
      "epoch": 1.1126760563380282,
      "grad_norm": 0.21022047102451324,
      "learning_rate": 0.00019779454673508402,
      "loss": 0.7581,
      "step": 553
    },
    {
      "epoch": 1.1146881287726358,
      "grad_norm": 0.20466671884059906,
      "learning_rate": 0.00019779052218533053,
      "loss": 0.7571,
      "step": 554
    },
    {
      "epoch": 1.1167002012072436,
      "grad_norm": 0.20733490586280823,
      "learning_rate": 0.00019778649763557704,
      "loss": 0.768,
      "step": 555
    },
    {
      "epoch": 1.118712273641851,
      "grad_norm": 0.2161037027835846,
      "learning_rate": 0.00019778247308582355,
      "loss": 0.724,
      "step": 556
    },
    {
      "epoch": 1.1207243460764587,
      "grad_norm": 0.2121858447790146,
      "learning_rate": 0.00019777844853607004,
      "loss": 0.7662,
      "step": 557
    },
    {
      "epoch": 1.1227364185110664,
      "grad_norm": 0.22775545716285706,
      "learning_rate": 0.00019777442398631655,
      "loss": 0.7651,
      "step": 558
    },
    {
      "epoch": 1.124748490945674,
      "grad_norm": 0.21751080453395844,
      "learning_rate": 0.00019777039943656303,
      "loss": 0.7876,
      "step": 559
    },
    {
      "epoch": 1.1267605633802817,
      "grad_norm": 0.21002468466758728,
      "learning_rate": 0.00019776637488680955,
      "loss": 0.7424,
      "step": 560
    },
    {
      "epoch": 1.1287726358148893,
      "grad_norm": 0.2153823971748352,
      "learning_rate": 0.00019776235033705606,
      "loss": 0.784,
      "step": 561
    },
    {
      "epoch": 1.130784708249497,
      "grad_norm": 0.216207355260849,
      "learning_rate": 0.00019775832578730254,
      "loss": 0.7276,
      "step": 562
    },
    {
      "epoch": 1.1327967806841046,
      "grad_norm": 0.2010664939880371,
      "learning_rate": 0.00019775430123754906,
      "loss": 0.7977,
      "step": 563
    },
    {
      "epoch": 1.1348088531187122,
      "grad_norm": 0.20186467468738556,
      "learning_rate": 0.00019775027668779557,
      "loss": 0.711,
      "step": 564
    },
    {
      "epoch": 1.13682092555332,
      "grad_norm": 0.21316972374916077,
      "learning_rate": 0.00019774625213804208,
      "loss": 0.7632,
      "step": 565
    },
    {
      "epoch": 1.1388329979879275,
      "grad_norm": 0.2019370049238205,
      "learning_rate": 0.00019774222758828857,
      "loss": 0.7594,
      "step": 566
    },
    {
      "epoch": 1.1408450704225352,
      "grad_norm": 0.20187628269195557,
      "learning_rate": 0.00019773820303853508,
      "loss": 0.8001,
      "step": 567
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.19514036178588867,
      "learning_rate": 0.00019773417848878156,
      "loss": 0.7351,
      "step": 568
    },
    {
      "epoch": 1.1448692152917506,
      "grad_norm": 0.2114318460226059,
      "learning_rate": 0.00019773015393902808,
      "loss": 0.7548,
      "step": 569
    },
    {
      "epoch": 1.1468812877263581,
      "grad_norm": 0.20212087035179138,
      "learning_rate": 0.0001977261293892746,
      "loss": 0.7715,
      "step": 570
    },
    {
      "epoch": 1.1488933601609659,
      "grad_norm": 0.20212677121162415,
      "learning_rate": 0.0001977221048395211,
      "loss": 0.7789,
      "step": 571
    },
    {
      "epoch": 1.1509054325955734,
      "grad_norm": 0.20668752491474152,
      "learning_rate": 0.00019771808028976758,
      "loss": 0.7721,
      "step": 572
    },
    {
      "epoch": 1.1529175050301812,
      "grad_norm": 0.2033763974905014,
      "learning_rate": 0.0001977140557400141,
      "loss": 0.7675,
      "step": 573
    },
    {
      "epoch": 1.1549295774647887,
      "grad_norm": 0.20231887698173523,
      "learning_rate": 0.00019771003119026058,
      "loss": 0.739,
      "step": 574
    },
    {
      "epoch": 1.1569416498993963,
      "grad_norm": 0.2053561955690384,
      "learning_rate": 0.00019770600664050712,
      "loss": 0.7613,
      "step": 575
    },
    {
      "epoch": 1.158953722334004,
      "grad_norm": 0.21580059826374054,
      "learning_rate": 0.0001977019820907536,
      "loss": 0.7953,
      "step": 576
    },
    {
      "epoch": 1.1609657947686116,
      "grad_norm": 0.21033544838428497,
      "learning_rate": 0.00019769795754100012,
      "loss": 0.7768,
      "step": 577
    },
    {
      "epoch": 1.1629778672032194,
      "grad_norm": 0.20164597034454346,
      "learning_rate": 0.0001976939329912466,
      "loss": 0.7808,
      "step": 578
    },
    {
      "epoch": 1.164989939637827,
      "grad_norm": 0.21885445713996887,
      "learning_rate": 0.00019768990844149312,
      "loss": 0.7695,
      "step": 579
    },
    {
      "epoch": 1.1670020120724347,
      "grad_norm": 0.2032032161951065,
      "learning_rate": 0.00019768588389173963,
      "loss": 0.7606,
      "step": 580
    },
    {
      "epoch": 1.1690140845070423,
      "grad_norm": 0.20284625887870789,
      "learning_rate": 0.00019768185934198614,
      "loss": 0.764,
      "step": 581
    },
    {
      "epoch": 1.1710261569416498,
      "grad_norm": 0.2039438784122467,
      "learning_rate": 0.00019767783479223263,
      "loss": 0.7922,
      "step": 582
    },
    {
      "epoch": 1.1730382293762576,
      "grad_norm": 0.20502698421478271,
      "learning_rate": 0.00019767381024247914,
      "loss": 0.7472,
      "step": 583
    },
    {
      "epoch": 1.1750503018108651,
      "grad_norm": 0.20149368047714233,
      "learning_rate": 0.00019766978569272562,
      "loss": 0.7601,
      "step": 584
    },
    {
      "epoch": 1.1770623742454729,
      "grad_norm": 0.2037273645401001,
      "learning_rate": 0.00019766576114297216,
      "loss": 0.7519,
      "step": 585
    },
    {
      "epoch": 1.1790744466800804,
      "grad_norm": 0.20320996642112732,
      "learning_rate": 0.00019766173659321865,
      "loss": 0.7517,
      "step": 586
    },
    {
      "epoch": 1.1810865191146882,
      "grad_norm": 0.2131882756948471,
      "learning_rate": 0.00019765771204346516,
      "loss": 0.7921,
      "step": 587
    },
    {
      "epoch": 1.1830985915492958,
      "grad_norm": 0.21050211787223816,
      "learning_rate": 0.00019765368749371164,
      "loss": 0.7619,
      "step": 588
    },
    {
      "epoch": 1.1851106639839033,
      "grad_norm": 0.200298473238945,
      "learning_rate": 0.00019764966294395816,
      "loss": 0.7703,
      "step": 589
    },
    {
      "epoch": 1.187122736418511,
      "grad_norm": 0.22403664886951447,
      "learning_rate": 0.00019764563839420467,
      "loss": 0.7795,
      "step": 590
    },
    {
      "epoch": 1.1891348088531186,
      "grad_norm": 0.2043384313583374,
      "learning_rate": 0.00019764161384445115,
      "loss": 0.7476,
      "step": 591
    },
    {
      "epoch": 1.1911468812877264,
      "grad_norm": 0.20886701345443726,
      "learning_rate": 0.00019763758929469767,
      "loss": 0.7445,
      "step": 592
    },
    {
      "epoch": 1.193158953722334,
      "grad_norm": 0.19960036873817444,
      "learning_rate": 0.00019763356474494418,
      "loss": 0.754,
      "step": 593
    },
    {
      "epoch": 1.1951710261569417,
      "grad_norm": 0.20719648897647858,
      "learning_rate": 0.00019762954019519066,
      "loss": 0.7586,
      "step": 594
    },
    {
      "epoch": 1.1971830985915493,
      "grad_norm": 0.2005016803741455,
      "learning_rate": 0.00019762551564543718,
      "loss": 0.8027,
      "step": 595
    },
    {
      "epoch": 1.199195171026157,
      "grad_norm": 0.19724833965301514,
      "learning_rate": 0.0001976214910956837,
      "loss": 0.7482,
      "step": 596
    },
    {
      "epoch": 1.2012072434607646,
      "grad_norm": 0.20417791604995728,
      "learning_rate": 0.00019761746654593017,
      "loss": 0.7531,
      "step": 597
    },
    {
      "epoch": 1.2032193158953723,
      "grad_norm": 0.19432702660560608,
      "learning_rate": 0.00019761344199617669,
      "loss": 0.72,
      "step": 598
    },
    {
      "epoch": 1.20523138832998,
      "grad_norm": 0.2057623565196991,
      "learning_rate": 0.00019760941744642317,
      "loss": 0.7987,
      "step": 599
    },
    {
      "epoch": 1.2072434607645874,
      "grad_norm": 0.21003559231758118,
      "learning_rate": 0.0001976053928966697,
      "loss": 0.7668,
      "step": 600
    },
    {
      "epoch": 1.2092555331991952,
      "grad_norm": 0.23363059759140015,
      "learning_rate": 0.0001976013683469162,
      "loss": 0.7671,
      "step": 601
    },
    {
      "epoch": 1.2112676056338028,
      "grad_norm": 0.21596045792102814,
      "learning_rate": 0.0001975973437971627,
      "loss": 0.758,
      "step": 602
    },
    {
      "epoch": 1.2132796780684105,
      "grad_norm": 0.21107515692710876,
      "learning_rate": 0.0001975933192474092,
      "loss": 0.722,
      "step": 603
    },
    {
      "epoch": 1.215291750503018,
      "grad_norm": 0.2200825959444046,
      "learning_rate": 0.0001975892946976557,
      "loss": 0.7448,
      "step": 604
    },
    {
      "epoch": 1.2173038229376258,
      "grad_norm": 0.22009983658790588,
      "learning_rate": 0.00019758527014790222,
      "loss": 0.8128,
      "step": 605
    },
    {
      "epoch": 1.2193158953722334,
      "grad_norm": 0.22689726948738098,
      "learning_rate": 0.00019758124559814873,
      "loss": 0.7622,
      "step": 606
    },
    {
      "epoch": 1.221327967806841,
      "grad_norm": 0.21176286041736603,
      "learning_rate": 0.00019757722104839521,
      "loss": 0.7516,
      "step": 607
    },
    {
      "epoch": 1.2233400402414487,
      "grad_norm": 0.20055902004241943,
      "learning_rate": 0.00019757319649864173,
      "loss": 0.7547,
      "step": 608
    },
    {
      "epoch": 1.2253521126760563,
      "grad_norm": 0.20834776759147644,
      "learning_rate": 0.0001975691719488882,
      "loss": 0.7355,
      "step": 609
    },
    {
      "epoch": 1.227364185110664,
      "grad_norm": 0.20672868192195892,
      "learning_rate": 0.00019756514739913475,
      "loss": 0.7789,
      "step": 610
    },
    {
      "epoch": 1.2293762575452716,
      "grad_norm": 0.21192163228988647,
      "learning_rate": 0.00019756112284938124,
      "loss": 0.7787,
      "step": 611
    },
    {
      "epoch": 1.2313883299798793,
      "grad_norm": 0.2409549355506897,
      "learning_rate": 0.00019755709829962775,
      "loss": 0.7566,
      "step": 612
    },
    {
      "epoch": 1.233400402414487,
      "grad_norm": 0.2371678501367569,
      "learning_rate": 0.00019755307374987423,
      "loss": 0.761,
      "step": 613
    },
    {
      "epoch": 1.2354124748490944,
      "grad_norm": 0.22088083624839783,
      "learning_rate": 0.00019754904920012075,
      "loss": 0.8066,
      "step": 614
    },
    {
      "epoch": 1.2374245472837022,
      "grad_norm": 0.21411436796188354,
      "learning_rate": 0.00019754502465036723,
      "loss": 0.7938,
      "step": 615
    },
    {
      "epoch": 1.2394366197183098,
      "grad_norm": 0.20965738594532013,
      "learning_rate": 0.00019754100010061377,
      "loss": 0.7622,
      "step": 616
    },
    {
      "epoch": 1.2414486921529175,
      "grad_norm": 0.20462395250797272,
      "learning_rate": 0.00019753697555086026,
      "loss": 0.7561,
      "step": 617
    },
    {
      "epoch": 1.243460764587525,
      "grad_norm": 0.20522043108940125,
      "learning_rate": 0.00019753295100110677,
      "loss": 0.7901,
      "step": 618
    },
    {
      "epoch": 1.2454728370221329,
      "grad_norm": 0.20623259246349335,
      "learning_rate": 0.00019752892645135325,
      "loss": 0.777,
      "step": 619
    },
    {
      "epoch": 1.2474849094567404,
      "grad_norm": 0.2130839079618454,
      "learning_rate": 0.00019752490190159976,
      "loss": 0.7907,
      "step": 620
    },
    {
      "epoch": 1.2494969818913482,
      "grad_norm": 0.21515990793704987,
      "learning_rate": 0.00019752087735184628,
      "loss": 0.7928,
      "step": 621
    },
    {
      "epoch": 1.2515090543259557,
      "grad_norm": 0.2033543884754181,
      "learning_rate": 0.0001975168528020928,
      "loss": 0.7669,
      "step": 622
    },
    {
      "epoch": 1.2535211267605635,
      "grad_norm": 0.21592438220977783,
      "learning_rate": 0.00019751282825233927,
      "loss": 0.7576,
      "step": 623
    },
    {
      "epoch": 1.255533199195171,
      "grad_norm": 0.22473298013210297,
      "learning_rate": 0.00019750880370258579,
      "loss": 0.8038,
      "step": 624
    },
    {
      "epoch": 1.2575452716297786,
      "grad_norm": 0.20827411115169525,
      "learning_rate": 0.00019750477915283227,
      "loss": 0.752,
      "step": 625
    },
    {
      "epoch": 1.2595573440643864,
      "grad_norm": 0.21449071168899536,
      "learning_rate": 0.00019750075460307878,
      "loss": 0.7699,
      "step": 626
    },
    {
      "epoch": 1.261569416498994,
      "grad_norm": 0.20645339787006378,
      "learning_rate": 0.0001974967300533253,
      "loss": 0.76,
      "step": 627
    },
    {
      "epoch": 1.2635814889336017,
      "grad_norm": 0.21296434104442596,
      "learning_rate": 0.0001974927055035718,
      "loss": 0.7902,
      "step": 628
    },
    {
      "epoch": 1.2655935613682092,
      "grad_norm": 0.21170911192893982,
      "learning_rate": 0.0001974886809538183,
      "loss": 0.7551,
      "step": 629
    },
    {
      "epoch": 1.267605633802817,
      "grad_norm": 0.21484392881393433,
      "learning_rate": 0.0001974846564040648,
      "loss": 0.7343,
      "step": 630
    },
    {
      "epoch": 1.2696177062374245,
      "grad_norm": 0.21116198599338531,
      "learning_rate": 0.00019748063185431132,
      "loss": 0.7933,
      "step": 631
    },
    {
      "epoch": 1.271629778672032,
      "grad_norm": 0.22254735231399536,
      "learning_rate": 0.0001974766073045578,
      "loss": 0.758,
      "step": 632
    },
    {
      "epoch": 1.2736418511066399,
      "grad_norm": 0.22698113322257996,
      "learning_rate": 0.00019747258275480431,
      "loss": 0.755,
      "step": 633
    },
    {
      "epoch": 1.2756539235412474,
      "grad_norm": 0.20800569653511047,
      "learning_rate": 0.0001974685582050508,
      "loss": 0.7755,
      "step": 634
    },
    {
      "epoch": 1.2776659959758552,
      "grad_norm": 0.20542915165424347,
      "learning_rate": 0.0001974645336552973,
      "loss": 0.7454,
      "step": 635
    },
    {
      "epoch": 1.2796780684104627,
      "grad_norm": 0.21945194900035858,
      "learning_rate": 0.00019746050910554382,
      "loss": 0.7606,
      "step": 636
    },
    {
      "epoch": 1.2816901408450705,
      "grad_norm": 0.21387560665607452,
      "learning_rate": 0.00019745648455579034,
      "loss": 0.7828,
      "step": 637
    },
    {
      "epoch": 1.283702213279678,
      "grad_norm": 0.1996394693851471,
      "learning_rate": 0.00019745246000603682,
      "loss": 0.7675,
      "step": 638
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.20758141577243805,
      "learning_rate": 0.00019744843545628333,
      "loss": 0.7955,
      "step": 639
    },
    {
      "epoch": 1.2877263581488934,
      "grad_norm": 0.21964871883392334,
      "learning_rate": 0.00019744441090652982,
      "loss": 0.751,
      "step": 640
    },
    {
      "epoch": 1.2897384305835011,
      "grad_norm": 0.21721269190311432,
      "learning_rate": 0.00019744038635677636,
      "loss": 0.7887,
      "step": 641
    },
    {
      "epoch": 1.2917505030181087,
      "grad_norm": 0.2022242546081543,
      "learning_rate": 0.00019743636180702284,
      "loss": 0.7612,
      "step": 642
    },
    {
      "epoch": 1.2937625754527162,
      "grad_norm": 0.2137438952922821,
      "learning_rate": 0.00019743233725726936,
      "loss": 0.7851,
      "step": 643
    },
    {
      "epoch": 1.295774647887324,
      "grad_norm": 0.19720867276191711,
      "learning_rate": 0.00019742831270751584,
      "loss": 0.736,
      "step": 644
    },
    {
      "epoch": 1.2977867203219315,
      "grad_norm": 0.20351798832416534,
      "learning_rate": 0.00019742428815776235,
      "loss": 0.776,
      "step": 645
    },
    {
      "epoch": 1.2997987927565393,
      "grad_norm": 0.2259402722120285,
      "learning_rate": 0.00019742026360800887,
      "loss": 0.813,
      "step": 646
    },
    {
      "epoch": 1.3018108651911469,
      "grad_norm": 0.21649900078773499,
      "learning_rate": 0.00019741623905825538,
      "loss": 0.7547,
      "step": 647
    },
    {
      "epoch": 1.3038229376257546,
      "grad_norm": 0.20813582837581635,
      "learning_rate": 0.00019741221450850186,
      "loss": 0.7524,
      "step": 648
    },
    {
      "epoch": 1.3058350100603622,
      "grad_norm": 0.2044193595647812,
      "learning_rate": 0.00019740818995874837,
      "loss": 0.7661,
      "step": 649
    },
    {
      "epoch": 1.3078470824949697,
      "grad_norm": 0.20814503729343414,
      "learning_rate": 0.00019740416540899486,
      "loss": 0.7501,
      "step": 650
    },
    {
      "epoch": 1.3098591549295775,
      "grad_norm": 0.21570748090744019,
      "learning_rate": 0.0001974001408592414,
      "loss": 0.7389,
      "step": 651
    },
    {
      "epoch": 1.311871227364185,
      "grad_norm": 0.21798443794250488,
      "learning_rate": 0.00019739611630948788,
      "loss": 0.7733,
      "step": 652
    },
    {
      "epoch": 1.3138832997987928,
      "grad_norm": 0.2294517308473587,
      "learning_rate": 0.0001973920917597344,
      "loss": 0.7718,
      "step": 653
    },
    {
      "epoch": 1.3158953722334004,
      "grad_norm": 0.2212398499250412,
      "learning_rate": 0.00019738806720998088,
      "loss": 0.7633,
      "step": 654
    },
    {
      "epoch": 1.3179074446680081,
      "grad_norm": 0.19762824475765228,
      "learning_rate": 0.0001973840426602274,
      "loss": 0.7902,
      "step": 655
    },
    {
      "epoch": 1.3199195171026157,
      "grad_norm": 0.20931509137153625,
      "learning_rate": 0.0001973800181104739,
      "loss": 0.7433,
      "step": 656
    },
    {
      "epoch": 1.3219315895372232,
      "grad_norm": 0.21270273625850677,
      "learning_rate": 0.00019737599356072042,
      "loss": 0.7343,
      "step": 657
    },
    {
      "epoch": 1.323943661971831,
      "grad_norm": 0.19439436495304108,
      "learning_rate": 0.0001973719690109669,
      "loss": 0.7368,
      "step": 658
    },
    {
      "epoch": 1.3259557344064388,
      "grad_norm": 0.20726308226585388,
      "learning_rate": 0.00019736794446121342,
      "loss": 0.7904,
      "step": 659
    },
    {
      "epoch": 1.3279678068410463,
      "grad_norm": 0.22074249386787415,
      "learning_rate": 0.0001973639199114599,
      "loss": 0.7873,
      "step": 660
    },
    {
      "epoch": 1.3299798792756539,
      "grad_norm": 0.21804845333099365,
      "learning_rate": 0.0001973598953617064,
      "loss": 0.7884,
      "step": 661
    },
    {
      "epoch": 1.3319919517102616,
      "grad_norm": 0.21696162223815918,
      "learning_rate": 0.00019735587081195293,
      "loss": 0.7476,
      "step": 662
    },
    {
      "epoch": 1.3340040241448692,
      "grad_norm": 0.20787188410758972,
      "learning_rate": 0.00019735184626219944,
      "loss": 0.7443,
      "step": 663
    },
    {
      "epoch": 1.3360160965794767,
      "grad_norm": 0.22317229211330414,
      "learning_rate": 0.00019734782171244592,
      "loss": 0.8201,
      "step": 664
    },
    {
      "epoch": 1.3380281690140845,
      "grad_norm": 0.20821146667003632,
      "learning_rate": 0.00019734379716269243,
      "loss": 0.7117,
      "step": 665
    },
    {
      "epoch": 1.3400402414486923,
      "grad_norm": 0.20800772309303284,
      "learning_rate": 0.00019733977261293895,
      "loss": 0.7765,
      "step": 666
    },
    {
      "epoch": 1.3420523138832998,
      "grad_norm": 0.21091477572917938,
      "learning_rate": 0.00019733574806318543,
      "loss": 0.7383,
      "step": 667
    },
    {
      "epoch": 1.3440643863179074,
      "grad_norm": 0.21927161514759064,
      "learning_rate": 0.00019733172351343194,
      "loss": 0.7332,
      "step": 668
    },
    {
      "epoch": 1.3460764587525151,
      "grad_norm": 0.21032051742076874,
      "learning_rate": 0.00019732769896367843,
      "loss": 0.7707,
      "step": 669
    },
    {
      "epoch": 1.3480885311871227,
      "grad_norm": 0.21803025901317596,
      "learning_rate": 0.00019732367441392494,
      "loss": 0.7205,
      "step": 670
    },
    {
      "epoch": 1.3501006036217305,
      "grad_norm": 0.23637065291404724,
      "learning_rate": 0.00019731964986417145,
      "loss": 0.7606,
      "step": 671
    },
    {
      "epoch": 1.352112676056338,
      "grad_norm": 0.2136673927307129,
      "learning_rate": 0.00019731562531441797,
      "loss": 0.7719,
      "step": 672
    },
    {
      "epoch": 1.3541247484909458,
      "grad_norm": 0.20740057528018951,
      "learning_rate": 0.00019731160076466445,
      "loss": 0.7821,
      "step": 673
    },
    {
      "epoch": 1.3561368209255533,
      "grad_norm": 0.20438742637634277,
      "learning_rate": 0.00019730757621491096,
      "loss": 0.7566,
      "step": 674
    },
    {
      "epoch": 1.3581488933601609,
      "grad_norm": 0.20936450362205505,
      "learning_rate": 0.00019730355166515745,
      "loss": 0.7519,
      "step": 675
    },
    {
      "epoch": 1.3601609657947686,
      "grad_norm": 0.21308772265911102,
      "learning_rate": 0.000197299527115404,
      "loss": 0.7302,
      "step": 676
    },
    {
      "epoch": 1.3621730382293762,
      "grad_norm": 0.20447133481502533,
      "learning_rate": 0.00019729550256565047,
      "loss": 0.7558,
      "step": 677
    },
    {
      "epoch": 1.364185110663984,
      "grad_norm": 0.2230728268623352,
      "learning_rate": 0.00019729147801589699,
      "loss": 0.7695,
      "step": 678
    },
    {
      "epoch": 1.3661971830985915,
      "grad_norm": 0.21498748660087585,
      "learning_rate": 0.00019728745346614347,
      "loss": 0.756,
      "step": 679
    },
    {
      "epoch": 1.3682092555331993,
      "grad_norm": 0.23097459971904755,
      "learning_rate": 0.00019728342891638998,
      "loss": 0.7809,
      "step": 680
    },
    {
      "epoch": 1.3702213279678068,
      "grad_norm": 0.20427601039409637,
      "learning_rate": 0.0001972794043666365,
      "loss": 0.7552,
      "step": 681
    },
    {
      "epoch": 1.3722334004024144,
      "grad_norm": 0.20513728260993958,
      "learning_rate": 0.000197275379816883,
      "loss": 0.7989,
      "step": 682
    },
    {
      "epoch": 1.3742454728370221,
      "grad_norm": 0.22099773585796356,
      "learning_rate": 0.0001972713552671295,
      "loss": 0.7967,
      "step": 683
    },
    {
      "epoch": 1.37625754527163,
      "grad_norm": 0.2149510383605957,
      "learning_rate": 0.000197267330717376,
      "loss": 0.7878,
      "step": 684
    },
    {
      "epoch": 1.3782696177062375,
      "grad_norm": 0.2090742141008377,
      "learning_rate": 0.0001972633061676225,
      "loss": 0.7645,
      "step": 685
    },
    {
      "epoch": 1.380281690140845,
      "grad_norm": 0.2059699445962906,
      "learning_rate": 0.00019725928161786903,
      "loss": 0.7308,
      "step": 686
    },
    {
      "epoch": 1.3822937625754528,
      "grad_norm": 0.20286206901073456,
      "learning_rate": 0.00019725525706811551,
      "loss": 0.7225,
      "step": 687
    },
    {
      "epoch": 1.3843058350100603,
      "grad_norm": 0.22567929327487946,
      "learning_rate": 0.00019725123251836203,
      "loss": 0.8008,
      "step": 688
    },
    {
      "epoch": 1.3863179074446679,
      "grad_norm": 0.21656584739685059,
      "learning_rate": 0.0001972472079686085,
      "loss": 0.777,
      "step": 689
    },
    {
      "epoch": 1.3883299798792756,
      "grad_norm": 0.2151181697845459,
      "learning_rate": 0.00019724318341885502,
      "loss": 0.7539,
      "step": 690
    },
    {
      "epoch": 1.3903420523138834,
      "grad_norm": 0.20684084296226501,
      "learning_rate": 0.00019723915886910154,
      "loss": 0.7758,
      "step": 691
    },
    {
      "epoch": 1.392354124748491,
      "grad_norm": 0.1992534101009369,
      "learning_rate": 0.00019723513431934805,
      "loss": 0.7369,
      "step": 692
    },
    {
      "epoch": 1.3943661971830985,
      "grad_norm": 0.21022792160511017,
      "learning_rate": 0.00019723110976959453,
      "loss": 0.7535,
      "step": 693
    },
    {
      "epoch": 1.3963782696177063,
      "grad_norm": 0.21063795685768127,
      "learning_rate": 0.00019722708521984105,
      "loss": 0.7611,
      "step": 694
    },
    {
      "epoch": 1.3983903420523138,
      "grad_norm": 0.21195973455905914,
      "learning_rate": 0.00019722306067008753,
      "loss": 0.7324,
      "step": 695
    },
    {
      "epoch": 1.4004024144869216,
      "grad_norm": 0.20900064706802368,
      "learning_rate": 0.00019721903612033404,
      "loss": 0.795,
      "step": 696
    },
    {
      "epoch": 1.4024144869215291,
      "grad_norm": 0.21372279524803162,
      "learning_rate": 0.00019721501157058055,
      "loss": 0.7751,
      "step": 697
    },
    {
      "epoch": 1.404426559356137,
      "grad_norm": 0.20673927664756775,
      "learning_rate": 0.00019721098702082707,
      "loss": 0.7396,
      "step": 698
    },
    {
      "epoch": 1.4064386317907445,
      "grad_norm": 0.2162758857011795,
      "learning_rate": 0.00019720696247107355,
      "loss": 0.7717,
      "step": 699
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 0.20211324095726013,
      "learning_rate": 0.00019720293792132006,
      "loss": 0.7364,
      "step": 700
    },
    {
      "epoch": 1.4104627766599598,
      "grad_norm": 0.20936228334903717,
      "learning_rate": 0.00019719891337156658,
      "loss": 0.693,
      "step": 701
    },
    {
      "epoch": 1.4124748490945673,
      "grad_norm": 0.21009227633476257,
      "learning_rate": 0.00019719488882181306,
      "loss": 0.7467,
      "step": 702
    },
    {
      "epoch": 1.414486921529175,
      "grad_norm": 0.22510768473148346,
      "learning_rate": 0.00019719086427205957,
      "loss": 0.7545,
      "step": 703
    },
    {
      "epoch": 1.4164989939637826,
      "grad_norm": 0.20975717902183533,
      "learning_rate": 0.00019718683972230606,
      "loss": 0.7498,
      "step": 704
    },
    {
      "epoch": 1.4185110663983904,
      "grad_norm": 0.21049906313419342,
      "learning_rate": 0.00019718281517255257,
      "loss": 0.7509,
      "step": 705
    },
    {
      "epoch": 1.420523138832998,
      "grad_norm": 0.2157151699066162,
      "learning_rate": 0.00019717879062279908,
      "loss": 0.7294,
      "step": 706
    },
    {
      "epoch": 1.4225352112676055,
      "grad_norm": 0.21654784679412842,
      "learning_rate": 0.0001971747660730456,
      "loss": 0.8119,
      "step": 707
    },
    {
      "epoch": 1.4245472837022133,
      "grad_norm": 0.20369063317775726,
      "learning_rate": 0.00019717074152329208,
      "loss": 0.7671,
      "step": 708
    },
    {
      "epoch": 1.426559356136821,
      "grad_norm": 0.20590493083000183,
      "learning_rate": 0.0001971667169735386,
      "loss": 0.7272,
      "step": 709
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.2145368456840515,
      "learning_rate": 0.00019716269242378508,
      "loss": 0.7626,
      "step": 710
    },
    {
      "epoch": 1.4305835010060362,
      "grad_norm": 0.21053309738636017,
      "learning_rate": 0.00019715866787403162,
      "loss": 0.7595,
      "step": 711
    },
    {
      "epoch": 1.432595573440644,
      "grad_norm": 0.20612013339996338,
      "learning_rate": 0.0001971546433242781,
      "loss": 0.777,
      "step": 712
    },
    {
      "epoch": 1.4346076458752515,
      "grad_norm": 0.20882627367973328,
      "learning_rate": 0.00019715061877452461,
      "loss": 0.743,
      "step": 713
    },
    {
      "epoch": 1.436619718309859,
      "grad_norm": 0.21823306381702423,
      "learning_rate": 0.0001971465942247711,
      "loss": 0.7693,
      "step": 714
    },
    {
      "epoch": 1.4386317907444668,
      "grad_norm": 0.21115143597126007,
      "learning_rate": 0.0001971425696750176,
      "loss": 0.8018,
      "step": 715
    },
    {
      "epoch": 1.4406438631790746,
      "grad_norm": 0.21651187539100647,
      "learning_rate": 0.00019713854512526412,
      "loss": 0.7804,
      "step": 716
    },
    {
      "epoch": 1.442655935613682,
      "grad_norm": 0.22482594847679138,
      "learning_rate": 0.00019713452057551064,
      "loss": 0.7548,
      "step": 717
    },
    {
      "epoch": 1.4446680080482897,
      "grad_norm": 0.19835259020328522,
      "learning_rate": 0.00019713049602575712,
      "loss": 0.7636,
      "step": 718
    },
    {
      "epoch": 1.4466800804828974,
      "grad_norm": 0.19563961029052734,
      "learning_rate": 0.00019712647147600363,
      "loss": 0.7731,
      "step": 719
    },
    {
      "epoch": 1.448692152917505,
      "grad_norm": 0.22199906408786774,
      "learning_rate": 0.00019712244692625012,
      "loss": 0.7672,
      "step": 720
    },
    {
      "epoch": 1.4507042253521127,
      "grad_norm": 0.212428018450737,
      "learning_rate": 0.00019711842237649666,
      "loss": 0.7567,
      "step": 721
    },
    {
      "epoch": 1.4527162977867203,
      "grad_norm": 0.20680256187915802,
      "learning_rate": 0.00019711439782674314,
      "loss": 0.777,
      "step": 722
    },
    {
      "epoch": 1.454728370221328,
      "grad_norm": 0.2154480367898941,
      "learning_rate": 0.00019711037327698966,
      "loss": 0.7748,
      "step": 723
    },
    {
      "epoch": 1.4567404426559356,
      "grad_norm": 0.20773887634277344,
      "learning_rate": 0.00019710634872723614,
      "loss": 0.7987,
      "step": 724
    },
    {
      "epoch": 1.4587525150905432,
      "grad_norm": 0.21689879894256592,
      "learning_rate": 0.00019710232417748265,
      "loss": 0.7372,
      "step": 725
    },
    {
      "epoch": 1.460764587525151,
      "grad_norm": 0.22635528445243835,
      "learning_rate": 0.00019709829962772917,
      "loss": 0.8073,
      "step": 726
    },
    {
      "epoch": 1.4627766599597585,
      "grad_norm": 0.21420136094093323,
      "learning_rate": 0.00019709427507797568,
      "loss": 0.7872,
      "step": 727
    },
    {
      "epoch": 1.4647887323943662,
      "grad_norm": 0.21741244196891785,
      "learning_rate": 0.00019709025052822216,
      "loss": 0.7349,
      "step": 728
    },
    {
      "epoch": 1.4668008048289738,
      "grad_norm": 0.20862679183483124,
      "learning_rate": 0.00019708622597846867,
      "loss": 0.7385,
      "step": 729
    },
    {
      "epoch": 1.4688128772635816,
      "grad_norm": 0.2154298573732376,
      "learning_rate": 0.00019708220142871516,
      "loss": 0.7883,
      "step": 730
    },
    {
      "epoch": 1.470824949698189,
      "grad_norm": 0.20357312262058258,
      "learning_rate": 0.00019707817687896167,
      "loss": 0.7428,
      "step": 731
    },
    {
      "epoch": 1.4728370221327967,
      "grad_norm": 0.19600915908813477,
      "learning_rate": 0.00019707415232920818,
      "loss": 0.7561,
      "step": 732
    },
    {
      "epoch": 1.4748490945674044,
      "grad_norm": 0.20344989001750946,
      "learning_rate": 0.0001970701277794547,
      "loss": 0.7873,
      "step": 733
    },
    {
      "epoch": 1.4768611670020122,
      "grad_norm": 0.22857217490673065,
      "learning_rate": 0.00019706610322970118,
      "loss": 0.7995,
      "step": 734
    },
    {
      "epoch": 1.4788732394366197,
      "grad_norm": 0.21100227534770966,
      "learning_rate": 0.0001970620786799477,
      "loss": 0.7773,
      "step": 735
    },
    {
      "epoch": 1.4808853118712273,
      "grad_norm": 0.22742469608783722,
      "learning_rate": 0.0001970580541301942,
      "loss": 0.7501,
      "step": 736
    },
    {
      "epoch": 1.482897384305835,
      "grad_norm": 0.20228612422943115,
      "learning_rate": 0.0001970540295804407,
      "loss": 0.7134,
      "step": 737
    },
    {
      "epoch": 1.4849094567404426,
      "grad_norm": 0.2166685163974762,
      "learning_rate": 0.0001970500050306872,
      "loss": 0.75,
      "step": 738
    },
    {
      "epoch": 1.4869215291750504,
      "grad_norm": 0.20305536687374115,
      "learning_rate": 0.0001970459804809337,
      "loss": 0.7742,
      "step": 739
    },
    {
      "epoch": 1.488933601609658,
      "grad_norm": 0.20236632227897644,
      "learning_rate": 0.0001970419559311802,
      "loss": 0.7551,
      "step": 740
    },
    {
      "epoch": 1.4909456740442657,
      "grad_norm": 0.20708313584327698,
      "learning_rate": 0.0001970379313814267,
      "loss": 0.7532,
      "step": 741
    },
    {
      "epoch": 1.4929577464788732,
      "grad_norm": 0.20442654192447662,
      "learning_rate": 0.00019703390683167323,
      "loss": 0.7659,
      "step": 742
    },
    {
      "epoch": 1.4949698189134808,
      "grad_norm": 0.20589937269687653,
      "learning_rate": 0.0001970298822819197,
      "loss": 0.7211,
      "step": 743
    },
    {
      "epoch": 1.4969818913480886,
      "grad_norm": 0.20875293016433716,
      "learning_rate": 0.00019702585773216622,
      "loss": 0.7382,
      "step": 744
    },
    {
      "epoch": 1.4989939637826961,
      "grad_norm": 0.19612406194210052,
      "learning_rate": 0.0001970218331824127,
      "loss": 0.7569,
      "step": 745
    },
    {
      "epoch": 1.5010060362173037,
      "grad_norm": 0.20576655864715576,
      "learning_rate": 0.00019701780863265925,
      "loss": 0.7552,
      "step": 746
    },
    {
      "epoch": 1.5030181086519114,
      "grad_norm": 0.20444756746292114,
      "learning_rate": 0.00019701378408290573,
      "loss": 0.7736,
      "step": 747
    },
    {
      "epoch": 1.5050301810865192,
      "grad_norm": 0.21568745374679565,
      "learning_rate": 0.00019700975953315224,
      "loss": 0.738,
      "step": 748
    },
    {
      "epoch": 1.5070422535211268,
      "grad_norm": 0.2154405415058136,
      "learning_rate": 0.00019700573498339873,
      "loss": 0.7405,
      "step": 749
    },
    {
      "epoch": 1.5090543259557343,
      "grad_norm": 0.19835495948791504,
      "learning_rate": 0.00019700171043364524,
      "loss": 0.7562,
      "step": 750
    },
    {
      "epoch": 1.511066398390342,
      "grad_norm": 0.21415363252162933,
      "learning_rate": 0.00019699768588389175,
      "loss": 0.7813,
      "step": 751
    },
    {
      "epoch": 1.5130784708249498,
      "grad_norm": 0.2129097431898117,
      "learning_rate": 0.00019699366133413827,
      "loss": 0.7569,
      "step": 752
    },
    {
      "epoch": 1.5150905432595574,
      "grad_norm": 0.22925405204296112,
      "learning_rate": 0.00019698963678438475,
      "loss": 0.7899,
      "step": 753
    },
    {
      "epoch": 1.517102615694165,
      "grad_norm": 0.21179541945457458,
      "learning_rate": 0.00019698561223463126,
      "loss": 0.7714,
      "step": 754
    },
    {
      "epoch": 1.5191146881287727,
      "grad_norm": 0.19958153367042542,
      "learning_rate": 0.00019698158768487775,
      "loss": 0.7535,
      "step": 755
    },
    {
      "epoch": 1.5211267605633803,
      "grad_norm": 0.21140442788600922,
      "learning_rate": 0.0001969775631351243,
      "loss": 0.7547,
      "step": 756
    },
    {
      "epoch": 1.5231388329979878,
      "grad_norm": 0.2154538482427597,
      "learning_rate": 0.00019697353858537077,
      "loss": 0.7734,
      "step": 757
    },
    {
      "epoch": 1.5251509054325956,
      "grad_norm": 0.2158554047346115,
      "learning_rate": 0.00019696951403561729,
      "loss": 0.806,
      "step": 758
    },
    {
      "epoch": 1.5271629778672033,
      "grad_norm": 0.21962596476078033,
      "learning_rate": 0.00019696548948586377,
      "loss": 0.7921,
      "step": 759
    },
    {
      "epoch": 1.529175050301811,
      "grad_norm": 0.20667310059070587,
      "learning_rate": 0.00019696146493611028,
      "loss": 0.7602,
      "step": 760
    },
    {
      "epoch": 1.5311871227364184,
      "grad_norm": 0.20896445214748383,
      "learning_rate": 0.0001969574403863568,
      "loss": 0.7227,
      "step": 761
    },
    {
      "epoch": 1.5331991951710262,
      "grad_norm": 0.20068536698818207,
      "learning_rate": 0.0001969534158366033,
      "loss": 0.7369,
      "step": 762
    },
    {
      "epoch": 1.5352112676056338,
      "grad_norm": 0.19465771317481995,
      "learning_rate": 0.0001969493912868498,
      "loss": 0.7518,
      "step": 763
    },
    {
      "epoch": 1.5372233400402413,
      "grad_norm": 0.20654593408107758,
      "learning_rate": 0.0001969453667370963,
      "loss": 0.7546,
      "step": 764
    },
    {
      "epoch": 1.539235412474849,
      "grad_norm": 0.20687700808048248,
      "learning_rate": 0.0001969413421873428,
      "loss": 0.7208,
      "step": 765
    },
    {
      "epoch": 1.5412474849094568,
      "grad_norm": 0.2115265130996704,
      "learning_rate": 0.0001969373176375893,
      "loss": 0.7423,
      "step": 766
    },
    {
      "epoch": 1.5432595573440644,
      "grad_norm": 0.2157900035381317,
      "learning_rate": 0.00019693329308783581,
      "loss": 0.7668,
      "step": 767
    },
    {
      "epoch": 1.545271629778672,
      "grad_norm": 0.2253667563199997,
      "learning_rate": 0.0001969292685380823,
      "loss": 0.7829,
      "step": 768
    },
    {
      "epoch": 1.5472837022132797,
      "grad_norm": 0.20430664718151093,
      "learning_rate": 0.0001969252439883288,
      "loss": 0.7199,
      "step": 769
    },
    {
      "epoch": 1.5492957746478875,
      "grad_norm": 0.20871588587760925,
      "learning_rate": 0.00019692121943857532,
      "loss": 0.7557,
      "step": 770
    },
    {
      "epoch": 1.5513078470824948,
      "grad_norm": 0.19725710153579712,
      "learning_rate": 0.00019691719488882184,
      "loss": 0.7789,
      "step": 771
    },
    {
      "epoch": 1.5533199195171026,
      "grad_norm": 0.20831726491451263,
      "learning_rate": 0.00019691317033906832,
      "loss": 0.7138,
      "step": 772
    },
    {
      "epoch": 1.5553319919517103,
      "grad_norm": 0.20949296653270721,
      "learning_rate": 0.00019690914578931483,
      "loss": 0.7385,
      "step": 773
    },
    {
      "epoch": 1.557344064386318,
      "grad_norm": 0.20810428261756897,
      "learning_rate": 0.00019690512123956132,
      "loss": 0.7535,
      "step": 774
    },
    {
      "epoch": 1.5593561368209254,
      "grad_norm": 0.2106880247592926,
      "learning_rate": 0.00019690109668980783,
      "loss": 0.7663,
      "step": 775
    },
    {
      "epoch": 1.5613682092555332,
      "grad_norm": 0.2078884392976761,
      "learning_rate": 0.00019689707214005434,
      "loss": 0.72,
      "step": 776
    },
    {
      "epoch": 1.563380281690141,
      "grad_norm": 0.20912811160087585,
      "learning_rate": 0.00019689304759030085,
      "loss": 0.7399,
      "step": 777
    },
    {
      "epoch": 1.5653923541247485,
      "grad_norm": 0.20990392565727234,
      "learning_rate": 0.00019688902304054734,
      "loss": 0.7728,
      "step": 778
    },
    {
      "epoch": 1.567404426559356,
      "grad_norm": 0.21032798290252686,
      "learning_rate": 0.00019688499849079385,
      "loss": 0.7852,
      "step": 779
    },
    {
      "epoch": 1.5694164989939638,
      "grad_norm": 0.22323305904865265,
      "learning_rate": 0.00019688097394104034,
      "loss": 0.7425,
      "step": 780
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.21333369612693787,
      "learning_rate": 0.00019687694939128688,
      "loss": 0.765,
      "step": 781
    },
    {
      "epoch": 1.573440643863179,
      "grad_norm": 0.2137414813041687,
      "learning_rate": 0.00019687292484153336,
      "loss": 0.8188,
      "step": 782
    },
    {
      "epoch": 1.5754527162977867,
      "grad_norm": 0.2097903937101364,
      "learning_rate": 0.00019686890029177987,
      "loss": 0.7686,
      "step": 783
    },
    {
      "epoch": 1.5774647887323945,
      "grad_norm": 0.2061568945646286,
      "learning_rate": 0.00019686487574202636,
      "loss": 0.7843,
      "step": 784
    },
    {
      "epoch": 1.579476861167002,
      "grad_norm": 0.2201550453901291,
      "learning_rate": 0.00019686085119227287,
      "loss": 0.7789,
      "step": 785
    },
    {
      "epoch": 1.5814889336016096,
      "grad_norm": 0.21457263827323914,
      "learning_rate": 0.00019685682664251938,
      "loss": 0.7805,
      "step": 786
    },
    {
      "epoch": 1.5835010060362174,
      "grad_norm": 0.22020882368087769,
      "learning_rate": 0.0001968528020927659,
      "loss": 0.7145,
      "step": 787
    },
    {
      "epoch": 1.585513078470825,
      "grad_norm": 0.22095006704330444,
      "learning_rate": 0.00019684877754301238,
      "loss": 0.7762,
      "step": 788
    },
    {
      "epoch": 1.5875251509054324,
      "grad_norm": 0.2285294234752655,
      "learning_rate": 0.0001968447529932589,
      "loss": 0.7685,
      "step": 789
    },
    {
      "epoch": 1.5895372233400402,
      "grad_norm": 0.21465645730495453,
      "learning_rate": 0.00019684072844350538,
      "loss": 0.8011,
      "step": 790
    },
    {
      "epoch": 1.591549295774648,
      "grad_norm": 0.21062113344669342,
      "learning_rate": 0.00019683670389375192,
      "loss": 0.7542,
      "step": 791
    },
    {
      "epoch": 1.5935613682092555,
      "grad_norm": 0.22823649644851685,
      "learning_rate": 0.0001968326793439984,
      "loss": 0.7944,
      "step": 792
    },
    {
      "epoch": 1.595573440643863,
      "grad_norm": 0.21474406123161316,
      "learning_rate": 0.00019682865479424491,
      "loss": 0.7899,
      "step": 793
    },
    {
      "epoch": 1.5975855130784709,
      "grad_norm": 0.20207299292087555,
      "learning_rate": 0.0001968246302444914,
      "loss": 0.736,
      "step": 794
    },
    {
      "epoch": 1.5995975855130786,
      "grad_norm": 0.21118280291557312,
      "learning_rate": 0.0001968206056947379,
      "loss": 0.7596,
      "step": 795
    },
    {
      "epoch": 1.6016096579476862,
      "grad_norm": 0.20614570379257202,
      "learning_rate": 0.00019681658114498442,
      "loss": 0.779,
      "step": 796
    },
    {
      "epoch": 1.6036217303822937,
      "grad_norm": 0.21871444582939148,
      "learning_rate": 0.00019681255659523094,
      "loss": 0.7739,
      "step": 797
    },
    {
      "epoch": 1.6056338028169015,
      "grad_norm": 0.2082602083683014,
      "learning_rate": 0.00019680853204547742,
      "loss": 0.7494,
      "step": 798
    },
    {
      "epoch": 1.607645875251509,
      "grad_norm": 0.21173761785030365,
      "learning_rate": 0.00019680450749572393,
      "loss": 0.7658,
      "step": 799
    },
    {
      "epoch": 1.6096579476861166,
      "grad_norm": 0.21997703611850739,
      "learning_rate": 0.00019680048294597042,
      "loss": 0.8065,
      "step": 800
    },
    {
      "epoch": 1.6116700201207244,
      "grad_norm": 0.20764707028865814,
      "learning_rate": 0.00019679645839621693,
      "loss": 0.7495,
      "step": 801
    },
    {
      "epoch": 1.6136820925553321,
      "grad_norm": 0.20136268436908722,
      "learning_rate": 0.00019679243384646344,
      "loss": 0.739,
      "step": 802
    },
    {
      "epoch": 1.6156941649899397,
      "grad_norm": 0.1993347704410553,
      "learning_rate": 0.00019678840929670993,
      "loss": 0.7368,
      "step": 803
    },
    {
      "epoch": 1.6177062374245472,
      "grad_norm": 0.21055534482002258,
      "learning_rate": 0.00019678438474695644,
      "loss": 0.7268,
      "step": 804
    },
    {
      "epoch": 1.619718309859155,
      "grad_norm": 0.2030448466539383,
      "learning_rate": 0.00019678036019720295,
      "loss": 0.7749,
      "step": 805
    },
    {
      "epoch": 1.6217303822937625,
      "grad_norm": 0.2208346724510193,
      "learning_rate": 0.00019677633564744946,
      "loss": 0.7684,
      "step": 806
    },
    {
      "epoch": 1.62374245472837,
      "grad_norm": 0.20889361202716827,
      "learning_rate": 0.00019677231109769595,
      "loss": 0.756,
      "step": 807
    },
    {
      "epoch": 1.6257545271629779,
      "grad_norm": 0.20858529210090637,
      "learning_rate": 0.00019676828654794246,
      "loss": 0.7616,
      "step": 808
    },
    {
      "epoch": 1.6277665995975856,
      "grad_norm": 0.21428059041500092,
      "learning_rate": 0.00019676426199818895,
      "loss": 0.7742,
      "step": 809
    },
    {
      "epoch": 1.6297786720321932,
      "grad_norm": 0.20781828463077545,
      "learning_rate": 0.00019676023744843546,
      "loss": 0.7698,
      "step": 810
    },
    {
      "epoch": 1.6317907444668007,
      "grad_norm": 0.2132347673177719,
      "learning_rate": 0.00019675621289868197,
      "loss": 0.7978,
      "step": 811
    },
    {
      "epoch": 1.6338028169014085,
      "grad_norm": 0.2128157913684845,
      "learning_rate": 0.00019675218834892848,
      "loss": 0.7525,
      "step": 812
    },
    {
      "epoch": 1.635814889336016,
      "grad_norm": 0.20979738235473633,
      "learning_rate": 0.00019674816379917497,
      "loss": 0.7904,
      "step": 813
    },
    {
      "epoch": 1.6378269617706236,
      "grad_norm": 0.21527855098247528,
      "learning_rate": 0.00019674413924942148,
      "loss": 0.7452,
      "step": 814
    },
    {
      "epoch": 1.6398390342052314,
      "grad_norm": 0.20412810146808624,
      "learning_rate": 0.00019674011469966797,
      "loss": 0.7397,
      "step": 815
    },
    {
      "epoch": 1.6418511066398391,
      "grad_norm": 0.20678669214248657,
      "learning_rate": 0.0001967360901499145,
      "loss": 0.7551,
      "step": 816
    },
    {
      "epoch": 1.6438631790744467,
      "grad_norm": 0.23092319071292877,
      "learning_rate": 0.000196732065600161,
      "loss": 0.7893,
      "step": 817
    },
    {
      "epoch": 1.6458752515090542,
      "grad_norm": 0.2228238433599472,
      "learning_rate": 0.0001967280410504075,
      "loss": 0.7534,
      "step": 818
    },
    {
      "epoch": 1.647887323943662,
      "grad_norm": 0.20642365515232086,
      "learning_rate": 0.000196724016500654,
      "loss": 0.734,
      "step": 819
    },
    {
      "epoch": 1.6498993963782698,
      "grad_norm": 0.2100323587656021,
      "learning_rate": 0.0001967199919509005,
      "loss": 0.7523,
      "step": 820
    },
    {
      "epoch": 1.6519114688128773,
      "grad_norm": 0.2172761708498001,
      "learning_rate": 0.000196715967401147,
      "loss": 0.7809,
      "step": 821
    },
    {
      "epoch": 1.6539235412474849,
      "grad_norm": 0.20762042701244354,
      "learning_rate": 0.00019671194285139352,
      "loss": 0.7811,
      "step": 822
    },
    {
      "epoch": 1.6559356136820926,
      "grad_norm": 0.22608041763305664,
      "learning_rate": 0.00019670791830164,
      "loss": 0.7498,
      "step": 823
    },
    {
      "epoch": 1.6579476861167002,
      "grad_norm": 0.22158052027225494,
      "learning_rate": 0.00019670389375188652,
      "loss": 0.7741,
      "step": 824
    },
    {
      "epoch": 1.6599597585513077,
      "grad_norm": 0.2091297209262848,
      "learning_rate": 0.000196699869202133,
      "loss": 0.7672,
      "step": 825
    },
    {
      "epoch": 1.6619718309859155,
      "grad_norm": 0.20737625658512115,
      "learning_rate": 0.00019669584465237955,
      "loss": 0.7399,
      "step": 826
    },
    {
      "epoch": 1.6639839034205233,
      "grad_norm": 0.2102729082107544,
      "learning_rate": 0.00019669182010262603,
      "loss": 0.7358,
      "step": 827
    },
    {
      "epoch": 1.6659959758551308,
      "grad_norm": 0.22154349088668823,
      "learning_rate": 0.00019668779555287254,
      "loss": 0.7779,
      "step": 828
    },
    {
      "epoch": 1.6680080482897384,
      "grad_norm": 0.22724100947380066,
      "learning_rate": 0.00019668377100311903,
      "loss": 0.7667,
      "step": 829
    },
    {
      "epoch": 1.6700201207243461,
      "grad_norm": 0.21467465162277222,
      "learning_rate": 0.00019667974645336554,
      "loss": 0.7423,
      "step": 830
    },
    {
      "epoch": 1.6720321931589537,
      "grad_norm": 0.20942510664463043,
      "learning_rate": 0.00019667572190361205,
      "loss": 0.7199,
      "step": 831
    },
    {
      "epoch": 1.6740442655935612,
      "grad_norm": 0.21452704071998596,
      "learning_rate": 0.00019667169735385857,
      "loss": 0.7552,
      "step": 832
    },
    {
      "epoch": 1.676056338028169,
      "grad_norm": 0.20982596278190613,
      "learning_rate": 0.00019666767280410505,
      "loss": 0.7611,
      "step": 833
    },
    {
      "epoch": 1.6780684104627768,
      "grad_norm": 0.19132955372333527,
      "learning_rate": 0.00019666364825435156,
      "loss": 0.7336,
      "step": 834
    },
    {
      "epoch": 1.6800804828973843,
      "grad_norm": 0.19157543778419495,
      "learning_rate": 0.00019665962370459805,
      "loss": 0.7392,
      "step": 835
    },
    {
      "epoch": 1.6820925553319919,
      "grad_norm": 0.24869048595428467,
      "learning_rate": 0.00019665559915484456,
      "loss": 0.7457,
      "step": 836
    },
    {
      "epoch": 1.6841046277665996,
      "grad_norm": 0.237411230802536,
      "learning_rate": 0.00019665157460509107,
      "loss": 0.7573,
      "step": 837
    },
    {
      "epoch": 1.6861167002012074,
      "grad_norm": 0.21248352527618408,
      "learning_rate": 0.00019664755005533756,
      "loss": 0.7764,
      "step": 838
    },
    {
      "epoch": 1.6881287726358147,
      "grad_norm": 0.21844811737537384,
      "learning_rate": 0.00019664352550558407,
      "loss": 0.7668,
      "step": 839
    },
    {
      "epoch": 1.6901408450704225,
      "grad_norm": 0.20942743122577667,
      "learning_rate": 0.00019663950095583058,
      "loss": 0.7994,
      "step": 840
    },
    {
      "epoch": 1.6921529175050303,
      "grad_norm": 0.20664839446544647,
      "learning_rate": 0.0001966354764060771,
      "loss": 0.7632,
      "step": 841
    },
    {
      "epoch": 1.6941649899396378,
      "grad_norm": 0.21607957780361176,
      "learning_rate": 0.00019663145185632358,
      "loss": 0.7616,
      "step": 842
    },
    {
      "epoch": 1.6961770623742454,
      "grad_norm": 0.21770325303077698,
      "learning_rate": 0.0001966274273065701,
      "loss": 0.7638,
      "step": 843
    },
    {
      "epoch": 1.6981891348088531,
      "grad_norm": 0.21301789581775665,
      "learning_rate": 0.00019662340275681658,
      "loss": 0.7956,
      "step": 844
    },
    {
      "epoch": 1.700201207243461,
      "grad_norm": 0.221811443567276,
      "learning_rate": 0.0001966193782070631,
      "loss": 0.7848,
      "step": 845
    },
    {
      "epoch": 1.7022132796780685,
      "grad_norm": 0.2162182331085205,
      "learning_rate": 0.0001966153536573096,
      "loss": 0.7042,
      "step": 846
    },
    {
      "epoch": 1.704225352112676,
      "grad_norm": 0.20948706567287445,
      "learning_rate": 0.0001966113291075561,
      "loss": 0.7241,
      "step": 847
    },
    {
      "epoch": 1.7062374245472838,
      "grad_norm": 0.21286815404891968,
      "learning_rate": 0.0001966073045578026,
      "loss": 0.7446,
      "step": 848
    },
    {
      "epoch": 1.7082494969818913,
      "grad_norm": 0.22723998129367828,
      "learning_rate": 0.0001966032800080491,
      "loss": 0.742,
      "step": 849
    },
    {
      "epoch": 1.7102615694164989,
      "grad_norm": 0.2255285084247589,
      "learning_rate": 0.0001965992554582956,
      "loss": 0.7411,
      "step": 850
    },
    {
      "epoch": 1.7122736418511066,
      "grad_norm": 0.1998584270477295,
      "learning_rate": 0.00019659523090854214,
      "loss": 0.7271,
      "step": 851
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.20902371406555176,
      "learning_rate": 0.00019659120635878862,
      "loss": 0.7725,
      "step": 852
    },
    {
      "epoch": 1.716297786720322,
      "grad_norm": 0.20470239222049713,
      "learning_rate": 0.00019658718180903513,
      "loss": 0.7648,
      "step": 853
    },
    {
      "epoch": 1.7183098591549295,
      "grad_norm": 0.20082302391529083,
      "learning_rate": 0.00019658315725928162,
      "loss": 0.7172,
      "step": 854
    },
    {
      "epoch": 1.7203219315895373,
      "grad_norm": 0.21218958497047424,
      "learning_rate": 0.00019657913270952813,
      "loss": 0.769,
      "step": 855
    },
    {
      "epoch": 1.7223340040241448,
      "grad_norm": 0.20643708109855652,
      "learning_rate": 0.00019657510815977461,
      "loss": 0.7382,
      "step": 856
    },
    {
      "epoch": 1.7243460764587524,
      "grad_norm": 0.20632877945899963,
      "learning_rate": 0.00019657108361002115,
      "loss": 0.7483,
      "step": 857
    },
    {
      "epoch": 1.7263581488933601,
      "grad_norm": 0.20667202770709991,
      "learning_rate": 0.00019656705906026764,
      "loss": 0.7544,
      "step": 858
    },
    {
      "epoch": 1.728370221327968,
      "grad_norm": 0.20653125643730164,
      "learning_rate": 0.00019656303451051415,
      "loss": 0.7562,
      "step": 859
    },
    {
      "epoch": 1.7303822937625755,
      "grad_norm": 0.21634671092033386,
      "learning_rate": 0.00019655900996076064,
      "loss": 0.7834,
      "step": 860
    },
    {
      "epoch": 1.732394366197183,
      "grad_norm": 0.20219971239566803,
      "learning_rate": 0.00019655498541100715,
      "loss": 0.7781,
      "step": 861
    },
    {
      "epoch": 1.7344064386317908,
      "grad_norm": 0.20286354422569275,
      "learning_rate": 0.00019655096086125366,
      "loss": 0.7403,
      "step": 862
    },
    {
      "epoch": 1.7364185110663986,
      "grad_norm": 0.20048809051513672,
      "learning_rate": 0.00019654693631150017,
      "loss": 0.767,
      "step": 863
    },
    {
      "epoch": 1.7384305835010059,
      "grad_norm": 0.2086467742919922,
      "learning_rate": 0.00019654291176174666,
      "loss": 0.734,
      "step": 864
    },
    {
      "epoch": 1.7404426559356136,
      "grad_norm": 0.21448853611946106,
      "learning_rate": 0.00019653888721199317,
      "loss": 0.7172,
      "step": 865
    },
    {
      "epoch": 1.7424547283702214,
      "grad_norm": 0.21043464541435242,
      "learning_rate": 0.00019653486266223966,
      "loss": 0.7606,
      "step": 866
    },
    {
      "epoch": 1.744466800804829,
      "grad_norm": 0.2084220051765442,
      "learning_rate": 0.0001965308381124862,
      "loss": 0.7266,
      "step": 867
    },
    {
      "epoch": 1.7464788732394365,
      "grad_norm": 0.2305024266242981,
      "learning_rate": 0.00019652681356273268,
      "loss": 0.7789,
      "step": 868
    },
    {
      "epoch": 1.7484909456740443,
      "grad_norm": 0.2282792180776596,
      "learning_rate": 0.0001965227890129792,
      "loss": 0.7752,
      "step": 869
    },
    {
      "epoch": 1.750503018108652,
      "grad_norm": 0.2119024246931076,
      "learning_rate": 0.00019651876446322568,
      "loss": 0.7284,
      "step": 870
    },
    {
      "epoch": 1.7525150905432596,
      "grad_norm": 0.2109227478504181,
      "learning_rate": 0.0001965147399134722,
      "loss": 0.7587,
      "step": 871
    },
    {
      "epoch": 1.7545271629778671,
      "grad_norm": 0.22048455476760864,
      "learning_rate": 0.0001965107153637187,
      "loss": 0.7926,
      "step": 872
    },
    {
      "epoch": 1.756539235412475,
      "grad_norm": 0.2084798514842987,
      "learning_rate": 0.0001965066908139652,
      "loss": 0.7401,
      "step": 873
    },
    {
      "epoch": 1.7585513078470825,
      "grad_norm": 0.19949446618556976,
      "learning_rate": 0.0001965026662642117,
      "loss": 0.7425,
      "step": 874
    },
    {
      "epoch": 1.76056338028169,
      "grad_norm": 0.2058166116476059,
      "learning_rate": 0.0001964986417144582,
      "loss": 0.728,
      "step": 875
    },
    {
      "epoch": 1.7625754527162978,
      "grad_norm": 0.21692286431789398,
      "learning_rate": 0.0001964946171647047,
      "loss": 0.7575,
      "step": 876
    },
    {
      "epoch": 1.7645875251509056,
      "grad_norm": 0.23520472645759583,
      "learning_rate": 0.0001964905926149512,
      "loss": 0.7706,
      "step": 877
    },
    {
      "epoch": 1.766599597585513,
      "grad_norm": 0.2213881015777588,
      "learning_rate": 0.00019648656806519772,
      "loss": 0.7273,
      "step": 878
    },
    {
      "epoch": 1.7686116700201207,
      "grad_norm": 0.2254532426595688,
      "learning_rate": 0.0001964825435154442,
      "loss": 0.7292,
      "step": 879
    },
    {
      "epoch": 1.7706237424547284,
      "grad_norm": 0.2246842086315155,
      "learning_rate": 0.00019647851896569072,
      "loss": 0.7358,
      "step": 880
    },
    {
      "epoch": 1.772635814889336,
      "grad_norm": 0.21725532412528992,
      "learning_rate": 0.0001964744944159372,
      "loss": 0.7738,
      "step": 881
    },
    {
      "epoch": 1.7746478873239435,
      "grad_norm": 0.21387183666229248,
      "learning_rate": 0.00019647046986618374,
      "loss": 0.7926,
      "step": 882
    },
    {
      "epoch": 1.7766599597585513,
      "grad_norm": 0.22884145379066467,
      "learning_rate": 0.00019646644531643023,
      "loss": 0.7513,
      "step": 883
    },
    {
      "epoch": 1.778672032193159,
      "grad_norm": 0.20692187547683716,
      "learning_rate": 0.00019646242076667674,
      "loss": 0.7529,
      "step": 884
    },
    {
      "epoch": 1.7806841046277666,
      "grad_norm": 0.21151526272296906,
      "learning_rate": 0.00019645839621692323,
      "loss": 0.7698,
      "step": 885
    },
    {
      "epoch": 1.7826961770623742,
      "grad_norm": 0.2459927648305893,
      "learning_rate": 0.00019645437166716974,
      "loss": 0.8004,
      "step": 886
    },
    {
      "epoch": 1.784708249496982,
      "grad_norm": 0.2248574197292328,
      "learning_rate": 0.00019645034711741625,
      "loss": 0.7502,
      "step": 887
    },
    {
      "epoch": 1.7867203219315897,
      "grad_norm": 0.20655979216098785,
      "learning_rate": 0.00019644632256766276,
      "loss": 0.7376,
      "step": 888
    },
    {
      "epoch": 1.788732394366197,
      "grad_norm": 0.20476384460926056,
      "learning_rate": 0.00019644229801790925,
      "loss": 0.7088,
      "step": 889
    },
    {
      "epoch": 1.7907444668008048,
      "grad_norm": 0.22074662148952484,
      "learning_rate": 0.00019643827346815576,
      "loss": 0.7438,
      "step": 890
    },
    {
      "epoch": 1.7927565392354126,
      "grad_norm": 0.22786259651184082,
      "learning_rate": 0.00019643424891840224,
      "loss": 0.7636,
      "step": 891
    },
    {
      "epoch": 1.79476861167002,
      "grad_norm": 0.22532124817371368,
      "learning_rate": 0.00019643022436864878,
      "loss": 0.7766,
      "step": 892
    },
    {
      "epoch": 1.7967806841046277,
      "grad_norm": 0.20602385699748993,
      "learning_rate": 0.00019642619981889527,
      "loss": 0.7433,
      "step": 893
    },
    {
      "epoch": 1.7987927565392354,
      "grad_norm": 0.20606547594070435,
      "learning_rate": 0.00019642217526914178,
      "loss": 0.7287,
      "step": 894
    },
    {
      "epoch": 1.8008048289738432,
      "grad_norm": 0.21245788037776947,
      "learning_rate": 0.00019641815071938827,
      "loss": 0.759,
      "step": 895
    },
    {
      "epoch": 1.8028169014084507,
      "grad_norm": 0.2071320116519928,
      "learning_rate": 0.00019641412616963478,
      "loss": 0.7687,
      "step": 896
    },
    {
      "epoch": 1.8048289738430583,
      "grad_norm": 0.20539934933185577,
      "learning_rate": 0.0001964101016198813,
      "loss": 0.7319,
      "step": 897
    },
    {
      "epoch": 1.806841046277666,
      "grad_norm": 0.2194489687681198,
      "learning_rate": 0.0001964060770701278,
      "loss": 0.7761,
      "step": 898
    },
    {
      "epoch": 1.8088531187122736,
      "grad_norm": 0.21471616625785828,
      "learning_rate": 0.0001964020525203743,
      "loss": 0.7262,
      "step": 899
    },
    {
      "epoch": 1.8108651911468812,
      "grad_norm": 0.21998406946659088,
      "learning_rate": 0.0001963980279706208,
      "loss": 0.7543,
      "step": 900
    },
    {
      "epoch": 1.812877263581489,
      "grad_norm": 0.20536790788173676,
      "learning_rate": 0.00019639400342086729,
      "loss": 0.7285,
      "step": 901
    },
    {
      "epoch": 1.8148893360160967,
      "grad_norm": 0.20303194224834442,
      "learning_rate": 0.00019638997887111382,
      "loss": 0.7373,
      "step": 902
    },
    {
      "epoch": 1.8169014084507042,
      "grad_norm": 0.2195051610469818,
      "learning_rate": 0.0001963859543213603,
      "loss": 0.7439,
      "step": 903
    },
    {
      "epoch": 1.8189134808853118,
      "grad_norm": 0.20657269656658173,
      "learning_rate": 0.00019638192977160682,
      "loss": 0.7187,
      "step": 904
    },
    {
      "epoch": 1.8209255533199196,
      "grad_norm": 0.20427612960338593,
      "learning_rate": 0.0001963779052218533,
      "loss": 0.7303,
      "step": 905
    },
    {
      "epoch": 1.8229376257545271,
      "grad_norm": 0.20989790558815002,
      "learning_rate": 0.00019637388067209982,
      "loss": 0.7666,
      "step": 906
    },
    {
      "epoch": 1.8249496981891347,
      "grad_norm": 0.2111741304397583,
      "learning_rate": 0.00019636985612234633,
      "loss": 0.7814,
      "step": 907
    },
    {
      "epoch": 1.8269617706237424,
      "grad_norm": 0.21670198440551758,
      "learning_rate": 0.00019636583157259282,
      "loss": 0.7927,
      "step": 908
    },
    {
      "epoch": 1.8289738430583502,
      "grad_norm": 0.212752565741539,
      "learning_rate": 0.00019636180702283933,
      "loss": 0.7424,
      "step": 909
    },
    {
      "epoch": 1.8309859154929577,
      "grad_norm": 0.21217291057109833,
      "learning_rate": 0.00019635778247308581,
      "loss": 0.7692,
      "step": 910
    },
    {
      "epoch": 1.8329979879275653,
      "grad_norm": 0.20339812338352203,
      "learning_rate": 0.00019635375792333233,
      "loss": 0.7218,
      "step": 911
    },
    {
      "epoch": 1.835010060362173,
      "grad_norm": 0.20201808214187622,
      "learning_rate": 0.00019634973337357884,
      "loss": 0.7865,
      "step": 912
    },
    {
      "epoch": 1.8370221327967808,
      "grad_norm": 0.19938881695270538,
      "learning_rate": 0.00019634570882382535,
      "loss": 0.6826,
      "step": 913
    },
    {
      "epoch": 1.8390342052313882,
      "grad_norm": 0.21199378371238708,
      "learning_rate": 0.00019634168427407184,
      "loss": 0.7506,
      "step": 914
    },
    {
      "epoch": 1.841046277665996,
      "grad_norm": 0.2158687710762024,
      "learning_rate": 0.00019633765972431835,
      "loss": 0.7545,
      "step": 915
    },
    {
      "epoch": 1.8430583501006037,
      "grad_norm": 0.21462085843086243,
      "learning_rate": 0.00019633363517456483,
      "loss": 0.7763,
      "step": 916
    },
    {
      "epoch": 1.8450704225352113,
      "grad_norm": 0.22707857191562653,
      "learning_rate": 0.00019632961062481137,
      "loss": 0.7638,
      "step": 917
    },
    {
      "epoch": 1.8470824949698188,
      "grad_norm": 0.21630097925662994,
      "learning_rate": 0.00019632558607505786,
      "loss": 0.73,
      "step": 918
    },
    {
      "epoch": 1.8490945674044266,
      "grad_norm": 0.2189587950706482,
      "learning_rate": 0.00019632156152530437,
      "loss": 0.7634,
      "step": 919
    },
    {
      "epoch": 1.8511066398390343,
      "grad_norm": 0.22511105239391327,
      "learning_rate": 0.00019631753697555085,
      "loss": 0.7557,
      "step": 920
    },
    {
      "epoch": 1.8531187122736419,
      "grad_norm": 0.21189647912979126,
      "learning_rate": 0.00019631351242579737,
      "loss": 0.7502,
      "step": 921
    },
    {
      "epoch": 1.8551307847082494,
      "grad_norm": 0.20478157699108124,
      "learning_rate": 0.00019630948787604388,
      "loss": 0.7669,
      "step": 922
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.20955193042755127,
      "learning_rate": 0.0001963054633262904,
      "loss": 0.7678,
      "step": 923
    },
    {
      "epoch": 1.8591549295774648,
      "grad_norm": 0.22325338423252106,
      "learning_rate": 0.00019630143877653688,
      "loss": 0.7505,
      "step": 924
    },
    {
      "epoch": 1.8611670020120723,
      "grad_norm": 0.21398597955703735,
      "learning_rate": 0.0001962974142267834,
      "loss": 0.739,
      "step": 925
    },
    {
      "epoch": 1.86317907444668,
      "grad_norm": 0.21419407427310944,
      "learning_rate": 0.00019629338967702987,
      "loss": 0.7596,
      "step": 926
    },
    {
      "epoch": 1.8651911468812878,
      "grad_norm": 0.20634624361991882,
      "learning_rate": 0.0001962893651272764,
      "loss": 0.7383,
      "step": 927
    },
    {
      "epoch": 1.8672032193158954,
      "grad_norm": 0.21698173880577087,
      "learning_rate": 0.0001962853405775229,
      "loss": 0.7771,
      "step": 928
    },
    {
      "epoch": 1.869215291750503,
      "grad_norm": 0.2050512433052063,
      "learning_rate": 0.0001962813160277694,
      "loss": 0.7542,
      "step": 929
    },
    {
      "epoch": 1.8712273641851107,
      "grad_norm": 0.20445042848587036,
      "learning_rate": 0.0001962772914780159,
      "loss": 0.7762,
      "step": 930
    },
    {
      "epoch": 1.8732394366197183,
      "grad_norm": 0.20562149584293365,
      "learning_rate": 0.0001962732669282624,
      "loss": 0.7988,
      "step": 931
    },
    {
      "epoch": 1.8752515090543258,
      "grad_norm": 0.208319291472435,
      "learning_rate": 0.00019626924237850892,
      "loss": 0.7445,
      "step": 932
    },
    {
      "epoch": 1.8772635814889336,
      "grad_norm": 0.19369593262672424,
      "learning_rate": 0.00019626521782875543,
      "loss": 0.7368,
      "step": 933
    },
    {
      "epoch": 1.8792756539235413,
      "grad_norm": 0.19976818561553955,
      "learning_rate": 0.00019626119327900192,
      "loss": 0.7441,
      "step": 934
    },
    {
      "epoch": 1.881287726358149,
      "grad_norm": 0.2028803527355194,
      "learning_rate": 0.00019625716872924843,
      "loss": 0.7705,
      "step": 935
    },
    {
      "epoch": 1.8832997987927564,
      "grad_norm": 0.20908714830875397,
      "learning_rate": 0.00019625314417949491,
      "loss": 0.7425,
      "step": 936
    },
    {
      "epoch": 1.8853118712273642,
      "grad_norm": 0.20224106311798096,
      "learning_rate": 0.00019624911962974143,
      "loss": 0.7562,
      "step": 937
    },
    {
      "epoch": 1.887323943661972,
      "grad_norm": 0.22056549787521362,
      "learning_rate": 0.00019624509507998794,
      "loss": 0.7697,
      "step": 938
    },
    {
      "epoch": 1.8893360160965795,
      "grad_norm": 0.1991543173789978,
      "learning_rate": 0.00019624107053023445,
      "loss": 0.7217,
      "step": 939
    },
    {
      "epoch": 1.891348088531187,
      "grad_norm": 0.2048669010400772,
      "learning_rate": 0.00019623704598048094,
      "loss": 0.7438,
      "step": 940
    },
    {
      "epoch": 1.8933601609657948,
      "grad_norm": 0.2037402242422104,
      "learning_rate": 0.00019623302143072745,
      "loss": 0.6962,
      "step": 941
    },
    {
      "epoch": 1.8953722334004024,
      "grad_norm": 0.2079252451658249,
      "learning_rate": 0.00019622899688097396,
      "loss": 0.7246,
      "step": 942
    },
    {
      "epoch": 1.89738430583501,
      "grad_norm": 0.2071739137172699,
      "learning_rate": 0.00019622497233122045,
      "loss": 0.7236,
      "step": 943
    },
    {
      "epoch": 1.8993963782696177,
      "grad_norm": 0.21113547682762146,
      "learning_rate": 0.00019622094778146696,
      "loss": 0.7899,
      "step": 944
    },
    {
      "epoch": 1.9014084507042255,
      "grad_norm": 0.20807835459709167,
      "learning_rate": 0.00019621692323171344,
      "loss": 0.7132,
      "step": 945
    },
    {
      "epoch": 1.903420523138833,
      "grad_norm": 0.20087450742721558,
      "learning_rate": 0.00019621289868195996,
      "loss": 0.7381,
      "step": 946
    },
    {
      "epoch": 1.9054325955734406,
      "grad_norm": 0.2135852426290512,
      "learning_rate": 0.00019620887413220647,
      "loss": 0.7508,
      "step": 947
    },
    {
      "epoch": 1.9074446680080483,
      "grad_norm": 0.20814770460128784,
      "learning_rate": 0.00019620484958245298,
      "loss": 0.7173,
      "step": 948
    },
    {
      "epoch": 1.909456740442656,
      "grad_norm": 0.20177525281906128,
      "learning_rate": 0.00019620082503269947,
      "loss": 0.7627,
      "step": 949
    },
    {
      "epoch": 1.9114688128772634,
      "grad_norm": 0.20799563825130463,
      "learning_rate": 0.00019619680048294598,
      "loss": 0.7475,
      "step": 950
    },
    {
      "epoch": 1.9134808853118712,
      "grad_norm": 0.2004767656326294,
      "learning_rate": 0.00019619277593319246,
      "loss": 0.7812,
      "step": 951
    },
    {
      "epoch": 1.915492957746479,
      "grad_norm": 0.221574604511261,
      "learning_rate": 0.000196188751383439,
      "loss": 0.7299,
      "step": 952
    },
    {
      "epoch": 1.9175050301810865,
      "grad_norm": 0.20303137600421906,
      "learning_rate": 0.0001961847268336855,
      "loss": 0.7384,
      "step": 953
    },
    {
      "epoch": 1.919517102615694,
      "grad_norm": 0.20567819476127625,
      "learning_rate": 0.000196180702283932,
      "loss": 0.736,
      "step": 954
    },
    {
      "epoch": 1.9215291750503019,
      "grad_norm": 0.21297413110733032,
      "learning_rate": 0.00019617667773417848,
      "loss": 0.7542,
      "step": 955
    },
    {
      "epoch": 1.9235412474849096,
      "grad_norm": 0.2091938555240631,
      "learning_rate": 0.000196172653184425,
      "loss": 0.7574,
      "step": 956
    },
    {
      "epoch": 1.925553319919517,
      "grad_norm": 0.22853755950927734,
      "learning_rate": 0.0001961686286346715,
      "loss": 0.7207,
      "step": 957
    },
    {
      "epoch": 1.9275653923541247,
      "grad_norm": 0.21111193299293518,
      "learning_rate": 0.00019616460408491802,
      "loss": 0.7861,
      "step": 958
    },
    {
      "epoch": 1.9295774647887325,
      "grad_norm": 0.2270459681749344,
      "learning_rate": 0.0001961605795351645,
      "loss": 0.7652,
      "step": 959
    },
    {
      "epoch": 1.93158953722334,
      "grad_norm": 0.2105388045310974,
      "learning_rate": 0.00019615655498541102,
      "loss": 0.7414,
      "step": 960
    },
    {
      "epoch": 1.9336016096579476,
      "grad_norm": 0.19753362238407135,
      "learning_rate": 0.0001961525304356575,
      "loss": 0.7471,
      "step": 961
    },
    {
      "epoch": 1.9356136820925554,
      "grad_norm": 0.19533070921897888,
      "learning_rate": 0.00019614850588590404,
      "loss": 0.7575,
      "step": 962
    },
    {
      "epoch": 1.9376257545271631,
      "grad_norm": 0.20125333964824677,
      "learning_rate": 0.00019614448133615053,
      "loss": 0.7709,
      "step": 963
    },
    {
      "epoch": 1.9396378269617707,
      "grad_norm": 0.20801472663879395,
      "learning_rate": 0.00019614045678639704,
      "loss": 0.7538,
      "step": 964
    },
    {
      "epoch": 1.9416498993963782,
      "grad_norm": 0.21507273614406586,
      "learning_rate": 0.00019613643223664352,
      "loss": 0.7105,
      "step": 965
    },
    {
      "epoch": 1.943661971830986,
      "grad_norm": 0.2029217630624771,
      "learning_rate": 0.00019613240768689004,
      "loss": 0.7517,
      "step": 966
    },
    {
      "epoch": 1.9456740442655935,
      "grad_norm": 0.20860172808170319,
      "learning_rate": 0.00019612838313713655,
      "loss": 0.7564,
      "step": 967
    },
    {
      "epoch": 1.947686116700201,
      "grad_norm": 0.2082442045211792,
      "learning_rate": 0.00019612435858738306,
      "loss": 0.7651,
      "step": 968
    },
    {
      "epoch": 1.9496981891348089,
      "grad_norm": 0.2068198174238205,
      "learning_rate": 0.00019612033403762955,
      "loss": 0.7304,
      "step": 969
    },
    {
      "epoch": 1.9517102615694166,
      "grad_norm": 0.20857945084571838,
      "learning_rate": 0.00019611630948787606,
      "loss": 0.7816,
      "step": 970
    },
    {
      "epoch": 1.9537223340040242,
      "grad_norm": 0.21462532877922058,
      "learning_rate": 0.00019611228493812254,
      "loss": 0.7132,
      "step": 971
    },
    {
      "epoch": 1.9557344064386317,
      "grad_norm": 0.22361581027507782,
      "learning_rate": 0.00019610826038836906,
      "loss": 0.7626,
      "step": 972
    },
    {
      "epoch": 1.9577464788732395,
      "grad_norm": 0.2091909795999527,
      "learning_rate": 0.00019610423583861557,
      "loss": 0.7375,
      "step": 973
    },
    {
      "epoch": 1.959758551307847,
      "grad_norm": 0.22352983057498932,
      "learning_rate": 0.00019610021128886208,
      "loss": 0.7489,
      "step": 974
    },
    {
      "epoch": 1.9617706237424546,
      "grad_norm": 0.21736223995685577,
      "learning_rate": 0.00019609618673910857,
      "loss": 0.7658,
      "step": 975
    },
    {
      "epoch": 1.9637826961770624,
      "grad_norm": 0.22969214618206024,
      "learning_rate": 0.00019609216218935508,
      "loss": 0.7738,
      "step": 976
    },
    {
      "epoch": 1.9657947686116701,
      "grad_norm": 0.19458293914794922,
      "learning_rate": 0.0001960881376396016,
      "loss": 0.7269,
      "step": 977
    },
    {
      "epoch": 1.9678068410462777,
      "grad_norm": 0.21650508046150208,
      "learning_rate": 0.00019608411308984808,
      "loss": 0.7336,
      "step": 978
    },
    {
      "epoch": 1.9698189134808852,
      "grad_norm": 0.20622272789478302,
      "learning_rate": 0.0001960800885400946,
      "loss": 0.7293,
      "step": 979
    },
    {
      "epoch": 1.971830985915493,
      "grad_norm": 0.2201029360294342,
      "learning_rate": 0.00019607606399034107,
      "loss": 0.7447,
      "step": 980
    },
    {
      "epoch": 1.9738430583501008,
      "grad_norm": 0.21460536122322083,
      "learning_rate": 0.00019607203944058758,
      "loss": 0.7326,
      "step": 981
    },
    {
      "epoch": 1.975855130784708,
      "grad_norm": 0.21130818128585815,
      "learning_rate": 0.0001960680148908341,
      "loss": 0.7584,
      "step": 982
    },
    {
      "epoch": 1.9778672032193159,
      "grad_norm": 0.24147556722164154,
      "learning_rate": 0.0001960639903410806,
      "loss": 0.7606,
      "step": 983
    },
    {
      "epoch": 1.9798792756539236,
      "grad_norm": 0.21984367072582245,
      "learning_rate": 0.0001960599657913271,
      "loss": 0.7765,
      "step": 984
    },
    {
      "epoch": 1.9818913480885312,
      "grad_norm": 0.22601091861724854,
      "learning_rate": 0.0001960559412415736,
      "loss": 0.7326,
      "step": 985
    },
    {
      "epoch": 1.9839034205231387,
      "grad_norm": 0.20082849264144897,
      "learning_rate": 0.0001960519166918201,
      "loss": 0.7621,
      "step": 986
    },
    {
      "epoch": 1.9859154929577465,
      "grad_norm": 0.2112150490283966,
      "learning_rate": 0.00019604789214206663,
      "loss": 0.7704,
      "step": 987
    },
    {
      "epoch": 1.9879275653923543,
      "grad_norm": 0.21881426870822906,
      "learning_rate": 0.00019604386759231312,
      "loss": 0.773,
      "step": 988
    },
    {
      "epoch": 1.9899396378269618,
      "grad_norm": 0.20639117062091827,
      "learning_rate": 0.00019603984304255963,
      "loss": 0.7617,
      "step": 989
    },
    {
      "epoch": 1.9919517102615694,
      "grad_norm": 0.20783422887325287,
      "learning_rate": 0.0001960358184928061,
      "loss": 0.7695,
      "step": 990
    },
    {
      "epoch": 1.9939637826961771,
      "grad_norm": 0.21761909127235413,
      "learning_rate": 0.00019603179394305263,
      "loss": 0.7571,
      "step": 991
    },
    {
      "epoch": 1.9959758551307847,
      "grad_norm": 0.212938591837883,
      "learning_rate": 0.00019602776939329914,
      "loss": 0.7712,
      "step": 992
    },
    {
      "epoch": 1.9979879275653922,
      "grad_norm": 0.2203359454870224,
      "learning_rate": 0.00019602374484354565,
      "loss": 0.742,
      "step": 993
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.22630161046981812,
      "learning_rate": 0.00019601972029379214,
      "loss": 0.7589,
      "step": 994
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.7495424151420593,
      "eval_runtime": 49.8942,
      "eval_samples_per_second": 19.882,
      "eval_steps_per_second": 2.485,
      "step": 994
    },
    {
      "epoch": 2.0020120724346078,
      "grad_norm": 0.2078024446964264,
      "learning_rate": 0.00019601569574403865,
      "loss": 0.7283,
      "step": 995
    },
    {
      "epoch": 2.004024144869215,
      "grad_norm": 0.20462891459465027,
      "learning_rate": 0.00019601167119428513,
      "loss": 0.7209,
      "step": 996
    },
    {
      "epoch": 2.006036217303823,
      "grad_norm": 0.21443834900856018,
      "learning_rate": 0.00019600764664453167,
      "loss": 0.7436,
      "step": 997
    },
    {
      "epoch": 2.0080482897384306,
      "grad_norm": 0.21526779234409332,
      "learning_rate": 0.00019600362209477816,
      "loss": 0.764,
      "step": 998
    },
    {
      "epoch": 2.0100603621730384,
      "grad_norm": 0.21751011908054352,
      "learning_rate": 0.00019599959754502467,
      "loss": 0.723,
      "step": 999
    },
    {
      "epoch": 2.0120724346076457,
      "grad_norm": 0.21349070966243744,
      "learning_rate": 0.00019599557299527115,
      "loss": 0.7313,
      "step": 1000
    },
    {
      "epoch": 2.0140845070422535,
      "grad_norm": 0.21679405868053436,
      "learning_rate": 0.00019599154844551767,
      "loss": 0.7193,
      "step": 1001
    },
    {
      "epoch": 2.0160965794768613,
      "grad_norm": 0.22702181339263916,
      "learning_rate": 0.00019598752389576418,
      "loss": 0.7032,
      "step": 1002
    },
    {
      "epoch": 2.0181086519114686,
      "grad_norm": 0.20086944103240967,
      "learning_rate": 0.0001959834993460107,
      "loss": 0.6724,
      "step": 1003
    },
    {
      "epoch": 2.0201207243460764,
      "grad_norm": 0.2455434501171112,
      "learning_rate": 0.00019597947479625718,
      "loss": 0.7283,
      "step": 1004
    },
    {
      "epoch": 2.022132796780684,
      "grad_norm": 0.22428341209888458,
      "learning_rate": 0.0001959754502465037,
      "loss": 0.7527,
      "step": 1005
    },
    {
      "epoch": 2.024144869215292,
      "grad_norm": 0.2080901712179184,
      "learning_rate": 0.00019597142569675017,
      "loss": 0.7237,
      "step": 1006
    },
    {
      "epoch": 2.0261569416498992,
      "grad_norm": 0.20451980829238892,
      "learning_rate": 0.00019596740114699669,
      "loss": 0.735,
      "step": 1007
    },
    {
      "epoch": 2.028169014084507,
      "grad_norm": 0.2188626080751419,
      "learning_rate": 0.0001959633765972432,
      "loss": 0.733,
      "step": 1008
    },
    {
      "epoch": 2.0301810865191148,
      "grad_norm": 0.2101321816444397,
      "learning_rate": 0.0001959593520474897,
      "loss": 0.7226,
      "step": 1009
    },
    {
      "epoch": 2.0321931589537225,
      "grad_norm": 0.26255521178245544,
      "learning_rate": 0.0001959553274977362,
      "loss": 0.6956,
      "step": 1010
    },
    {
      "epoch": 2.03420523138833,
      "grad_norm": 0.2138528674840927,
      "learning_rate": 0.0001959513029479827,
      "loss": 0.7344,
      "step": 1011
    },
    {
      "epoch": 2.0362173038229376,
      "grad_norm": 0.21882277727127075,
      "learning_rate": 0.00019594727839822922,
      "loss": 0.7729,
      "step": 1012
    },
    {
      "epoch": 2.0382293762575454,
      "grad_norm": 0.2366427183151245,
      "learning_rate": 0.0001959432538484757,
      "loss": 0.7899,
      "step": 1013
    },
    {
      "epoch": 2.0402414486921527,
      "grad_norm": 0.21182477474212646,
      "learning_rate": 0.00019593922929872222,
      "loss": 0.7297,
      "step": 1014
    },
    {
      "epoch": 2.0422535211267605,
      "grad_norm": 0.2246573269367218,
      "learning_rate": 0.0001959352047489687,
      "loss": 0.7177,
      "step": 1015
    },
    {
      "epoch": 2.0442655935613683,
      "grad_norm": 0.22012117505073547,
      "learning_rate": 0.00019593118019921521,
      "loss": 0.6707,
      "step": 1016
    },
    {
      "epoch": 2.046277665995976,
      "grad_norm": 0.21346008777618408,
      "learning_rate": 0.00019592715564946173,
      "loss": 0.7093,
      "step": 1017
    },
    {
      "epoch": 2.0482897384305834,
      "grad_norm": 0.2703637182712555,
      "learning_rate": 0.00019592313109970824,
      "loss": 0.7157,
      "step": 1018
    },
    {
      "epoch": 2.050301810865191,
      "grad_norm": 0.2274174690246582,
      "learning_rate": 0.00019591910654995472,
      "loss": 0.7162,
      "step": 1019
    },
    {
      "epoch": 2.052313883299799,
      "grad_norm": 0.24872732162475586,
      "learning_rate": 0.00019591508200020124,
      "loss": 0.7115,
      "step": 1020
    },
    {
      "epoch": 2.0543259557344062,
      "grad_norm": 0.23569177091121674,
      "learning_rate": 0.00019591105745044772,
      "loss": 0.7092,
      "step": 1021
    },
    {
      "epoch": 2.056338028169014,
      "grad_norm": 0.2066155970096588,
      "learning_rate": 0.00019590703290069426,
      "loss": 0.7228,
      "step": 1022
    },
    {
      "epoch": 2.058350100603622,
      "grad_norm": 0.2184535562992096,
      "learning_rate": 0.00019590300835094075,
      "loss": 0.7496,
      "step": 1023
    },
    {
      "epoch": 2.0603621730382295,
      "grad_norm": 0.24144303798675537,
      "learning_rate": 0.00019589898380118726,
      "loss": 0.6897,
      "step": 1024
    },
    {
      "epoch": 2.062374245472837,
      "grad_norm": 0.2146894633769989,
      "learning_rate": 0.00019589495925143374,
      "loss": 0.717,
      "step": 1025
    },
    {
      "epoch": 2.0643863179074446,
      "grad_norm": 0.22522462904453278,
      "learning_rate": 0.00019589093470168026,
      "loss": 0.7304,
      "step": 1026
    },
    {
      "epoch": 2.0663983903420524,
      "grad_norm": 0.23332828283309937,
      "learning_rate": 0.00019588691015192677,
      "loss": 0.736,
      "step": 1027
    },
    {
      "epoch": 2.0684104627766597,
      "grad_norm": 0.22559721767902374,
      "learning_rate": 0.00019588288560217328,
      "loss": 0.7099,
      "step": 1028
    },
    {
      "epoch": 2.0704225352112675,
      "grad_norm": 0.22198188304901123,
      "learning_rate": 0.00019587886105241976,
      "loss": 0.7505,
      "step": 1029
    },
    {
      "epoch": 2.0724346076458753,
      "grad_norm": 0.22670753300189972,
      "learning_rate": 0.00019587483650266628,
      "loss": 0.7266,
      "step": 1030
    },
    {
      "epoch": 2.074446680080483,
      "grad_norm": 0.2133053094148636,
      "learning_rate": 0.00019587081195291276,
      "loss": 0.7054,
      "step": 1031
    },
    {
      "epoch": 2.0764587525150904,
      "grad_norm": 0.22441518306732178,
      "learning_rate": 0.0001958667874031593,
      "loss": 0.7597,
      "step": 1032
    },
    {
      "epoch": 2.078470824949698,
      "grad_norm": 0.21463987231254578,
      "learning_rate": 0.0001958627628534058,
      "loss": 0.729,
      "step": 1033
    },
    {
      "epoch": 2.080482897384306,
      "grad_norm": 0.22579041123390198,
      "learning_rate": 0.0001958587383036523,
      "loss": 0.7039,
      "step": 1034
    },
    {
      "epoch": 2.0824949698189137,
      "grad_norm": 0.2147577702999115,
      "learning_rate": 0.00019585471375389878,
      "loss": 0.7447,
      "step": 1035
    },
    {
      "epoch": 2.084507042253521,
      "grad_norm": 0.21698956191539764,
      "learning_rate": 0.0001958506892041453,
      "loss": 0.738,
      "step": 1036
    },
    {
      "epoch": 2.086519114688129,
      "grad_norm": 0.23796893656253815,
      "learning_rate": 0.0001958466646543918,
      "loss": 0.7653,
      "step": 1037
    },
    {
      "epoch": 2.0885311871227366,
      "grad_norm": 0.22188763320446014,
      "learning_rate": 0.00019584264010463832,
      "loss": 0.7339,
      "step": 1038
    },
    {
      "epoch": 2.090543259557344,
      "grad_norm": 0.2093288004398346,
      "learning_rate": 0.0001958386155548848,
      "loss": 0.7125,
      "step": 1039
    },
    {
      "epoch": 2.0925553319919517,
      "grad_norm": 0.2150883674621582,
      "learning_rate": 0.00019583459100513132,
      "loss": 0.7217,
      "step": 1040
    },
    {
      "epoch": 2.0945674044265594,
      "grad_norm": 0.20912392437458038,
      "learning_rate": 0.0001958305664553778,
      "loss": 0.6944,
      "step": 1041
    },
    {
      "epoch": 2.096579476861167,
      "grad_norm": 0.2257704734802246,
      "learning_rate": 0.00019582654190562432,
      "loss": 0.727,
      "step": 1042
    },
    {
      "epoch": 2.0985915492957745,
      "grad_norm": 0.22726179659366608,
      "learning_rate": 0.00019582251735587083,
      "loss": 0.7501,
      "step": 1043
    },
    {
      "epoch": 2.1006036217303823,
      "grad_norm": 0.21637652814388275,
      "learning_rate": 0.00019581849280611734,
      "loss": 0.731,
      "step": 1044
    },
    {
      "epoch": 2.10261569416499,
      "grad_norm": 0.21031637489795685,
      "learning_rate": 0.00019581446825636382,
      "loss": 0.6767,
      "step": 1045
    },
    {
      "epoch": 2.1046277665995974,
      "grad_norm": 0.20820903778076172,
      "learning_rate": 0.00019581044370661034,
      "loss": 0.7184,
      "step": 1046
    },
    {
      "epoch": 2.106639839034205,
      "grad_norm": 0.22535440325737,
      "learning_rate": 0.00019580641915685685,
      "loss": 0.7346,
      "step": 1047
    },
    {
      "epoch": 2.108651911468813,
      "grad_norm": 0.224481463432312,
      "learning_rate": 0.00019580239460710333,
      "loss": 0.7249,
      "step": 1048
    },
    {
      "epoch": 2.1106639839034207,
      "grad_norm": 0.21487440168857574,
      "learning_rate": 0.00019579837005734985,
      "loss": 0.7386,
      "step": 1049
    },
    {
      "epoch": 2.112676056338028,
      "grad_norm": 0.21640177071094513,
      "learning_rate": 0.00019579434550759633,
      "loss": 0.7091,
      "step": 1050
    },
    {
      "epoch": 2.114688128772636,
      "grad_norm": 0.24776634573936462,
      "learning_rate": 0.00019579032095784284,
      "loss": 0.7355,
      "step": 1051
    },
    {
      "epoch": 2.1167002012072436,
      "grad_norm": 0.2195567637681961,
      "learning_rate": 0.00019578629640808936,
      "loss": 0.7302,
      "step": 1052
    },
    {
      "epoch": 2.118712273641851,
      "grad_norm": 0.22157254815101624,
      "learning_rate": 0.00019578227185833587,
      "loss": 0.7208,
      "step": 1053
    },
    {
      "epoch": 2.1207243460764587,
      "grad_norm": 0.21733660995960236,
      "learning_rate": 0.00019577824730858235,
      "loss": 0.6977,
      "step": 1054
    },
    {
      "epoch": 2.1227364185110664,
      "grad_norm": 0.22552838921546936,
      "learning_rate": 0.00019577422275882887,
      "loss": 0.7403,
      "step": 1055
    },
    {
      "epoch": 2.124748490945674,
      "grad_norm": 0.21870660781860352,
      "learning_rate": 0.00019577019820907535,
      "loss": 0.716,
      "step": 1056
    },
    {
      "epoch": 2.1267605633802815,
      "grad_norm": 0.22497577965259552,
      "learning_rate": 0.0001957661736593219,
      "loss": 0.735,
      "step": 1057
    },
    {
      "epoch": 2.1287726358148893,
      "grad_norm": 0.23465850949287415,
      "learning_rate": 0.00019576214910956838,
      "loss": 0.7426,
      "step": 1058
    },
    {
      "epoch": 2.130784708249497,
      "grad_norm": 0.2300845980644226,
      "learning_rate": 0.0001957581245598149,
      "loss": 0.7081,
      "step": 1059
    },
    {
      "epoch": 2.132796780684105,
      "grad_norm": 0.21497440338134766,
      "learning_rate": 0.00019575410001006137,
      "loss": 0.7177,
      "step": 1060
    },
    {
      "epoch": 2.134808853118712,
      "grad_norm": 0.2243235856294632,
      "learning_rate": 0.00019575007546030788,
      "loss": 0.7754,
      "step": 1061
    },
    {
      "epoch": 2.13682092555332,
      "grad_norm": 0.2272304892539978,
      "learning_rate": 0.0001957460509105544,
      "loss": 0.7349,
      "step": 1062
    },
    {
      "epoch": 2.1388329979879277,
      "grad_norm": 0.2150789350271225,
      "learning_rate": 0.0001957420263608009,
      "loss": 0.7019,
      "step": 1063
    },
    {
      "epoch": 2.140845070422535,
      "grad_norm": 0.22273512184619904,
      "learning_rate": 0.0001957380018110474,
      "loss": 0.7436,
      "step": 1064
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.2269275188446045,
      "learning_rate": 0.0001957339772612939,
      "loss": 0.6667,
      "step": 1065
    },
    {
      "epoch": 2.1448692152917506,
      "grad_norm": 0.21763776242733002,
      "learning_rate": 0.0001957299527115404,
      "loss": 0.731,
      "step": 1066
    },
    {
      "epoch": 2.1468812877263583,
      "grad_norm": 0.2411067634820938,
      "learning_rate": 0.00019572592816178693,
      "loss": 0.7342,
      "step": 1067
    },
    {
      "epoch": 2.1488933601609657,
      "grad_norm": 0.22410370409488678,
      "learning_rate": 0.00019572190361203342,
      "loss": 0.7345,
      "step": 1068
    },
    {
      "epoch": 2.1509054325955734,
      "grad_norm": 0.23248526453971863,
      "learning_rate": 0.00019571787906227993,
      "loss": 0.7529,
      "step": 1069
    },
    {
      "epoch": 2.152917505030181,
      "grad_norm": 0.23583808541297913,
      "learning_rate": 0.0001957138545125264,
      "loss": 0.7125,
      "step": 1070
    },
    {
      "epoch": 2.1549295774647885,
      "grad_norm": 0.21569496393203735,
      "learning_rate": 0.00019570982996277293,
      "loss": 0.7134,
      "step": 1071
    },
    {
      "epoch": 2.1569416498993963,
      "grad_norm": 0.22502373158931732,
      "learning_rate": 0.00019570580541301944,
      "loss": 0.7399,
      "step": 1072
    },
    {
      "epoch": 2.158953722334004,
      "grad_norm": 0.23513898253440857,
      "learning_rate": 0.00019570178086326595,
      "loss": 0.7385,
      "step": 1073
    },
    {
      "epoch": 2.160965794768612,
      "grad_norm": 0.23341679573059082,
      "learning_rate": 0.00019569775631351244,
      "loss": 0.7518,
      "step": 1074
    },
    {
      "epoch": 2.162977867203219,
      "grad_norm": 0.20629821717739105,
      "learning_rate": 0.00019569373176375895,
      "loss": 0.7189,
      "step": 1075
    },
    {
      "epoch": 2.164989939637827,
      "grad_norm": 0.23354648053646088,
      "learning_rate": 0.00019568970721400543,
      "loss": 0.7079,
      "step": 1076
    },
    {
      "epoch": 2.1670020120724347,
      "grad_norm": 0.22285717725753784,
      "learning_rate": 0.00019568568266425194,
      "loss": 0.7517,
      "step": 1077
    },
    {
      "epoch": 2.169014084507042,
      "grad_norm": 0.23190023005008698,
      "learning_rate": 0.00019568165811449846,
      "loss": 0.7388,
      "step": 1078
    },
    {
      "epoch": 2.17102615694165,
      "grad_norm": 0.22539037466049194,
      "learning_rate": 0.00019567763356474494,
      "loss": 0.7191,
      "step": 1079
    },
    {
      "epoch": 2.1730382293762576,
      "grad_norm": 0.22606568038463593,
      "learning_rate": 0.00019567360901499145,
      "loss": 0.7681,
      "step": 1080
    },
    {
      "epoch": 2.1750503018108653,
      "grad_norm": 0.2116852104663849,
      "learning_rate": 0.00019566958446523797,
      "loss": 0.7369,
      "step": 1081
    },
    {
      "epoch": 2.1770623742454727,
      "grad_norm": 0.22341620922088623,
      "learning_rate": 0.00019566555991548448,
      "loss": 0.7099,
      "step": 1082
    },
    {
      "epoch": 2.1790744466800804,
      "grad_norm": 0.21974040567874908,
      "learning_rate": 0.00019566153536573096,
      "loss": 0.7635,
      "step": 1083
    },
    {
      "epoch": 2.181086519114688,
      "grad_norm": 0.21749591827392578,
      "learning_rate": 0.00019565751081597748,
      "loss": 0.7371,
      "step": 1084
    },
    {
      "epoch": 2.183098591549296,
      "grad_norm": 0.2174331247806549,
      "learning_rate": 0.00019565348626622396,
      "loss": 0.6787,
      "step": 1085
    },
    {
      "epoch": 2.1851106639839033,
      "grad_norm": 0.21105673909187317,
      "learning_rate": 0.00019564946171647047,
      "loss": 0.7074,
      "step": 1086
    },
    {
      "epoch": 2.187122736418511,
      "grad_norm": 0.21161188185214996,
      "learning_rate": 0.00019564543716671699,
      "loss": 0.7254,
      "step": 1087
    },
    {
      "epoch": 2.189134808853119,
      "grad_norm": 0.21386748552322388,
      "learning_rate": 0.0001956414126169635,
      "loss": 0.7298,
      "step": 1088
    },
    {
      "epoch": 2.191146881287726,
      "grad_norm": 0.2253909707069397,
      "learning_rate": 0.00019563738806720998,
      "loss": 0.7295,
      "step": 1089
    },
    {
      "epoch": 2.193158953722334,
      "grad_norm": 0.22630882263183594,
      "learning_rate": 0.0001956333635174565,
      "loss": 0.7217,
      "step": 1090
    },
    {
      "epoch": 2.1951710261569417,
      "grad_norm": 0.23182858526706696,
      "learning_rate": 0.00019562933896770298,
      "loss": 0.7378,
      "step": 1091
    },
    {
      "epoch": 2.1971830985915495,
      "grad_norm": 0.21632249653339386,
      "learning_rate": 0.00019562531441794952,
      "loss": 0.7018,
      "step": 1092
    },
    {
      "epoch": 2.199195171026157,
      "grad_norm": 0.2322763055562973,
      "learning_rate": 0.000195621289868196,
      "loss": 0.7151,
      "step": 1093
    },
    {
      "epoch": 2.2012072434607646,
      "grad_norm": 0.2142789661884308,
      "learning_rate": 0.00019561726531844252,
      "loss": 0.692,
      "step": 1094
    },
    {
      "epoch": 2.2032193158953723,
      "grad_norm": 0.22465743124485016,
      "learning_rate": 0.000195613240768689,
      "loss": 0.7241,
      "step": 1095
    },
    {
      "epoch": 2.20523138832998,
      "grad_norm": 0.22172342240810394,
      "learning_rate": 0.00019560921621893551,
      "loss": 0.727,
      "step": 1096
    },
    {
      "epoch": 2.2072434607645874,
      "grad_norm": 0.22791250050067902,
      "learning_rate": 0.000195605191669182,
      "loss": 0.7247,
      "step": 1097
    },
    {
      "epoch": 2.209255533199195,
      "grad_norm": 0.22387269139289856,
      "learning_rate": 0.00019560116711942854,
      "loss": 0.7016,
      "step": 1098
    },
    {
      "epoch": 2.211267605633803,
      "grad_norm": 0.2252112478017807,
      "learning_rate": 0.00019559714256967502,
      "loss": 0.7431,
      "step": 1099
    },
    {
      "epoch": 2.2132796780684103,
      "grad_norm": 0.21953225135803223,
      "learning_rate": 0.00019559311801992154,
      "loss": 0.7714,
      "step": 1100
    },
    {
      "epoch": 2.215291750503018,
      "grad_norm": 0.21820029616355896,
      "learning_rate": 0.00019558909347016802,
      "loss": 0.7552,
      "step": 1101
    },
    {
      "epoch": 2.217303822937626,
      "grad_norm": 0.2231789380311966,
      "learning_rate": 0.00019558506892041453,
      "loss": 0.7306,
      "step": 1102
    },
    {
      "epoch": 2.219315895372233,
      "grad_norm": 0.22359232604503632,
      "learning_rate": 0.00019558104437066105,
      "loss": 0.7457,
      "step": 1103
    },
    {
      "epoch": 2.221327967806841,
      "grad_norm": 0.21252141892910004,
      "learning_rate": 0.00019557701982090756,
      "loss": 0.7154,
      "step": 1104
    },
    {
      "epoch": 2.2233400402414487,
      "grad_norm": 0.2062349170446396,
      "learning_rate": 0.00019557299527115404,
      "loss": 0.7334,
      "step": 1105
    },
    {
      "epoch": 2.2253521126760565,
      "grad_norm": 0.2205323576927185,
      "learning_rate": 0.00019556897072140055,
      "loss": 0.7128,
      "step": 1106
    },
    {
      "epoch": 2.227364185110664,
      "grad_norm": 0.2229357808828354,
      "learning_rate": 0.00019556494617164704,
      "loss": 0.7361,
      "step": 1107
    },
    {
      "epoch": 2.2293762575452716,
      "grad_norm": 0.2132570743560791,
      "learning_rate": 0.00019556092162189358,
      "loss": 0.6966,
      "step": 1108
    },
    {
      "epoch": 2.2313883299798793,
      "grad_norm": 0.21937550604343414,
      "learning_rate": 0.00019555689707214006,
      "loss": 0.6834,
      "step": 1109
    },
    {
      "epoch": 2.233400402414487,
      "grad_norm": 0.22622068226337433,
      "learning_rate": 0.00019555287252238658,
      "loss": 0.7063,
      "step": 1110
    },
    {
      "epoch": 2.2354124748490944,
      "grad_norm": 0.23759396374225616,
      "learning_rate": 0.00019554884797263306,
      "loss": 0.7766,
      "step": 1111
    },
    {
      "epoch": 2.237424547283702,
      "grad_norm": 0.24106621742248535,
      "learning_rate": 0.00019554482342287957,
      "loss": 0.7137,
      "step": 1112
    },
    {
      "epoch": 2.23943661971831,
      "grad_norm": 0.2336236536502838,
      "learning_rate": 0.00019554079887312609,
      "loss": 0.6988,
      "step": 1113
    },
    {
      "epoch": 2.2414486921529173,
      "grad_norm": 0.22725346684455872,
      "learning_rate": 0.00019553677432337257,
      "loss": 0.697,
      "step": 1114
    },
    {
      "epoch": 2.243460764587525,
      "grad_norm": 0.21636982262134552,
      "learning_rate": 0.00019553274977361908,
      "loss": 0.725,
      "step": 1115
    },
    {
      "epoch": 2.245472837022133,
      "grad_norm": 0.2231416553258896,
      "learning_rate": 0.0001955287252238656,
      "loss": 0.71,
      "step": 1116
    },
    {
      "epoch": 2.2474849094567406,
      "grad_norm": 0.23423047363758087,
      "learning_rate": 0.00019552470067411208,
      "loss": 0.7501,
      "step": 1117
    },
    {
      "epoch": 2.249496981891348,
      "grad_norm": 0.21886217594146729,
      "learning_rate": 0.0001955206761243586,
      "loss": 0.7353,
      "step": 1118
    },
    {
      "epoch": 2.2515090543259557,
      "grad_norm": 0.21197298169136047,
      "learning_rate": 0.0001955166515746051,
      "loss": 0.7118,
      "step": 1119
    },
    {
      "epoch": 2.2535211267605635,
      "grad_norm": 0.222966730594635,
      "learning_rate": 0.0001955126270248516,
      "loss": 0.7446,
      "step": 1120
    },
    {
      "epoch": 2.2555331991951713,
      "grad_norm": 0.22135990858078003,
      "learning_rate": 0.0001955086024750981,
      "loss": 0.7121,
      "step": 1121
    },
    {
      "epoch": 2.2575452716297786,
      "grad_norm": 0.21919547021389008,
      "learning_rate": 0.0001955045779253446,
      "loss": 0.7276,
      "step": 1122
    },
    {
      "epoch": 2.2595573440643864,
      "grad_norm": 0.21692952513694763,
      "learning_rate": 0.00019550055337559113,
      "loss": 0.7779,
      "step": 1123
    },
    {
      "epoch": 2.261569416498994,
      "grad_norm": 0.20794297754764557,
      "learning_rate": 0.0001954965288258376,
      "loss": 0.7009,
      "step": 1124
    },
    {
      "epoch": 2.2635814889336014,
      "grad_norm": 0.21707825362682343,
      "learning_rate": 0.00019549250427608412,
      "loss": 0.7018,
      "step": 1125
    },
    {
      "epoch": 2.265593561368209,
      "grad_norm": 0.22363953292369843,
      "learning_rate": 0.0001954884797263306,
      "loss": 0.7115,
      "step": 1126
    },
    {
      "epoch": 2.267605633802817,
      "grad_norm": 0.24312065541744232,
      "learning_rate": 0.00019548445517657712,
      "loss": 0.7677,
      "step": 1127
    },
    {
      "epoch": 2.2696177062374243,
      "grad_norm": 0.21689391136169434,
      "learning_rate": 0.00019548043062682363,
      "loss": 0.7103,
      "step": 1128
    },
    {
      "epoch": 2.271629778672032,
      "grad_norm": 0.21869690716266632,
      "learning_rate": 0.00019547640607707015,
      "loss": 0.7094,
      "step": 1129
    },
    {
      "epoch": 2.27364185110664,
      "grad_norm": 0.22521549463272095,
      "learning_rate": 0.00019547238152731663,
      "loss": 0.684,
      "step": 1130
    },
    {
      "epoch": 2.2756539235412476,
      "grad_norm": 0.2331799566745758,
      "learning_rate": 0.00019546835697756314,
      "loss": 0.7139,
      "step": 1131
    },
    {
      "epoch": 2.277665995975855,
      "grad_norm": 0.22694076597690582,
      "learning_rate": 0.00019546433242780963,
      "loss": 0.7168,
      "step": 1132
    },
    {
      "epoch": 2.2796780684104627,
      "grad_norm": 0.2319418042898178,
      "learning_rate": 0.00019546030787805617,
      "loss": 0.743,
      "step": 1133
    },
    {
      "epoch": 2.2816901408450705,
      "grad_norm": 0.226470485329628,
      "learning_rate": 0.00019545628332830265,
      "loss": 0.7366,
      "step": 1134
    },
    {
      "epoch": 2.2837022132796783,
      "grad_norm": 0.2236812561750412,
      "learning_rate": 0.00019545225877854917,
      "loss": 0.721,
      "step": 1135
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.22117513418197632,
      "learning_rate": 0.00019544823422879565,
      "loss": 0.7216,
      "step": 1136
    },
    {
      "epoch": 2.2877263581488934,
      "grad_norm": 0.22003132104873657,
      "learning_rate": 0.00019544420967904216,
      "loss": 0.7316,
      "step": 1137
    },
    {
      "epoch": 2.289738430583501,
      "grad_norm": 0.21399424970149994,
      "learning_rate": 0.00019544018512928867,
      "loss": 0.7007,
      "step": 1138
    },
    {
      "epoch": 2.2917505030181085,
      "grad_norm": 0.21045240759849548,
      "learning_rate": 0.0001954361605795352,
      "loss": 0.7208,
      "step": 1139
    },
    {
      "epoch": 2.2937625754527162,
      "grad_norm": 0.23665018379688263,
      "learning_rate": 0.00019543213602978167,
      "loss": 0.7759,
      "step": 1140
    },
    {
      "epoch": 2.295774647887324,
      "grad_norm": 0.22128668427467346,
      "learning_rate": 0.00019542811148002818,
      "loss": 0.6863,
      "step": 1141
    },
    {
      "epoch": 2.2977867203219318,
      "grad_norm": 0.21908453106880188,
      "learning_rate": 0.00019542408693027467,
      "loss": 0.74,
      "step": 1142
    },
    {
      "epoch": 2.299798792756539,
      "grad_norm": 0.21523210406303406,
      "learning_rate": 0.0001954200623805212,
      "loss": 0.7045,
      "step": 1143
    },
    {
      "epoch": 2.301810865191147,
      "grad_norm": 0.216562882065773,
      "learning_rate": 0.0001954160378307677,
      "loss": 0.7085,
      "step": 1144
    },
    {
      "epoch": 2.3038229376257546,
      "grad_norm": 0.22708746790885925,
      "learning_rate": 0.0001954120132810142,
      "loss": 0.7157,
      "step": 1145
    },
    {
      "epoch": 2.3058350100603624,
      "grad_norm": 0.23390686511993408,
      "learning_rate": 0.0001954079887312607,
      "loss": 0.7426,
      "step": 1146
    },
    {
      "epoch": 2.3078470824949697,
      "grad_norm": 0.2272133082151413,
      "learning_rate": 0.0001954039641815072,
      "loss": 0.7273,
      "step": 1147
    },
    {
      "epoch": 2.3098591549295775,
      "grad_norm": 0.23259462416172028,
      "learning_rate": 0.00019539993963175372,
      "loss": 0.7022,
      "step": 1148
    },
    {
      "epoch": 2.3118712273641853,
      "grad_norm": 0.24191604554653168,
      "learning_rate": 0.0001953959150820002,
      "loss": 0.7266,
      "step": 1149
    },
    {
      "epoch": 2.3138832997987926,
      "grad_norm": 0.22005783021450043,
      "learning_rate": 0.0001953918905322467,
      "loss": 0.732,
      "step": 1150
    },
    {
      "epoch": 2.3158953722334004,
      "grad_norm": 0.21566690504550934,
      "learning_rate": 0.00019538786598249323,
      "loss": 0.7133,
      "step": 1151
    },
    {
      "epoch": 2.317907444668008,
      "grad_norm": 0.21812306344509125,
      "learning_rate": 0.0001953838414327397,
      "loss": 0.7141,
      "step": 1152
    },
    {
      "epoch": 2.3199195171026155,
      "grad_norm": 0.24069833755493164,
      "learning_rate": 0.00019537981688298622,
      "loss": 0.707,
      "step": 1153
    },
    {
      "epoch": 2.3219315895372232,
      "grad_norm": 0.22204045951366425,
      "learning_rate": 0.00019537579233323273,
      "loss": 0.733,
      "step": 1154
    },
    {
      "epoch": 2.323943661971831,
      "grad_norm": 0.22771279513835907,
      "learning_rate": 0.00019537176778347922,
      "loss": 0.744,
      "step": 1155
    },
    {
      "epoch": 2.3259557344064388,
      "grad_norm": 0.224864661693573,
      "learning_rate": 0.00019536774323372573,
      "loss": 0.7481,
      "step": 1156
    },
    {
      "epoch": 2.327967806841046,
      "grad_norm": 0.2194974571466446,
      "learning_rate": 0.00019536371868397222,
      "loss": 0.7355,
      "step": 1157
    },
    {
      "epoch": 2.329979879275654,
      "grad_norm": 0.2260061651468277,
      "learning_rate": 0.00019535969413421876,
      "loss": 0.794,
      "step": 1158
    },
    {
      "epoch": 2.3319919517102616,
      "grad_norm": 0.22463354468345642,
      "learning_rate": 0.00019535566958446524,
      "loss": 0.7591,
      "step": 1159
    },
    {
      "epoch": 2.3340040241448694,
      "grad_norm": 0.21642526984214783,
      "learning_rate": 0.00019535164503471175,
      "loss": 0.7143,
      "step": 1160
    },
    {
      "epoch": 2.3360160965794767,
      "grad_norm": 0.22307853400707245,
      "learning_rate": 0.00019534762048495824,
      "loss": 0.7668,
      "step": 1161
    },
    {
      "epoch": 2.3380281690140845,
      "grad_norm": 0.22506868839263916,
      "learning_rate": 0.00019534359593520475,
      "loss": 0.6915,
      "step": 1162
    },
    {
      "epoch": 2.3400402414486923,
      "grad_norm": 0.24076876044273376,
      "learning_rate": 0.00019533957138545126,
      "loss": 0.7303,
      "step": 1163
    },
    {
      "epoch": 2.3420523138832996,
      "grad_norm": 0.20958903431892395,
      "learning_rate": 0.00019533554683569778,
      "loss": 0.7264,
      "step": 1164
    },
    {
      "epoch": 2.3440643863179074,
      "grad_norm": 0.2330329269170761,
      "learning_rate": 0.00019533152228594426,
      "loss": 0.8207,
      "step": 1165
    },
    {
      "epoch": 2.346076458752515,
      "grad_norm": 0.21484404802322388,
      "learning_rate": 0.00019532749773619077,
      "loss": 0.712,
      "step": 1166
    },
    {
      "epoch": 2.348088531187123,
      "grad_norm": 0.21463127434253693,
      "learning_rate": 0.00019532347318643726,
      "loss": 0.7034,
      "step": 1167
    },
    {
      "epoch": 2.3501006036217302,
      "grad_norm": 0.24414056539535522,
      "learning_rate": 0.0001953194486366838,
      "loss": 0.8083,
      "step": 1168
    },
    {
      "epoch": 2.352112676056338,
      "grad_norm": 0.22531233727931976,
      "learning_rate": 0.00019531542408693028,
      "loss": 0.6911,
      "step": 1169
    },
    {
      "epoch": 2.3541247484909458,
      "grad_norm": 0.22135724127292633,
      "learning_rate": 0.0001953113995371768,
      "loss": 0.7233,
      "step": 1170
    },
    {
      "epoch": 2.3561368209255535,
      "grad_norm": 0.22009636461734772,
      "learning_rate": 0.00019530737498742328,
      "loss": 0.7411,
      "step": 1171
    },
    {
      "epoch": 2.358148893360161,
      "grad_norm": 0.24227476119995117,
      "learning_rate": 0.0001953033504376698,
      "loss": 0.7037,
      "step": 1172
    },
    {
      "epoch": 2.3601609657947686,
      "grad_norm": 0.2251187413930893,
      "learning_rate": 0.0001952993258879163,
      "loss": 0.713,
      "step": 1173
    },
    {
      "epoch": 2.3621730382293764,
      "grad_norm": 0.21349172294139862,
      "learning_rate": 0.00019529530133816282,
      "loss": 0.6392,
      "step": 1174
    },
    {
      "epoch": 2.3641851106639837,
      "grad_norm": 0.2385302186012268,
      "learning_rate": 0.0001952912767884093,
      "loss": 0.7422,
      "step": 1175
    },
    {
      "epoch": 2.3661971830985915,
      "grad_norm": 0.21010756492614746,
      "learning_rate": 0.00019528725223865581,
      "loss": 0.6926,
      "step": 1176
    },
    {
      "epoch": 2.3682092555331993,
      "grad_norm": 0.21192649006843567,
      "learning_rate": 0.0001952832276889023,
      "loss": 0.6823,
      "step": 1177
    },
    {
      "epoch": 2.3702213279678066,
      "grad_norm": 0.2179497331380844,
      "learning_rate": 0.00019527920313914884,
      "loss": 0.7288,
      "step": 1178
    },
    {
      "epoch": 2.3722334004024144,
      "grad_norm": 0.21463130414485931,
      "learning_rate": 0.00019527517858939532,
      "loss": 0.6895,
      "step": 1179
    },
    {
      "epoch": 2.374245472837022,
      "grad_norm": 0.22280487418174744,
      "learning_rate": 0.00019527115403964184,
      "loss": 0.725,
      "step": 1180
    },
    {
      "epoch": 2.37625754527163,
      "grad_norm": 0.2239341288805008,
      "learning_rate": 0.00019526712948988832,
      "loss": 0.7506,
      "step": 1181
    },
    {
      "epoch": 2.3782696177062372,
      "grad_norm": 0.22592604160308838,
      "learning_rate": 0.00019526310494013483,
      "loss": 0.6698,
      "step": 1182
    },
    {
      "epoch": 2.380281690140845,
      "grad_norm": 0.21926315128803253,
      "learning_rate": 0.00019525908039038135,
      "loss": 0.6959,
      "step": 1183
    },
    {
      "epoch": 2.3822937625754528,
      "grad_norm": 0.22814469039440155,
      "learning_rate": 0.00019525505584062783,
      "loss": 0.7152,
      "step": 1184
    },
    {
      "epoch": 2.3843058350100605,
      "grad_norm": 0.22436727583408356,
      "learning_rate": 0.00019525103129087434,
      "loss": 0.7113,
      "step": 1185
    },
    {
      "epoch": 2.386317907444668,
      "grad_norm": 0.23778027296066284,
      "learning_rate": 0.00019524700674112085,
      "loss": 0.7843,
      "step": 1186
    },
    {
      "epoch": 2.3883299798792756,
      "grad_norm": 0.21239584684371948,
      "learning_rate": 0.00019524298219136734,
      "loss": 0.699,
      "step": 1187
    },
    {
      "epoch": 2.3903420523138834,
      "grad_norm": 0.21349874138832092,
      "learning_rate": 0.00019523895764161385,
      "loss": 0.7215,
      "step": 1188
    },
    {
      "epoch": 2.3923541247484907,
      "grad_norm": 0.22732676565647125,
      "learning_rate": 0.00019523493309186036,
      "loss": 0.6894,
      "step": 1189
    },
    {
      "epoch": 2.3943661971830985,
      "grad_norm": 0.22261472046375275,
      "learning_rate": 0.00019523090854210685,
      "loss": 0.6963,
      "step": 1190
    },
    {
      "epoch": 2.3963782696177063,
      "grad_norm": 0.2345701903104782,
      "learning_rate": 0.00019522688399235336,
      "loss": 0.7289,
      "step": 1191
    },
    {
      "epoch": 2.398390342052314,
      "grad_norm": 0.2311418503522873,
      "learning_rate": 0.00019522285944259985,
      "loss": 0.7345,
      "step": 1192
    },
    {
      "epoch": 2.4004024144869214,
      "grad_norm": 0.22071833908557892,
      "learning_rate": 0.00019521883489284639,
      "loss": 0.7019,
      "step": 1193
    },
    {
      "epoch": 2.402414486921529,
      "grad_norm": 0.21855713427066803,
      "learning_rate": 0.00019521481034309287,
      "loss": 0.6973,
      "step": 1194
    },
    {
      "epoch": 2.404426559356137,
      "grad_norm": 0.22856329381465912,
      "learning_rate": 0.00019521078579333938,
      "loss": 0.7231,
      "step": 1195
    },
    {
      "epoch": 2.4064386317907447,
      "grad_norm": 0.21865320205688477,
      "learning_rate": 0.00019520676124358587,
      "loss": 0.7277,
      "step": 1196
    },
    {
      "epoch": 2.408450704225352,
      "grad_norm": 0.22070430219173431,
      "learning_rate": 0.00019520273669383238,
      "loss": 0.6971,
      "step": 1197
    },
    {
      "epoch": 2.41046277665996,
      "grad_norm": 0.2297634333372116,
      "learning_rate": 0.0001951987121440789,
      "loss": 0.7171,
      "step": 1198
    },
    {
      "epoch": 2.4124748490945676,
      "grad_norm": 0.2223893254995346,
      "learning_rate": 0.0001951946875943254,
      "loss": 0.7243,
      "step": 1199
    },
    {
      "epoch": 2.414486921529175,
      "grad_norm": 0.20932993292808533,
      "learning_rate": 0.0001951906630445719,
      "loss": 0.6753,
      "step": 1200
    },
    {
      "epoch": 2.4164989939637826,
      "grad_norm": 0.21853981912136078,
      "learning_rate": 0.0001951866384948184,
      "loss": 0.7169,
      "step": 1201
    },
    {
      "epoch": 2.4185110663983904,
      "grad_norm": 0.23193979263305664,
      "learning_rate": 0.0001951826139450649,
      "loss": 0.7562,
      "step": 1202
    },
    {
      "epoch": 2.4205231388329977,
      "grad_norm": 0.24766360223293304,
      "learning_rate": 0.00019517858939531143,
      "loss": 0.7171,
      "step": 1203
    },
    {
      "epoch": 2.4225352112676055,
      "grad_norm": 0.23922309279441833,
      "learning_rate": 0.0001951745648455579,
      "loss": 0.7707,
      "step": 1204
    },
    {
      "epoch": 2.4245472837022133,
      "grad_norm": 0.24040721356868744,
      "learning_rate": 0.00019517054029580442,
      "loss": 0.7409,
      "step": 1205
    },
    {
      "epoch": 2.426559356136821,
      "grad_norm": 0.23042923212051392,
      "learning_rate": 0.0001951665157460509,
      "loss": 0.7075,
      "step": 1206
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.21719443798065186,
      "learning_rate": 0.00019516249119629742,
      "loss": 0.6945,
      "step": 1207
    },
    {
      "epoch": 2.430583501006036,
      "grad_norm": 0.22770053148269653,
      "learning_rate": 0.00019515846664654393,
      "loss": 0.7364,
      "step": 1208
    },
    {
      "epoch": 2.432595573440644,
      "grad_norm": 0.22061486542224884,
      "learning_rate": 0.00019515444209679045,
      "loss": 0.7,
      "step": 1209
    },
    {
      "epoch": 2.4346076458752517,
      "grad_norm": 0.21705392003059387,
      "learning_rate": 0.00019515041754703693,
      "loss": 0.6959,
      "step": 1210
    },
    {
      "epoch": 2.436619718309859,
      "grad_norm": 0.22367382049560547,
      "learning_rate": 0.00019514639299728344,
      "loss": 0.7117,
      "step": 1211
    },
    {
      "epoch": 2.438631790744467,
      "grad_norm": 0.21909764409065247,
      "learning_rate": 0.00019514236844752993,
      "loss": 0.7276,
      "step": 1212
    },
    {
      "epoch": 2.4406438631790746,
      "grad_norm": 0.2177736759185791,
      "learning_rate": 0.00019513834389777647,
      "loss": 0.7348,
      "step": 1213
    },
    {
      "epoch": 2.442655935613682,
      "grad_norm": 0.2325657308101654,
      "learning_rate": 0.00019513431934802295,
      "loss": 0.7659,
      "step": 1214
    },
    {
      "epoch": 2.4446680080482897,
      "grad_norm": 0.22497643530368805,
      "learning_rate": 0.00019513029479826946,
      "loss": 0.7205,
      "step": 1215
    },
    {
      "epoch": 2.4466800804828974,
      "grad_norm": 0.22348393499851227,
      "learning_rate": 0.00019512627024851595,
      "loss": 0.7456,
      "step": 1216
    },
    {
      "epoch": 2.448692152917505,
      "grad_norm": 0.21979817748069763,
      "learning_rate": 0.00019512224569876246,
      "loss": 0.7261,
      "step": 1217
    },
    {
      "epoch": 2.4507042253521125,
      "grad_norm": 0.22498130798339844,
      "learning_rate": 0.00019511822114900897,
      "loss": 0.6926,
      "step": 1218
    },
    {
      "epoch": 2.4527162977867203,
      "grad_norm": 0.22214563190937042,
      "learning_rate": 0.00019511419659925546,
      "loss": 0.7259,
      "step": 1219
    },
    {
      "epoch": 2.454728370221328,
      "grad_norm": 0.22627070546150208,
      "learning_rate": 0.00019511017204950197,
      "loss": 0.7282,
      "step": 1220
    },
    {
      "epoch": 2.456740442655936,
      "grad_norm": 0.22157244384288788,
      "learning_rate": 0.00019510614749974848,
      "loss": 0.7359,
      "step": 1221
    },
    {
      "epoch": 2.458752515090543,
      "grad_norm": 0.20920580625534058,
      "learning_rate": 0.00019510212294999497,
      "loss": 0.7284,
      "step": 1222
    },
    {
      "epoch": 2.460764587525151,
      "grad_norm": 0.21106824278831482,
      "learning_rate": 0.00019509809840024148,
      "loss": 0.7271,
      "step": 1223
    },
    {
      "epoch": 2.4627766599597587,
      "grad_norm": 0.21271522343158722,
      "learning_rate": 0.000195094073850488,
      "loss": 0.7152,
      "step": 1224
    },
    {
      "epoch": 2.464788732394366,
      "grad_norm": 0.21234001219272614,
      "learning_rate": 0.00019509004930073448,
      "loss": 0.725,
      "step": 1225
    },
    {
      "epoch": 2.466800804828974,
      "grad_norm": 0.2202625870704651,
      "learning_rate": 0.000195086024750981,
      "loss": 0.7059,
      "step": 1226
    },
    {
      "epoch": 2.4688128772635816,
      "grad_norm": 0.2122834473848343,
      "learning_rate": 0.00019508200020122748,
      "loss": 0.6835,
      "step": 1227
    },
    {
      "epoch": 2.470824949698189,
      "grad_norm": 0.22763259708881378,
      "learning_rate": 0.00019507797565147402,
      "loss": 0.7265,
      "step": 1228
    },
    {
      "epoch": 2.4728370221327967,
      "grad_norm": 0.22533319890499115,
      "learning_rate": 0.0001950739511017205,
      "loss": 0.7261,
      "step": 1229
    },
    {
      "epoch": 2.4748490945674044,
      "grad_norm": 0.2380005121231079,
      "learning_rate": 0.000195069926551967,
      "loss": 0.7555,
      "step": 1230
    },
    {
      "epoch": 2.476861167002012,
      "grad_norm": 0.21382874250411987,
      "learning_rate": 0.0001950659020022135,
      "loss": 0.7105,
      "step": 1231
    },
    {
      "epoch": 2.4788732394366195,
      "grad_norm": 0.2362089455127716,
      "learning_rate": 0.00019506187745246,
      "loss": 0.7093,
      "step": 1232
    },
    {
      "epoch": 2.4808853118712273,
      "grad_norm": 0.2343980073928833,
      "learning_rate": 0.00019505785290270652,
      "loss": 0.7179,
      "step": 1233
    },
    {
      "epoch": 2.482897384305835,
      "grad_norm": 0.21556539833545685,
      "learning_rate": 0.00019505382835295303,
      "loss": 0.7238,
      "step": 1234
    },
    {
      "epoch": 2.484909456740443,
      "grad_norm": 0.22675657272338867,
      "learning_rate": 0.00019504980380319952,
      "loss": 0.6742,
      "step": 1235
    },
    {
      "epoch": 2.48692152917505,
      "grad_norm": 0.20922210812568665,
      "learning_rate": 0.00019504577925344603,
      "loss": 0.729,
      "step": 1236
    },
    {
      "epoch": 2.488933601609658,
      "grad_norm": 0.22957925498485565,
      "learning_rate": 0.00019504175470369252,
      "loss": 0.7254,
      "step": 1237
    },
    {
      "epoch": 2.4909456740442657,
      "grad_norm": 0.22696161270141602,
      "learning_rate": 0.00019503773015393906,
      "loss": 0.7482,
      "step": 1238
    },
    {
      "epoch": 2.492957746478873,
      "grad_norm": 0.21925029158592224,
      "learning_rate": 0.00019503370560418554,
      "loss": 0.7091,
      "step": 1239
    },
    {
      "epoch": 2.494969818913481,
      "grad_norm": 0.23265793919563293,
      "learning_rate": 0.00019502968105443205,
      "loss": 0.7123,
      "step": 1240
    },
    {
      "epoch": 2.4969818913480886,
      "grad_norm": 0.2243814915418625,
      "learning_rate": 0.00019502565650467854,
      "loss": 0.7206,
      "step": 1241
    },
    {
      "epoch": 2.4989939637826963,
      "grad_norm": 0.21718215942382812,
      "learning_rate": 0.00019502163195492505,
      "loss": 0.7073,
      "step": 1242
    },
    {
      "epoch": 2.5010060362173037,
      "grad_norm": 0.2336428463459015,
      "learning_rate": 0.00019501760740517156,
      "loss": 0.6523,
      "step": 1243
    },
    {
      "epoch": 2.5030181086519114,
      "grad_norm": 0.23301252722740173,
      "learning_rate": 0.00019501358285541808,
      "loss": 0.7335,
      "step": 1244
    },
    {
      "epoch": 2.505030181086519,
      "grad_norm": 0.20970484614372253,
      "learning_rate": 0.00019500955830566456,
      "loss": 0.7167,
      "step": 1245
    },
    {
      "epoch": 2.507042253521127,
      "grad_norm": 0.2401803880929947,
      "learning_rate": 0.00019500553375591107,
      "loss": 0.7253,
      "step": 1246
    },
    {
      "epoch": 2.5090543259557343,
      "grad_norm": 0.21255552768707275,
      "learning_rate": 0.00019500150920615756,
      "loss": 0.7113,
      "step": 1247
    },
    {
      "epoch": 2.511066398390342,
      "grad_norm": 0.2258751541376114,
      "learning_rate": 0.0001949974846564041,
      "loss": 0.775,
      "step": 1248
    },
    {
      "epoch": 2.51307847082495,
      "grad_norm": 0.21513110399246216,
      "learning_rate": 0.00019499346010665058,
      "loss": 0.7321,
      "step": 1249
    },
    {
      "epoch": 2.515090543259557,
      "grad_norm": 0.21566863358020782,
      "learning_rate": 0.0001949894355568971,
      "loss": 0.7033,
      "step": 1250
    },
    {
      "epoch": 2.517102615694165,
      "grad_norm": 0.2110251635313034,
      "learning_rate": 0.00019498541100714358,
      "loss": 0.7076,
      "step": 1251
    },
    {
      "epoch": 2.5191146881287727,
      "grad_norm": 0.2142709642648697,
      "learning_rate": 0.0001949813864573901,
      "loss": 0.6848,
      "step": 1252
    },
    {
      "epoch": 2.52112676056338,
      "grad_norm": 0.22790470719337463,
      "learning_rate": 0.0001949773619076366,
      "loss": 0.7295,
      "step": 1253
    },
    {
      "epoch": 2.523138832997988,
      "grad_norm": 0.2220717817544937,
      "learning_rate": 0.0001949733373578831,
      "loss": 0.696,
      "step": 1254
    },
    {
      "epoch": 2.5251509054325956,
      "grad_norm": 0.23218922317028046,
      "learning_rate": 0.0001949693128081296,
      "loss": 0.7167,
      "step": 1255
    },
    {
      "epoch": 2.5271629778672033,
      "grad_norm": 0.22046971321105957,
      "learning_rate": 0.00019496528825837609,
      "loss": 0.7446,
      "step": 1256
    },
    {
      "epoch": 2.529175050301811,
      "grad_norm": 0.22494420409202576,
      "learning_rate": 0.0001949612637086226,
      "loss": 0.7195,
      "step": 1257
    },
    {
      "epoch": 2.5311871227364184,
      "grad_norm": 0.229350745677948,
      "learning_rate": 0.0001949572391588691,
      "loss": 0.7421,
      "step": 1258
    },
    {
      "epoch": 2.533199195171026,
      "grad_norm": 0.21751584112644196,
      "learning_rate": 0.00019495321460911562,
      "loss": 0.7081,
      "step": 1259
    },
    {
      "epoch": 2.535211267605634,
      "grad_norm": 0.22339756786823273,
      "learning_rate": 0.0001949491900593621,
      "loss": 0.7116,
      "step": 1260
    },
    {
      "epoch": 2.5372233400402413,
      "grad_norm": 0.22141075134277344,
      "learning_rate": 0.00019494516550960862,
      "loss": 0.7369,
      "step": 1261
    },
    {
      "epoch": 2.539235412474849,
      "grad_norm": 0.2083786129951477,
      "learning_rate": 0.0001949411409598551,
      "loss": 0.7252,
      "step": 1262
    },
    {
      "epoch": 2.541247484909457,
      "grad_norm": 0.23904642462730408,
      "learning_rate": 0.00019493711641010164,
      "loss": 0.7611,
      "step": 1263
    },
    {
      "epoch": 2.543259557344064,
      "grad_norm": 0.2205447256565094,
      "learning_rate": 0.00019493309186034813,
      "loss": 0.7464,
      "step": 1264
    },
    {
      "epoch": 2.545271629778672,
      "grad_norm": 0.22058899700641632,
      "learning_rate": 0.00019492906731059464,
      "loss": 0.7389,
      "step": 1265
    },
    {
      "epoch": 2.5472837022132797,
      "grad_norm": 0.23618444800376892,
      "learning_rate": 0.00019492504276084113,
      "loss": 0.7425,
      "step": 1266
    },
    {
      "epoch": 2.5492957746478875,
      "grad_norm": 0.2095669060945511,
      "learning_rate": 0.00019492101821108764,
      "loss": 0.721,
      "step": 1267
    },
    {
      "epoch": 2.551307847082495,
      "grad_norm": 0.21835742890834808,
      "learning_rate": 0.00019491699366133415,
      "loss": 0.7629,
      "step": 1268
    },
    {
      "epoch": 2.5533199195171026,
      "grad_norm": 0.20906507968902588,
      "learning_rate": 0.00019491296911158066,
      "loss": 0.6944,
      "step": 1269
    },
    {
      "epoch": 2.5553319919517103,
      "grad_norm": 0.24561288952827454,
      "learning_rate": 0.00019490894456182715,
      "loss": 0.7428,
      "step": 1270
    },
    {
      "epoch": 2.557344064386318,
      "grad_norm": 0.22216928005218506,
      "learning_rate": 0.00019490492001207366,
      "loss": 0.7059,
      "step": 1271
    },
    {
      "epoch": 2.5593561368209254,
      "grad_norm": 0.21753519773483276,
      "learning_rate": 0.00019490089546232015,
      "loss": 0.7442,
      "step": 1272
    },
    {
      "epoch": 2.561368209255533,
      "grad_norm": 0.22109082341194153,
      "learning_rate": 0.00019489687091256669,
      "loss": 0.6957,
      "step": 1273
    },
    {
      "epoch": 2.563380281690141,
      "grad_norm": 0.2459147423505783,
      "learning_rate": 0.00019489284636281317,
      "loss": 0.7706,
      "step": 1274
    },
    {
      "epoch": 2.5653923541247483,
      "grad_norm": 0.23066487908363342,
      "learning_rate": 0.00019488882181305968,
      "loss": 0.7473,
      "step": 1275
    },
    {
      "epoch": 2.567404426559356,
      "grad_norm": 0.23743943870067596,
      "learning_rate": 0.00019488479726330617,
      "loss": 0.7248,
      "step": 1276
    },
    {
      "epoch": 2.569416498993964,
      "grad_norm": 0.23035912215709686,
      "learning_rate": 0.00019488077271355268,
      "loss": 0.7635,
      "step": 1277
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.21707361936569214,
      "learning_rate": 0.0001948767481637992,
      "loss": 0.7228,
      "step": 1278
    },
    {
      "epoch": 2.573440643863179,
      "grad_norm": 0.21992574632167816,
      "learning_rate": 0.0001948727236140457,
      "loss": 0.6964,
      "step": 1279
    },
    {
      "epoch": 2.5754527162977867,
      "grad_norm": 0.21680162847042084,
      "learning_rate": 0.0001948686990642922,
      "loss": 0.7241,
      "step": 1280
    },
    {
      "epoch": 2.5774647887323945,
      "grad_norm": 0.2194765955209732,
      "learning_rate": 0.0001948646745145387,
      "loss": 0.7256,
      "step": 1281
    },
    {
      "epoch": 2.5794768611670023,
      "grad_norm": 0.21174263954162598,
      "learning_rate": 0.0001948606499647852,
      "loss": 0.7067,
      "step": 1282
    },
    {
      "epoch": 2.5814889336016096,
      "grad_norm": 0.2403241991996765,
      "learning_rate": 0.0001948566254150317,
      "loss": 0.7643,
      "step": 1283
    },
    {
      "epoch": 2.5835010060362174,
      "grad_norm": 0.22762376070022583,
      "learning_rate": 0.0001948526008652782,
      "loss": 0.719,
      "step": 1284
    },
    {
      "epoch": 2.585513078470825,
      "grad_norm": 0.2199140191078186,
      "learning_rate": 0.00019484857631552472,
      "loss": 0.7216,
      "step": 1285
    },
    {
      "epoch": 2.5875251509054324,
      "grad_norm": 0.22334909439086914,
      "learning_rate": 0.0001948445517657712,
      "loss": 0.7702,
      "step": 1286
    },
    {
      "epoch": 2.58953722334004,
      "grad_norm": 0.20840111374855042,
      "learning_rate": 0.00019484052721601772,
      "loss": 0.7221,
      "step": 1287
    },
    {
      "epoch": 2.591549295774648,
      "grad_norm": 0.22067224979400635,
      "learning_rate": 0.00019483650266626423,
      "loss": 0.7285,
      "step": 1288
    },
    {
      "epoch": 2.5935613682092553,
      "grad_norm": 0.21125638484954834,
      "learning_rate": 0.00019483247811651072,
      "loss": 0.7263,
      "step": 1289
    },
    {
      "epoch": 2.595573440643863,
      "grad_norm": 0.2177157998085022,
      "learning_rate": 0.00019482845356675723,
      "loss": 0.7221,
      "step": 1290
    },
    {
      "epoch": 2.597585513078471,
      "grad_norm": 0.21551911532878876,
      "learning_rate": 0.00019482442901700372,
      "loss": 0.7226,
      "step": 1291
    },
    {
      "epoch": 2.5995975855130786,
      "grad_norm": 0.22304406762123108,
      "learning_rate": 0.00019482040446725023,
      "loss": 0.7225,
      "step": 1292
    },
    {
      "epoch": 2.6016096579476864,
      "grad_norm": 0.22157172858715057,
      "learning_rate": 0.00019481637991749674,
      "loss": 0.6773,
      "step": 1293
    },
    {
      "epoch": 2.6036217303822937,
      "grad_norm": 0.24007858335971832,
      "learning_rate": 0.00019481235536774325,
      "loss": 0.7406,
      "step": 1294
    },
    {
      "epoch": 2.6056338028169015,
      "grad_norm": 0.21795116364955902,
      "learning_rate": 0.00019480833081798974,
      "loss": 0.7293,
      "step": 1295
    },
    {
      "epoch": 2.6076458752515093,
      "grad_norm": 0.2311878800392151,
      "learning_rate": 0.00019480430626823625,
      "loss": 0.7422,
      "step": 1296
    },
    {
      "epoch": 2.6096579476861166,
      "grad_norm": 0.23162014782428741,
      "learning_rate": 0.00019480028171848273,
      "loss": 0.732,
      "step": 1297
    },
    {
      "epoch": 2.6116700201207244,
      "grad_norm": 0.21602213382720947,
      "learning_rate": 0.00019479625716872927,
      "loss": 0.7117,
      "step": 1298
    },
    {
      "epoch": 2.613682092555332,
      "grad_norm": 0.2173287719488144,
      "learning_rate": 0.00019479223261897576,
      "loss": 0.7115,
      "step": 1299
    },
    {
      "epoch": 2.6156941649899395,
      "grad_norm": 0.24096271395683289,
      "learning_rate": 0.00019478820806922227,
      "loss": 0.7344,
      "step": 1300
    },
    {
      "epoch": 2.6177062374245472,
      "grad_norm": 0.23769330978393555,
      "learning_rate": 0.00019478418351946876,
      "loss": 0.7456,
      "step": 1301
    },
    {
      "epoch": 2.619718309859155,
      "grad_norm": 0.2347463071346283,
      "learning_rate": 0.00019478015896971527,
      "loss": 0.7714,
      "step": 1302
    },
    {
      "epoch": 2.6217303822937623,
      "grad_norm": 0.20931492745876312,
      "learning_rate": 0.00019477613441996178,
      "loss": 0.7031,
      "step": 1303
    },
    {
      "epoch": 2.62374245472837,
      "grad_norm": 0.2160550206899643,
      "learning_rate": 0.0001947721098702083,
      "loss": 0.7129,
      "step": 1304
    },
    {
      "epoch": 2.625754527162978,
      "grad_norm": 0.22317765653133392,
      "learning_rate": 0.00019476808532045478,
      "loss": 0.7452,
      "step": 1305
    },
    {
      "epoch": 2.6277665995975856,
      "grad_norm": 0.21865354478359222,
      "learning_rate": 0.0001947640607707013,
      "loss": 0.6952,
      "step": 1306
    },
    {
      "epoch": 2.6297786720321934,
      "grad_norm": 0.214321568608284,
      "learning_rate": 0.00019476003622094778,
      "loss": 0.7115,
      "step": 1307
    },
    {
      "epoch": 2.6317907444668007,
      "grad_norm": 0.21741387248039246,
      "learning_rate": 0.00019475601167119432,
      "loss": 0.6917,
      "step": 1308
    },
    {
      "epoch": 2.6338028169014085,
      "grad_norm": 0.2201167494058609,
      "learning_rate": 0.0001947519871214408,
      "loss": 0.7586,
      "step": 1309
    },
    {
      "epoch": 2.6358148893360163,
      "grad_norm": 0.22324398159980774,
      "learning_rate": 0.0001947479625716873,
      "loss": 0.7501,
      "step": 1310
    },
    {
      "epoch": 2.6378269617706236,
      "grad_norm": 0.22209767997264862,
      "learning_rate": 0.0001947439380219338,
      "loss": 0.6757,
      "step": 1311
    },
    {
      "epoch": 2.6398390342052314,
      "grad_norm": 0.21419495344161987,
      "learning_rate": 0.0001947399134721803,
      "loss": 0.6954,
      "step": 1312
    },
    {
      "epoch": 2.641851106639839,
      "grad_norm": 0.2079227715730667,
      "learning_rate": 0.00019473588892242682,
      "loss": 0.7269,
      "step": 1313
    },
    {
      "epoch": 2.6438631790744465,
      "grad_norm": 0.22130849957466125,
      "learning_rate": 0.00019473186437267333,
      "loss": 0.7205,
      "step": 1314
    },
    {
      "epoch": 2.6458752515090542,
      "grad_norm": 0.22448155283927917,
      "learning_rate": 0.00019472783982291982,
      "loss": 0.6948,
      "step": 1315
    },
    {
      "epoch": 2.647887323943662,
      "grad_norm": 0.2245672643184662,
      "learning_rate": 0.00019472381527316633,
      "loss": 0.7079,
      "step": 1316
    },
    {
      "epoch": 2.6498993963782698,
      "grad_norm": 0.22670993208885193,
      "learning_rate": 0.00019471979072341282,
      "loss": 0.6683,
      "step": 1317
    },
    {
      "epoch": 2.6519114688128775,
      "grad_norm": 0.22399300336837769,
      "learning_rate": 0.00019471576617365933,
      "loss": 0.7383,
      "step": 1318
    },
    {
      "epoch": 2.653923541247485,
      "grad_norm": 0.22954148054122925,
      "learning_rate": 0.00019471174162390584,
      "loss": 0.7217,
      "step": 1319
    },
    {
      "epoch": 2.6559356136820926,
      "grad_norm": 0.24333107471466064,
      "learning_rate": 0.00019470771707415235,
      "loss": 0.7405,
      "step": 1320
    },
    {
      "epoch": 2.6579476861167004,
      "grad_norm": 0.2292836308479309,
      "learning_rate": 0.00019470369252439884,
      "loss": 0.7207,
      "step": 1321
    },
    {
      "epoch": 2.6599597585513077,
      "grad_norm": 0.2229667454957962,
      "learning_rate": 0.00019469966797464535,
      "loss": 0.7662,
      "step": 1322
    },
    {
      "epoch": 2.6619718309859155,
      "grad_norm": 0.22797301411628723,
      "learning_rate": 0.00019469564342489186,
      "loss": 0.7177,
      "step": 1323
    },
    {
      "epoch": 2.6639839034205233,
      "grad_norm": 0.24602057039737701,
      "learning_rate": 0.00019469161887513835,
      "loss": 0.7654,
      "step": 1324
    },
    {
      "epoch": 2.6659959758551306,
      "grad_norm": 0.22905194759368896,
      "learning_rate": 0.00019468759432538486,
      "loss": 0.6891,
      "step": 1325
    },
    {
      "epoch": 2.6680080482897384,
      "grad_norm": 0.21149665117263794,
      "learning_rate": 0.00019468356977563135,
      "loss": 0.6629,
      "step": 1326
    },
    {
      "epoch": 2.670020120724346,
      "grad_norm": 0.22910387814044952,
      "learning_rate": 0.00019467954522587786,
      "loss": 0.7694,
      "step": 1327
    },
    {
      "epoch": 2.6720321931589535,
      "grad_norm": 0.23805655539035797,
      "learning_rate": 0.00019467552067612437,
      "loss": 0.7059,
      "step": 1328
    },
    {
      "epoch": 2.6740442655935612,
      "grad_norm": 0.2404222935438156,
      "learning_rate": 0.00019467149612637088,
      "loss": 0.7774,
      "step": 1329
    },
    {
      "epoch": 2.676056338028169,
      "grad_norm": 0.23223519325256348,
      "learning_rate": 0.00019466747157661737,
      "loss": 0.77,
      "step": 1330
    },
    {
      "epoch": 2.6780684104627768,
      "grad_norm": 0.21868066489696503,
      "learning_rate": 0.00019466344702686388,
      "loss": 0.7065,
      "step": 1331
    },
    {
      "epoch": 2.6800804828973845,
      "grad_norm": 0.21611808240413666,
      "learning_rate": 0.00019465942247711036,
      "loss": 0.7109,
      "step": 1332
    },
    {
      "epoch": 2.682092555331992,
      "grad_norm": 0.21455343067646027,
      "learning_rate": 0.0001946553979273569,
      "loss": 0.6741,
      "step": 1333
    },
    {
      "epoch": 2.6841046277665996,
      "grad_norm": 0.23264755308628082,
      "learning_rate": 0.0001946513733776034,
      "loss": 0.7257,
      "step": 1334
    },
    {
      "epoch": 2.6861167002012074,
      "grad_norm": 0.21570612490177155,
      "learning_rate": 0.0001946473488278499,
      "loss": 0.7116,
      "step": 1335
    },
    {
      "epoch": 2.6881287726358147,
      "grad_norm": 0.2227158546447754,
      "learning_rate": 0.00019464332427809639,
      "loss": 0.7056,
      "step": 1336
    },
    {
      "epoch": 2.6901408450704225,
      "grad_norm": 0.23470275104045868,
      "learning_rate": 0.0001946392997283429,
      "loss": 0.7167,
      "step": 1337
    },
    {
      "epoch": 2.6921529175050303,
      "grad_norm": 0.22076714038848877,
      "learning_rate": 0.0001946352751785894,
      "loss": 0.7284,
      "step": 1338
    },
    {
      "epoch": 2.6941649899396376,
      "grad_norm": 0.21596193313598633,
      "learning_rate": 0.00019463125062883592,
      "loss": 0.7053,
      "step": 1339
    },
    {
      "epoch": 2.6961770623742454,
      "grad_norm": 0.22446809709072113,
      "learning_rate": 0.0001946272260790824,
      "loss": 0.731,
      "step": 1340
    },
    {
      "epoch": 2.698189134808853,
      "grad_norm": 0.2147655189037323,
      "learning_rate": 0.00019462320152932892,
      "loss": 0.724,
      "step": 1341
    },
    {
      "epoch": 2.700201207243461,
      "grad_norm": 0.23018060624599457,
      "learning_rate": 0.0001946191769795754,
      "loss": 0.7255,
      "step": 1342
    },
    {
      "epoch": 2.7022132796780687,
      "grad_norm": 0.2112770527601242,
      "learning_rate": 0.00019461515242982192,
      "loss": 0.7103,
      "step": 1343
    },
    {
      "epoch": 2.704225352112676,
      "grad_norm": 0.21728335320949554,
      "learning_rate": 0.00019461112788006843,
      "loss": 0.7269,
      "step": 1344
    },
    {
      "epoch": 2.7062374245472838,
      "grad_norm": 0.22883202135562897,
      "learning_rate": 0.00019460710333031494,
      "loss": 0.7388,
      "step": 1345
    },
    {
      "epoch": 2.7082494969818915,
      "grad_norm": 0.2190951555967331,
      "learning_rate": 0.00019460307878056143,
      "loss": 0.7276,
      "step": 1346
    },
    {
      "epoch": 2.710261569416499,
      "grad_norm": 0.2122119665145874,
      "learning_rate": 0.00019459905423080794,
      "loss": 0.7123,
      "step": 1347
    },
    {
      "epoch": 2.7122736418511066,
      "grad_norm": 0.230897456407547,
      "learning_rate": 0.00019459502968105442,
      "loss": 0.7572,
      "step": 1348
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 0.22241999208927155,
      "learning_rate": 0.00019459100513130096,
      "loss": 0.7062,
      "step": 1349
    },
    {
      "epoch": 2.7162977867203217,
      "grad_norm": 0.21681924164295197,
      "learning_rate": 0.00019458698058154745,
      "loss": 0.707,
      "step": 1350
    },
    {
      "epoch": 2.7183098591549295,
      "grad_norm": 0.2220483273267746,
      "learning_rate": 0.00019458295603179396,
      "loss": 0.7399,
      "step": 1351
    },
    {
      "epoch": 2.7203219315895373,
      "grad_norm": 0.23297980427742004,
      "learning_rate": 0.00019457893148204045,
      "loss": 0.7479,
      "step": 1352
    },
    {
      "epoch": 2.7223340040241446,
      "grad_norm": 0.21625134348869324,
      "learning_rate": 0.00019457490693228696,
      "loss": 0.7152,
      "step": 1353
    },
    {
      "epoch": 2.7243460764587524,
      "grad_norm": 0.22453522682189941,
      "learning_rate": 0.00019457088238253347,
      "loss": 0.6912,
      "step": 1354
    },
    {
      "epoch": 2.72635814889336,
      "grad_norm": 0.22431714832782745,
      "learning_rate": 0.00019456685783277998,
      "loss": 0.745,
      "step": 1355
    },
    {
      "epoch": 2.728370221327968,
      "grad_norm": 0.22845309972763062,
      "learning_rate": 0.00019456283328302647,
      "loss": 0.7424,
      "step": 1356
    },
    {
      "epoch": 2.7303822937625757,
      "grad_norm": 0.2191452831029892,
      "learning_rate": 0.00019455880873327298,
      "loss": 0.6992,
      "step": 1357
    },
    {
      "epoch": 2.732394366197183,
      "grad_norm": 0.2281181365251541,
      "learning_rate": 0.00019455478418351947,
      "loss": 0.6779,
      "step": 1358
    },
    {
      "epoch": 2.734406438631791,
      "grad_norm": 0.21140417456626892,
      "learning_rate": 0.00019455075963376598,
      "loss": 0.7234,
      "step": 1359
    },
    {
      "epoch": 2.7364185110663986,
      "grad_norm": 0.21321403980255127,
      "learning_rate": 0.0001945467350840125,
      "loss": 0.7191,
      "step": 1360
    },
    {
      "epoch": 2.738430583501006,
      "grad_norm": 0.24473045766353607,
      "learning_rate": 0.00019454271053425897,
      "loss": 0.7804,
      "step": 1361
    },
    {
      "epoch": 2.7404426559356136,
      "grad_norm": 0.22449825704097748,
      "learning_rate": 0.0001945386859845055,
      "loss": 0.7148,
      "step": 1362
    },
    {
      "epoch": 2.7424547283702214,
      "grad_norm": 0.2166704535484314,
      "learning_rate": 0.000194534661434752,
      "loss": 0.7508,
      "step": 1363
    },
    {
      "epoch": 2.7444668008048287,
      "grad_norm": 0.2228545993566513,
      "learning_rate": 0.0001945306368849985,
      "loss": 0.7356,
      "step": 1364
    },
    {
      "epoch": 2.7464788732394365,
      "grad_norm": 0.22510428726673126,
      "learning_rate": 0.000194526612335245,
      "loss": 0.6982,
      "step": 1365
    },
    {
      "epoch": 2.7484909456740443,
      "grad_norm": 0.20665518939495087,
      "learning_rate": 0.0001945225877854915,
      "loss": 0.7142,
      "step": 1366
    },
    {
      "epoch": 2.750503018108652,
      "grad_norm": 0.2180193066596985,
      "learning_rate": 0.000194518563235738,
      "loss": 0.7195,
      "step": 1367
    },
    {
      "epoch": 2.75251509054326,
      "grad_norm": 0.22327888011932373,
      "learning_rate": 0.0001945145386859845,
      "loss": 0.7284,
      "step": 1368
    },
    {
      "epoch": 2.754527162977867,
      "grad_norm": 0.21593859791755676,
      "learning_rate": 0.00019451051413623102,
      "loss": 0.6818,
      "step": 1369
    },
    {
      "epoch": 2.756539235412475,
      "grad_norm": 0.2251414656639099,
      "learning_rate": 0.00019450648958647753,
      "loss": 0.7102,
      "step": 1370
    },
    {
      "epoch": 2.7585513078470827,
      "grad_norm": 0.2253330945968628,
      "learning_rate": 0.00019450246503672402,
      "loss": 0.7228,
      "step": 1371
    },
    {
      "epoch": 2.76056338028169,
      "grad_norm": 0.21569426357746124,
      "learning_rate": 0.00019449844048697053,
      "loss": 0.7172,
      "step": 1372
    },
    {
      "epoch": 2.762575452716298,
      "grad_norm": 0.23466460406780243,
      "learning_rate": 0.000194494415937217,
      "loss": 0.7547,
      "step": 1373
    },
    {
      "epoch": 2.7645875251509056,
      "grad_norm": 0.21779361367225647,
      "learning_rate": 0.00019449039138746355,
      "loss": 0.7119,
      "step": 1374
    },
    {
      "epoch": 2.766599597585513,
      "grad_norm": 0.2151523232460022,
      "learning_rate": 0.00019448636683771004,
      "loss": 0.6858,
      "step": 1375
    },
    {
      "epoch": 2.7686116700201207,
      "grad_norm": 0.20679441094398499,
      "learning_rate": 0.00019448234228795655,
      "loss": 0.746,
      "step": 1376
    },
    {
      "epoch": 2.7706237424547284,
      "grad_norm": 0.23008590936660767,
      "learning_rate": 0.00019447831773820303,
      "loss": 0.7178,
      "step": 1377
    },
    {
      "epoch": 2.7726358148893357,
      "grad_norm": 0.20714528858661652,
      "learning_rate": 0.00019447429318844955,
      "loss": 0.6899,
      "step": 1378
    },
    {
      "epoch": 2.7746478873239435,
      "grad_norm": 0.23046374320983887,
      "learning_rate": 0.00019447026863869606,
      "loss": 0.6723,
      "step": 1379
    },
    {
      "epoch": 2.7766599597585513,
      "grad_norm": 0.21999171376228333,
      "learning_rate": 0.00019446624408894257,
      "loss": 0.7469,
      "step": 1380
    },
    {
      "epoch": 2.778672032193159,
      "grad_norm": 0.21577945351600647,
      "learning_rate": 0.00019446221953918906,
      "loss": 0.721,
      "step": 1381
    },
    {
      "epoch": 2.780684104627767,
      "grad_norm": 0.23180745542049408,
      "learning_rate": 0.00019445819498943557,
      "loss": 0.8058,
      "step": 1382
    },
    {
      "epoch": 2.782696177062374,
      "grad_norm": 0.2297973930835724,
      "learning_rate": 0.00019445417043968205,
      "loss": 0.7508,
      "step": 1383
    },
    {
      "epoch": 2.784708249496982,
      "grad_norm": 0.2142428159713745,
      "learning_rate": 0.0001944501458899286,
      "loss": 0.6859,
      "step": 1384
    },
    {
      "epoch": 2.7867203219315897,
      "grad_norm": 0.21095602214336395,
      "learning_rate": 0.00019444612134017508,
      "loss": 0.7266,
      "step": 1385
    },
    {
      "epoch": 2.788732394366197,
      "grad_norm": 0.21569474041461945,
      "learning_rate": 0.0001944420967904216,
      "loss": 0.729,
      "step": 1386
    },
    {
      "epoch": 2.790744466800805,
      "grad_norm": 0.2225250005722046,
      "learning_rate": 0.00019443807224066808,
      "loss": 0.7322,
      "step": 1387
    },
    {
      "epoch": 2.7927565392354126,
      "grad_norm": 0.21737822890281677,
      "learning_rate": 0.0001944340476909146,
      "loss": 0.7176,
      "step": 1388
    },
    {
      "epoch": 2.79476861167002,
      "grad_norm": 0.2282789647579193,
      "learning_rate": 0.0001944300231411611,
      "loss": 0.7274,
      "step": 1389
    },
    {
      "epoch": 2.7967806841046277,
      "grad_norm": 0.2245185226202011,
      "learning_rate": 0.0001944259985914076,
      "loss": 0.7114,
      "step": 1390
    },
    {
      "epoch": 2.7987927565392354,
      "grad_norm": 0.20871195197105408,
      "learning_rate": 0.0001944219740416541,
      "loss": 0.7181,
      "step": 1391
    },
    {
      "epoch": 2.800804828973843,
      "grad_norm": 0.2334602326154709,
      "learning_rate": 0.0001944179494919006,
      "loss": 0.7427,
      "step": 1392
    },
    {
      "epoch": 2.802816901408451,
      "grad_norm": 0.22106532752513885,
      "learning_rate": 0.0001944139249421471,
      "loss": 0.67,
      "step": 1393
    },
    {
      "epoch": 2.8048289738430583,
      "grad_norm": 0.21452564001083374,
      "learning_rate": 0.0001944099003923936,
      "loss": 0.7017,
      "step": 1394
    },
    {
      "epoch": 2.806841046277666,
      "grad_norm": 0.21718469262123108,
      "learning_rate": 0.00019440587584264012,
      "loss": 0.7101,
      "step": 1395
    },
    {
      "epoch": 2.808853118712274,
      "grad_norm": 0.2161032259464264,
      "learning_rate": 0.0001944018512928866,
      "loss": 0.7081,
      "step": 1396
    },
    {
      "epoch": 2.810865191146881,
      "grad_norm": 0.22502003610134125,
      "learning_rate": 0.00019439782674313312,
      "loss": 0.7014,
      "step": 1397
    },
    {
      "epoch": 2.812877263581489,
      "grad_norm": 0.23443610966205597,
      "learning_rate": 0.0001943938021933796,
      "loss": 0.7551,
      "step": 1398
    },
    {
      "epoch": 2.8148893360160967,
      "grad_norm": 0.21269704401493073,
      "learning_rate": 0.00019438977764362614,
      "loss": 0.719,
      "step": 1399
    },
    {
      "epoch": 2.816901408450704,
      "grad_norm": 0.236770898103714,
      "learning_rate": 0.00019438575309387263,
      "loss": 0.7418,
      "step": 1400
    },
    {
      "epoch": 2.818913480885312,
      "grad_norm": 0.23499830067157745,
      "learning_rate": 0.00019438172854411914,
      "loss": 0.7211,
      "step": 1401
    },
    {
      "epoch": 2.8209255533199196,
      "grad_norm": 0.21285898983478546,
      "learning_rate": 0.00019437770399436562,
      "loss": 0.7335,
      "step": 1402
    },
    {
      "epoch": 2.822937625754527,
      "grad_norm": 0.2160852998495102,
      "learning_rate": 0.00019437367944461214,
      "loss": 0.7398,
      "step": 1403
    },
    {
      "epoch": 2.8249496981891347,
      "grad_norm": 0.22379526495933533,
      "learning_rate": 0.00019436965489485865,
      "loss": 0.6811,
      "step": 1404
    },
    {
      "epoch": 2.8269617706237424,
      "grad_norm": 0.22497913241386414,
      "learning_rate": 0.00019436563034510516,
      "loss": 0.7518,
      "step": 1405
    },
    {
      "epoch": 2.82897384305835,
      "grad_norm": 0.2520255744457245,
      "learning_rate": 0.00019436160579535165,
      "loss": 0.7401,
      "step": 1406
    },
    {
      "epoch": 2.830985915492958,
      "grad_norm": 0.22560161352157593,
      "learning_rate": 0.00019435758124559816,
      "loss": 0.7333,
      "step": 1407
    },
    {
      "epoch": 2.8329979879275653,
      "grad_norm": 0.21530015766620636,
      "learning_rate": 0.00019435355669584464,
      "loss": 0.7159,
      "step": 1408
    },
    {
      "epoch": 2.835010060362173,
      "grad_norm": 0.21982063353061676,
      "learning_rate": 0.00019434953214609118,
      "loss": 0.6927,
      "step": 1409
    },
    {
      "epoch": 2.837022132796781,
      "grad_norm": 0.2273745983839035,
      "learning_rate": 0.00019434550759633767,
      "loss": 0.7472,
      "step": 1410
    },
    {
      "epoch": 2.839034205231388,
      "grad_norm": 0.22533515095710754,
      "learning_rate": 0.00019434148304658418,
      "loss": 0.7173,
      "step": 1411
    },
    {
      "epoch": 2.841046277665996,
      "grad_norm": 0.21778221428394318,
      "learning_rate": 0.00019433745849683066,
      "loss": 0.7201,
      "step": 1412
    },
    {
      "epoch": 2.8430583501006037,
      "grad_norm": 0.21139328181743622,
      "learning_rate": 0.00019433343394707718,
      "loss": 0.7034,
      "step": 1413
    },
    {
      "epoch": 2.845070422535211,
      "grad_norm": 0.21264135837554932,
      "learning_rate": 0.0001943294093973237,
      "loss": 0.7484,
      "step": 1414
    },
    {
      "epoch": 2.847082494969819,
      "grad_norm": 0.21046246588230133,
      "learning_rate": 0.0001943253848475702,
      "loss": 0.7152,
      "step": 1415
    },
    {
      "epoch": 2.8490945674044266,
      "grad_norm": 0.208870530128479,
      "learning_rate": 0.00019432136029781669,
      "loss": 0.7548,
      "step": 1416
    },
    {
      "epoch": 2.8511066398390343,
      "grad_norm": 0.20827952027320862,
      "learning_rate": 0.0001943173357480632,
      "loss": 0.7115,
      "step": 1417
    },
    {
      "epoch": 2.853118712273642,
      "grad_norm": 0.22339506447315216,
      "learning_rate": 0.00019431331119830968,
      "loss": 0.7347,
      "step": 1418
    },
    {
      "epoch": 2.8551307847082494,
      "grad_norm": 0.21758340299129486,
      "learning_rate": 0.00019430928664855622,
      "loss": 0.6666,
      "step": 1419
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.21514892578125,
      "learning_rate": 0.0001943052620988027,
      "loss": 0.7384,
      "step": 1420
    },
    {
      "epoch": 2.859154929577465,
      "grad_norm": 0.21019931137561798,
      "learning_rate": 0.00019430123754904922,
      "loss": 0.7185,
      "step": 1421
    },
    {
      "epoch": 2.8611670020120723,
      "grad_norm": 0.21937526762485504,
      "learning_rate": 0.0001942972129992957,
      "loss": 0.7398,
      "step": 1422
    },
    {
      "epoch": 2.86317907444668,
      "grad_norm": 0.21996064484119415,
      "learning_rate": 0.00019429318844954222,
      "loss": 0.7034,
      "step": 1423
    },
    {
      "epoch": 2.865191146881288,
      "grad_norm": 0.2172410786151886,
      "learning_rate": 0.00019428916389978873,
      "loss": 0.7373,
      "step": 1424
    },
    {
      "epoch": 2.867203219315895,
      "grad_norm": 0.2278648018836975,
      "learning_rate": 0.00019428513935003521,
      "loss": 0.7243,
      "step": 1425
    },
    {
      "epoch": 2.869215291750503,
      "grad_norm": 0.21246767044067383,
      "learning_rate": 0.00019428111480028173,
      "loss": 0.6835,
      "step": 1426
    },
    {
      "epoch": 2.8712273641851107,
      "grad_norm": 0.21846234798431396,
      "learning_rate": 0.00019427709025052824,
      "loss": 0.7333,
      "step": 1427
    },
    {
      "epoch": 2.873239436619718,
      "grad_norm": 0.22595345973968506,
      "learning_rate": 0.00019427306570077472,
      "loss": 0.6986,
      "step": 1428
    },
    {
      "epoch": 2.875251509054326,
      "grad_norm": 0.21092256903648376,
      "learning_rate": 0.00019426904115102124,
      "loss": 0.7206,
      "step": 1429
    },
    {
      "epoch": 2.8772635814889336,
      "grad_norm": 0.21631361544132233,
      "learning_rate": 0.00019426501660126775,
      "loss": 0.6711,
      "step": 1430
    },
    {
      "epoch": 2.8792756539235413,
      "grad_norm": 0.2061423361301422,
      "learning_rate": 0.00019426099205151423,
      "loss": 0.7331,
      "step": 1431
    },
    {
      "epoch": 2.881287726358149,
      "grad_norm": 0.2306017279624939,
      "learning_rate": 0.00019425696750176075,
      "loss": 0.7244,
      "step": 1432
    },
    {
      "epoch": 2.8832997987927564,
      "grad_norm": 0.22381602227687836,
      "learning_rate": 0.00019425294295200723,
      "loss": 0.7216,
      "step": 1433
    },
    {
      "epoch": 2.885311871227364,
      "grad_norm": 0.21132892370224,
      "learning_rate": 0.00019424891840225377,
      "loss": 0.6969,
      "step": 1434
    },
    {
      "epoch": 2.887323943661972,
      "grad_norm": 0.22839681804180145,
      "learning_rate": 0.00019424489385250026,
      "loss": 0.7578,
      "step": 1435
    },
    {
      "epoch": 2.8893360160965793,
      "grad_norm": 0.2351103574037552,
      "learning_rate": 0.00019424086930274677,
      "loss": 0.7248,
      "step": 1436
    },
    {
      "epoch": 2.891348088531187,
      "grad_norm": 0.21571794152259827,
      "learning_rate": 0.00019423684475299325,
      "loss": 0.7024,
      "step": 1437
    },
    {
      "epoch": 2.893360160965795,
      "grad_norm": 0.2206469476222992,
      "learning_rate": 0.00019423282020323976,
      "loss": 0.7553,
      "step": 1438
    },
    {
      "epoch": 2.895372233400402,
      "grad_norm": 0.2570860683917999,
      "learning_rate": 0.00019422879565348628,
      "loss": 0.7008,
      "step": 1439
    },
    {
      "epoch": 2.89738430583501,
      "grad_norm": 0.24947449564933777,
      "learning_rate": 0.0001942247711037328,
      "loss": 0.7774,
      "step": 1440
    },
    {
      "epoch": 2.8993963782696177,
      "grad_norm": 0.2281508892774582,
      "learning_rate": 0.00019422074655397927,
      "loss": 0.7067,
      "step": 1441
    },
    {
      "epoch": 2.9014084507042255,
      "grad_norm": 0.22344401478767395,
      "learning_rate": 0.0001942167220042258,
      "loss": 0.7221,
      "step": 1442
    },
    {
      "epoch": 2.9034205231388333,
      "grad_norm": 0.21377891302108765,
      "learning_rate": 0.00019421269745447227,
      "loss": 0.7042,
      "step": 1443
    },
    {
      "epoch": 2.9054325955734406,
      "grad_norm": 0.2192000150680542,
      "learning_rate": 0.0001942086729047188,
      "loss": 0.7579,
      "step": 1444
    },
    {
      "epoch": 2.9074446680080483,
      "grad_norm": 0.22757887840270996,
      "learning_rate": 0.0001942046483549653,
      "loss": 0.7572,
      "step": 1445
    },
    {
      "epoch": 2.909456740442656,
      "grad_norm": 0.21139311790466309,
      "learning_rate": 0.0001942006238052118,
      "loss": 0.7282,
      "step": 1446
    },
    {
      "epoch": 2.9114688128772634,
      "grad_norm": 0.21711201965808868,
      "learning_rate": 0.0001941965992554583,
      "loss": 0.7267,
      "step": 1447
    },
    {
      "epoch": 2.913480885311871,
      "grad_norm": 0.22894568741321564,
      "learning_rate": 0.0001941925747057048,
      "loss": 0.7201,
      "step": 1448
    },
    {
      "epoch": 2.915492957746479,
      "grad_norm": 0.2169509083032608,
      "learning_rate": 0.00019418855015595132,
      "loss": 0.7188,
      "step": 1449
    },
    {
      "epoch": 2.9175050301810863,
      "grad_norm": 0.21396222710609436,
      "learning_rate": 0.00019418452560619783,
      "loss": 0.7316,
      "step": 1450
    },
    {
      "epoch": 2.919517102615694,
      "grad_norm": 0.21325118839740753,
      "learning_rate": 0.00019418050105644432,
      "loss": 0.7546,
      "step": 1451
    },
    {
      "epoch": 2.921529175050302,
      "grad_norm": 0.22020955383777618,
      "learning_rate": 0.00019417647650669083,
      "loss": 0.7212,
      "step": 1452
    },
    {
      "epoch": 2.9235412474849096,
      "grad_norm": 0.2194664478302002,
      "learning_rate": 0.0001941724519569373,
      "loss": 0.7338,
      "step": 1453
    },
    {
      "epoch": 2.925553319919517,
      "grad_norm": 0.21235331892967224,
      "learning_rate": 0.00019416842740718385,
      "loss": 0.7022,
      "step": 1454
    },
    {
      "epoch": 2.9275653923541247,
      "grad_norm": 0.2125311642885208,
      "learning_rate": 0.00019416440285743034,
      "loss": 0.7271,
      "step": 1455
    },
    {
      "epoch": 2.9295774647887325,
      "grad_norm": 0.22755461931228638,
      "learning_rate": 0.00019416037830767685,
      "loss": 0.7373,
      "step": 1456
    },
    {
      "epoch": 2.9315895372233403,
      "grad_norm": 0.21935437619686127,
      "learning_rate": 0.00019415635375792333,
      "loss": 0.7037,
      "step": 1457
    },
    {
      "epoch": 2.9336016096579476,
      "grad_norm": 0.21465788781642914,
      "learning_rate": 0.00019415232920816985,
      "loss": 0.7332,
      "step": 1458
    },
    {
      "epoch": 2.9356136820925554,
      "grad_norm": 0.2125743329524994,
      "learning_rate": 0.00019414830465841636,
      "loss": 0.744,
      "step": 1459
    },
    {
      "epoch": 2.937625754527163,
      "grad_norm": 0.2488010972738266,
      "learning_rate": 0.00019414428010866284,
      "loss": 0.7186,
      "step": 1460
    },
    {
      "epoch": 2.9396378269617705,
      "grad_norm": 0.22573551535606384,
      "learning_rate": 0.00019414025555890936,
      "loss": 0.7337,
      "step": 1461
    },
    {
      "epoch": 2.941649899396378,
      "grad_norm": 0.217473104596138,
      "learning_rate": 0.00019413623100915587,
      "loss": 0.7179,
      "step": 1462
    },
    {
      "epoch": 2.943661971830986,
      "grad_norm": 0.22589647769927979,
      "learning_rate": 0.00019413220645940235,
      "loss": 0.6789,
      "step": 1463
    },
    {
      "epoch": 2.9456740442655933,
      "grad_norm": 0.23169223964214325,
      "learning_rate": 0.00019412818190964887,
      "loss": 0.7179,
      "step": 1464
    },
    {
      "epoch": 2.947686116700201,
      "grad_norm": 0.23736709356307983,
      "learning_rate": 0.00019412415735989538,
      "loss": 0.7459,
      "step": 1465
    },
    {
      "epoch": 2.949698189134809,
      "grad_norm": 0.23110832273960114,
      "learning_rate": 0.00019412013281014186,
      "loss": 0.7466,
      "step": 1466
    },
    {
      "epoch": 2.9517102615694166,
      "grad_norm": 0.2379557490348816,
      "learning_rate": 0.00019411610826038838,
      "loss": 0.7439,
      "step": 1467
    },
    {
      "epoch": 2.9537223340040244,
      "grad_norm": 0.2262466847896576,
      "learning_rate": 0.00019411208371063486,
      "loss": 0.6812,
      "step": 1468
    },
    {
      "epoch": 2.9557344064386317,
      "grad_norm": 0.2236003279685974,
      "learning_rate": 0.0001941080591608814,
      "loss": 0.7087,
      "step": 1469
    },
    {
      "epoch": 2.9577464788732395,
      "grad_norm": 0.21755774319171906,
      "learning_rate": 0.00019410403461112788,
      "loss": 0.7171,
      "step": 1470
    },
    {
      "epoch": 2.9597585513078473,
      "grad_norm": 0.2253326028585434,
      "learning_rate": 0.0001941000100613744,
      "loss": 0.7203,
      "step": 1471
    },
    {
      "epoch": 2.9617706237424546,
      "grad_norm": 0.22633923590183258,
      "learning_rate": 0.00019409598551162088,
      "loss": 0.7423,
      "step": 1472
    },
    {
      "epoch": 2.9637826961770624,
      "grad_norm": 0.21477045118808746,
      "learning_rate": 0.0001940919609618674,
      "loss": 0.7649,
      "step": 1473
    },
    {
      "epoch": 2.96579476861167,
      "grad_norm": 0.21680766344070435,
      "learning_rate": 0.0001940879364121139,
      "loss": 0.659,
      "step": 1474
    },
    {
      "epoch": 2.9678068410462775,
      "grad_norm": 0.2165469378232956,
      "learning_rate": 0.00019408391186236042,
      "loss": 0.6796,
      "step": 1475
    },
    {
      "epoch": 2.9698189134808852,
      "grad_norm": 0.23519395291805267,
      "learning_rate": 0.0001940798873126069,
      "loss": 0.6759,
      "step": 1476
    },
    {
      "epoch": 2.971830985915493,
      "grad_norm": 0.21354959905147552,
      "learning_rate": 0.00019407586276285342,
      "loss": 0.7186,
      "step": 1477
    },
    {
      "epoch": 2.9738430583501008,
      "grad_norm": 0.2353467047214508,
      "learning_rate": 0.0001940718382130999,
      "loss": 0.7486,
      "step": 1478
    },
    {
      "epoch": 2.975855130784708,
      "grad_norm": 0.2232659012079239,
      "learning_rate": 0.00019406781366334644,
      "loss": 0.7374,
      "step": 1479
    },
    {
      "epoch": 2.977867203219316,
      "grad_norm": 0.2152753323316574,
      "learning_rate": 0.00019406378911359293,
      "loss": 0.722,
      "step": 1480
    },
    {
      "epoch": 2.9798792756539236,
      "grad_norm": 0.2118883728981018,
      "learning_rate": 0.00019405976456383944,
      "loss": 0.7555,
      "step": 1481
    },
    {
      "epoch": 2.9818913480885314,
      "grad_norm": 0.21467061340808868,
      "learning_rate": 0.00019405574001408592,
      "loss": 0.7411,
      "step": 1482
    },
    {
      "epoch": 2.9839034205231387,
      "grad_norm": 0.21578456461429596,
      "learning_rate": 0.00019405171546433244,
      "loss": 0.7403,
      "step": 1483
    },
    {
      "epoch": 2.9859154929577465,
      "grad_norm": 0.20994828641414642,
      "learning_rate": 0.00019404769091457895,
      "loss": 0.684,
      "step": 1484
    },
    {
      "epoch": 2.9879275653923543,
      "grad_norm": 0.2375311553478241,
      "learning_rate": 0.00019404366636482546,
      "loss": 0.7322,
      "step": 1485
    },
    {
      "epoch": 2.9899396378269616,
      "grad_norm": 0.2262299358844757,
      "learning_rate": 0.00019403964181507194,
      "loss": 0.7263,
      "step": 1486
    },
    {
      "epoch": 2.9919517102615694,
      "grad_norm": 0.22886845469474792,
      "learning_rate": 0.00019403561726531846,
      "loss": 0.716,
      "step": 1487
    },
    {
      "epoch": 2.993963782696177,
      "grad_norm": 0.2280508279800415,
      "learning_rate": 0.00019403159271556494,
      "loss": 0.7324,
      "step": 1488
    },
    {
      "epoch": 2.9959758551307845,
      "grad_norm": 0.226213276386261,
      "learning_rate": 0.00019402756816581148,
      "loss": 0.6864,
      "step": 1489
    },
    {
      "epoch": 2.9979879275653922,
      "grad_norm": 0.2188388854265213,
      "learning_rate": 0.00019402354361605797,
      "loss": 0.7506,
      "step": 1490
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.22364717721939087,
      "learning_rate": 0.00019401951906630448,
      "loss": 0.7302,
      "step": 1491
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.7322601675987244,
      "eval_runtime": 49.8888,
      "eval_samples_per_second": 19.884,
      "eval_steps_per_second": 2.486,
      "step": 1491
    },
    {
      "epoch": 3.0020120724346078,
      "grad_norm": 0.2288115918636322,
      "learning_rate": 0.00019401549451655096,
      "loss": 0.6673,
      "step": 1492
    },
    {
      "epoch": 3.004024144869215,
      "grad_norm": 0.23095141351222992,
      "learning_rate": 0.00019401146996679748,
      "loss": 0.7008,
      "step": 1493
    },
    {
      "epoch": 3.006036217303823,
      "grad_norm": 0.21420244872570038,
      "learning_rate": 0.000194007445417044,
      "loss": 0.6862,
      "step": 1494
    },
    {
      "epoch": 3.0080482897384306,
      "grad_norm": 0.22422319650650024,
      "learning_rate": 0.00019400342086729047,
      "loss": 0.6548,
      "step": 1495
    },
    {
      "epoch": 3.0100603621730384,
      "grad_norm": 0.22187627851963043,
      "learning_rate": 0.00019399939631753699,
      "loss": 0.6922,
      "step": 1496
    },
    {
      "epoch": 3.0120724346076457,
      "grad_norm": 0.2215452343225479,
      "learning_rate": 0.0001939953717677835,
      "loss": 0.6813,
      "step": 1497
    },
    {
      "epoch": 3.0140845070422535,
      "grad_norm": 0.23705746233463287,
      "learning_rate": 0.00019399134721802998,
      "loss": 0.7251,
      "step": 1498
    },
    {
      "epoch": 3.0160965794768613,
      "grad_norm": 0.2156999111175537,
      "learning_rate": 0.0001939873226682765,
      "loss": 0.671,
      "step": 1499
    },
    {
      "epoch": 3.0181086519114686,
      "grad_norm": 0.23607784509658813,
      "learning_rate": 0.000193983298118523,
      "loss": 0.7096,
      "step": 1500
    },
    {
      "epoch": 3.0201207243460764,
      "grad_norm": 0.21470679342746735,
      "learning_rate": 0.0001939792735687695,
      "loss": 0.6578,
      "step": 1501
    },
    {
      "epoch": 3.022132796780684,
      "grad_norm": 0.22877804934978485,
      "learning_rate": 0.000193975249019016,
      "loss": 0.6789,
      "step": 1502
    },
    {
      "epoch": 3.024144869215292,
      "grad_norm": 0.23345847427845,
      "learning_rate": 0.0001939712244692625,
      "loss": 0.7088,
      "step": 1503
    },
    {
      "epoch": 3.0261569416498992,
      "grad_norm": 0.2136765569448471,
      "learning_rate": 0.00019396719991950903,
      "loss": 0.6771,
      "step": 1504
    },
    {
      "epoch": 3.028169014084507,
      "grad_norm": 0.25079345703125,
      "learning_rate": 0.00019396317536975551,
      "loss": 0.6864,
      "step": 1505
    },
    {
      "epoch": 3.0301810865191148,
      "grad_norm": 0.23510105907917023,
      "learning_rate": 0.00019395915082000203,
      "loss": 0.6939,
      "step": 1506
    },
    {
      "epoch": 3.0321931589537225,
      "grad_norm": 0.21597059071063995,
      "learning_rate": 0.0001939551262702485,
      "loss": 0.677,
      "step": 1507
    },
    {
      "epoch": 3.03420523138833,
      "grad_norm": 0.23120160400867462,
      "learning_rate": 0.00019395110172049502,
      "loss": 0.6864,
      "step": 1508
    },
    {
      "epoch": 3.0362173038229376,
      "grad_norm": 0.24223916232585907,
      "learning_rate": 0.00019394707717074154,
      "loss": 0.6999,
      "step": 1509
    },
    {
      "epoch": 3.0382293762575454,
      "grad_norm": 0.2331368774175644,
      "learning_rate": 0.00019394305262098805,
      "loss": 0.6746,
      "step": 1510
    },
    {
      "epoch": 3.0402414486921527,
      "grad_norm": 0.22362633049488068,
      "learning_rate": 0.00019393902807123453,
      "loss": 0.677,
      "step": 1511
    },
    {
      "epoch": 3.0422535211267605,
      "grad_norm": 0.2352025955915451,
      "learning_rate": 0.00019393500352148105,
      "loss": 0.6952,
      "step": 1512
    },
    {
      "epoch": 3.0442655935613683,
      "grad_norm": 0.23838204145431519,
      "learning_rate": 0.00019393097897172753,
      "loss": 0.6761,
      "step": 1513
    },
    {
      "epoch": 3.046277665995976,
      "grad_norm": 0.2420731633901596,
      "learning_rate": 0.00019392695442197407,
      "loss": 0.7254,
      "step": 1514
    },
    {
      "epoch": 3.0482897384305834,
      "grad_norm": 0.2501656413078308,
      "learning_rate": 0.00019392292987222056,
      "loss": 0.7225,
      "step": 1515
    },
    {
      "epoch": 3.050301810865191,
      "grad_norm": 0.241062730550766,
      "learning_rate": 0.00019391890532246707,
      "loss": 0.6835,
      "step": 1516
    },
    {
      "epoch": 3.052313883299799,
      "grad_norm": 0.25063711404800415,
      "learning_rate": 0.00019391488077271355,
      "loss": 0.683,
      "step": 1517
    },
    {
      "epoch": 3.0543259557344062,
      "grad_norm": 0.22450530529022217,
      "learning_rate": 0.00019391085622296006,
      "loss": 0.6738,
      "step": 1518
    },
    {
      "epoch": 3.056338028169014,
      "grad_norm": 0.23053570091724396,
      "learning_rate": 0.00019390683167320658,
      "loss": 0.6723,
      "step": 1519
    },
    {
      "epoch": 3.058350100603622,
      "grad_norm": 0.22707599401474,
      "learning_rate": 0.0001939028071234531,
      "loss": 0.6893,
      "step": 1520
    },
    {
      "epoch": 3.0603621730382295,
      "grad_norm": 0.24449801445007324,
      "learning_rate": 0.00019389878257369957,
      "loss": 0.7536,
      "step": 1521
    },
    {
      "epoch": 3.062374245472837,
      "grad_norm": 0.22409170866012573,
      "learning_rate": 0.00019389475802394609,
      "loss": 0.6838,
      "step": 1522
    },
    {
      "epoch": 3.0643863179074446,
      "grad_norm": 0.24016624689102173,
      "learning_rate": 0.00019389073347419257,
      "loss": 0.6686,
      "step": 1523
    },
    {
      "epoch": 3.0663983903420524,
      "grad_norm": 0.2292516976594925,
      "learning_rate": 0.0001938867089244391,
      "loss": 0.6786,
      "step": 1524
    },
    {
      "epoch": 3.0684104627766597,
      "grad_norm": 0.2407192885875702,
      "learning_rate": 0.0001938826843746856,
      "loss": 0.6789,
      "step": 1525
    },
    {
      "epoch": 3.0704225352112675,
      "grad_norm": 0.23031266033649445,
      "learning_rate": 0.0001938786598249321,
      "loss": 0.6542,
      "step": 1526
    },
    {
      "epoch": 3.0724346076458753,
      "grad_norm": 0.2321528196334839,
      "learning_rate": 0.0001938746352751786,
      "loss": 0.6692,
      "step": 1527
    },
    {
      "epoch": 3.074446680080483,
      "grad_norm": 0.21714365482330322,
      "learning_rate": 0.0001938706107254251,
      "loss": 0.6638,
      "step": 1528
    },
    {
      "epoch": 3.0764587525150904,
      "grad_norm": 0.2339906394481659,
      "learning_rate": 0.00019386658617567162,
      "loss": 0.6827,
      "step": 1529
    },
    {
      "epoch": 3.078470824949698,
      "grad_norm": 0.2329816371202469,
      "learning_rate": 0.0001938625616259181,
      "loss": 0.6933,
      "step": 1530
    },
    {
      "epoch": 3.080482897384306,
      "grad_norm": 0.22818970680236816,
      "learning_rate": 0.00019385853707616462,
      "loss": 0.6864,
      "step": 1531
    },
    {
      "epoch": 3.0824949698189137,
      "grad_norm": 0.2173224836587906,
      "learning_rate": 0.00019385451252641113,
      "loss": 0.6127,
      "step": 1532
    },
    {
      "epoch": 3.084507042253521,
      "grad_norm": 0.2379104644060135,
      "learning_rate": 0.0001938504879766576,
      "loss": 0.6986,
      "step": 1533
    },
    {
      "epoch": 3.086519114688129,
      "grad_norm": 0.24372950196266174,
      "learning_rate": 0.00019384646342690412,
      "loss": 0.6936,
      "step": 1534
    },
    {
      "epoch": 3.0885311871227366,
      "grad_norm": 0.23294900357723236,
      "learning_rate": 0.00019384243887715064,
      "loss": 0.678,
      "step": 1535
    },
    {
      "epoch": 3.090543259557344,
      "grad_norm": 0.2308502048254013,
      "learning_rate": 0.00019383841432739712,
      "loss": 0.7345,
      "step": 1536
    },
    {
      "epoch": 3.0925553319919517,
      "grad_norm": 0.2390635460615158,
      "learning_rate": 0.00019383438977764363,
      "loss": 0.7129,
      "step": 1537
    },
    {
      "epoch": 3.0945674044265594,
      "grad_norm": 0.22401224076747894,
      "learning_rate": 0.00019383036522789012,
      "loss": 0.6781,
      "step": 1538
    },
    {
      "epoch": 3.096579476861167,
      "grad_norm": 0.2362419217824936,
      "learning_rate": 0.00019382634067813666,
      "loss": 0.7083,
      "step": 1539
    },
    {
      "epoch": 3.0985915492957745,
      "grad_norm": 0.2453593760728836,
      "learning_rate": 0.00019382231612838314,
      "loss": 0.6932,
      "step": 1540
    },
    {
      "epoch": 3.1006036217303823,
      "grad_norm": 0.23825597763061523,
      "learning_rate": 0.00019381829157862966,
      "loss": 0.6988,
      "step": 1541
    },
    {
      "epoch": 3.10261569416499,
      "grad_norm": 0.2439289093017578,
      "learning_rate": 0.00019381426702887614,
      "loss": 0.7232,
      "step": 1542
    },
    {
      "epoch": 3.1046277665995974,
      "grad_norm": 0.23932553827762604,
      "learning_rate": 0.00019381024247912265,
      "loss": 0.704,
      "step": 1543
    },
    {
      "epoch": 3.106639839034205,
      "grad_norm": 0.22808237373828888,
      "learning_rate": 0.00019380621792936917,
      "loss": 0.6836,
      "step": 1544
    },
    {
      "epoch": 3.108651911468813,
      "grad_norm": 0.2501825988292694,
      "learning_rate": 0.00019380219337961568,
      "loss": 0.743,
      "step": 1545
    },
    {
      "epoch": 3.1106639839034207,
      "grad_norm": 0.24106761813163757,
      "learning_rate": 0.00019379816882986216,
      "loss": 0.6826,
      "step": 1546
    },
    {
      "epoch": 3.112676056338028,
      "grad_norm": 0.23269017040729523,
      "learning_rate": 0.00019379414428010867,
      "loss": 0.6938,
      "step": 1547
    },
    {
      "epoch": 3.114688128772636,
      "grad_norm": 0.23573511838912964,
      "learning_rate": 0.00019379011973035516,
      "loss": 0.6787,
      "step": 1548
    },
    {
      "epoch": 3.1167002012072436,
      "grad_norm": 0.241473987698555,
      "learning_rate": 0.0001937860951806017,
      "loss": 0.7307,
      "step": 1549
    },
    {
      "epoch": 3.118712273641851,
      "grad_norm": 0.24507950246334076,
      "learning_rate": 0.00019378207063084818,
      "loss": 0.7383,
      "step": 1550
    },
    {
      "epoch": 3.1207243460764587,
      "grad_norm": 0.2357393503189087,
      "learning_rate": 0.0001937780460810947,
      "loss": 0.7156,
      "step": 1551
    },
    {
      "epoch": 3.1227364185110664,
      "grad_norm": 0.2341334968805313,
      "learning_rate": 0.00019377402153134118,
      "loss": 0.6909,
      "step": 1552
    },
    {
      "epoch": 3.124748490945674,
      "grad_norm": 0.2235124409198761,
      "learning_rate": 0.0001937699969815877,
      "loss": 0.6916,
      "step": 1553
    },
    {
      "epoch": 3.1267605633802815,
      "grad_norm": 0.23351562023162842,
      "learning_rate": 0.0001937659724318342,
      "loss": 0.7112,
      "step": 1554
    },
    {
      "epoch": 3.1287726358148893,
      "grad_norm": 0.2427181750535965,
      "learning_rate": 0.00019376194788208072,
      "loss": 0.7235,
      "step": 1555
    },
    {
      "epoch": 3.130784708249497,
      "grad_norm": 0.2347429394721985,
      "learning_rate": 0.0001937579233323272,
      "loss": 0.6697,
      "step": 1556
    },
    {
      "epoch": 3.132796780684105,
      "grad_norm": 0.23682181537151337,
      "learning_rate": 0.00019375389878257372,
      "loss": 0.6547,
      "step": 1557
    },
    {
      "epoch": 3.134808853118712,
      "grad_norm": 0.23529097437858582,
      "learning_rate": 0.0001937498742328202,
      "loss": 0.7119,
      "step": 1558
    },
    {
      "epoch": 3.13682092555332,
      "grad_norm": 0.22314417362213135,
      "learning_rate": 0.00019374584968306674,
      "loss": 0.6751,
      "step": 1559
    },
    {
      "epoch": 3.1388329979879277,
      "grad_norm": 0.23784451186656952,
      "learning_rate": 0.00019374182513331323,
      "loss": 0.6982,
      "step": 1560
    },
    {
      "epoch": 3.140845070422535,
      "grad_norm": 0.25025174021720886,
      "learning_rate": 0.00019373780058355974,
      "loss": 0.6917,
      "step": 1561
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.23013122379779816,
      "learning_rate": 0.00019373377603380622,
      "loss": 0.7013,
      "step": 1562
    },
    {
      "epoch": 3.1448692152917506,
      "grad_norm": 0.22727376222610474,
      "learning_rate": 0.00019372975148405273,
      "loss": 0.6675,
      "step": 1563
    },
    {
      "epoch": 3.1468812877263583,
      "grad_norm": 0.2482883632183075,
      "learning_rate": 0.00019372572693429925,
      "loss": 0.7328,
      "step": 1564
    },
    {
      "epoch": 3.1488933601609657,
      "grad_norm": 0.23325049877166748,
      "learning_rate": 0.00019372170238454573,
      "loss": 0.6749,
      "step": 1565
    },
    {
      "epoch": 3.1509054325955734,
      "grad_norm": 0.2367517650127411,
      "learning_rate": 0.00019371767783479224,
      "loss": 0.7065,
      "step": 1566
    },
    {
      "epoch": 3.152917505030181,
      "grad_norm": 0.23786009848117828,
      "learning_rate": 0.00019371365328503873,
      "loss": 0.7094,
      "step": 1567
    },
    {
      "epoch": 3.1549295774647885,
      "grad_norm": 0.239426389336586,
      "learning_rate": 0.00019370962873528524,
      "loss": 0.7732,
      "step": 1568
    },
    {
      "epoch": 3.1569416498993963,
      "grad_norm": 0.22523419559001923,
      "learning_rate": 0.00019370560418553175,
      "loss": 0.6834,
      "step": 1569
    },
    {
      "epoch": 3.158953722334004,
      "grad_norm": 0.24072127044200897,
      "learning_rate": 0.00019370157963577827,
      "loss": 0.6782,
      "step": 1570
    },
    {
      "epoch": 3.160965794768612,
      "grad_norm": 0.23402808606624603,
      "learning_rate": 0.00019369755508602475,
      "loss": 0.6806,
      "step": 1571
    },
    {
      "epoch": 3.162977867203219,
      "grad_norm": 0.2326715737581253,
      "learning_rate": 0.00019369353053627126,
      "loss": 0.6881,
      "step": 1572
    },
    {
      "epoch": 3.164989939637827,
      "grad_norm": 0.23219874501228333,
      "learning_rate": 0.00019368950598651775,
      "loss": 0.6599,
      "step": 1573
    },
    {
      "epoch": 3.1670020120724347,
      "grad_norm": 0.2232244908809662,
      "learning_rate": 0.0001936854814367643,
      "loss": 0.688,
      "step": 1574
    },
    {
      "epoch": 3.169014084507042,
      "grad_norm": 0.24831262230873108,
      "learning_rate": 0.00019368145688701077,
      "loss": 0.6647,
      "step": 1575
    },
    {
      "epoch": 3.17102615694165,
      "grad_norm": 0.2334568053483963,
      "learning_rate": 0.00019367743233725729,
      "loss": 0.6867,
      "step": 1576
    },
    {
      "epoch": 3.1730382293762576,
      "grad_norm": 0.23272565007209778,
      "learning_rate": 0.00019367340778750377,
      "loss": 0.7066,
      "step": 1577
    },
    {
      "epoch": 3.1750503018108653,
      "grad_norm": 0.24975252151489258,
      "learning_rate": 0.00019366938323775028,
      "loss": 0.7325,
      "step": 1578
    },
    {
      "epoch": 3.1770623742454727,
      "grad_norm": 0.23822475969791412,
      "learning_rate": 0.0001936653586879968,
      "loss": 0.6889,
      "step": 1579
    },
    {
      "epoch": 3.1790744466800804,
      "grad_norm": 0.24311676621437073,
      "learning_rate": 0.0001936613341382433,
      "loss": 0.6556,
      "step": 1580
    },
    {
      "epoch": 3.181086519114688,
      "grad_norm": 0.23737850785255432,
      "learning_rate": 0.0001936573095884898,
      "loss": 0.6626,
      "step": 1581
    },
    {
      "epoch": 3.183098591549296,
      "grad_norm": 0.24144905805587769,
      "learning_rate": 0.0001936532850387363,
      "loss": 0.7132,
      "step": 1582
    },
    {
      "epoch": 3.1851106639839033,
      "grad_norm": 0.24398837983608246,
      "learning_rate": 0.0001936492604889828,
      "loss": 0.7083,
      "step": 1583
    },
    {
      "epoch": 3.187122736418511,
      "grad_norm": 0.23310869932174683,
      "learning_rate": 0.0001936452359392293,
      "loss": 0.6685,
      "step": 1584
    },
    {
      "epoch": 3.189134808853119,
      "grad_norm": 0.23013900220394135,
      "learning_rate": 0.00019364121138947581,
      "loss": 0.7037,
      "step": 1585
    },
    {
      "epoch": 3.191146881287726,
      "grad_norm": 0.24031099677085876,
      "learning_rate": 0.00019363718683972233,
      "loss": 0.7307,
      "step": 1586
    },
    {
      "epoch": 3.193158953722334,
      "grad_norm": 0.22480575740337372,
      "learning_rate": 0.0001936331622899688,
      "loss": 0.6485,
      "step": 1587
    },
    {
      "epoch": 3.1951710261569417,
      "grad_norm": 0.23206421732902527,
      "learning_rate": 0.00019362913774021532,
      "loss": 0.7075,
      "step": 1588
    },
    {
      "epoch": 3.1971830985915495,
      "grad_norm": 0.22770735621452332,
      "learning_rate": 0.0001936251131904618,
      "loss": 0.6858,
      "step": 1589
    },
    {
      "epoch": 3.199195171026157,
      "grad_norm": 0.23083961009979248,
      "learning_rate": 0.00019362108864070835,
      "loss": 0.6459,
      "step": 1590
    },
    {
      "epoch": 3.2012072434607646,
      "grad_norm": 0.22901275753974915,
      "learning_rate": 0.00019361706409095483,
      "loss": 0.6984,
      "step": 1591
    },
    {
      "epoch": 3.2032193158953723,
      "grad_norm": 0.22130410373210907,
      "learning_rate": 0.00019361303954120135,
      "loss": 0.6717,
      "step": 1592
    },
    {
      "epoch": 3.20523138832998,
      "grad_norm": 0.24415892362594604,
      "learning_rate": 0.00019360901499144783,
      "loss": 0.6872,
      "step": 1593
    },
    {
      "epoch": 3.2072434607645874,
      "grad_norm": 0.22701625525951385,
      "learning_rate": 0.00019360499044169434,
      "loss": 0.6932,
      "step": 1594
    },
    {
      "epoch": 3.209255533199195,
      "grad_norm": 0.2380877435207367,
      "learning_rate": 0.00019360096589194085,
      "loss": 0.708,
      "step": 1595
    },
    {
      "epoch": 3.211267605633803,
      "grad_norm": 0.2361469268798828,
      "learning_rate": 0.00019359694134218737,
      "loss": 0.6736,
      "step": 1596
    },
    {
      "epoch": 3.2132796780684103,
      "grad_norm": 0.24890758097171783,
      "learning_rate": 0.00019359291679243385,
      "loss": 0.7032,
      "step": 1597
    },
    {
      "epoch": 3.215291750503018,
      "grad_norm": 0.2476334422826767,
      "learning_rate": 0.00019358889224268036,
      "loss": 0.6916,
      "step": 1598
    },
    {
      "epoch": 3.217303822937626,
      "grad_norm": 0.23517827689647675,
      "learning_rate": 0.00019358486769292685,
      "loss": 0.722,
      "step": 1599
    },
    {
      "epoch": 3.219315895372233,
      "grad_norm": 0.22720196843147278,
      "learning_rate": 0.00019358084314317336,
      "loss": 0.7084,
      "step": 1600
    },
    {
      "epoch": 3.221327967806841,
      "grad_norm": 0.22522392868995667,
      "learning_rate": 0.00019357681859341987,
      "loss": 0.654,
      "step": 1601
    },
    {
      "epoch": 3.2233400402414487,
      "grad_norm": 0.23076897859573364,
      "learning_rate": 0.00019357279404366636,
      "loss": 0.716,
      "step": 1602
    },
    {
      "epoch": 3.2253521126760565,
      "grad_norm": 0.25559869408607483,
      "learning_rate": 0.00019356876949391287,
      "loss": 0.7302,
      "step": 1603
    },
    {
      "epoch": 3.227364185110664,
      "grad_norm": 0.24436190724372864,
      "learning_rate": 0.00019356474494415938,
      "loss": 0.706,
      "step": 1604
    },
    {
      "epoch": 3.2293762575452716,
      "grad_norm": 0.24444495141506195,
      "learning_rate": 0.0001935607203944059,
      "loss": 0.6951,
      "step": 1605
    },
    {
      "epoch": 3.2313883299798793,
      "grad_norm": 0.2426472157239914,
      "learning_rate": 0.00019355669584465238,
      "loss": 0.7259,
      "step": 1606
    },
    {
      "epoch": 3.233400402414487,
      "grad_norm": 0.2392481565475464,
      "learning_rate": 0.0001935526712948989,
      "loss": 0.7027,
      "step": 1607
    },
    {
      "epoch": 3.2354124748490944,
      "grad_norm": 0.23010025918483734,
      "learning_rate": 0.00019354864674514538,
      "loss": 0.6719,
      "step": 1608
    },
    {
      "epoch": 3.237424547283702,
      "grad_norm": 0.22361761331558228,
      "learning_rate": 0.0001935446221953919,
      "loss": 0.7326,
      "step": 1609
    },
    {
      "epoch": 3.23943661971831,
      "grad_norm": 0.22662372887134552,
      "learning_rate": 0.0001935405976456384,
      "loss": 0.6985,
      "step": 1610
    },
    {
      "epoch": 3.2414486921529173,
      "grad_norm": 0.2359430193901062,
      "learning_rate": 0.00019353657309588491,
      "loss": 0.7231,
      "step": 1611
    },
    {
      "epoch": 3.243460764587525,
      "grad_norm": 0.21573294699192047,
      "learning_rate": 0.0001935325485461314,
      "loss": 0.6861,
      "step": 1612
    },
    {
      "epoch": 3.245472837022133,
      "grad_norm": 0.23242530226707458,
      "learning_rate": 0.0001935285239963779,
      "loss": 0.6856,
      "step": 1613
    },
    {
      "epoch": 3.2474849094567406,
      "grad_norm": 0.23349560797214508,
      "learning_rate": 0.0001935244994466244,
      "loss": 0.6578,
      "step": 1614
    },
    {
      "epoch": 3.249496981891348,
      "grad_norm": 0.24682140350341797,
      "learning_rate": 0.00019352047489687094,
      "loss": 0.6812,
      "step": 1615
    },
    {
      "epoch": 3.2515090543259557,
      "grad_norm": 0.2537659704685211,
      "learning_rate": 0.00019351645034711742,
      "loss": 0.7075,
      "step": 1616
    },
    {
      "epoch": 3.2535211267605635,
      "grad_norm": 0.2556670010089874,
      "learning_rate": 0.00019351242579736393,
      "loss": 0.7102,
      "step": 1617
    },
    {
      "epoch": 3.2555331991951713,
      "grad_norm": 0.2476445883512497,
      "learning_rate": 0.00019350840124761042,
      "loss": 0.6808,
      "step": 1618
    },
    {
      "epoch": 3.2575452716297786,
      "grad_norm": 0.2270820140838623,
      "learning_rate": 0.00019350437669785693,
      "loss": 0.6685,
      "step": 1619
    },
    {
      "epoch": 3.2595573440643864,
      "grad_norm": 0.22584518790245056,
      "learning_rate": 0.00019350035214810344,
      "loss": 0.6902,
      "step": 1620
    },
    {
      "epoch": 3.261569416498994,
      "grad_norm": 0.23419739305973053,
      "learning_rate": 0.00019349632759834996,
      "loss": 0.6891,
      "step": 1621
    },
    {
      "epoch": 3.2635814889336014,
      "grad_norm": 0.22486180067062378,
      "learning_rate": 0.00019349230304859644,
      "loss": 0.6961,
      "step": 1622
    },
    {
      "epoch": 3.265593561368209,
      "grad_norm": 0.22825174033641815,
      "learning_rate": 0.00019348827849884295,
      "loss": 0.6795,
      "step": 1623
    },
    {
      "epoch": 3.267605633802817,
      "grad_norm": 0.22938846051692963,
      "learning_rate": 0.00019348425394908944,
      "loss": 0.71,
      "step": 1624
    },
    {
      "epoch": 3.2696177062374243,
      "grad_norm": 0.2365417331457138,
      "learning_rate": 0.00019348022939933598,
      "loss": 0.7079,
      "step": 1625
    },
    {
      "epoch": 3.271629778672032,
      "grad_norm": 0.2621385455131531,
      "learning_rate": 0.00019347620484958246,
      "loss": 0.7395,
      "step": 1626
    },
    {
      "epoch": 3.27364185110664,
      "grad_norm": 0.24521179497241974,
      "learning_rate": 0.00019347218029982897,
      "loss": 0.6676,
      "step": 1627
    },
    {
      "epoch": 3.2756539235412476,
      "grad_norm": 0.23920822143554688,
      "learning_rate": 0.00019346815575007546,
      "loss": 0.6921,
      "step": 1628
    },
    {
      "epoch": 3.277665995975855,
      "grad_norm": 0.26846086978912354,
      "learning_rate": 0.00019346413120032197,
      "loss": 0.688,
      "step": 1629
    },
    {
      "epoch": 3.2796780684104627,
      "grad_norm": 0.26285499334335327,
      "learning_rate": 0.00019346010665056848,
      "loss": 0.7143,
      "step": 1630
    },
    {
      "epoch": 3.2816901408450705,
      "grad_norm": 0.2303270846605301,
      "learning_rate": 0.000193456082100815,
      "loss": 0.7213,
      "step": 1631
    },
    {
      "epoch": 3.2837022132796783,
      "grad_norm": 0.2427266538143158,
      "learning_rate": 0.00019345205755106148,
      "loss": 0.687,
      "step": 1632
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.2456742227077484,
      "learning_rate": 0.000193448033001308,
      "loss": 0.6728,
      "step": 1633
    },
    {
      "epoch": 3.2877263581488934,
      "grad_norm": 0.24202226102352142,
      "learning_rate": 0.00019344400845155448,
      "loss": 0.7243,
      "step": 1634
    },
    {
      "epoch": 3.289738430583501,
      "grad_norm": 0.23919853568077087,
      "learning_rate": 0.000193439983901801,
      "loss": 0.6868,
      "step": 1635
    },
    {
      "epoch": 3.2917505030181085,
      "grad_norm": 0.24643924832344055,
      "learning_rate": 0.0001934359593520475,
      "loss": 0.6953,
      "step": 1636
    },
    {
      "epoch": 3.2937625754527162,
      "grad_norm": 0.23467685282230377,
      "learning_rate": 0.000193431934802294,
      "loss": 0.7503,
      "step": 1637
    },
    {
      "epoch": 3.295774647887324,
      "grad_norm": 0.22755925357341766,
      "learning_rate": 0.0001934279102525405,
      "loss": 0.688,
      "step": 1638
    },
    {
      "epoch": 3.2977867203219318,
      "grad_norm": 0.23792873322963715,
      "learning_rate": 0.000193423885702787,
      "loss": 0.6866,
      "step": 1639
    },
    {
      "epoch": 3.299798792756539,
      "grad_norm": 0.2377803921699524,
      "learning_rate": 0.00019341986115303353,
      "loss": 0.6688,
      "step": 1640
    },
    {
      "epoch": 3.301810865191147,
      "grad_norm": 0.23498551547527313,
      "learning_rate": 0.00019341583660328,
      "loss": 0.6914,
      "step": 1641
    },
    {
      "epoch": 3.3038229376257546,
      "grad_norm": 0.23545697331428528,
      "learning_rate": 0.00019341181205352652,
      "loss": 0.7011,
      "step": 1642
    },
    {
      "epoch": 3.3058350100603624,
      "grad_norm": 0.2351248860359192,
      "learning_rate": 0.000193407787503773,
      "loss": 0.6337,
      "step": 1643
    },
    {
      "epoch": 3.3078470824949697,
      "grad_norm": 0.2417202591896057,
      "learning_rate": 0.00019340376295401952,
      "loss": 0.7413,
      "step": 1644
    },
    {
      "epoch": 3.3098591549295775,
      "grad_norm": 0.23126135766506195,
      "learning_rate": 0.00019339973840426603,
      "loss": 0.7328,
      "step": 1645
    },
    {
      "epoch": 3.3118712273641853,
      "grad_norm": 0.23581936955451965,
      "learning_rate": 0.00019339571385451254,
      "loss": 0.679,
      "step": 1646
    },
    {
      "epoch": 3.3138832997987926,
      "grad_norm": 0.2332908809185028,
      "learning_rate": 0.00019339168930475903,
      "loss": 0.6683,
      "step": 1647
    },
    {
      "epoch": 3.3158953722334004,
      "grad_norm": 0.24273304641246796,
      "learning_rate": 0.00019338766475500554,
      "loss": 0.6721,
      "step": 1648
    },
    {
      "epoch": 3.317907444668008,
      "grad_norm": 0.23548336327075958,
      "learning_rate": 0.00019338364020525203,
      "loss": 0.6913,
      "step": 1649
    },
    {
      "epoch": 3.3199195171026155,
      "grad_norm": 0.23314456641674042,
      "learning_rate": 0.00019337961565549857,
      "loss": 0.6948,
      "step": 1650
    },
    {
      "epoch": 3.3219315895372232,
      "grad_norm": 0.23316319286823273,
      "learning_rate": 0.00019337559110574505,
      "loss": 0.6885,
      "step": 1651
    },
    {
      "epoch": 3.323943661971831,
      "grad_norm": 0.2430439442396164,
      "learning_rate": 0.00019337156655599156,
      "loss": 0.6918,
      "step": 1652
    },
    {
      "epoch": 3.3259557344064388,
      "grad_norm": 0.24075660109519958,
      "learning_rate": 0.00019336754200623805,
      "loss": 0.7112,
      "step": 1653
    },
    {
      "epoch": 3.327967806841046,
      "grad_norm": 0.24451972544193268,
      "learning_rate": 0.00019336351745648456,
      "loss": 0.7047,
      "step": 1654
    },
    {
      "epoch": 3.329979879275654,
      "grad_norm": 0.23005972802639008,
      "learning_rate": 0.00019335949290673107,
      "loss": 0.6776,
      "step": 1655
    },
    {
      "epoch": 3.3319919517102616,
      "grad_norm": 0.2299434393644333,
      "learning_rate": 0.00019335546835697759,
      "loss": 0.6684,
      "step": 1656
    },
    {
      "epoch": 3.3340040241448694,
      "grad_norm": 0.23448602855205536,
      "learning_rate": 0.00019335144380722407,
      "loss": 0.7542,
      "step": 1657
    },
    {
      "epoch": 3.3360160965794767,
      "grad_norm": 0.23163555562496185,
      "learning_rate": 0.00019334741925747058,
      "loss": 0.6858,
      "step": 1658
    },
    {
      "epoch": 3.3380281690140845,
      "grad_norm": 0.22920002043247223,
      "learning_rate": 0.00019334339470771707,
      "loss": 0.7086,
      "step": 1659
    },
    {
      "epoch": 3.3400402414486923,
      "grad_norm": 0.23280680179595947,
      "learning_rate": 0.0001933393701579636,
      "loss": 0.7144,
      "step": 1660
    },
    {
      "epoch": 3.3420523138832996,
      "grad_norm": 0.22355933487415314,
      "learning_rate": 0.0001933353456082101,
      "loss": 0.6878,
      "step": 1661
    },
    {
      "epoch": 3.3440643863179074,
      "grad_norm": 0.23020680248737335,
      "learning_rate": 0.0001933313210584566,
      "loss": 0.6994,
      "step": 1662
    },
    {
      "epoch": 3.346076458752515,
      "grad_norm": 0.235915869474411,
      "learning_rate": 0.0001933272965087031,
      "loss": 0.7194,
      "step": 1663
    },
    {
      "epoch": 3.348088531187123,
      "grad_norm": 0.2359795868396759,
      "learning_rate": 0.0001933232719589496,
      "loss": 0.7077,
      "step": 1664
    },
    {
      "epoch": 3.3501006036217302,
      "grad_norm": 0.2547214925289154,
      "learning_rate": 0.00019331924740919611,
      "loss": 0.7208,
      "step": 1665
    },
    {
      "epoch": 3.352112676056338,
      "grad_norm": 0.23850099742412567,
      "learning_rate": 0.00019331522285944263,
      "loss": 0.6824,
      "step": 1666
    },
    {
      "epoch": 3.3541247484909458,
      "grad_norm": 0.23075681924819946,
      "learning_rate": 0.0001933111983096891,
      "loss": 0.6721,
      "step": 1667
    },
    {
      "epoch": 3.3561368209255535,
      "grad_norm": 0.22795021533966064,
      "learning_rate": 0.00019330717375993562,
      "loss": 0.6889,
      "step": 1668
    },
    {
      "epoch": 3.358148893360161,
      "grad_norm": 0.24509678781032562,
      "learning_rate": 0.0001933031492101821,
      "loss": 0.7064,
      "step": 1669
    },
    {
      "epoch": 3.3601609657947686,
      "grad_norm": 0.24212022125720978,
      "learning_rate": 0.00019329912466042862,
      "loss": 0.7027,
      "step": 1670
    },
    {
      "epoch": 3.3621730382293764,
      "grad_norm": 0.22350876033306122,
      "learning_rate": 0.00019329510011067513,
      "loss": 0.7184,
      "step": 1671
    },
    {
      "epoch": 3.3641851106639837,
      "grad_norm": 0.23483906686306,
      "learning_rate": 0.00019329107556092162,
      "loss": 0.7412,
      "step": 1672
    },
    {
      "epoch": 3.3661971830985915,
      "grad_norm": 0.23336803913116455,
      "learning_rate": 0.00019328705101116813,
      "loss": 0.7075,
      "step": 1673
    },
    {
      "epoch": 3.3682092555331993,
      "grad_norm": 0.24298439919948578,
      "learning_rate": 0.00019328302646141464,
      "loss": 0.7255,
      "step": 1674
    },
    {
      "epoch": 3.3702213279678066,
      "grad_norm": 0.2245032638311386,
      "learning_rate": 0.00019327900191166115,
      "loss": 0.7136,
      "step": 1675
    },
    {
      "epoch": 3.3722334004024144,
      "grad_norm": 0.23400451242923737,
      "learning_rate": 0.00019327497736190764,
      "loss": 0.7099,
      "step": 1676
    },
    {
      "epoch": 3.374245472837022,
      "grad_norm": 0.23064638674259186,
      "learning_rate": 0.00019327095281215415,
      "loss": 0.7059,
      "step": 1677
    },
    {
      "epoch": 3.37625754527163,
      "grad_norm": 0.24114173650741577,
      "learning_rate": 0.00019326692826240064,
      "loss": 0.6863,
      "step": 1678
    },
    {
      "epoch": 3.3782696177062372,
      "grad_norm": 0.2315332442522049,
      "learning_rate": 0.00019326290371264715,
      "loss": 0.7054,
      "step": 1679
    },
    {
      "epoch": 3.380281690140845,
      "grad_norm": 0.23176191747188568,
      "learning_rate": 0.00019325887916289366,
      "loss": 0.6813,
      "step": 1680
    },
    {
      "epoch": 3.3822937625754528,
      "grad_norm": 0.24265705049037933,
      "learning_rate": 0.00019325485461314017,
      "loss": 0.6835,
      "step": 1681
    },
    {
      "epoch": 3.3843058350100605,
      "grad_norm": 0.23040176928043365,
      "learning_rate": 0.00019325083006338666,
      "loss": 0.7139,
      "step": 1682
    },
    {
      "epoch": 3.386317907444668,
      "grad_norm": 0.24002894759178162,
      "learning_rate": 0.00019324680551363317,
      "loss": 0.6987,
      "step": 1683
    },
    {
      "epoch": 3.3883299798792756,
      "grad_norm": 0.2370009571313858,
      "learning_rate": 0.00019324278096387966,
      "loss": 0.7186,
      "step": 1684
    },
    {
      "epoch": 3.3903420523138834,
      "grad_norm": 0.23135505616664886,
      "learning_rate": 0.0001932387564141262,
      "loss": 0.7022,
      "step": 1685
    },
    {
      "epoch": 3.3923541247484907,
      "grad_norm": 0.22227144241333008,
      "learning_rate": 0.00019323473186437268,
      "loss": 0.7294,
      "step": 1686
    },
    {
      "epoch": 3.3943661971830985,
      "grad_norm": 0.22825627028942108,
      "learning_rate": 0.0001932307073146192,
      "loss": 0.682,
      "step": 1687
    },
    {
      "epoch": 3.3963782696177063,
      "grad_norm": 0.24375247955322266,
      "learning_rate": 0.00019322668276486568,
      "loss": 0.7159,
      "step": 1688
    },
    {
      "epoch": 3.398390342052314,
      "grad_norm": 0.2273947298526764,
      "learning_rate": 0.0001932226582151122,
      "loss": 0.6501,
      "step": 1689
    },
    {
      "epoch": 3.4004024144869214,
      "grad_norm": 0.22578850388526917,
      "learning_rate": 0.0001932186336653587,
      "loss": 0.692,
      "step": 1690
    },
    {
      "epoch": 3.402414486921529,
      "grad_norm": 0.2405782788991928,
      "learning_rate": 0.00019321460911560521,
      "loss": 0.7234,
      "step": 1691
    },
    {
      "epoch": 3.404426559356137,
      "grad_norm": 0.2402074635028839,
      "learning_rate": 0.0001932105845658517,
      "loss": 0.6755,
      "step": 1692
    },
    {
      "epoch": 3.4064386317907447,
      "grad_norm": 0.24572886526584625,
      "learning_rate": 0.0001932065600160982,
      "loss": 0.7081,
      "step": 1693
    },
    {
      "epoch": 3.408450704225352,
      "grad_norm": 0.2215237021446228,
      "learning_rate": 0.0001932025354663447,
      "loss": 0.6627,
      "step": 1694
    },
    {
      "epoch": 3.41046277665996,
      "grad_norm": 0.22467245161533356,
      "learning_rate": 0.00019319851091659124,
      "loss": 0.7113,
      "step": 1695
    },
    {
      "epoch": 3.4124748490945676,
      "grad_norm": 0.23315025866031647,
      "learning_rate": 0.00019319448636683772,
      "loss": 0.665,
      "step": 1696
    },
    {
      "epoch": 3.414486921529175,
      "grad_norm": 0.2370494306087494,
      "learning_rate": 0.00019319046181708423,
      "loss": 0.7057,
      "step": 1697
    },
    {
      "epoch": 3.4164989939637826,
      "grad_norm": 0.2305634617805481,
      "learning_rate": 0.00019318643726733072,
      "loss": 0.7054,
      "step": 1698
    },
    {
      "epoch": 3.4185110663983904,
      "grad_norm": 0.2346886545419693,
      "learning_rate": 0.00019318241271757723,
      "loss": 0.6754,
      "step": 1699
    },
    {
      "epoch": 3.4205231388329977,
      "grad_norm": 0.22591523826122284,
      "learning_rate": 0.00019317838816782374,
      "loss": 0.6585,
      "step": 1700
    },
    {
      "epoch": 3.4225352112676055,
      "grad_norm": 0.23382115364074707,
      "learning_rate": 0.00019317436361807026,
      "loss": 0.7021,
      "step": 1701
    },
    {
      "epoch": 3.4245472837022133,
      "grad_norm": 0.2479792982339859,
      "learning_rate": 0.00019317033906831674,
      "loss": 0.733,
      "step": 1702
    },
    {
      "epoch": 3.426559356136821,
      "grad_norm": 0.24973587691783905,
      "learning_rate": 0.00019316631451856325,
      "loss": 0.6984,
      "step": 1703
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 0.23889632523059845,
      "learning_rate": 0.00019316228996880974,
      "loss": 0.6549,
      "step": 1704
    },
    {
      "epoch": 3.430583501006036,
      "grad_norm": 0.24756863713264465,
      "learning_rate": 0.00019315826541905625,
      "loss": 0.6893,
      "step": 1705
    },
    {
      "epoch": 3.432595573440644,
      "grad_norm": 0.23855429887771606,
      "learning_rate": 0.00019315424086930276,
      "loss": 0.7061,
      "step": 1706
    },
    {
      "epoch": 3.4346076458752517,
      "grad_norm": 0.24378447234630585,
      "learning_rate": 0.00019315021631954925,
      "loss": 0.7092,
      "step": 1707
    },
    {
      "epoch": 3.436619718309859,
      "grad_norm": 0.22513903677463531,
      "learning_rate": 0.00019314619176979576,
      "loss": 0.628,
      "step": 1708
    },
    {
      "epoch": 3.438631790744467,
      "grad_norm": 0.26139378547668457,
      "learning_rate": 0.00019314216722004227,
      "loss": 0.7067,
      "step": 1709
    },
    {
      "epoch": 3.4406438631790746,
      "grad_norm": 0.23601561784744263,
      "learning_rate": 0.00019313814267028878,
      "loss": 0.6856,
      "step": 1710
    },
    {
      "epoch": 3.442655935613682,
      "grad_norm": 0.22472642362117767,
      "learning_rate": 0.00019313411812053527,
      "loss": 0.7001,
      "step": 1711
    },
    {
      "epoch": 3.4446680080482897,
      "grad_norm": 0.2582346498966217,
      "learning_rate": 0.00019313009357078178,
      "loss": 0.7036,
      "step": 1712
    },
    {
      "epoch": 3.4466800804828974,
      "grad_norm": 0.23306459188461304,
      "learning_rate": 0.00019312606902102827,
      "loss": 0.7181,
      "step": 1713
    },
    {
      "epoch": 3.448692152917505,
      "grad_norm": 0.23093391954898834,
      "learning_rate": 0.00019312204447127478,
      "loss": 0.7117,
      "step": 1714
    },
    {
      "epoch": 3.4507042253521125,
      "grad_norm": 0.23566818237304688,
      "learning_rate": 0.0001931180199215213,
      "loss": 0.7174,
      "step": 1715
    },
    {
      "epoch": 3.4527162977867203,
      "grad_norm": 0.23943156003952026,
      "learning_rate": 0.0001931139953717678,
      "loss": 0.7157,
      "step": 1716
    },
    {
      "epoch": 3.454728370221328,
      "grad_norm": 0.23877495527267456,
      "learning_rate": 0.0001931099708220143,
      "loss": 0.7061,
      "step": 1717
    },
    {
      "epoch": 3.456740442655936,
      "grad_norm": 0.22226683795452118,
      "learning_rate": 0.0001931059462722608,
      "loss": 0.6969,
      "step": 1718
    },
    {
      "epoch": 3.458752515090543,
      "grad_norm": 0.2399270236492157,
      "learning_rate": 0.00019310192172250729,
      "loss": 0.6576,
      "step": 1719
    },
    {
      "epoch": 3.460764587525151,
      "grad_norm": 0.2500087320804596,
      "learning_rate": 0.00019309789717275382,
      "loss": 0.7662,
      "step": 1720
    },
    {
      "epoch": 3.4627766599597587,
      "grad_norm": 0.2248287796974182,
      "learning_rate": 0.0001930938726230003,
      "loss": 0.6168,
      "step": 1721
    },
    {
      "epoch": 3.464788732394366,
      "grad_norm": 0.22595219314098358,
      "learning_rate": 0.00019308984807324682,
      "loss": 0.6609,
      "step": 1722
    },
    {
      "epoch": 3.466800804828974,
      "grad_norm": 0.24905194342136383,
      "learning_rate": 0.0001930858235234933,
      "loss": 0.709,
      "step": 1723
    },
    {
      "epoch": 3.4688128772635816,
      "grad_norm": 0.2413015067577362,
      "learning_rate": 0.00019308179897373982,
      "loss": 0.7006,
      "step": 1724
    },
    {
      "epoch": 3.470824949698189,
      "grad_norm": 0.23821258544921875,
      "learning_rate": 0.00019307777442398633,
      "loss": 0.6677,
      "step": 1725
    },
    {
      "epoch": 3.4728370221327967,
      "grad_norm": 0.23075628280639648,
      "learning_rate": 0.00019307374987423284,
      "loss": 0.7072,
      "step": 1726
    },
    {
      "epoch": 3.4748490945674044,
      "grad_norm": 0.23380498588085175,
      "learning_rate": 0.00019306972532447933,
      "loss": 0.6739,
      "step": 1727
    },
    {
      "epoch": 3.476861167002012,
      "grad_norm": 0.24640358984470367,
      "learning_rate": 0.00019306570077472584,
      "loss": 0.7365,
      "step": 1728
    },
    {
      "epoch": 3.4788732394366195,
      "grad_norm": 0.2408212423324585,
      "learning_rate": 0.00019306167622497233,
      "loss": 0.6926,
      "step": 1729
    },
    {
      "epoch": 3.4808853118712273,
      "grad_norm": 0.23619797825813293,
      "learning_rate": 0.00019305765167521887,
      "loss": 0.6691,
      "step": 1730
    },
    {
      "epoch": 3.482897384305835,
      "grad_norm": 0.23910418152809143,
      "learning_rate": 0.00019305362712546535,
      "loss": 0.6889,
      "step": 1731
    },
    {
      "epoch": 3.484909456740443,
      "grad_norm": 0.233292818069458,
      "learning_rate": 0.00019304960257571186,
      "loss": 0.7102,
      "step": 1732
    },
    {
      "epoch": 3.48692152917505,
      "grad_norm": 0.22905509173870087,
      "learning_rate": 0.00019304557802595835,
      "loss": 0.695,
      "step": 1733
    },
    {
      "epoch": 3.488933601609658,
      "grad_norm": 0.22058171033859253,
      "learning_rate": 0.00019304155347620486,
      "loss": 0.6818,
      "step": 1734
    },
    {
      "epoch": 3.4909456740442657,
      "grad_norm": 0.2331046760082245,
      "learning_rate": 0.00019303752892645137,
      "loss": 0.7027,
      "step": 1735
    },
    {
      "epoch": 3.492957746478873,
      "grad_norm": 0.2355024516582489,
      "learning_rate": 0.00019303350437669788,
      "loss": 0.7062,
      "step": 1736
    },
    {
      "epoch": 3.494969818913481,
      "grad_norm": 0.2298189103603363,
      "learning_rate": 0.00019302947982694437,
      "loss": 0.6943,
      "step": 1737
    },
    {
      "epoch": 3.4969818913480886,
      "grad_norm": 0.23487921059131622,
      "learning_rate": 0.00019302545527719088,
      "loss": 0.7134,
      "step": 1738
    },
    {
      "epoch": 3.4989939637826963,
      "grad_norm": 0.24066010117530823,
      "learning_rate": 0.00019302143072743737,
      "loss": 0.7734,
      "step": 1739
    },
    {
      "epoch": 3.5010060362173037,
      "grad_norm": 0.24782365560531616,
      "learning_rate": 0.00019301740617768388,
      "loss": 0.7077,
      "step": 1740
    },
    {
      "epoch": 3.5030181086519114,
      "grad_norm": 0.2420543134212494,
      "learning_rate": 0.0001930133816279304,
      "loss": 0.6883,
      "step": 1741
    },
    {
      "epoch": 3.505030181086519,
      "grad_norm": 0.24932658672332764,
      "learning_rate": 0.00019300935707817688,
      "loss": 0.7158,
      "step": 1742
    },
    {
      "epoch": 3.507042253521127,
      "grad_norm": 0.2347479611635208,
      "learning_rate": 0.0001930053325284234,
      "loss": 0.7202,
      "step": 1743
    },
    {
      "epoch": 3.5090543259557343,
      "grad_norm": 0.2292327880859375,
      "learning_rate": 0.00019300130797866987,
      "loss": 0.7127,
      "step": 1744
    },
    {
      "epoch": 3.511066398390342,
      "grad_norm": 0.2525351047515869,
      "learning_rate": 0.0001929972834289164,
      "loss": 0.6872,
      "step": 1745
    },
    {
      "epoch": 3.51307847082495,
      "grad_norm": 0.22831976413726807,
      "learning_rate": 0.0001929932588791629,
      "loss": 0.6779,
      "step": 1746
    },
    {
      "epoch": 3.515090543259557,
      "grad_norm": 0.27987077832221985,
      "learning_rate": 0.0001929892343294094,
      "loss": 0.729,
      "step": 1747
    },
    {
      "epoch": 3.517102615694165,
      "grad_norm": 0.24398759007453918,
      "learning_rate": 0.0001929852097796559,
      "loss": 0.7145,
      "step": 1748
    },
    {
      "epoch": 3.5191146881287727,
      "grad_norm": 0.24292579293251038,
      "learning_rate": 0.0001929811852299024,
      "loss": 0.7307,
      "step": 1749
    },
    {
      "epoch": 3.52112676056338,
      "grad_norm": 0.23463967442512512,
      "learning_rate": 0.00019297716068014892,
      "loss": 0.6724,
      "step": 1750
    },
    {
      "epoch": 3.523138832997988,
      "grad_norm": 0.2338467240333557,
      "learning_rate": 0.00019297313613039543,
      "loss": 0.6967,
      "step": 1751
    },
    {
      "epoch": 3.5251509054325956,
      "grad_norm": 0.22566086053848267,
      "learning_rate": 0.00019296911158064192,
      "loss": 0.732,
      "step": 1752
    },
    {
      "epoch": 3.5271629778672033,
      "grad_norm": 0.23761647939682007,
      "learning_rate": 0.00019296508703088843,
      "loss": 0.7066,
      "step": 1753
    },
    {
      "epoch": 3.529175050301811,
      "grad_norm": 0.24294739961624146,
      "learning_rate": 0.00019296106248113491,
      "loss": 0.7266,
      "step": 1754
    },
    {
      "epoch": 3.5311871227364184,
      "grad_norm": 0.24149510264396667,
      "learning_rate": 0.00019295703793138145,
      "loss": 0.7226,
      "step": 1755
    },
    {
      "epoch": 3.533199195171026,
      "grad_norm": 0.22705937922000885,
      "learning_rate": 0.00019295301338162794,
      "loss": 0.657,
      "step": 1756
    },
    {
      "epoch": 3.535211267605634,
      "grad_norm": 0.23701685667037964,
      "learning_rate": 0.00019294898883187445,
      "loss": 0.6778,
      "step": 1757
    },
    {
      "epoch": 3.5372233400402413,
      "grad_norm": 0.24094973504543304,
      "learning_rate": 0.00019294496428212094,
      "loss": 0.6728,
      "step": 1758
    },
    {
      "epoch": 3.539235412474849,
      "grad_norm": 0.2323753982782364,
      "learning_rate": 0.00019294093973236745,
      "loss": 0.6335,
      "step": 1759
    },
    {
      "epoch": 3.541247484909457,
      "grad_norm": 0.24481141567230225,
      "learning_rate": 0.00019293691518261396,
      "loss": 0.7304,
      "step": 1760
    },
    {
      "epoch": 3.543259557344064,
      "grad_norm": 0.2471548467874527,
      "learning_rate": 0.00019293289063286047,
      "loss": 0.7094,
      "step": 1761
    },
    {
      "epoch": 3.545271629778672,
      "grad_norm": 0.24029098451137543,
      "learning_rate": 0.00019292886608310696,
      "loss": 0.665,
      "step": 1762
    },
    {
      "epoch": 3.5472837022132797,
      "grad_norm": 0.2328212410211563,
      "learning_rate": 0.00019292484153335347,
      "loss": 0.7333,
      "step": 1763
    },
    {
      "epoch": 3.5492957746478875,
      "grad_norm": 0.2484590858221054,
      "learning_rate": 0.00019292081698359996,
      "loss": 0.7015,
      "step": 1764
    },
    {
      "epoch": 3.551307847082495,
      "grad_norm": 0.24538247287273407,
      "learning_rate": 0.0001929167924338465,
      "loss": 0.6751,
      "step": 1765
    },
    {
      "epoch": 3.5533199195171026,
      "grad_norm": 0.23368537425994873,
      "learning_rate": 0.00019291276788409298,
      "loss": 0.6915,
      "step": 1766
    },
    {
      "epoch": 3.5553319919517103,
      "grad_norm": 0.2258591204881668,
      "learning_rate": 0.0001929087433343395,
      "loss": 0.6545,
      "step": 1767
    },
    {
      "epoch": 3.557344064386318,
      "grad_norm": 0.22948600351810455,
      "learning_rate": 0.00019290471878458598,
      "loss": 0.6441,
      "step": 1768
    },
    {
      "epoch": 3.5593561368209254,
      "grad_norm": 0.2344496250152588,
      "learning_rate": 0.0001929006942348325,
      "loss": 0.7316,
      "step": 1769
    },
    {
      "epoch": 3.561368209255533,
      "grad_norm": 0.23352712392807007,
      "learning_rate": 0.000192896669685079,
      "loss": 0.6943,
      "step": 1770
    },
    {
      "epoch": 3.563380281690141,
      "grad_norm": 0.23165737092494965,
      "learning_rate": 0.0001928926451353255,
      "loss": 0.6667,
      "step": 1771
    },
    {
      "epoch": 3.5653923541247483,
      "grad_norm": 0.23233571648597717,
      "learning_rate": 0.000192888620585572,
      "loss": 0.6965,
      "step": 1772
    },
    {
      "epoch": 3.567404426559356,
      "grad_norm": 0.22402173280715942,
      "learning_rate": 0.0001928845960358185,
      "loss": 0.6622,
      "step": 1773
    },
    {
      "epoch": 3.569416498993964,
      "grad_norm": 0.24608848989009857,
      "learning_rate": 0.000192880571486065,
      "loss": 0.7117,
      "step": 1774
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 0.2287045419216156,
      "learning_rate": 0.0001928765469363115,
      "loss": 0.6589,
      "step": 1775
    },
    {
      "epoch": 3.573440643863179,
      "grad_norm": 0.24539415538311005,
      "learning_rate": 0.00019287252238655802,
      "loss": 0.7123,
      "step": 1776
    },
    {
      "epoch": 3.5754527162977867,
      "grad_norm": 0.23217026889324188,
      "learning_rate": 0.0001928684978368045,
      "loss": 0.6631,
      "step": 1777
    },
    {
      "epoch": 3.5774647887323945,
      "grad_norm": 0.2358051836490631,
      "learning_rate": 0.00019286447328705102,
      "loss": 0.7155,
      "step": 1778
    },
    {
      "epoch": 3.5794768611670023,
      "grad_norm": 0.238157719373703,
      "learning_rate": 0.0001928604487372975,
      "loss": 0.7064,
      "step": 1779
    },
    {
      "epoch": 3.5814889336016096,
      "grad_norm": 0.21969357132911682,
      "learning_rate": 0.00019285642418754404,
      "loss": 0.663,
      "step": 1780
    },
    {
      "epoch": 3.5835010060362174,
      "grad_norm": 0.2223249226808548,
      "learning_rate": 0.00019285239963779053,
      "loss": 0.7311,
      "step": 1781
    },
    {
      "epoch": 3.585513078470825,
      "grad_norm": 0.23833346366882324,
      "learning_rate": 0.00019284837508803704,
      "loss": 0.6897,
      "step": 1782
    },
    {
      "epoch": 3.5875251509054324,
      "grad_norm": 0.2231534719467163,
      "learning_rate": 0.00019284435053828353,
      "loss": 0.6582,
      "step": 1783
    },
    {
      "epoch": 3.58953722334004,
      "grad_norm": 0.23681452870368958,
      "learning_rate": 0.00019284032598853004,
      "loss": 0.674,
      "step": 1784
    },
    {
      "epoch": 3.591549295774648,
      "grad_norm": 0.23664651811122894,
      "learning_rate": 0.00019283630143877655,
      "loss": 0.6732,
      "step": 1785
    },
    {
      "epoch": 3.5935613682092553,
      "grad_norm": 0.23505382239818573,
      "learning_rate": 0.00019283227688902306,
      "loss": 0.7184,
      "step": 1786
    },
    {
      "epoch": 3.595573440643863,
      "grad_norm": 0.23388473689556122,
      "learning_rate": 0.00019282825233926955,
      "loss": 0.6654,
      "step": 1787
    },
    {
      "epoch": 3.597585513078471,
      "grad_norm": 0.25290006399154663,
      "learning_rate": 0.00019282422778951606,
      "loss": 0.6639,
      "step": 1788
    },
    {
      "epoch": 3.5995975855130786,
      "grad_norm": 0.24411596357822418,
      "learning_rate": 0.00019282020323976254,
      "loss": 0.7239,
      "step": 1789
    },
    {
      "epoch": 3.6016096579476864,
      "grad_norm": 0.2332068383693695,
      "learning_rate": 0.00019281617869000908,
      "loss": 0.6701,
      "step": 1790
    },
    {
      "epoch": 3.6036217303822937,
      "grad_norm": 0.22910623252391815,
      "learning_rate": 0.00019281215414025557,
      "loss": 0.6557,
      "step": 1791
    },
    {
      "epoch": 3.6056338028169015,
      "grad_norm": 0.23075483739376068,
      "learning_rate": 0.00019280812959050208,
      "loss": 0.7231,
      "step": 1792
    },
    {
      "epoch": 3.6076458752515093,
      "grad_norm": 0.235329732298851,
      "learning_rate": 0.00019280410504074857,
      "loss": 0.7522,
      "step": 1793
    },
    {
      "epoch": 3.6096579476861166,
      "grad_norm": 0.22289817035198212,
      "learning_rate": 0.00019280008049099508,
      "loss": 0.6849,
      "step": 1794
    },
    {
      "epoch": 3.6116700201207244,
      "grad_norm": 0.2496640682220459,
      "learning_rate": 0.0001927960559412416,
      "loss": 0.7471,
      "step": 1795
    },
    {
      "epoch": 3.613682092555332,
      "grad_norm": 0.2592909038066864,
      "learning_rate": 0.0001927920313914881,
      "loss": 0.6769,
      "step": 1796
    },
    {
      "epoch": 3.6156941649899395,
      "grad_norm": 0.2544625997543335,
      "learning_rate": 0.0001927880068417346,
      "loss": 0.7151,
      "step": 1797
    },
    {
      "epoch": 3.6177062374245472,
      "grad_norm": 0.25509247183799744,
      "learning_rate": 0.0001927839822919811,
      "loss": 0.6976,
      "step": 1798
    },
    {
      "epoch": 3.619718309859155,
      "grad_norm": 0.2591218948364258,
      "learning_rate": 0.00019277995774222759,
      "loss": 0.6369,
      "step": 1799
    },
    {
      "epoch": 3.6217303822937623,
      "grad_norm": 0.228357195854187,
      "learning_rate": 0.00019277593319247412,
      "loss": 0.6692,
      "step": 1800
    },
    {
      "epoch": 3.62374245472837,
      "grad_norm": 0.23765087127685547,
      "learning_rate": 0.0001927719086427206,
      "loss": 0.7084,
      "step": 1801
    },
    {
      "epoch": 3.625754527162978,
      "grad_norm": 0.22777436673641205,
      "learning_rate": 0.00019276788409296712,
      "loss": 0.7074,
      "step": 1802
    },
    {
      "epoch": 3.6277665995975856,
      "grad_norm": 0.24658247828483582,
      "learning_rate": 0.0001927638595432136,
      "loss": 0.7068,
      "step": 1803
    },
    {
      "epoch": 3.6297786720321934,
      "grad_norm": 0.22675152122974396,
      "learning_rate": 0.00019275983499346012,
      "loss": 0.6877,
      "step": 1804
    },
    {
      "epoch": 3.6317907444668007,
      "grad_norm": 0.2432154268026352,
      "learning_rate": 0.00019275581044370663,
      "loss": 0.654,
      "step": 1805
    },
    {
      "epoch": 3.6338028169014085,
      "grad_norm": 0.2565009593963623,
      "learning_rate": 0.00019275178589395312,
      "loss": 0.7068,
      "step": 1806
    },
    {
      "epoch": 3.6358148893360163,
      "grad_norm": 0.22320066392421722,
      "learning_rate": 0.00019274776134419963,
      "loss": 0.703,
      "step": 1807
    },
    {
      "epoch": 3.6378269617706236,
      "grad_norm": 0.2575012743473053,
      "learning_rate": 0.00019274373679444614,
      "loss": 0.7057,
      "step": 1808
    },
    {
      "epoch": 3.6398390342052314,
      "grad_norm": 0.21812804043293,
      "learning_rate": 0.00019273971224469263,
      "loss": 0.6566,
      "step": 1809
    },
    {
      "epoch": 3.641851106639839,
      "grad_norm": 0.22933615744113922,
      "learning_rate": 0.00019273568769493914,
      "loss": 0.6931,
      "step": 1810
    },
    {
      "epoch": 3.6438631790744465,
      "grad_norm": 0.2298492193222046,
      "learning_rate": 0.00019273166314518565,
      "loss": 0.6967,
      "step": 1811
    },
    {
      "epoch": 3.6458752515090542,
      "grad_norm": 0.22739674150943756,
      "learning_rate": 0.00019272763859543214,
      "loss": 0.6823,
      "step": 1812
    },
    {
      "epoch": 3.647887323943662,
      "grad_norm": 0.23180244863033295,
      "learning_rate": 0.00019272361404567865,
      "loss": 0.6939,
      "step": 1813
    },
    {
      "epoch": 3.6498993963782698,
      "grad_norm": 0.2478729486465454,
      "learning_rate": 0.00019271958949592513,
      "loss": 0.666,
      "step": 1814
    },
    {
      "epoch": 3.6519114688128775,
      "grad_norm": 0.24066227674484253,
      "learning_rate": 0.00019271556494617167,
      "loss": 0.6997,
      "step": 1815
    },
    {
      "epoch": 3.653923541247485,
      "grad_norm": 0.25469326972961426,
      "learning_rate": 0.00019271154039641816,
      "loss": 0.717,
      "step": 1816
    },
    {
      "epoch": 3.6559356136820926,
      "grad_norm": 0.24576079845428467,
      "learning_rate": 0.00019270751584666467,
      "loss": 0.7383,
      "step": 1817
    },
    {
      "epoch": 3.6579476861167004,
      "grad_norm": 0.24239559471607208,
      "learning_rate": 0.00019270349129691115,
      "loss": 0.6959,
      "step": 1818
    },
    {
      "epoch": 3.6599597585513077,
      "grad_norm": 0.23503166437149048,
      "learning_rate": 0.00019269946674715767,
      "loss": 0.6991,
      "step": 1819
    },
    {
      "epoch": 3.6619718309859155,
      "grad_norm": 0.23571762442588806,
      "learning_rate": 0.00019269544219740418,
      "loss": 0.6555,
      "step": 1820
    },
    {
      "epoch": 3.6639839034205233,
      "grad_norm": 0.22440765798091888,
      "learning_rate": 0.0001926914176476507,
      "loss": 0.6955,
      "step": 1821
    },
    {
      "epoch": 3.6659959758551306,
      "grad_norm": 0.23005710542201996,
      "learning_rate": 0.00019268739309789718,
      "loss": 0.6942,
      "step": 1822
    },
    {
      "epoch": 3.6680080482897384,
      "grad_norm": 0.22071383893489838,
      "learning_rate": 0.0001926833685481437,
      "loss": 0.6832,
      "step": 1823
    },
    {
      "epoch": 3.670020120724346,
      "grad_norm": 0.25749924778938293,
      "learning_rate": 0.00019267934399839017,
      "loss": 0.6733,
      "step": 1824
    },
    {
      "epoch": 3.6720321931589535,
      "grad_norm": 0.23496240377426147,
      "learning_rate": 0.00019267531944863669,
      "loss": 0.6955,
      "step": 1825
    },
    {
      "epoch": 3.6740442655935612,
      "grad_norm": 0.24058936536312103,
      "learning_rate": 0.0001926712948988832,
      "loss": 0.6882,
      "step": 1826
    },
    {
      "epoch": 3.676056338028169,
      "grad_norm": 0.24593773484230042,
      "learning_rate": 0.0001926672703491297,
      "loss": 0.7601,
      "step": 1827
    },
    {
      "epoch": 3.6780684104627768,
      "grad_norm": 0.2334287166595459,
      "learning_rate": 0.0001926632457993762,
      "loss": 0.6853,
      "step": 1828
    },
    {
      "epoch": 3.6800804828973845,
      "grad_norm": 0.24321489036083221,
      "learning_rate": 0.0001926592212496227,
      "loss": 0.6854,
      "step": 1829
    },
    {
      "epoch": 3.682092555331992,
      "grad_norm": 0.23245175182819366,
      "learning_rate": 0.0001926551966998692,
      "loss": 0.6971,
      "step": 1830
    },
    {
      "epoch": 3.6841046277665996,
      "grad_norm": 0.249057337641716,
      "learning_rate": 0.00019265117215011573,
      "loss": 0.6926,
      "step": 1831
    },
    {
      "epoch": 3.6861167002012074,
      "grad_norm": 0.2399148792028427,
      "learning_rate": 0.00019264714760036222,
      "loss": 0.7272,
      "step": 1832
    },
    {
      "epoch": 3.6881287726358147,
      "grad_norm": 0.2403377741575241,
      "learning_rate": 0.00019264312305060873,
      "loss": 0.7174,
      "step": 1833
    },
    {
      "epoch": 3.6901408450704225,
      "grad_norm": 0.23352690041065216,
      "learning_rate": 0.00019263909850085521,
      "loss": 0.6915,
      "step": 1834
    },
    {
      "epoch": 3.6921529175050303,
      "grad_norm": 0.234411358833313,
      "learning_rate": 0.00019263507395110173,
      "loss": 0.6865,
      "step": 1835
    },
    {
      "epoch": 3.6941649899396376,
      "grad_norm": 0.23683148622512817,
      "learning_rate": 0.00019263104940134824,
      "loss": 0.6783,
      "step": 1836
    },
    {
      "epoch": 3.6961770623742454,
      "grad_norm": 0.2477303147315979,
      "learning_rate": 0.00019262702485159475,
      "loss": 0.7181,
      "step": 1837
    },
    {
      "epoch": 3.698189134808853,
      "grad_norm": 0.2221735417842865,
      "learning_rate": 0.00019262300030184124,
      "loss": 0.6765,
      "step": 1838
    },
    {
      "epoch": 3.700201207243461,
      "grad_norm": 0.23052603006362915,
      "learning_rate": 0.00019261897575208775,
      "loss": 0.6882,
      "step": 1839
    },
    {
      "epoch": 3.7022132796780687,
      "grad_norm": 0.24196867644786835,
      "learning_rate": 0.00019261495120233423,
      "loss": 0.7047,
      "step": 1840
    },
    {
      "epoch": 3.704225352112676,
      "grad_norm": 0.25070443749427795,
      "learning_rate": 0.00019261092665258075,
      "loss": 0.7273,
      "step": 1841
    },
    {
      "epoch": 3.7062374245472838,
      "grad_norm": 0.24138924479484558,
      "learning_rate": 0.00019260690210282726,
      "loss": 0.6937,
      "step": 1842
    },
    {
      "epoch": 3.7082494969818915,
      "grad_norm": 0.245374396443367,
      "learning_rate": 0.00019260287755307377,
      "loss": 0.7361,
      "step": 1843
    },
    {
      "epoch": 3.710261569416499,
      "grad_norm": 0.22566305100917816,
      "learning_rate": 0.00019259885300332026,
      "loss": 0.6939,
      "step": 1844
    },
    {
      "epoch": 3.7122736418511066,
      "grad_norm": 0.23332878947257996,
      "learning_rate": 0.00019259482845356677,
      "loss": 0.6828,
      "step": 1845
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 0.22843143343925476,
      "learning_rate": 0.00019259080390381328,
      "loss": 0.6738,
      "step": 1846
    },
    {
      "epoch": 3.7162977867203217,
      "grad_norm": 0.22433574497699738,
      "learning_rate": 0.00019258677935405977,
      "loss": 0.6989,
      "step": 1847
    },
    {
      "epoch": 3.7183098591549295,
      "grad_norm": 0.2267366498708725,
      "learning_rate": 0.00019258275480430628,
      "loss": 0.6956,
      "step": 1848
    },
    {
      "epoch": 3.7203219315895373,
      "grad_norm": 0.26738131046295166,
      "learning_rate": 0.00019257873025455276,
      "loss": 0.6991,
      "step": 1849
    },
    {
      "epoch": 3.7223340040241446,
      "grad_norm": 0.2350640445947647,
      "learning_rate": 0.00019257470570479927,
      "loss": 0.681,
      "step": 1850
    },
    {
      "epoch": 3.7243460764587524,
      "grad_norm": 0.23159289360046387,
      "learning_rate": 0.0001925706811550458,
      "loss": 0.7147,
      "step": 1851
    },
    {
      "epoch": 3.72635814889336,
      "grad_norm": 0.2257286012172699,
      "learning_rate": 0.0001925666566052923,
      "loss": 0.7158,
      "step": 1852
    },
    {
      "epoch": 3.728370221327968,
      "grad_norm": 0.2321460247039795,
      "learning_rate": 0.00019256263205553878,
      "loss": 0.653,
      "step": 1853
    },
    {
      "epoch": 3.7303822937625757,
      "grad_norm": 0.2368355691432953,
      "learning_rate": 0.0001925586075057853,
      "loss": 0.6636,
      "step": 1854
    },
    {
      "epoch": 3.732394366197183,
      "grad_norm": 0.23776549100875854,
      "learning_rate": 0.00019255458295603178,
      "loss": 0.6774,
      "step": 1855
    },
    {
      "epoch": 3.734406438631791,
      "grad_norm": 0.2322676032781601,
      "learning_rate": 0.00019255055840627832,
      "loss": 0.6944,
      "step": 1856
    },
    {
      "epoch": 3.7364185110663986,
      "grad_norm": 0.24380552768707275,
      "learning_rate": 0.0001925465338565248,
      "loss": 0.6758,
      "step": 1857
    },
    {
      "epoch": 3.738430583501006,
      "grad_norm": 0.23134292662143707,
      "learning_rate": 0.00019254250930677132,
      "loss": 0.6677,
      "step": 1858
    },
    {
      "epoch": 3.7404426559356136,
      "grad_norm": 0.23585349321365356,
      "learning_rate": 0.0001925384847570178,
      "loss": 0.7443,
      "step": 1859
    },
    {
      "epoch": 3.7424547283702214,
      "grad_norm": 0.22528059780597687,
      "learning_rate": 0.00019253446020726432,
      "loss": 0.6664,
      "step": 1860
    },
    {
      "epoch": 3.7444668008048287,
      "grad_norm": 0.2302093505859375,
      "learning_rate": 0.00019253043565751083,
      "loss": 0.6696,
      "step": 1861
    },
    {
      "epoch": 3.7464788732394365,
      "grad_norm": 0.23391468822956085,
      "learning_rate": 0.00019252641110775734,
      "loss": 0.6744,
      "step": 1862
    },
    {
      "epoch": 3.7484909456740443,
      "grad_norm": 0.24261294305324554,
      "learning_rate": 0.00019252238655800382,
      "loss": 0.6651,
      "step": 1863
    },
    {
      "epoch": 3.750503018108652,
      "grad_norm": 0.23779040575027466,
      "learning_rate": 0.00019251836200825034,
      "loss": 0.6794,
      "step": 1864
    },
    {
      "epoch": 3.75251509054326,
      "grad_norm": 0.25951477885246277,
      "learning_rate": 0.00019251433745849682,
      "loss": 0.686,
      "step": 1865
    },
    {
      "epoch": 3.754527162977867,
      "grad_norm": 0.21841871738433838,
      "learning_rate": 0.00019251031290874336,
      "loss": 0.6616,
      "step": 1866
    },
    {
      "epoch": 3.756539235412475,
      "grad_norm": 0.24156349897384644,
      "learning_rate": 0.00019250628835898985,
      "loss": 0.7065,
      "step": 1867
    },
    {
      "epoch": 3.7585513078470827,
      "grad_norm": 0.24517448246479034,
      "learning_rate": 0.00019250226380923636,
      "loss": 0.6819,
      "step": 1868
    },
    {
      "epoch": 3.76056338028169,
      "grad_norm": 0.24590948224067688,
      "learning_rate": 0.00019249823925948284,
      "loss": 0.7176,
      "step": 1869
    },
    {
      "epoch": 3.762575452716298,
      "grad_norm": 0.2156120240688324,
      "learning_rate": 0.00019249421470972936,
      "loss": 0.6767,
      "step": 1870
    },
    {
      "epoch": 3.7645875251509056,
      "grad_norm": 0.24259857833385468,
      "learning_rate": 0.00019249019015997587,
      "loss": 0.6832,
      "step": 1871
    },
    {
      "epoch": 3.766599597585513,
      "grad_norm": 0.23170143365859985,
      "learning_rate": 0.00019248616561022238,
      "loss": 0.6918,
      "step": 1872
    },
    {
      "epoch": 3.7686116700201207,
      "grad_norm": 0.23169013857841492,
      "learning_rate": 0.00019248214106046887,
      "loss": 0.6579,
      "step": 1873
    },
    {
      "epoch": 3.7706237424547284,
      "grad_norm": 0.22326813638210297,
      "learning_rate": 0.00019247811651071538,
      "loss": 0.7124,
      "step": 1874
    },
    {
      "epoch": 3.7726358148893357,
      "grad_norm": 0.24542170763015747,
      "learning_rate": 0.00019247409196096186,
      "loss": 0.7359,
      "step": 1875
    },
    {
      "epoch": 3.7746478873239435,
      "grad_norm": 0.24756203591823578,
      "learning_rate": 0.00019247006741120838,
      "loss": 0.684,
      "step": 1876
    },
    {
      "epoch": 3.7766599597585513,
      "grad_norm": 0.21786275506019592,
      "learning_rate": 0.0001924660428614549,
      "loss": 0.7118,
      "step": 1877
    },
    {
      "epoch": 3.778672032193159,
      "grad_norm": 0.24560287594795227,
      "learning_rate": 0.0001924620183117014,
      "loss": 0.7071,
      "step": 1878
    },
    {
      "epoch": 3.780684104627767,
      "grad_norm": 0.2301536500453949,
      "learning_rate": 0.00019245799376194788,
      "loss": 0.6715,
      "step": 1879
    },
    {
      "epoch": 3.782696177062374,
      "grad_norm": 0.24076026678085327,
      "learning_rate": 0.0001924539692121944,
      "loss": 0.6951,
      "step": 1880
    },
    {
      "epoch": 3.784708249496982,
      "grad_norm": 0.23411822319030762,
      "learning_rate": 0.0001924499446624409,
      "loss": 0.7042,
      "step": 1881
    },
    {
      "epoch": 3.7867203219315897,
      "grad_norm": 0.22956258058547974,
      "learning_rate": 0.0001924459201126874,
      "loss": 0.7344,
      "step": 1882
    },
    {
      "epoch": 3.788732394366197,
      "grad_norm": 0.22116142511367798,
      "learning_rate": 0.0001924418955629339,
      "loss": 0.6906,
      "step": 1883
    },
    {
      "epoch": 3.790744466800805,
      "grad_norm": 0.24112854897975922,
      "learning_rate": 0.0001924378710131804,
      "loss": 0.6979,
      "step": 1884
    },
    {
      "epoch": 3.7927565392354126,
      "grad_norm": 0.22920964658260345,
      "learning_rate": 0.0001924338464634269,
      "loss": 0.6732,
      "step": 1885
    },
    {
      "epoch": 3.79476861167002,
      "grad_norm": 0.22989526391029358,
      "learning_rate": 0.00019242982191367342,
      "loss": 0.6601,
      "step": 1886
    },
    {
      "epoch": 3.7967806841046277,
      "grad_norm": 0.2302427589893341,
      "learning_rate": 0.00019242579736391993,
      "loss": 0.6991,
      "step": 1887
    },
    {
      "epoch": 3.7987927565392354,
      "grad_norm": 0.23613174259662628,
      "learning_rate": 0.00019242177281416641,
      "loss": 0.6817,
      "step": 1888
    },
    {
      "epoch": 3.800804828973843,
      "grad_norm": 0.23247130215168,
      "learning_rate": 0.00019241774826441293,
      "loss": 0.6655,
      "step": 1889
    },
    {
      "epoch": 3.802816901408451,
      "grad_norm": 0.23156596720218658,
      "learning_rate": 0.0001924137237146594,
      "loss": 0.6741,
      "step": 1890
    },
    {
      "epoch": 3.8048289738430583,
      "grad_norm": 0.23107436299324036,
      "learning_rate": 0.00019240969916490595,
      "loss": 0.6839,
      "step": 1891
    },
    {
      "epoch": 3.806841046277666,
      "grad_norm": 0.2389271855354309,
      "learning_rate": 0.00019240567461515244,
      "loss": 0.6894,
      "step": 1892
    },
    {
      "epoch": 3.808853118712274,
      "grad_norm": 0.23330508172512054,
      "learning_rate": 0.00019240165006539895,
      "loss": 0.6645,
      "step": 1893
    },
    {
      "epoch": 3.810865191146881,
      "grad_norm": 0.23858395218849182,
      "learning_rate": 0.00019239762551564543,
      "loss": 0.6892,
      "step": 1894
    },
    {
      "epoch": 3.812877263581489,
      "grad_norm": 0.2281372994184494,
      "learning_rate": 0.00019239360096589194,
      "loss": 0.7074,
      "step": 1895
    },
    {
      "epoch": 3.8148893360160967,
      "grad_norm": 0.215658500790596,
      "learning_rate": 0.00019238957641613846,
      "loss": 0.6926,
      "step": 1896
    },
    {
      "epoch": 3.816901408450704,
      "grad_norm": 0.23650960624217987,
      "learning_rate": 0.00019238555186638497,
      "loss": 0.6936,
      "step": 1897
    },
    {
      "epoch": 3.818913480885312,
      "grad_norm": 0.2482113242149353,
      "learning_rate": 0.00019238152731663145,
      "loss": 0.7132,
      "step": 1898
    },
    {
      "epoch": 3.8209255533199196,
      "grad_norm": 0.2459903359413147,
      "learning_rate": 0.00019237750276687797,
      "loss": 0.6629,
      "step": 1899
    },
    {
      "epoch": 3.822937625754527,
      "grad_norm": 0.2291335016489029,
      "learning_rate": 0.00019237347821712445,
      "loss": 0.7079,
      "step": 1900
    },
    {
      "epoch": 3.8249496981891347,
      "grad_norm": 0.23784224689006805,
      "learning_rate": 0.000192369453667371,
      "loss": 0.6996,
      "step": 1901
    },
    {
      "epoch": 3.8269617706237424,
      "grad_norm": 0.23849911987781525,
      "learning_rate": 0.00019236542911761748,
      "loss": 0.741,
      "step": 1902
    },
    {
      "epoch": 3.82897384305835,
      "grad_norm": 0.23108957707881927,
      "learning_rate": 0.000192361404567864,
      "loss": 0.6872,
      "step": 1903
    },
    {
      "epoch": 3.830985915492958,
      "grad_norm": 0.22143703699111938,
      "learning_rate": 0.00019235738001811047,
      "loss": 0.679,
      "step": 1904
    },
    {
      "epoch": 3.8329979879275653,
      "grad_norm": 0.2405409961938858,
      "learning_rate": 0.00019235335546835699,
      "loss": 0.7089,
      "step": 1905
    },
    {
      "epoch": 3.835010060362173,
      "grad_norm": 0.23092776536941528,
      "learning_rate": 0.0001923493309186035,
      "loss": 0.679,
      "step": 1906
    },
    {
      "epoch": 3.837022132796781,
      "grad_norm": 0.23625332117080688,
      "learning_rate": 0.00019234530636885,
      "loss": 0.6831,
      "step": 1907
    },
    {
      "epoch": 3.839034205231388,
      "grad_norm": 0.2334945648908615,
      "learning_rate": 0.0001923412818190965,
      "loss": 0.7123,
      "step": 1908
    },
    {
      "epoch": 3.841046277665996,
      "grad_norm": 0.23112644255161285,
      "learning_rate": 0.000192337257269343,
      "loss": 0.711,
      "step": 1909
    },
    {
      "epoch": 3.8430583501006037,
      "grad_norm": 0.23293301463127136,
      "learning_rate": 0.0001923332327195895,
      "loss": 0.7225,
      "step": 1910
    },
    {
      "epoch": 3.845070422535211,
      "grad_norm": 0.23490293323993683,
      "learning_rate": 0.000192329208169836,
      "loss": 0.6692,
      "step": 1911
    },
    {
      "epoch": 3.847082494969819,
      "grad_norm": 0.2449648529291153,
      "learning_rate": 0.00019232518362008252,
      "loss": 0.687,
      "step": 1912
    },
    {
      "epoch": 3.8490945674044266,
      "grad_norm": 0.23392529785633087,
      "learning_rate": 0.000192321159070329,
      "loss": 0.6638,
      "step": 1913
    },
    {
      "epoch": 3.8511066398390343,
      "grad_norm": 0.23738643527030945,
      "learning_rate": 0.00019231713452057551,
      "loss": 0.693,
      "step": 1914
    },
    {
      "epoch": 3.853118712273642,
      "grad_norm": 0.23016805946826935,
      "learning_rate": 0.00019231310997082203,
      "loss": 0.7264,
      "step": 1915
    },
    {
      "epoch": 3.8551307847082494,
      "grad_norm": 0.23194679617881775,
      "learning_rate": 0.00019230908542106854,
      "loss": 0.7009,
      "step": 1916
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 0.23972611129283905,
      "learning_rate": 0.00019230506087131502,
      "loss": 0.7082,
      "step": 1917
    },
    {
      "epoch": 3.859154929577465,
      "grad_norm": 0.2218748927116394,
      "learning_rate": 0.00019230103632156154,
      "loss": 0.6454,
      "step": 1918
    },
    {
      "epoch": 3.8611670020120723,
      "grad_norm": 0.2220298945903778,
      "learning_rate": 0.00019229701177180802,
      "loss": 0.7054,
      "step": 1919
    },
    {
      "epoch": 3.86317907444668,
      "grad_norm": 0.24045787751674652,
      "learning_rate": 0.00019229298722205453,
      "loss": 0.6452,
      "step": 1920
    },
    {
      "epoch": 3.865191146881288,
      "grad_norm": 0.22839391231536865,
      "learning_rate": 0.00019228896267230105,
      "loss": 0.6929,
      "step": 1921
    },
    {
      "epoch": 3.867203219315895,
      "grad_norm": 0.2290279120206833,
      "learning_rate": 0.00019228493812254756,
      "loss": 0.667,
      "step": 1922
    },
    {
      "epoch": 3.869215291750503,
      "grad_norm": 0.2594568431377411,
      "learning_rate": 0.00019228091357279404,
      "loss": 0.6802,
      "step": 1923
    },
    {
      "epoch": 3.8712273641851107,
      "grad_norm": 0.2270127832889557,
      "learning_rate": 0.00019227688902304056,
      "loss": 0.6728,
      "step": 1924
    },
    {
      "epoch": 3.873239436619718,
      "grad_norm": 0.24711106717586517,
      "learning_rate": 0.00019227286447328704,
      "loss": 0.7409,
      "step": 1925
    },
    {
      "epoch": 3.875251509054326,
      "grad_norm": 0.23279714584350586,
      "learning_rate": 0.00019226883992353358,
      "loss": 0.721,
      "step": 1926
    },
    {
      "epoch": 3.8772635814889336,
      "grad_norm": 0.2268342226743698,
      "learning_rate": 0.00019226481537378006,
      "loss": 0.6864,
      "step": 1927
    },
    {
      "epoch": 3.8792756539235413,
      "grad_norm": 0.23673877120018005,
      "learning_rate": 0.00019226079082402658,
      "loss": 0.7096,
      "step": 1928
    },
    {
      "epoch": 3.881287726358149,
      "grad_norm": 0.23135122656822205,
      "learning_rate": 0.00019225676627427306,
      "loss": 0.7128,
      "step": 1929
    },
    {
      "epoch": 3.8832997987927564,
      "grad_norm": 0.2532777786254883,
      "learning_rate": 0.00019225274172451957,
      "loss": 0.7567,
      "step": 1930
    },
    {
      "epoch": 3.885311871227364,
      "grad_norm": 0.25231924653053284,
      "learning_rate": 0.0001922487171747661,
      "loss": 0.6576,
      "step": 1931
    },
    {
      "epoch": 3.887323943661972,
      "grad_norm": 0.2485998570919037,
      "learning_rate": 0.0001922446926250126,
      "loss": 0.7182,
      "step": 1932
    },
    {
      "epoch": 3.8893360160965793,
      "grad_norm": 0.23617765307426453,
      "learning_rate": 0.00019224066807525908,
      "loss": 0.6947,
      "step": 1933
    },
    {
      "epoch": 3.891348088531187,
      "grad_norm": 0.23058456182479858,
      "learning_rate": 0.0001922366435255056,
      "loss": 0.6866,
      "step": 1934
    },
    {
      "epoch": 3.893360160965795,
      "grad_norm": 0.23214410245418549,
      "learning_rate": 0.00019223261897575208,
      "loss": 0.6994,
      "step": 1935
    },
    {
      "epoch": 3.895372233400402,
      "grad_norm": 0.22129850089550018,
      "learning_rate": 0.00019222859442599862,
      "loss": 0.7048,
      "step": 1936
    },
    {
      "epoch": 3.89738430583501,
      "grad_norm": 0.22793418169021606,
      "learning_rate": 0.0001922245698762451,
      "loss": 0.7008,
      "step": 1937
    },
    {
      "epoch": 3.8993963782696177,
      "grad_norm": 0.22823455929756165,
      "learning_rate": 0.00019222054532649162,
      "loss": 0.721,
      "step": 1938
    },
    {
      "epoch": 3.9014084507042255,
      "grad_norm": 0.21950305998325348,
      "learning_rate": 0.0001922165207767381,
      "loss": 0.7112,
      "step": 1939
    },
    {
      "epoch": 3.9034205231388333,
      "grad_norm": 0.2501836121082306,
      "learning_rate": 0.00019221249622698462,
      "loss": 0.7117,
      "step": 1940
    },
    {
      "epoch": 3.9054325955734406,
      "grad_norm": 0.23330600559711456,
      "learning_rate": 0.00019220847167723113,
      "loss": 0.6993,
      "step": 1941
    },
    {
      "epoch": 3.9074446680080483,
      "grad_norm": 0.2210969477891922,
      "learning_rate": 0.00019220444712747764,
      "loss": 0.7108,
      "step": 1942
    },
    {
      "epoch": 3.909456740442656,
      "grad_norm": 0.23726676404476166,
      "learning_rate": 0.00019220042257772412,
      "loss": 0.677,
      "step": 1943
    },
    {
      "epoch": 3.9114688128772634,
      "grad_norm": 0.2610090970993042,
      "learning_rate": 0.00019219639802797064,
      "loss": 0.6942,
      "step": 1944
    },
    {
      "epoch": 3.913480885311871,
      "grad_norm": 0.2382240891456604,
      "learning_rate": 0.00019219237347821712,
      "loss": 0.6832,
      "step": 1945
    },
    {
      "epoch": 3.915492957746479,
      "grad_norm": 0.24191652238368988,
      "learning_rate": 0.00019218834892846363,
      "loss": 0.6964,
      "step": 1946
    },
    {
      "epoch": 3.9175050301810863,
      "grad_norm": 0.2317599505186081,
      "learning_rate": 0.00019218432437871015,
      "loss": 0.699,
      "step": 1947
    },
    {
      "epoch": 3.919517102615694,
      "grad_norm": 0.2289998084306717,
      "learning_rate": 0.00019218029982895663,
      "loss": 0.6923,
      "step": 1948
    },
    {
      "epoch": 3.921529175050302,
      "grad_norm": 0.2361524999141693,
      "learning_rate": 0.00019217627527920314,
      "loss": 0.7316,
      "step": 1949
    },
    {
      "epoch": 3.9235412474849096,
      "grad_norm": 0.2457527071237564,
      "learning_rate": 0.00019217225072944966,
      "loss": 0.7294,
      "step": 1950
    },
    {
      "epoch": 3.925553319919517,
      "grad_norm": 0.2280920296907425,
      "learning_rate": 0.00019216822617969617,
      "loss": 0.6969,
      "step": 1951
    },
    {
      "epoch": 3.9275653923541247,
      "grad_norm": 0.23437270522117615,
      "learning_rate": 0.00019216420162994265,
      "loss": 0.6703,
      "step": 1952
    },
    {
      "epoch": 3.9295774647887325,
      "grad_norm": 0.2434079349040985,
      "learning_rate": 0.00019216017708018917,
      "loss": 0.6801,
      "step": 1953
    },
    {
      "epoch": 3.9315895372233403,
      "grad_norm": 0.2470364272594452,
      "learning_rate": 0.00019215615253043565,
      "loss": 0.717,
      "step": 1954
    },
    {
      "epoch": 3.9336016096579476,
      "grad_norm": 0.24356728792190552,
      "learning_rate": 0.00019215212798068216,
      "loss": 0.6892,
      "step": 1955
    },
    {
      "epoch": 3.9356136820925554,
      "grad_norm": 0.23730406165122986,
      "learning_rate": 0.00019214810343092868,
      "loss": 0.7113,
      "step": 1956
    },
    {
      "epoch": 3.937625754527163,
      "grad_norm": 0.21865668892860413,
      "learning_rate": 0.0001921440788811752,
      "loss": 0.6614,
      "step": 1957
    },
    {
      "epoch": 3.9396378269617705,
      "grad_norm": 0.2409597784280777,
      "learning_rate": 0.00019214005433142167,
      "loss": 0.6999,
      "step": 1958
    },
    {
      "epoch": 3.941649899396378,
      "grad_norm": 0.23715567588806152,
      "learning_rate": 0.00019213602978166818,
      "loss": 0.7161,
      "step": 1959
    },
    {
      "epoch": 3.943661971830986,
      "grad_norm": 0.23740948736667633,
      "learning_rate": 0.00019213200523191467,
      "loss": 0.6668,
      "step": 1960
    },
    {
      "epoch": 3.9456740442655933,
      "grad_norm": 0.2416006475687027,
      "learning_rate": 0.0001921279806821612,
      "loss": 0.7558,
      "step": 1961
    },
    {
      "epoch": 3.947686116700201,
      "grad_norm": 0.2251882255077362,
      "learning_rate": 0.0001921239561324077,
      "loss": 0.7179,
      "step": 1962
    },
    {
      "epoch": 3.949698189134809,
      "grad_norm": 0.2279040813446045,
      "learning_rate": 0.0001921199315826542,
      "loss": 0.7007,
      "step": 1963
    },
    {
      "epoch": 3.9517102615694166,
      "grad_norm": 0.2217198610305786,
      "learning_rate": 0.0001921159070329007,
      "loss": 0.6801,
      "step": 1964
    },
    {
      "epoch": 3.9537223340040244,
      "grad_norm": 0.23199497163295746,
      "learning_rate": 0.0001921118824831472,
      "loss": 0.7133,
      "step": 1965
    },
    {
      "epoch": 3.9557344064386317,
      "grad_norm": 0.23776012659072876,
      "learning_rate": 0.00019210785793339372,
      "loss": 0.6885,
      "step": 1966
    },
    {
      "epoch": 3.9577464788732395,
      "grad_norm": 0.24436809122562408,
      "learning_rate": 0.00019210383338364023,
      "loss": 0.6473,
      "step": 1967
    },
    {
      "epoch": 3.9597585513078473,
      "grad_norm": 0.23780950903892517,
      "learning_rate": 0.0001920998088338867,
      "loss": 0.6813,
      "step": 1968
    },
    {
      "epoch": 3.9617706237424546,
      "grad_norm": 0.23171818256378174,
      "learning_rate": 0.00019209578428413323,
      "loss": 0.6606,
      "step": 1969
    },
    {
      "epoch": 3.9637826961770624,
      "grad_norm": 0.2516558766365051,
      "learning_rate": 0.0001920917597343797,
      "loss": 0.7438,
      "step": 1970
    },
    {
      "epoch": 3.96579476861167,
      "grad_norm": 0.22207097709178925,
      "learning_rate": 0.00019208773518462625,
      "loss": 0.6576,
      "step": 1971
    },
    {
      "epoch": 3.9678068410462775,
      "grad_norm": 0.24184763431549072,
      "learning_rate": 0.00019208371063487274,
      "loss": 0.7008,
      "step": 1972
    },
    {
      "epoch": 3.9698189134808852,
      "grad_norm": 0.2252059131860733,
      "learning_rate": 0.00019207968608511925,
      "loss": 0.745,
      "step": 1973
    },
    {
      "epoch": 3.971830985915493,
      "grad_norm": 0.22978946566581726,
      "learning_rate": 0.00019207566153536573,
      "loss": 0.6591,
      "step": 1974
    },
    {
      "epoch": 3.9738430583501008,
      "grad_norm": 0.2240496277809143,
      "learning_rate": 0.00019207163698561224,
      "loss": 0.7411,
      "step": 1975
    },
    {
      "epoch": 3.975855130784708,
      "grad_norm": 0.23332056403160095,
      "learning_rate": 0.00019206761243585876,
      "loss": 0.6989,
      "step": 1976
    },
    {
      "epoch": 3.977867203219316,
      "grad_norm": 0.22128255665302277,
      "learning_rate": 0.00019206358788610527,
      "loss": 0.7116,
      "step": 1977
    },
    {
      "epoch": 3.9798792756539236,
      "grad_norm": 0.23008668422698975,
      "learning_rate": 0.00019205956333635175,
      "loss": 0.7215,
      "step": 1978
    },
    {
      "epoch": 3.9818913480885314,
      "grad_norm": 0.23738893866539001,
      "learning_rate": 0.00019205553878659827,
      "loss": 0.7246,
      "step": 1979
    },
    {
      "epoch": 3.9839034205231387,
      "grad_norm": 0.23592744767665863,
      "learning_rate": 0.00019205151423684475,
      "loss": 0.7228,
      "step": 1980
    },
    {
      "epoch": 3.9859154929577465,
      "grad_norm": 0.2348884791135788,
      "learning_rate": 0.00019204748968709126,
      "loss": 0.7258,
      "step": 1981
    },
    {
      "epoch": 3.9879275653923543,
      "grad_norm": 0.22140610218048096,
      "learning_rate": 0.00019204346513733778,
      "loss": 0.6615,
      "step": 1982
    },
    {
      "epoch": 3.9899396378269616,
      "grad_norm": 0.23999762535095215,
      "learning_rate": 0.00019203944058758426,
      "loss": 0.7301,
      "step": 1983
    },
    {
      "epoch": 3.9919517102615694,
      "grad_norm": 0.21324191987514496,
      "learning_rate": 0.00019203541603783077,
      "loss": 0.6782,
      "step": 1984
    },
    {
      "epoch": 3.993963782696177,
      "grad_norm": 0.23234009742736816,
      "learning_rate": 0.00019203139148807729,
      "loss": 0.7362,
      "step": 1985
    },
    {
      "epoch": 3.9959758551307845,
      "grad_norm": 0.24916483461856842,
      "learning_rate": 0.0001920273669383238,
      "loss": 0.7391,
      "step": 1986
    },
    {
      "epoch": 3.9979879275653922,
      "grad_norm": 0.23296374082565308,
      "learning_rate": 0.00019202334238857028,
      "loss": 0.7154,
      "step": 1987
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.24847015738487244,
      "learning_rate": 0.0001920193178388168,
      "loss": 0.7283,
      "step": 1988
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.7226448655128479,
      "eval_runtime": 49.8286,
      "eval_samples_per_second": 19.908,
      "eval_steps_per_second": 2.489,
      "step": 1988
    },
    {
      "epoch": 4.002012072434607,
      "grad_norm": 0.2598626911640167,
      "learning_rate": 0.00019201529328906328,
      "loss": 0.6564,
      "step": 1989
    },
    {
      "epoch": 4.0040241448692155,
      "grad_norm": 0.2350320667028427,
      "learning_rate": 0.0001920112687393098,
      "loss": 0.6895,
      "step": 1990
    },
    {
      "epoch": 4.006036217303823,
      "grad_norm": 0.2473616749048233,
      "learning_rate": 0.0001920072441895563,
      "loss": 0.6239,
      "step": 1991
    },
    {
      "epoch": 4.00804828973843,
      "grad_norm": 0.27403032779693604,
      "learning_rate": 0.00019200321963980282,
      "loss": 0.6906,
      "step": 1992
    },
    {
      "epoch": 4.010060362173038,
      "grad_norm": 0.2716118097305298,
      "learning_rate": 0.0001919991950900493,
      "loss": 0.661,
      "step": 1993
    },
    {
      "epoch": 4.012072434607646,
      "grad_norm": 0.28519225120544434,
      "learning_rate": 0.00019199517054029581,
      "loss": 0.6526,
      "step": 1994
    },
    {
      "epoch": 4.014084507042254,
      "grad_norm": 0.25795575976371765,
      "learning_rate": 0.0001919911459905423,
      "loss": 0.6425,
      "step": 1995
    },
    {
      "epoch": 4.016096579476861,
      "grad_norm": 0.2514059543609619,
      "learning_rate": 0.00019198712144078884,
      "loss": 0.6397,
      "step": 1996
    },
    {
      "epoch": 4.018108651911469,
      "grad_norm": 0.2397656887769699,
      "learning_rate": 0.00019198309689103532,
      "loss": 0.6711,
      "step": 1997
    },
    {
      "epoch": 4.020120724346077,
      "grad_norm": 0.24184304475784302,
      "learning_rate": 0.00019197907234128184,
      "loss": 0.6684,
      "step": 1998
    },
    {
      "epoch": 4.022132796780684,
      "grad_norm": 0.24233420193195343,
      "learning_rate": 0.00019197504779152832,
      "loss": 0.6735,
      "step": 1999
    },
    {
      "epoch": 4.0241448692152915,
      "grad_norm": 0.2524791657924652,
      "learning_rate": 0.00019197102324177483,
      "loss": 0.6781,
      "step": 2000
    },
    {
      "epoch": 4.0261569416499,
      "grad_norm": 0.26553505659103394,
      "learning_rate": 0.00019196699869202135,
      "loss": 0.645,
      "step": 2001
    },
    {
      "epoch": 4.028169014084507,
      "grad_norm": 0.2437235414981842,
      "learning_rate": 0.00019196297414226786,
      "loss": 0.6832,
      "step": 2002
    },
    {
      "epoch": 4.030181086519114,
      "grad_norm": 0.24902240931987762,
      "learning_rate": 0.00019195894959251434,
      "loss": 0.6784,
      "step": 2003
    },
    {
      "epoch": 4.0321931589537225,
      "grad_norm": 0.2441614270210266,
      "learning_rate": 0.00019195492504276085,
      "loss": 0.6285,
      "step": 2004
    },
    {
      "epoch": 4.03420523138833,
      "grad_norm": 0.27031856775283813,
      "learning_rate": 0.00019195090049300734,
      "loss": 0.6984,
      "step": 2005
    },
    {
      "epoch": 4.036217303822937,
      "grad_norm": 0.2521320879459381,
      "learning_rate": 0.00019194687594325388,
      "loss": 0.6388,
      "step": 2006
    },
    {
      "epoch": 4.038229376257545,
      "grad_norm": 0.24036750197410583,
      "learning_rate": 0.00019194285139350036,
      "loss": 0.646,
      "step": 2007
    },
    {
      "epoch": 4.040241448692153,
      "grad_norm": 0.2563973367214203,
      "learning_rate": 0.00019193882684374688,
      "loss": 0.6892,
      "step": 2008
    },
    {
      "epoch": 4.042253521126761,
      "grad_norm": 0.23272980749607086,
      "learning_rate": 0.00019193480229399336,
      "loss": 0.6533,
      "step": 2009
    },
    {
      "epoch": 4.044265593561368,
      "grad_norm": 0.24580314755439758,
      "learning_rate": 0.00019193077774423987,
      "loss": 0.6841,
      "step": 2010
    },
    {
      "epoch": 4.046277665995976,
      "grad_norm": 0.24550294876098633,
      "learning_rate": 0.00019192675319448639,
      "loss": 0.6506,
      "step": 2011
    },
    {
      "epoch": 4.048289738430584,
      "grad_norm": 0.26783764362335205,
      "learning_rate": 0.0001919227286447329,
      "loss": 0.6563,
      "step": 2012
    },
    {
      "epoch": 4.050301810865191,
      "grad_norm": 0.2741577625274658,
      "learning_rate": 0.00019191870409497938,
      "loss": 0.6953,
      "step": 2013
    },
    {
      "epoch": 4.0523138832997985,
      "grad_norm": 0.2639622986316681,
      "learning_rate": 0.0001919146795452259,
      "loss": 0.696,
      "step": 2014
    },
    {
      "epoch": 4.054325955734407,
      "grad_norm": 0.26250341534614563,
      "learning_rate": 0.00019191065499547238,
      "loss": 0.6858,
      "step": 2015
    },
    {
      "epoch": 4.056338028169014,
      "grad_norm": 0.2627863585948944,
      "learning_rate": 0.0001919066304457189,
      "loss": 0.6344,
      "step": 2016
    },
    {
      "epoch": 4.058350100603621,
      "grad_norm": 0.25507694482803345,
      "learning_rate": 0.0001919026058959654,
      "loss": 0.6207,
      "step": 2017
    },
    {
      "epoch": 4.0603621730382295,
      "grad_norm": 0.2671705186367035,
      "learning_rate": 0.0001918985813462119,
      "loss": 0.6894,
      "step": 2018
    },
    {
      "epoch": 4.062374245472837,
      "grad_norm": 0.2678779363632202,
      "learning_rate": 0.0001918945567964584,
      "loss": 0.6426,
      "step": 2019
    },
    {
      "epoch": 4.064386317907445,
      "grad_norm": 0.2637898921966553,
      "learning_rate": 0.00019189053224670491,
      "loss": 0.6519,
      "step": 2020
    },
    {
      "epoch": 4.066398390342052,
      "grad_norm": 0.24093830585479736,
      "learning_rate": 0.00019188650769695143,
      "loss": 0.6233,
      "step": 2021
    },
    {
      "epoch": 4.06841046277666,
      "grad_norm": 0.24330611526966095,
      "learning_rate": 0.0001918824831471979,
      "loss": 0.6507,
      "step": 2022
    },
    {
      "epoch": 4.070422535211268,
      "grad_norm": 0.25373324751853943,
      "learning_rate": 0.00019187845859744442,
      "loss": 0.6533,
      "step": 2023
    },
    {
      "epoch": 4.072434607645875,
      "grad_norm": 0.25173676013946533,
      "learning_rate": 0.0001918744340476909,
      "loss": 0.6495,
      "step": 2024
    },
    {
      "epoch": 4.074446680080483,
      "grad_norm": 0.24482721090316772,
      "learning_rate": 0.00019187040949793742,
      "loss": 0.6235,
      "step": 2025
    },
    {
      "epoch": 4.076458752515091,
      "grad_norm": 0.2531599700450897,
      "learning_rate": 0.00019186638494818393,
      "loss": 0.6911,
      "step": 2026
    },
    {
      "epoch": 4.078470824949698,
      "grad_norm": 0.25460612773895264,
      "learning_rate": 0.00019186236039843045,
      "loss": 0.6673,
      "step": 2027
    },
    {
      "epoch": 4.0804828973843055,
      "grad_norm": 0.24156071245670319,
      "learning_rate": 0.00019185833584867693,
      "loss": 0.6796,
      "step": 2028
    },
    {
      "epoch": 4.082494969818914,
      "grad_norm": 0.26300618052482605,
      "learning_rate": 0.00019185431129892344,
      "loss": 0.6622,
      "step": 2029
    },
    {
      "epoch": 4.084507042253521,
      "grad_norm": 0.262004554271698,
      "learning_rate": 0.00019185028674916993,
      "loss": 0.6524,
      "step": 2030
    },
    {
      "epoch": 4.086519114688128,
      "grad_norm": 0.22730819880962372,
      "learning_rate": 0.00019184626219941647,
      "loss": 0.6518,
      "step": 2031
    },
    {
      "epoch": 4.0885311871227366,
      "grad_norm": 0.26053091883659363,
      "learning_rate": 0.00019184223764966295,
      "loss": 0.6775,
      "step": 2032
    },
    {
      "epoch": 4.090543259557344,
      "grad_norm": 0.24195434153079987,
      "learning_rate": 0.00019183821309990947,
      "loss": 0.5969,
      "step": 2033
    },
    {
      "epoch": 4.092555331991952,
      "grad_norm": 0.2651550769805908,
      "learning_rate": 0.00019183418855015595,
      "loss": 0.7056,
      "step": 2034
    },
    {
      "epoch": 4.094567404426559,
      "grad_norm": 0.2533281147480011,
      "learning_rate": 0.00019183016400040246,
      "loss": 0.6543,
      "step": 2035
    },
    {
      "epoch": 4.096579476861167,
      "grad_norm": 0.23608127236366272,
      "learning_rate": 0.00019182613945064897,
      "loss": 0.688,
      "step": 2036
    },
    {
      "epoch": 4.098591549295775,
      "grad_norm": 0.2500976324081421,
      "learning_rate": 0.0001918221149008955,
      "loss": 0.6533,
      "step": 2037
    },
    {
      "epoch": 4.100603621730382,
      "grad_norm": 0.255687415599823,
      "learning_rate": 0.00019181809035114197,
      "loss": 0.6703,
      "step": 2038
    },
    {
      "epoch": 4.10261569416499,
      "grad_norm": 0.261843204498291,
      "learning_rate": 0.00019181406580138848,
      "loss": 0.6728,
      "step": 2039
    },
    {
      "epoch": 4.104627766599598,
      "grad_norm": 0.2469487339258194,
      "learning_rate": 0.00019181004125163497,
      "loss": 0.6488,
      "step": 2040
    },
    {
      "epoch": 4.106639839034205,
      "grad_norm": 0.2542511522769928,
      "learning_rate": 0.0001918060167018815,
      "loss": 0.6778,
      "step": 2041
    },
    {
      "epoch": 4.1086519114688125,
      "grad_norm": 0.24327674508094788,
      "learning_rate": 0.000191801992152128,
      "loss": 0.6575,
      "step": 2042
    },
    {
      "epoch": 4.110663983903421,
      "grad_norm": 0.2510167062282562,
      "learning_rate": 0.0001917979676023745,
      "loss": 0.6284,
      "step": 2043
    },
    {
      "epoch": 4.112676056338028,
      "grad_norm": 0.249923437833786,
      "learning_rate": 0.000191793943052621,
      "loss": 0.6442,
      "step": 2044
    },
    {
      "epoch": 4.114688128772636,
      "grad_norm": 0.24675175547599792,
      "learning_rate": 0.0001917899185028675,
      "loss": 0.6683,
      "step": 2045
    },
    {
      "epoch": 4.116700201207244,
      "grad_norm": 0.2442193478345871,
      "learning_rate": 0.00019178589395311402,
      "loss": 0.6739,
      "step": 2046
    },
    {
      "epoch": 4.118712273641851,
      "grad_norm": 0.24996230006217957,
      "learning_rate": 0.00019178186940336053,
      "loss": 0.6781,
      "step": 2047
    },
    {
      "epoch": 4.120724346076459,
      "grad_norm": 0.24932384490966797,
      "learning_rate": 0.000191777844853607,
      "loss": 0.6099,
      "step": 2048
    },
    {
      "epoch": 4.122736418511066,
      "grad_norm": 0.2448396384716034,
      "learning_rate": 0.00019177382030385353,
      "loss": 0.661,
      "step": 2049
    },
    {
      "epoch": 4.124748490945674,
      "grad_norm": 0.24644187092781067,
      "learning_rate": 0.0001917697957541,
      "loss": 0.658,
      "step": 2050
    },
    {
      "epoch": 4.126760563380282,
      "grad_norm": 0.23897001147270203,
      "learning_rate": 0.00019176577120434652,
      "loss": 0.6788,
      "step": 2051
    },
    {
      "epoch": 4.128772635814889,
      "grad_norm": 0.26707923412323,
      "learning_rate": 0.00019176174665459303,
      "loss": 0.6538,
      "step": 2052
    },
    {
      "epoch": 4.130784708249497,
      "grad_norm": 0.2475564032793045,
      "learning_rate": 0.00019175772210483952,
      "loss": 0.7115,
      "step": 2053
    },
    {
      "epoch": 4.132796780684105,
      "grad_norm": 0.24095818400382996,
      "learning_rate": 0.00019175369755508603,
      "loss": 0.6746,
      "step": 2054
    },
    {
      "epoch": 4.134808853118712,
      "grad_norm": 0.24550622701644897,
      "learning_rate": 0.00019174967300533252,
      "loss": 0.6628,
      "step": 2055
    },
    {
      "epoch": 4.1368209255533195,
      "grad_norm": 0.2448495775461197,
      "learning_rate": 0.00019174564845557906,
      "loss": 0.6301,
      "step": 2056
    },
    {
      "epoch": 4.138832997987928,
      "grad_norm": 0.24670813977718353,
      "learning_rate": 0.00019174162390582554,
      "loss": 0.6679,
      "step": 2057
    },
    {
      "epoch": 4.140845070422535,
      "grad_norm": 0.24138143658638,
      "learning_rate": 0.00019173759935607205,
      "loss": 0.646,
      "step": 2058
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 0.24808178842067719,
      "learning_rate": 0.00019173357480631854,
      "loss": 0.671,
      "step": 2059
    },
    {
      "epoch": 4.144869215291751,
      "grad_norm": 0.24059553444385529,
      "learning_rate": 0.00019172955025656505,
      "loss": 0.65,
      "step": 2060
    },
    {
      "epoch": 4.146881287726358,
      "grad_norm": 0.2464541494846344,
      "learning_rate": 0.00019172552570681156,
      "loss": 0.6552,
      "step": 2061
    },
    {
      "epoch": 4.148893360160966,
      "grad_norm": 0.25317931175231934,
      "learning_rate": 0.00019172150115705808,
      "loss": 0.6664,
      "step": 2062
    },
    {
      "epoch": 4.150905432595573,
      "grad_norm": 0.24394290149211884,
      "learning_rate": 0.00019171747660730456,
      "loss": 0.681,
      "step": 2063
    },
    {
      "epoch": 4.152917505030181,
      "grad_norm": 0.2581652104854584,
      "learning_rate": 0.00019171345205755107,
      "loss": 0.7069,
      "step": 2064
    },
    {
      "epoch": 4.154929577464789,
      "grad_norm": 0.27646884322166443,
      "learning_rate": 0.00019170942750779756,
      "loss": 0.6902,
      "step": 2065
    },
    {
      "epoch": 4.156941649899396,
      "grad_norm": 0.25098341703414917,
      "learning_rate": 0.00019170540295804407,
      "loss": 0.685,
      "step": 2066
    },
    {
      "epoch": 4.158953722334004,
      "grad_norm": 0.24929063022136688,
      "learning_rate": 0.00019170137840829058,
      "loss": 0.6691,
      "step": 2067
    },
    {
      "epoch": 4.160965794768612,
      "grad_norm": 0.24946734309196472,
      "learning_rate": 0.0001916973538585371,
      "loss": 0.6645,
      "step": 2068
    },
    {
      "epoch": 4.162977867203219,
      "grad_norm": 0.2397080957889557,
      "learning_rate": 0.00019169332930878358,
      "loss": 0.6547,
      "step": 2069
    },
    {
      "epoch": 4.164989939637827,
      "grad_norm": 0.2621222734451294,
      "learning_rate": 0.0001916893047590301,
      "loss": 0.6373,
      "step": 2070
    },
    {
      "epoch": 4.167002012072435,
      "grad_norm": 0.26127222180366516,
      "learning_rate": 0.00019168528020927658,
      "loss": 0.6823,
      "step": 2071
    },
    {
      "epoch": 4.169014084507042,
      "grad_norm": 0.2393663227558136,
      "learning_rate": 0.00019168125565952312,
      "loss": 0.6328,
      "step": 2072
    },
    {
      "epoch": 4.17102615694165,
      "grad_norm": 0.25134164094924927,
      "learning_rate": 0.0001916772311097696,
      "loss": 0.656,
      "step": 2073
    },
    {
      "epoch": 4.173038229376258,
      "grad_norm": 0.25071385502815247,
      "learning_rate": 0.00019167320656001611,
      "loss": 0.6687,
      "step": 2074
    },
    {
      "epoch": 4.175050301810865,
      "grad_norm": 0.2636944055557251,
      "learning_rate": 0.0001916691820102626,
      "loss": 0.6814,
      "step": 2075
    },
    {
      "epoch": 4.177062374245473,
      "grad_norm": 0.2715793550014496,
      "learning_rate": 0.0001916651574605091,
      "loss": 0.657,
      "step": 2076
    },
    {
      "epoch": 4.17907444668008,
      "grad_norm": 0.2635582685470581,
      "learning_rate": 0.00019166113291075562,
      "loss": 0.6613,
      "step": 2077
    },
    {
      "epoch": 4.181086519114688,
      "grad_norm": 0.24337469041347504,
      "learning_rate": 0.00019165710836100214,
      "loss": 0.6549,
      "step": 2078
    },
    {
      "epoch": 4.183098591549296,
      "grad_norm": 0.23874641954898834,
      "learning_rate": 0.00019165308381124862,
      "loss": 0.6565,
      "step": 2079
    },
    {
      "epoch": 4.185110663983903,
      "grad_norm": 0.24666403234004974,
      "learning_rate": 0.00019164905926149513,
      "loss": 0.6496,
      "step": 2080
    },
    {
      "epoch": 4.187122736418511,
      "grad_norm": 0.24231067299842834,
      "learning_rate": 0.00019164503471174162,
      "loss": 0.6686,
      "step": 2081
    },
    {
      "epoch": 4.189134808853119,
      "grad_norm": 0.26011958718299866,
      "learning_rate": 0.00019164101016198813,
      "loss": 0.6524,
      "step": 2082
    },
    {
      "epoch": 4.191146881287726,
      "grad_norm": 0.26048803329467773,
      "learning_rate": 0.00019163698561223464,
      "loss": 0.6822,
      "step": 2083
    },
    {
      "epoch": 4.193158953722334,
      "grad_norm": 0.2411729246377945,
      "learning_rate": 0.00019163296106248115,
      "loss": 0.6409,
      "step": 2084
    },
    {
      "epoch": 4.195171026156942,
      "grad_norm": 0.24880942702293396,
      "learning_rate": 0.00019162893651272764,
      "loss": 0.6777,
      "step": 2085
    },
    {
      "epoch": 4.197183098591549,
      "grad_norm": 0.25758904218673706,
      "learning_rate": 0.00019162491196297415,
      "loss": 0.6669,
      "step": 2086
    },
    {
      "epoch": 4.199195171026157,
      "grad_norm": 0.24849484860897064,
      "learning_rate": 0.00019162088741322066,
      "loss": 0.6791,
      "step": 2087
    },
    {
      "epoch": 4.201207243460765,
      "grad_norm": 0.23938621580600739,
      "learning_rate": 0.00019161686286346715,
      "loss": 0.6477,
      "step": 2088
    },
    {
      "epoch": 4.203219315895372,
      "grad_norm": 0.24315741658210754,
      "learning_rate": 0.00019161283831371366,
      "loss": 0.6746,
      "step": 2089
    },
    {
      "epoch": 4.20523138832998,
      "grad_norm": 0.24814654886722565,
      "learning_rate": 0.00019160881376396015,
      "loss": 0.6756,
      "step": 2090
    },
    {
      "epoch": 4.207243460764587,
      "grad_norm": 0.2603759169578552,
      "learning_rate": 0.00019160478921420666,
      "loss": 0.6764,
      "step": 2091
    },
    {
      "epoch": 4.209255533199195,
      "grad_norm": 0.2631041407585144,
      "learning_rate": 0.00019160076466445317,
      "loss": 0.7025,
      "step": 2092
    },
    {
      "epoch": 4.211267605633803,
      "grad_norm": 0.2415754795074463,
      "learning_rate": 0.00019159674011469968,
      "loss": 0.7021,
      "step": 2093
    },
    {
      "epoch": 4.21327967806841,
      "grad_norm": 0.24501265585422516,
      "learning_rate": 0.00019159271556494617,
      "loss": 0.6851,
      "step": 2094
    },
    {
      "epoch": 4.2152917505030185,
      "grad_norm": 0.24509674310684204,
      "learning_rate": 0.00019158869101519268,
      "loss": 0.6804,
      "step": 2095
    },
    {
      "epoch": 4.217303822937626,
      "grad_norm": 0.27070868015289307,
      "learning_rate": 0.00019158466646543917,
      "loss": 0.6764,
      "step": 2096
    },
    {
      "epoch": 4.219315895372233,
      "grad_norm": 0.2542610764503479,
      "learning_rate": 0.0001915806419156857,
      "loss": 0.6761,
      "step": 2097
    },
    {
      "epoch": 4.221327967806841,
      "grad_norm": 0.2570990324020386,
      "learning_rate": 0.0001915766173659322,
      "loss": 0.6552,
      "step": 2098
    },
    {
      "epoch": 4.223340040241449,
      "grad_norm": 0.23751747608184814,
      "learning_rate": 0.0001915725928161787,
      "loss": 0.6615,
      "step": 2099
    },
    {
      "epoch": 4.225352112676056,
      "grad_norm": 0.25504070520401,
      "learning_rate": 0.0001915685682664252,
      "loss": 0.6887,
      "step": 2100
    },
    {
      "epoch": 4.227364185110664,
      "grad_norm": 0.25126415491104126,
      "learning_rate": 0.0001915645437166717,
      "loss": 0.6728,
      "step": 2101
    },
    {
      "epoch": 4.229376257545272,
      "grad_norm": 0.2526719570159912,
      "learning_rate": 0.0001915605191669182,
      "loss": 0.6809,
      "step": 2102
    },
    {
      "epoch": 4.231388329979879,
      "grad_norm": 0.2511559724807739,
      "learning_rate": 0.00019155649461716472,
      "loss": 0.6903,
      "step": 2103
    },
    {
      "epoch": 4.233400402414487,
      "grad_norm": 0.245673269033432,
      "learning_rate": 0.0001915524700674112,
      "loss": 0.6452,
      "step": 2104
    },
    {
      "epoch": 4.2354124748490944,
      "grad_norm": 0.25292566418647766,
      "learning_rate": 0.00019154844551765772,
      "loss": 0.6882,
      "step": 2105
    },
    {
      "epoch": 4.237424547283702,
      "grad_norm": 0.25727686285972595,
      "learning_rate": 0.0001915444209679042,
      "loss": 0.6749,
      "step": 2106
    },
    {
      "epoch": 4.23943661971831,
      "grad_norm": 0.2637469470500946,
      "learning_rate": 0.00019154039641815075,
      "loss": 0.7046,
      "step": 2107
    },
    {
      "epoch": 4.241448692152917,
      "grad_norm": 0.2624885141849518,
      "learning_rate": 0.00019153637186839723,
      "loss": 0.6559,
      "step": 2108
    },
    {
      "epoch": 4.2434607645875255,
      "grad_norm": 0.26671865582466125,
      "learning_rate": 0.00019153234731864374,
      "loss": 0.6914,
      "step": 2109
    },
    {
      "epoch": 4.245472837022133,
      "grad_norm": 0.2469983845949173,
      "learning_rate": 0.00019152832276889023,
      "loss": 0.6218,
      "step": 2110
    },
    {
      "epoch": 4.24748490945674,
      "grad_norm": 0.25647416710853577,
      "learning_rate": 0.00019152429821913674,
      "loss": 0.711,
      "step": 2111
    },
    {
      "epoch": 4.249496981891348,
      "grad_norm": 0.24772123992443085,
      "learning_rate": 0.00019152027366938325,
      "loss": 0.6472,
      "step": 2112
    },
    {
      "epoch": 4.251509054325956,
      "grad_norm": 0.2778399586677551,
      "learning_rate": 0.00019151624911962977,
      "loss": 0.6117,
      "step": 2113
    },
    {
      "epoch": 4.253521126760563,
      "grad_norm": 0.2671236991882324,
      "learning_rate": 0.00019151222456987625,
      "loss": 0.6641,
      "step": 2114
    },
    {
      "epoch": 4.255533199195171,
      "grad_norm": 0.27354493737220764,
      "learning_rate": 0.00019150820002012276,
      "loss": 0.7078,
      "step": 2115
    },
    {
      "epoch": 4.257545271629779,
      "grad_norm": 0.2716752588748932,
      "learning_rate": 0.00019150417547036925,
      "loss": 0.6472,
      "step": 2116
    },
    {
      "epoch": 4.259557344064386,
      "grad_norm": 0.23597382009029388,
      "learning_rate": 0.00019150015092061576,
      "loss": 0.6364,
      "step": 2117
    },
    {
      "epoch": 4.261569416498994,
      "grad_norm": 0.25081172585487366,
      "learning_rate": 0.00019149612637086227,
      "loss": 0.6573,
      "step": 2118
    },
    {
      "epoch": 4.2635814889336014,
      "grad_norm": 0.2621295750141144,
      "learning_rate": 0.00019149210182110878,
      "loss": 0.6411,
      "step": 2119
    },
    {
      "epoch": 4.26559356136821,
      "grad_norm": 0.26449552178382874,
      "learning_rate": 0.00019148807727135527,
      "loss": 0.6887,
      "step": 2120
    },
    {
      "epoch": 4.267605633802817,
      "grad_norm": 0.24817000329494476,
      "learning_rate": 0.00019148405272160178,
      "loss": 0.6425,
      "step": 2121
    },
    {
      "epoch": 4.269617706237424,
      "grad_norm": 0.27120450139045715,
      "learning_rate": 0.0001914800281718483,
      "loss": 0.7238,
      "step": 2122
    },
    {
      "epoch": 4.2716297786720325,
      "grad_norm": 0.23415806889533997,
      "learning_rate": 0.00019147600362209478,
      "loss": 0.6223,
      "step": 2123
    },
    {
      "epoch": 4.27364185110664,
      "grad_norm": 0.2825368344783783,
      "learning_rate": 0.0001914719790723413,
      "loss": 0.7078,
      "step": 2124
    },
    {
      "epoch": 4.275653923541247,
      "grad_norm": 0.24471786618232727,
      "learning_rate": 0.00019146795452258778,
      "loss": 0.6924,
      "step": 2125
    },
    {
      "epoch": 4.277665995975855,
      "grad_norm": 0.2339487224817276,
      "learning_rate": 0.0001914639299728343,
      "loss": 0.6539,
      "step": 2126
    },
    {
      "epoch": 4.279678068410463,
      "grad_norm": 0.2520397901535034,
      "learning_rate": 0.0001914599054230808,
      "loss": 0.7079,
      "step": 2127
    },
    {
      "epoch": 4.28169014084507,
      "grad_norm": 0.2539738416671753,
      "learning_rate": 0.0001914558808733273,
      "loss": 0.7092,
      "step": 2128
    },
    {
      "epoch": 4.283702213279678,
      "grad_norm": 0.2548232972621918,
      "learning_rate": 0.0001914518563235738,
      "loss": 0.6576,
      "step": 2129
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.24831055104732513,
      "learning_rate": 0.0001914478317738203,
      "loss": 0.663,
      "step": 2130
    },
    {
      "epoch": 4.287726358148893,
      "grad_norm": 0.24951568245887756,
      "learning_rate": 0.0001914438072240668,
      "loss": 0.6657,
      "step": 2131
    },
    {
      "epoch": 4.289738430583501,
      "grad_norm": 0.2663450539112091,
      "learning_rate": 0.00019143978267431333,
      "loss": 0.6446,
      "step": 2132
    },
    {
      "epoch": 4.2917505030181085,
      "grad_norm": 0.2738293707370758,
      "learning_rate": 0.00019143575812455982,
      "loss": 0.7183,
      "step": 2133
    },
    {
      "epoch": 4.293762575452717,
      "grad_norm": 0.24269872903823853,
      "learning_rate": 0.00019143173357480633,
      "loss": 0.6319,
      "step": 2134
    },
    {
      "epoch": 4.295774647887324,
      "grad_norm": 0.25219088792800903,
      "learning_rate": 0.00019142770902505282,
      "loss": 0.6524,
      "step": 2135
    },
    {
      "epoch": 4.297786720321931,
      "grad_norm": 0.26175644993782043,
      "learning_rate": 0.00019142368447529933,
      "loss": 0.6573,
      "step": 2136
    },
    {
      "epoch": 4.2997987927565395,
      "grad_norm": 0.24303297698497772,
      "learning_rate": 0.00019141965992554584,
      "loss": 0.6775,
      "step": 2137
    },
    {
      "epoch": 4.301810865191147,
      "grad_norm": 0.24732257425785065,
      "learning_rate": 0.00019141563537579235,
      "loss": 0.6813,
      "step": 2138
    },
    {
      "epoch": 4.303822937625754,
      "grad_norm": 0.24309657514095306,
      "learning_rate": 0.00019141161082603884,
      "loss": 0.6518,
      "step": 2139
    },
    {
      "epoch": 4.305835010060362,
      "grad_norm": 0.2580138146877289,
      "learning_rate": 0.00019140758627628535,
      "loss": 0.6469,
      "step": 2140
    },
    {
      "epoch": 4.30784708249497,
      "grad_norm": 0.25585368275642395,
      "learning_rate": 0.00019140356172653184,
      "loss": 0.7425,
      "step": 2141
    },
    {
      "epoch": 4.309859154929577,
      "grad_norm": 0.2518506646156311,
      "learning_rate": 0.00019139953717677838,
      "loss": 0.7039,
      "step": 2142
    },
    {
      "epoch": 4.311871227364185,
      "grad_norm": 0.24892781674861908,
      "learning_rate": 0.00019139551262702486,
      "loss": 0.6587,
      "step": 2143
    },
    {
      "epoch": 4.313883299798793,
      "grad_norm": 0.2602398693561554,
      "learning_rate": 0.00019139148807727137,
      "loss": 0.6929,
      "step": 2144
    },
    {
      "epoch": 4.315895372233401,
      "grad_norm": 0.24641457200050354,
      "learning_rate": 0.00019138746352751786,
      "loss": 0.6846,
      "step": 2145
    },
    {
      "epoch": 4.317907444668008,
      "grad_norm": 0.2401539534330368,
      "learning_rate": 0.00019138343897776437,
      "loss": 0.6629,
      "step": 2146
    },
    {
      "epoch": 4.3199195171026155,
      "grad_norm": 0.2348228394985199,
      "learning_rate": 0.00019137941442801088,
      "loss": 0.6943,
      "step": 2147
    },
    {
      "epoch": 4.321931589537224,
      "grad_norm": 0.25574278831481934,
      "learning_rate": 0.0001913753898782574,
      "loss": 0.7018,
      "step": 2148
    },
    {
      "epoch": 4.323943661971831,
      "grad_norm": 0.2506854832172394,
      "learning_rate": 0.00019137136532850388,
      "loss": 0.6496,
      "step": 2149
    },
    {
      "epoch": 4.325955734406438,
      "grad_norm": 0.2613626718521118,
      "learning_rate": 0.0001913673407787504,
      "loss": 0.6709,
      "step": 2150
    },
    {
      "epoch": 4.3279678068410465,
      "grad_norm": 0.2607444226741791,
      "learning_rate": 0.00019136331622899688,
      "loss": 0.6731,
      "step": 2151
    },
    {
      "epoch": 4.329979879275654,
      "grad_norm": 0.24822919070720673,
      "learning_rate": 0.0001913592916792434,
      "loss": 0.6408,
      "step": 2152
    },
    {
      "epoch": 4.331991951710261,
      "grad_norm": 0.23574842512607574,
      "learning_rate": 0.0001913552671294899,
      "loss": 0.6591,
      "step": 2153
    },
    {
      "epoch": 4.334004024144869,
      "grad_norm": 0.2449333369731903,
      "learning_rate": 0.00019135124257973641,
      "loss": 0.6947,
      "step": 2154
    },
    {
      "epoch": 4.336016096579477,
      "grad_norm": 0.241030752658844,
      "learning_rate": 0.0001913472180299829,
      "loss": 0.662,
      "step": 2155
    },
    {
      "epoch": 4.338028169014084,
      "grad_norm": 0.24373319745063782,
      "learning_rate": 0.0001913431934802294,
      "loss": 0.6805,
      "step": 2156
    },
    {
      "epoch": 4.340040241448692,
      "grad_norm": 0.2445266842842102,
      "learning_rate": 0.00019133916893047592,
      "loss": 0.6669,
      "step": 2157
    },
    {
      "epoch": 4.3420523138833,
      "grad_norm": 0.2456342577934265,
      "learning_rate": 0.0001913351443807224,
      "loss": 0.6689,
      "step": 2158
    },
    {
      "epoch": 4.344064386317908,
      "grad_norm": 0.24587716162204742,
      "learning_rate": 0.00019133111983096892,
      "loss": 0.6696,
      "step": 2159
    },
    {
      "epoch": 4.346076458752515,
      "grad_norm": 0.2535189688205719,
      "learning_rate": 0.0001913270952812154,
      "loss": 0.6682,
      "step": 2160
    },
    {
      "epoch": 4.3480885311871225,
      "grad_norm": 0.25413912534713745,
      "learning_rate": 0.00019132307073146192,
      "loss": 0.6822,
      "step": 2161
    },
    {
      "epoch": 4.350100603621731,
      "grad_norm": 0.24885591864585876,
      "learning_rate": 0.00019131904618170843,
      "loss": 0.7252,
      "step": 2162
    },
    {
      "epoch": 4.352112676056338,
      "grad_norm": 0.2453955113887787,
      "learning_rate": 0.00019131502163195494,
      "loss": 0.6665,
      "step": 2163
    },
    {
      "epoch": 4.354124748490945,
      "grad_norm": 0.24832406640052795,
      "learning_rate": 0.00019131099708220143,
      "loss": 0.672,
      "step": 2164
    },
    {
      "epoch": 4.3561368209255535,
      "grad_norm": 0.2622406780719757,
      "learning_rate": 0.00019130697253244794,
      "loss": 0.7043,
      "step": 2165
    },
    {
      "epoch": 4.358148893360161,
      "grad_norm": 0.2517155110836029,
      "learning_rate": 0.00019130294798269442,
      "loss": 0.6467,
      "step": 2166
    },
    {
      "epoch": 4.360160965794769,
      "grad_norm": 0.24187034368515015,
      "learning_rate": 0.00019129892343294096,
      "loss": 0.6735,
      "step": 2167
    },
    {
      "epoch": 4.362173038229376,
      "grad_norm": 0.2518698573112488,
      "learning_rate": 0.00019129489888318745,
      "loss": 0.6584,
      "step": 2168
    },
    {
      "epoch": 4.364185110663984,
      "grad_norm": 0.25999268889427185,
      "learning_rate": 0.00019129087433343396,
      "loss": 0.6654,
      "step": 2169
    },
    {
      "epoch": 4.366197183098592,
      "grad_norm": 0.2634376585483551,
      "learning_rate": 0.00019128684978368045,
      "loss": 0.6602,
      "step": 2170
    },
    {
      "epoch": 4.368209255533199,
      "grad_norm": 0.24278639256954193,
      "learning_rate": 0.00019128282523392696,
      "loss": 0.6341,
      "step": 2171
    },
    {
      "epoch": 4.370221327967807,
      "grad_norm": 0.25620394945144653,
      "learning_rate": 0.00019127880068417347,
      "loss": 0.6308,
      "step": 2172
    },
    {
      "epoch": 4.372233400402415,
      "grad_norm": 0.25447481870651245,
      "learning_rate": 0.00019127477613441998,
      "loss": 0.6576,
      "step": 2173
    },
    {
      "epoch": 4.374245472837022,
      "grad_norm": 0.2621065676212311,
      "learning_rate": 0.00019127075158466647,
      "loss": 0.6938,
      "step": 2174
    },
    {
      "epoch": 4.3762575452716295,
      "grad_norm": 0.2429697960615158,
      "learning_rate": 0.00019126672703491298,
      "loss": 0.6257,
      "step": 2175
    },
    {
      "epoch": 4.378269617706238,
      "grad_norm": 0.2421499490737915,
      "learning_rate": 0.00019126270248515947,
      "loss": 0.6598,
      "step": 2176
    },
    {
      "epoch": 4.380281690140845,
      "grad_norm": 0.2421969473361969,
      "learning_rate": 0.000191258677935406,
      "loss": 0.6858,
      "step": 2177
    },
    {
      "epoch": 4.382293762575452,
      "grad_norm": 0.2684423625469208,
      "learning_rate": 0.0001912546533856525,
      "loss": 0.6897,
      "step": 2178
    },
    {
      "epoch": 4.3843058350100605,
      "grad_norm": 0.24386562407016754,
      "learning_rate": 0.000191250628835899,
      "loss": 0.654,
      "step": 2179
    },
    {
      "epoch": 4.386317907444668,
      "grad_norm": 0.24919025599956512,
      "learning_rate": 0.0001912466042861455,
      "loss": 0.6802,
      "step": 2180
    },
    {
      "epoch": 4.388329979879275,
      "grad_norm": 0.24876868724822998,
      "learning_rate": 0.000191242579736392,
      "loss": 0.655,
      "step": 2181
    },
    {
      "epoch": 4.390342052313883,
      "grad_norm": 0.24936161935329437,
      "learning_rate": 0.0001912385551866385,
      "loss": 0.6957,
      "step": 2182
    },
    {
      "epoch": 4.392354124748491,
      "grad_norm": 0.24863582849502563,
      "learning_rate": 0.00019123453063688502,
      "loss": 0.6523,
      "step": 2183
    },
    {
      "epoch": 4.394366197183099,
      "grad_norm": 0.26121222972869873,
      "learning_rate": 0.0001912305060871315,
      "loss": 0.6868,
      "step": 2184
    },
    {
      "epoch": 4.396378269617706,
      "grad_norm": 0.24563902616500854,
      "learning_rate": 0.00019122648153737802,
      "loss": 0.6574,
      "step": 2185
    },
    {
      "epoch": 4.398390342052314,
      "grad_norm": 0.2579136788845062,
      "learning_rate": 0.0001912224569876245,
      "loss": 0.6982,
      "step": 2186
    },
    {
      "epoch": 4.400402414486922,
      "grad_norm": 0.25210466980934143,
      "learning_rate": 0.00019121843243787102,
      "loss": 0.6452,
      "step": 2187
    },
    {
      "epoch": 4.402414486921529,
      "grad_norm": 0.24473033845424652,
      "learning_rate": 0.00019121440788811753,
      "loss": 0.6774,
      "step": 2188
    },
    {
      "epoch": 4.4044265593561365,
      "grad_norm": 0.24485152959823608,
      "learning_rate": 0.00019121038333836404,
      "loss": 0.6403,
      "step": 2189
    },
    {
      "epoch": 4.406438631790745,
      "grad_norm": 0.2584545314311981,
      "learning_rate": 0.00019120635878861053,
      "loss": 0.6701,
      "step": 2190
    },
    {
      "epoch": 4.408450704225352,
      "grad_norm": 0.2518920600414276,
      "learning_rate": 0.00019120233423885704,
      "loss": 0.6555,
      "step": 2191
    },
    {
      "epoch": 4.41046277665996,
      "grad_norm": 0.2494913935661316,
      "learning_rate": 0.00019119830968910355,
      "loss": 0.637,
      "step": 2192
    },
    {
      "epoch": 4.4124748490945676,
      "grad_norm": 0.26475679874420166,
      "learning_rate": 0.00019119428513935004,
      "loss": 0.67,
      "step": 2193
    },
    {
      "epoch": 4.414486921529175,
      "grad_norm": 0.2482447326183319,
      "learning_rate": 0.00019119026058959655,
      "loss": 0.6483,
      "step": 2194
    },
    {
      "epoch": 4.416498993963783,
      "grad_norm": 0.2462395429611206,
      "learning_rate": 0.00019118623603984303,
      "loss": 0.6529,
      "step": 2195
    },
    {
      "epoch": 4.41851106639839,
      "grad_norm": 0.2526775300502777,
      "learning_rate": 0.00019118221149008955,
      "loss": 0.6492,
      "step": 2196
    },
    {
      "epoch": 4.420523138832998,
      "grad_norm": 0.24999302625656128,
      "learning_rate": 0.00019117818694033606,
      "loss": 0.6316,
      "step": 2197
    },
    {
      "epoch": 4.422535211267606,
      "grad_norm": 0.25032365322113037,
      "learning_rate": 0.00019117416239058257,
      "loss": 0.6159,
      "step": 2198
    },
    {
      "epoch": 4.424547283702213,
      "grad_norm": 0.24789872765541077,
      "learning_rate": 0.00019117013784082906,
      "loss": 0.6513,
      "step": 2199
    },
    {
      "epoch": 4.426559356136821,
      "grad_norm": 0.24334919452667236,
      "learning_rate": 0.00019116611329107557,
      "loss": 0.6227,
      "step": 2200
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 0.24753357470035553,
      "learning_rate": 0.00019116208874132205,
      "loss": 0.6644,
      "step": 2201
    },
    {
      "epoch": 4.430583501006036,
      "grad_norm": 0.24918998777866364,
      "learning_rate": 0.0001911580641915686,
      "loss": 0.6941,
      "step": 2202
    },
    {
      "epoch": 4.4325955734406435,
      "grad_norm": 0.2516709864139557,
      "learning_rate": 0.00019115403964181508,
      "loss": 0.6554,
      "step": 2203
    },
    {
      "epoch": 4.434607645875252,
      "grad_norm": 0.24810691177845,
      "learning_rate": 0.0001911500150920616,
      "loss": 0.6684,
      "step": 2204
    },
    {
      "epoch": 4.436619718309859,
      "grad_norm": 0.24725322425365448,
      "learning_rate": 0.00019114599054230808,
      "loss": 0.6668,
      "step": 2205
    },
    {
      "epoch": 4.438631790744466,
      "grad_norm": 0.23944547772407532,
      "learning_rate": 0.0001911419659925546,
      "loss": 0.6569,
      "step": 2206
    },
    {
      "epoch": 4.440643863179075,
      "grad_norm": 0.23421253263950348,
      "learning_rate": 0.0001911379414428011,
      "loss": 0.6783,
      "step": 2207
    },
    {
      "epoch": 4.442655935613682,
      "grad_norm": 0.24074037373065948,
      "learning_rate": 0.0001911339168930476,
      "loss": 0.6608,
      "step": 2208
    },
    {
      "epoch": 4.44466800804829,
      "grad_norm": 0.24444788694381714,
      "learning_rate": 0.0001911298923432941,
      "loss": 0.708,
      "step": 2209
    },
    {
      "epoch": 4.446680080482897,
      "grad_norm": 0.23955921828746796,
      "learning_rate": 0.0001911258677935406,
      "loss": 0.6634,
      "step": 2210
    },
    {
      "epoch": 4.448692152917505,
      "grad_norm": 0.27242785692214966,
      "learning_rate": 0.0001911218432437871,
      "loss": 0.6866,
      "step": 2211
    },
    {
      "epoch": 4.450704225352113,
      "grad_norm": 0.24625438451766968,
      "learning_rate": 0.00019111781869403363,
      "loss": 0.6765,
      "step": 2212
    },
    {
      "epoch": 4.45271629778672,
      "grad_norm": 0.2546171247959137,
      "learning_rate": 0.00019111379414428012,
      "loss": 0.6709,
      "step": 2213
    },
    {
      "epoch": 4.454728370221328,
      "grad_norm": 0.24556374549865723,
      "learning_rate": 0.00019110976959452663,
      "loss": 0.6687,
      "step": 2214
    },
    {
      "epoch": 4.456740442655936,
      "grad_norm": 0.24735413491725922,
      "learning_rate": 0.00019110574504477312,
      "loss": 0.6706,
      "step": 2215
    },
    {
      "epoch": 4.458752515090543,
      "grad_norm": 0.26716792583465576,
      "learning_rate": 0.00019110172049501963,
      "loss": 0.6828,
      "step": 2216
    },
    {
      "epoch": 4.460764587525151,
      "grad_norm": 0.24902960658073425,
      "learning_rate": 0.00019109769594526614,
      "loss": 0.6782,
      "step": 2217
    },
    {
      "epoch": 4.462776659959759,
      "grad_norm": 0.24994345009326935,
      "learning_rate": 0.00019109367139551265,
      "loss": 0.6514,
      "step": 2218
    },
    {
      "epoch": 4.464788732394366,
      "grad_norm": 0.2480635792016983,
      "learning_rate": 0.00019108964684575914,
      "loss": 0.6899,
      "step": 2219
    },
    {
      "epoch": 4.466800804828974,
      "grad_norm": 0.25017350912094116,
      "learning_rate": 0.00019108562229600565,
      "loss": 0.6551,
      "step": 2220
    },
    {
      "epoch": 4.468812877263582,
      "grad_norm": 0.26264774799346924,
      "learning_rate": 0.00019108159774625214,
      "loss": 0.6618,
      "step": 2221
    },
    {
      "epoch": 4.470824949698189,
      "grad_norm": 0.24584025144577026,
      "learning_rate": 0.00019107757319649865,
      "loss": 0.692,
      "step": 2222
    },
    {
      "epoch": 4.472837022132797,
      "grad_norm": 0.26477405428886414,
      "learning_rate": 0.00019107354864674516,
      "loss": 0.7109,
      "step": 2223
    },
    {
      "epoch": 4.474849094567404,
      "grad_norm": 0.2533622980117798,
      "learning_rate": 0.00019106952409699167,
      "loss": 0.6885,
      "step": 2224
    },
    {
      "epoch": 4.476861167002012,
      "grad_norm": 0.2614116370677948,
      "learning_rate": 0.00019106549954723816,
      "loss": 0.6951,
      "step": 2225
    },
    {
      "epoch": 4.47887323943662,
      "grad_norm": 0.2482900768518448,
      "learning_rate": 0.00019106147499748467,
      "loss": 0.6935,
      "step": 2226
    },
    {
      "epoch": 4.480885311871227,
      "grad_norm": 0.23910151422023773,
      "learning_rate": 0.00019105745044773118,
      "loss": 0.6795,
      "step": 2227
    },
    {
      "epoch": 4.482897384305835,
      "grad_norm": 0.27117210626602173,
      "learning_rate": 0.00019105342589797767,
      "loss": 0.6725,
      "step": 2228
    },
    {
      "epoch": 4.484909456740443,
      "grad_norm": 0.24587999284267426,
      "learning_rate": 0.00019104940134822418,
      "loss": 0.6636,
      "step": 2229
    },
    {
      "epoch": 4.48692152917505,
      "grad_norm": 0.2622460126876831,
      "learning_rate": 0.00019104537679847066,
      "loss": 0.6828,
      "step": 2230
    },
    {
      "epoch": 4.4889336016096575,
      "grad_norm": 0.25438952445983887,
      "learning_rate": 0.00019104135224871718,
      "loss": 0.6579,
      "step": 2231
    },
    {
      "epoch": 4.490945674044266,
      "grad_norm": 0.2517537772655487,
      "learning_rate": 0.0001910373276989637,
      "loss": 0.6438,
      "step": 2232
    },
    {
      "epoch": 4.492957746478873,
      "grad_norm": 0.23708221316337585,
      "learning_rate": 0.0001910333031492102,
      "loss": 0.6459,
      "step": 2233
    },
    {
      "epoch": 4.494969818913481,
      "grad_norm": 0.2520623207092285,
      "learning_rate": 0.00019102927859945669,
      "loss": 0.6661,
      "step": 2234
    },
    {
      "epoch": 4.496981891348089,
      "grad_norm": 0.24655888974666595,
      "learning_rate": 0.0001910252540497032,
      "loss": 0.6578,
      "step": 2235
    },
    {
      "epoch": 4.498993963782696,
      "grad_norm": 0.2586548626422882,
      "learning_rate": 0.00019102122949994968,
      "loss": 0.6949,
      "step": 2236
    },
    {
      "epoch": 4.501006036217304,
      "grad_norm": 0.2452935427427292,
      "learning_rate": 0.00019101720495019622,
      "loss": 0.6629,
      "step": 2237
    },
    {
      "epoch": 4.503018108651911,
      "grad_norm": 0.25466686487197876,
      "learning_rate": 0.0001910131804004427,
      "loss": 0.686,
      "step": 2238
    },
    {
      "epoch": 4.505030181086519,
      "grad_norm": 0.24110420048236847,
      "learning_rate": 0.00019100915585068922,
      "loss": 0.642,
      "step": 2239
    },
    {
      "epoch": 4.507042253521127,
      "grad_norm": 0.23910875618457794,
      "learning_rate": 0.0001910051313009357,
      "loss": 0.667,
      "step": 2240
    },
    {
      "epoch": 4.509054325955734,
      "grad_norm": 0.24619998037815094,
      "learning_rate": 0.00019100110675118222,
      "loss": 0.6552,
      "step": 2241
    },
    {
      "epoch": 4.5110663983903425,
      "grad_norm": 0.24369224905967712,
      "learning_rate": 0.00019099708220142873,
      "loss": 0.6879,
      "step": 2242
    },
    {
      "epoch": 4.51307847082495,
      "grad_norm": 0.2540642321109772,
      "learning_rate": 0.00019099305765167524,
      "loss": 0.6525,
      "step": 2243
    },
    {
      "epoch": 4.515090543259557,
      "grad_norm": 0.25055018067359924,
      "learning_rate": 0.00019098903310192173,
      "loss": 0.6546,
      "step": 2244
    },
    {
      "epoch": 4.517102615694165,
      "grad_norm": 0.24226973950862885,
      "learning_rate": 0.00019098500855216824,
      "loss": 0.7031,
      "step": 2245
    },
    {
      "epoch": 4.519114688128773,
      "grad_norm": 0.25200676918029785,
      "learning_rate": 0.00019098098400241472,
      "loss": 0.6629,
      "step": 2246
    },
    {
      "epoch": 4.52112676056338,
      "grad_norm": 0.25144505500793457,
      "learning_rate": 0.00019097695945266126,
      "loss": 0.6839,
      "step": 2247
    },
    {
      "epoch": 4.523138832997988,
      "grad_norm": 0.25496408343315125,
      "learning_rate": 0.00019097293490290775,
      "loss": 0.6913,
      "step": 2248
    },
    {
      "epoch": 4.525150905432596,
      "grad_norm": 0.23690220713615417,
      "learning_rate": 0.00019096891035315426,
      "loss": 0.6855,
      "step": 2249
    },
    {
      "epoch": 4.527162977867203,
      "grad_norm": 0.2655439078807831,
      "learning_rate": 0.00019096488580340075,
      "loss": 0.6637,
      "step": 2250
    },
    {
      "epoch": 4.529175050301811,
      "grad_norm": 0.2517867386341095,
      "learning_rate": 0.00019096086125364726,
      "loss": 0.6736,
      "step": 2251
    },
    {
      "epoch": 4.531187122736418,
      "grad_norm": 0.24769996106624603,
      "learning_rate": 0.00019095683670389377,
      "loss": 0.659,
      "step": 2252
    },
    {
      "epoch": 4.533199195171026,
      "grad_norm": 0.2663964331150055,
      "learning_rate": 0.00019095281215414028,
      "loss": 0.7005,
      "step": 2253
    },
    {
      "epoch": 4.535211267605634,
      "grad_norm": 0.24016596376895905,
      "learning_rate": 0.00019094878760438677,
      "loss": 0.6748,
      "step": 2254
    },
    {
      "epoch": 4.537223340040241,
      "grad_norm": 0.2430531084537506,
      "learning_rate": 0.00019094476305463328,
      "loss": 0.6373,
      "step": 2255
    },
    {
      "epoch": 4.539235412474849,
      "grad_norm": 0.2560066282749176,
      "learning_rate": 0.00019094073850487977,
      "loss": 0.6817,
      "step": 2256
    },
    {
      "epoch": 4.541247484909457,
      "grad_norm": 0.24313563108444214,
      "learning_rate": 0.00019093671395512628,
      "loss": 0.672,
      "step": 2257
    },
    {
      "epoch": 4.543259557344064,
      "grad_norm": 0.25253623723983765,
      "learning_rate": 0.0001909326894053728,
      "loss": 0.6755,
      "step": 2258
    },
    {
      "epoch": 4.545271629778672,
      "grad_norm": 0.2544630765914917,
      "learning_rate": 0.00019092866485561927,
      "loss": 0.6975,
      "step": 2259
    },
    {
      "epoch": 4.54728370221328,
      "grad_norm": 0.272443950176239,
      "learning_rate": 0.0001909246403058658,
      "loss": 0.7034,
      "step": 2260
    },
    {
      "epoch": 4.549295774647887,
      "grad_norm": 0.2523559331893921,
      "learning_rate": 0.0001909206157561123,
      "loss": 0.6782,
      "step": 2261
    },
    {
      "epoch": 4.551307847082495,
      "grad_norm": 0.24997520446777344,
      "learning_rate": 0.0001909165912063588,
      "loss": 0.6648,
      "step": 2262
    },
    {
      "epoch": 4.553319919517103,
      "grad_norm": 0.23826956748962402,
      "learning_rate": 0.0001909125666566053,
      "loss": 0.6721,
      "step": 2263
    },
    {
      "epoch": 4.55533199195171,
      "grad_norm": 0.23989567160606384,
      "learning_rate": 0.0001909085421068518,
      "loss": 0.6685,
      "step": 2264
    },
    {
      "epoch": 4.557344064386318,
      "grad_norm": 0.24780207872390747,
      "learning_rate": 0.0001909045175570983,
      "loss": 0.6668,
      "step": 2265
    },
    {
      "epoch": 4.559356136820925,
      "grad_norm": 0.23841243982315063,
      "learning_rate": 0.0001909004930073448,
      "loss": 0.6306,
      "step": 2266
    },
    {
      "epoch": 4.561368209255534,
      "grad_norm": 0.24946986138820648,
      "learning_rate": 0.00019089646845759132,
      "loss": 0.6822,
      "step": 2267
    },
    {
      "epoch": 4.563380281690141,
      "grad_norm": 0.261059045791626,
      "learning_rate": 0.00019089244390783783,
      "loss": 0.6746,
      "step": 2268
    },
    {
      "epoch": 4.565392354124748,
      "grad_norm": 0.2513234615325928,
      "learning_rate": 0.00019088841935808432,
      "loss": 0.6781,
      "step": 2269
    },
    {
      "epoch": 4.5674044265593565,
      "grad_norm": 0.25830408930778503,
      "learning_rate": 0.00019088439480833083,
      "loss": 0.6844,
      "step": 2270
    },
    {
      "epoch": 4.569416498993964,
      "grad_norm": 0.2633650302886963,
      "learning_rate": 0.0001908803702585773,
      "loss": 0.6741,
      "step": 2271
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.2561405599117279,
      "learning_rate": 0.00019087634570882385,
      "loss": 0.6832,
      "step": 2272
    },
    {
      "epoch": 4.573440643863179,
      "grad_norm": 0.23445507884025574,
      "learning_rate": 0.00019087232115907034,
      "loss": 0.66,
      "step": 2273
    },
    {
      "epoch": 4.575452716297787,
      "grad_norm": 0.2681765854358673,
      "learning_rate": 0.00019086829660931685,
      "loss": 0.6634,
      "step": 2274
    },
    {
      "epoch": 4.577464788732394,
      "grad_norm": 0.24926486611366272,
      "learning_rate": 0.00019086427205956333,
      "loss": 0.6904,
      "step": 2275
    },
    {
      "epoch": 4.579476861167002,
      "grad_norm": 0.25439608097076416,
      "learning_rate": 0.00019086024750980985,
      "loss": 0.6453,
      "step": 2276
    },
    {
      "epoch": 4.58148893360161,
      "grad_norm": 0.24756327271461487,
      "learning_rate": 0.00019085622296005636,
      "loss": 0.6454,
      "step": 2277
    },
    {
      "epoch": 4.583501006036217,
      "grad_norm": 0.2643541395664215,
      "learning_rate": 0.00019085219841030287,
      "loss": 0.6948,
      "step": 2278
    },
    {
      "epoch": 4.585513078470825,
      "grad_norm": 0.24215076863765717,
      "learning_rate": 0.00019084817386054936,
      "loss": 0.6563,
      "step": 2279
    },
    {
      "epoch": 4.5875251509054324,
      "grad_norm": 0.2578771710395813,
      "learning_rate": 0.00019084414931079587,
      "loss": 0.6304,
      "step": 2280
    },
    {
      "epoch": 4.58953722334004,
      "grad_norm": 0.25889942049980164,
      "learning_rate": 0.00019084012476104235,
      "loss": 0.7059,
      "step": 2281
    },
    {
      "epoch": 4.591549295774648,
      "grad_norm": 0.25377970933914185,
      "learning_rate": 0.0001908361002112889,
      "loss": 0.6753,
      "step": 2282
    },
    {
      "epoch": 4.593561368209255,
      "grad_norm": 0.2477119117975235,
      "learning_rate": 0.00019083207566153538,
      "loss": 0.6709,
      "step": 2283
    },
    {
      "epoch": 4.5955734406438635,
      "grad_norm": 0.24294058978557587,
      "learning_rate": 0.0001908280511117819,
      "loss": 0.6683,
      "step": 2284
    },
    {
      "epoch": 4.597585513078471,
      "grad_norm": 0.24390645325183868,
      "learning_rate": 0.00019082402656202838,
      "loss": 0.6521,
      "step": 2285
    },
    {
      "epoch": 4.599597585513078,
      "grad_norm": 0.26376470923423767,
      "learning_rate": 0.0001908200020122749,
      "loss": 0.7028,
      "step": 2286
    },
    {
      "epoch": 4.601609657947686,
      "grad_norm": 0.25467029213905334,
      "learning_rate": 0.0001908159774625214,
      "loss": 0.661,
      "step": 2287
    },
    {
      "epoch": 4.603621730382294,
      "grad_norm": 0.2514357268810272,
      "learning_rate": 0.0001908119529127679,
      "loss": 0.6948,
      "step": 2288
    },
    {
      "epoch": 4.605633802816901,
      "grad_norm": 0.2659415304660797,
      "learning_rate": 0.0001908079283630144,
      "loss": 0.6742,
      "step": 2289
    },
    {
      "epoch": 4.607645875251509,
      "grad_norm": 0.2401396781206131,
      "learning_rate": 0.0001908039038132609,
      "loss": 0.6386,
      "step": 2290
    },
    {
      "epoch": 4.609657947686117,
      "grad_norm": 0.25269585847854614,
      "learning_rate": 0.0001907998792635074,
      "loss": 0.6873,
      "step": 2291
    },
    {
      "epoch": 4.611670020120725,
      "grad_norm": 0.27799931168556213,
      "learning_rate": 0.0001907958547137539,
      "loss": 0.692,
      "step": 2292
    },
    {
      "epoch": 4.613682092555332,
      "grad_norm": 0.2622451186180115,
      "learning_rate": 0.00019079183016400042,
      "loss": 0.6685,
      "step": 2293
    },
    {
      "epoch": 4.6156941649899395,
      "grad_norm": 0.2424694299697876,
      "learning_rate": 0.0001907878056142469,
      "loss": 0.6652,
      "step": 2294
    },
    {
      "epoch": 4.617706237424548,
      "grad_norm": 0.24266573786735535,
      "learning_rate": 0.00019078378106449342,
      "loss": 0.6936,
      "step": 2295
    },
    {
      "epoch": 4.619718309859155,
      "grad_norm": 0.24277283251285553,
      "learning_rate": 0.00019077975651473993,
      "loss": 0.6703,
      "step": 2296
    },
    {
      "epoch": 4.621730382293762,
      "grad_norm": 0.2509732246398926,
      "learning_rate": 0.00019077573196498644,
      "loss": 0.6701,
      "step": 2297
    },
    {
      "epoch": 4.6237424547283705,
      "grad_norm": 0.23213334381580353,
      "learning_rate": 0.00019077170741523293,
      "loss": 0.6312,
      "step": 2298
    },
    {
      "epoch": 4.625754527162978,
      "grad_norm": 0.2381870597600937,
      "learning_rate": 0.00019076768286547944,
      "loss": 0.6645,
      "step": 2299
    },
    {
      "epoch": 4.627766599597585,
      "grad_norm": 0.24703282117843628,
      "learning_rate": 0.00019076365831572592,
      "loss": 0.6989,
      "step": 2300
    },
    {
      "epoch": 4.629778672032193,
      "grad_norm": 0.25023457407951355,
      "learning_rate": 0.00019075963376597244,
      "loss": 0.7093,
      "step": 2301
    },
    {
      "epoch": 4.631790744466801,
      "grad_norm": 0.25796836614608765,
      "learning_rate": 0.00019075560921621895,
      "loss": 0.6688,
      "step": 2302
    },
    {
      "epoch": 4.633802816901408,
      "grad_norm": 0.2420940101146698,
      "learning_rate": 0.00019075158466646546,
      "loss": 0.7007,
      "step": 2303
    },
    {
      "epoch": 4.635814889336016,
      "grad_norm": 0.2527626156806946,
      "learning_rate": 0.00019074756011671195,
      "loss": 0.6741,
      "step": 2304
    },
    {
      "epoch": 4.637826961770624,
      "grad_norm": 0.2518651485443115,
      "learning_rate": 0.00019074353556695846,
      "loss": 0.6628,
      "step": 2305
    },
    {
      "epoch": 4.639839034205231,
      "grad_norm": 0.2458876073360443,
      "learning_rate": 0.00019073951101720494,
      "loss": 0.6559,
      "step": 2306
    },
    {
      "epoch": 4.641851106639839,
      "grad_norm": 0.2516920566558838,
      "learning_rate": 0.00019073548646745148,
      "loss": 0.6775,
      "step": 2307
    },
    {
      "epoch": 4.6438631790744465,
      "grad_norm": 0.24900344014167786,
      "learning_rate": 0.00019073146191769797,
      "loss": 0.7045,
      "step": 2308
    },
    {
      "epoch": 4.645875251509055,
      "grad_norm": 0.24839696288108826,
      "learning_rate": 0.00019072743736794448,
      "loss": 0.6601,
      "step": 2309
    },
    {
      "epoch": 4.647887323943662,
      "grad_norm": 0.24830104410648346,
      "learning_rate": 0.00019072341281819096,
      "loss": 0.6845,
      "step": 2310
    },
    {
      "epoch": 4.649899396378269,
      "grad_norm": 0.25312158465385437,
      "learning_rate": 0.00019071938826843748,
      "loss": 0.6876,
      "step": 2311
    },
    {
      "epoch": 4.6519114688128775,
      "grad_norm": 0.2394452542066574,
      "learning_rate": 0.00019071536371868396,
      "loss": 0.6709,
      "step": 2312
    },
    {
      "epoch": 4.653923541247485,
      "grad_norm": 0.25434377789497375,
      "learning_rate": 0.0001907113391689305,
      "loss": 0.6623,
      "step": 2313
    },
    {
      "epoch": 4.655935613682092,
      "grad_norm": 0.25397250056266785,
      "learning_rate": 0.00019070731461917699,
      "loss": 0.714,
      "step": 2314
    },
    {
      "epoch": 4.6579476861167,
      "grad_norm": 0.2553960978984833,
      "learning_rate": 0.0001907032900694235,
      "loss": 0.6314,
      "step": 2315
    },
    {
      "epoch": 4.659959758551308,
      "grad_norm": 0.26668021082878113,
      "learning_rate": 0.00019069926551966998,
      "loss": 0.6648,
      "step": 2316
    },
    {
      "epoch": 4.661971830985916,
      "grad_norm": 0.2835752069950104,
      "learning_rate": 0.0001906952409699165,
      "loss": 0.7086,
      "step": 2317
    },
    {
      "epoch": 4.663983903420523,
      "grad_norm": 0.24672497808933258,
      "learning_rate": 0.000190691216420163,
      "loss": 0.6939,
      "step": 2318
    },
    {
      "epoch": 4.665995975855131,
      "grad_norm": 0.24947984516620636,
      "learning_rate": 0.00019068719187040952,
      "loss": 0.6502,
      "step": 2319
    },
    {
      "epoch": 4.668008048289739,
      "grad_norm": 0.24643529951572418,
      "learning_rate": 0.000190683167320656,
      "loss": 0.688,
      "step": 2320
    },
    {
      "epoch": 4.670020120724346,
      "grad_norm": 0.24161115288734436,
      "learning_rate": 0.00019067914277090252,
      "loss": 0.7011,
      "step": 2321
    },
    {
      "epoch": 4.6720321931589535,
      "grad_norm": 0.25002333521842957,
      "learning_rate": 0.000190675118221149,
      "loss": 0.6833,
      "step": 2322
    },
    {
      "epoch": 4.674044265593562,
      "grad_norm": 0.24326813220977783,
      "learning_rate": 0.00019067109367139554,
      "loss": 0.6901,
      "step": 2323
    },
    {
      "epoch": 4.676056338028169,
      "grad_norm": 0.25354668498039246,
      "learning_rate": 0.00019066706912164203,
      "loss": 0.689,
      "step": 2324
    },
    {
      "epoch": 4.678068410462776,
      "grad_norm": 0.24427847564220428,
      "learning_rate": 0.00019066304457188854,
      "loss": 0.6809,
      "step": 2325
    },
    {
      "epoch": 4.6800804828973845,
      "grad_norm": 0.23847223818302155,
      "learning_rate": 0.00019065902002213502,
      "loss": 0.6156,
      "step": 2326
    },
    {
      "epoch": 4.682092555331992,
      "grad_norm": 0.27103427052497864,
      "learning_rate": 0.00019065499547238154,
      "loss": 0.7037,
      "step": 2327
    },
    {
      "epoch": 4.684104627766599,
      "grad_norm": 0.2533142566680908,
      "learning_rate": 0.00019065097092262805,
      "loss": 0.6901,
      "step": 2328
    },
    {
      "epoch": 4.686116700201207,
      "grad_norm": 0.25290024280548096,
      "learning_rate": 0.00019064694637287453,
      "loss": 0.659,
      "step": 2329
    },
    {
      "epoch": 4.688128772635815,
      "grad_norm": 0.2521469295024872,
      "learning_rate": 0.00019064292182312105,
      "loss": 0.6769,
      "step": 2330
    },
    {
      "epoch": 4.690140845070422,
      "grad_norm": 0.2828289866447449,
      "learning_rate": 0.00019063889727336756,
      "loss": 0.6623,
      "step": 2331
    },
    {
      "epoch": 4.69215291750503,
      "grad_norm": 0.24512119591236115,
      "learning_rate": 0.00019063487272361404,
      "loss": 0.6644,
      "step": 2332
    },
    {
      "epoch": 4.694164989939638,
      "grad_norm": 0.2613806128501892,
      "learning_rate": 0.00019063084817386056,
      "loss": 0.7107,
      "step": 2333
    },
    {
      "epoch": 4.696177062374246,
      "grad_norm": 0.24759581685066223,
      "learning_rate": 0.00019062682362410707,
      "loss": 0.6887,
      "step": 2334
    },
    {
      "epoch": 4.698189134808853,
      "grad_norm": 0.27470481395721436,
      "learning_rate": 0.00019062279907435355,
      "loss": 0.7151,
      "step": 2335
    },
    {
      "epoch": 4.7002012072434605,
      "grad_norm": 0.2319764941930771,
      "learning_rate": 0.00019061877452460006,
      "loss": 0.6654,
      "step": 2336
    },
    {
      "epoch": 4.702213279678069,
      "grad_norm": 0.2585032880306244,
      "learning_rate": 0.00019061474997484655,
      "loss": 0.6812,
      "step": 2337
    },
    {
      "epoch": 4.704225352112676,
      "grad_norm": 0.24650315940380096,
      "learning_rate": 0.0001906107254250931,
      "loss": 0.6649,
      "step": 2338
    },
    {
      "epoch": 4.706237424547283,
      "grad_norm": 0.2526162564754486,
      "learning_rate": 0.00019060670087533957,
      "loss": 0.6589,
      "step": 2339
    },
    {
      "epoch": 4.7082494969818915,
      "grad_norm": 0.2538827061653137,
      "learning_rate": 0.0001906026763255861,
      "loss": 0.6263,
      "step": 2340
    },
    {
      "epoch": 4.710261569416499,
      "grad_norm": 0.24514919519424438,
      "learning_rate": 0.00019059865177583257,
      "loss": 0.6473,
      "step": 2341
    },
    {
      "epoch": 4.712273641851107,
      "grad_norm": 0.23924961686134338,
      "learning_rate": 0.00019059462722607908,
      "loss": 0.6441,
      "step": 2342
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 0.24704484641551971,
      "learning_rate": 0.0001905906026763256,
      "loss": 0.6911,
      "step": 2343
    },
    {
      "epoch": 4.716297786720322,
      "grad_norm": 0.24072809517383575,
      "learning_rate": 0.0001905865781265721,
      "loss": 0.6788,
      "step": 2344
    },
    {
      "epoch": 4.71830985915493,
      "grad_norm": 0.24505864083766937,
      "learning_rate": 0.0001905825535768186,
      "loss": 0.6799,
      "step": 2345
    },
    {
      "epoch": 4.720321931589537,
      "grad_norm": 0.24556680023670197,
      "learning_rate": 0.0001905785290270651,
      "loss": 0.617,
      "step": 2346
    },
    {
      "epoch": 4.722334004024145,
      "grad_norm": 0.2318621277809143,
      "learning_rate": 0.0001905745044773116,
      "loss": 0.6306,
      "step": 2347
    },
    {
      "epoch": 4.724346076458753,
      "grad_norm": 0.2602083086967468,
      "learning_rate": 0.00019057047992755813,
      "loss": 0.7295,
      "step": 2348
    },
    {
      "epoch": 4.72635814889336,
      "grad_norm": 0.25146380066871643,
      "learning_rate": 0.00019056645537780462,
      "loss": 0.6892,
      "step": 2349
    },
    {
      "epoch": 4.7283702213279675,
      "grad_norm": 0.25228744745254517,
      "learning_rate": 0.00019056243082805113,
      "loss": 0.6772,
      "step": 2350
    },
    {
      "epoch": 4.730382293762576,
      "grad_norm": 0.24757525324821472,
      "learning_rate": 0.0001905584062782976,
      "loss": 0.7064,
      "step": 2351
    },
    {
      "epoch": 4.732394366197183,
      "grad_norm": 0.24993543326854706,
      "learning_rate": 0.00019055438172854412,
      "loss": 0.6968,
      "step": 2352
    },
    {
      "epoch": 4.73440643863179,
      "grad_norm": 0.24717892706394196,
      "learning_rate": 0.00019055035717879064,
      "loss": 0.6472,
      "step": 2353
    },
    {
      "epoch": 4.7364185110663986,
      "grad_norm": 0.26630115509033203,
      "learning_rate": 0.00019054633262903715,
      "loss": 0.6751,
      "step": 2354
    },
    {
      "epoch": 4.738430583501006,
      "grad_norm": 0.24890944361686707,
      "learning_rate": 0.00019054230807928363,
      "loss": 0.6657,
      "step": 2355
    },
    {
      "epoch": 4.740442655935613,
      "grad_norm": 0.24828539788722992,
      "learning_rate": 0.00019053828352953015,
      "loss": 0.6428,
      "step": 2356
    },
    {
      "epoch": 4.742454728370221,
      "grad_norm": 0.2654436230659485,
      "learning_rate": 0.00019053425897977663,
      "loss": 0.7127,
      "step": 2357
    },
    {
      "epoch": 4.744466800804829,
      "grad_norm": 0.2550791800022125,
      "learning_rate": 0.00019053023443002317,
      "loss": 0.6624,
      "step": 2358
    },
    {
      "epoch": 4.746478873239437,
      "grad_norm": 0.23924127221107483,
      "learning_rate": 0.00019052620988026966,
      "loss": 0.7011,
      "step": 2359
    },
    {
      "epoch": 4.748490945674044,
      "grad_norm": 0.24990198016166687,
      "learning_rate": 0.00019052218533051617,
      "loss": 0.6435,
      "step": 2360
    },
    {
      "epoch": 4.750503018108652,
      "grad_norm": 0.2552099823951721,
      "learning_rate": 0.00019051816078076265,
      "loss": 0.6418,
      "step": 2361
    },
    {
      "epoch": 4.75251509054326,
      "grad_norm": 0.2507632076740265,
      "learning_rate": 0.00019051413623100917,
      "loss": 0.6585,
      "step": 2362
    },
    {
      "epoch": 4.754527162977867,
      "grad_norm": 0.2602674663066864,
      "learning_rate": 0.00019051011168125568,
      "loss": 0.6731,
      "step": 2363
    },
    {
      "epoch": 4.7565392354124745,
      "grad_norm": 0.254999577999115,
      "learning_rate": 0.00019050608713150216,
      "loss": 0.7434,
      "step": 2364
    },
    {
      "epoch": 4.758551307847083,
      "grad_norm": 0.2421538233757019,
      "learning_rate": 0.00019050206258174868,
      "loss": 0.6877,
      "step": 2365
    },
    {
      "epoch": 4.76056338028169,
      "grad_norm": 0.24030129611492157,
      "learning_rate": 0.0001904980380319952,
      "loss": 0.6755,
      "step": 2366
    },
    {
      "epoch": 4.762575452716298,
      "grad_norm": 0.23458316922187805,
      "learning_rate": 0.00019049401348224167,
      "loss": 0.6276,
      "step": 2367
    },
    {
      "epoch": 4.7645875251509056,
      "grad_norm": 0.24553051590919495,
      "learning_rate": 0.00019048998893248818,
      "loss": 0.6859,
      "step": 2368
    },
    {
      "epoch": 4.766599597585513,
      "grad_norm": 0.25353607535362244,
      "learning_rate": 0.0001904859643827347,
      "loss": 0.6816,
      "step": 2369
    },
    {
      "epoch": 4.768611670020121,
      "grad_norm": 0.26785850524902344,
      "learning_rate": 0.00019048193983298118,
      "loss": 0.6892,
      "step": 2370
    },
    {
      "epoch": 4.770623742454728,
      "grad_norm": 0.24459676444530487,
      "learning_rate": 0.0001904779152832277,
      "loss": 0.6435,
      "step": 2371
    },
    {
      "epoch": 4.772635814889336,
      "grad_norm": 0.24086885154247284,
      "learning_rate": 0.00019047389073347418,
      "loss": 0.643,
      "step": 2372
    },
    {
      "epoch": 4.774647887323944,
      "grad_norm": 0.2368784099817276,
      "learning_rate": 0.00019046986618372072,
      "loss": 0.6929,
      "step": 2373
    },
    {
      "epoch": 4.776659959758551,
      "grad_norm": 0.2477876842021942,
      "learning_rate": 0.0001904658416339672,
      "loss": 0.6872,
      "step": 2374
    },
    {
      "epoch": 4.778672032193159,
      "grad_norm": 0.25844624638557434,
      "learning_rate": 0.00019046181708421372,
      "loss": 0.6846,
      "step": 2375
    },
    {
      "epoch": 4.780684104627767,
      "grad_norm": 0.2593514025211334,
      "learning_rate": 0.0001904577925344602,
      "loss": 0.6913,
      "step": 2376
    },
    {
      "epoch": 4.782696177062374,
      "grad_norm": 0.24916735291481018,
      "learning_rate": 0.0001904537679847067,
      "loss": 0.6967,
      "step": 2377
    },
    {
      "epoch": 4.7847082494969815,
      "grad_norm": 0.2564140856266022,
      "learning_rate": 0.00019044974343495323,
      "loss": 0.6725,
      "step": 2378
    },
    {
      "epoch": 4.78672032193159,
      "grad_norm": 0.23625382781028748,
      "learning_rate": 0.00019044571888519974,
      "loss": 0.672,
      "step": 2379
    },
    {
      "epoch": 4.788732394366197,
      "grad_norm": 0.2552345395088196,
      "learning_rate": 0.00019044169433544622,
      "loss": 0.6864,
      "step": 2380
    },
    {
      "epoch": 4.790744466800804,
      "grad_norm": 0.247276172041893,
      "learning_rate": 0.00019043766978569274,
      "loss": 0.6714,
      "step": 2381
    },
    {
      "epoch": 4.792756539235413,
      "grad_norm": 0.24684861302375793,
      "learning_rate": 0.00019043364523593922,
      "loss": 0.6925,
      "step": 2382
    },
    {
      "epoch": 4.79476861167002,
      "grad_norm": 0.25198879837989807,
      "learning_rate": 0.00019042962068618576,
      "loss": 0.6684,
      "step": 2383
    },
    {
      "epoch": 4.796780684104628,
      "grad_norm": 0.2482793629169464,
      "learning_rate": 0.00019042559613643224,
      "loss": 0.6616,
      "step": 2384
    },
    {
      "epoch": 4.798792756539235,
      "grad_norm": 0.24400679767131805,
      "learning_rate": 0.00019042157158667876,
      "loss": 0.6538,
      "step": 2385
    },
    {
      "epoch": 4.800804828973843,
      "grad_norm": 0.2307782918214798,
      "learning_rate": 0.00019041754703692524,
      "loss": 0.6456,
      "step": 2386
    },
    {
      "epoch": 4.802816901408451,
      "grad_norm": 0.24104978144168854,
      "learning_rate": 0.00019041352248717175,
      "loss": 0.6581,
      "step": 2387
    },
    {
      "epoch": 4.804828973843058,
      "grad_norm": 0.24551543593406677,
      "learning_rate": 0.00019040949793741827,
      "loss": 0.6561,
      "step": 2388
    },
    {
      "epoch": 4.806841046277666,
      "grad_norm": 0.25968414545059204,
      "learning_rate": 0.00019040547338766478,
      "loss": 0.6554,
      "step": 2389
    },
    {
      "epoch": 4.808853118712274,
      "grad_norm": 0.2416195124387741,
      "learning_rate": 0.00019040144883791126,
      "loss": 0.6741,
      "step": 2390
    },
    {
      "epoch": 4.810865191146881,
      "grad_norm": 0.25020498037338257,
      "learning_rate": 0.00019039742428815778,
      "loss": 0.6666,
      "step": 2391
    },
    {
      "epoch": 4.812877263581489,
      "grad_norm": 0.26948150992393494,
      "learning_rate": 0.00019039339973840426,
      "loss": 0.6647,
      "step": 2392
    },
    {
      "epoch": 4.814889336016097,
      "grad_norm": 0.2657564580440521,
      "learning_rate": 0.0001903893751886508,
      "loss": 0.6633,
      "step": 2393
    },
    {
      "epoch": 4.816901408450704,
      "grad_norm": 0.25106051564216614,
      "learning_rate": 0.00019038535063889729,
      "loss": 0.6847,
      "step": 2394
    },
    {
      "epoch": 4.818913480885312,
      "grad_norm": 0.24197472631931305,
      "learning_rate": 0.0001903813260891438,
      "loss": 0.6607,
      "step": 2395
    },
    {
      "epoch": 4.82092555331992,
      "grad_norm": 0.2687385380268097,
      "learning_rate": 0.00019037730153939028,
      "loss": 0.7228,
      "step": 2396
    },
    {
      "epoch": 4.822937625754527,
      "grad_norm": 0.25035977363586426,
      "learning_rate": 0.0001903732769896368,
      "loss": 0.6825,
      "step": 2397
    },
    {
      "epoch": 4.824949698189135,
      "grad_norm": 0.2362363487482071,
      "learning_rate": 0.0001903692524398833,
      "loss": 0.6496,
      "step": 2398
    },
    {
      "epoch": 4.826961770623742,
      "grad_norm": 0.2530083954334259,
      "learning_rate": 0.0001903652278901298,
      "loss": 0.7038,
      "step": 2399
    },
    {
      "epoch": 4.82897384305835,
      "grad_norm": 0.2572874128818512,
      "learning_rate": 0.0001903612033403763,
      "loss": 0.6537,
      "step": 2400
    },
    {
      "epoch": 4.830985915492958,
      "grad_norm": 0.24542567133903503,
      "learning_rate": 0.0001903571787906228,
      "loss": 0.6513,
      "step": 2401
    },
    {
      "epoch": 4.832997987927565,
      "grad_norm": 0.25077342987060547,
      "learning_rate": 0.0001903531542408693,
      "loss": 0.6842,
      "step": 2402
    },
    {
      "epoch": 4.835010060362173,
      "grad_norm": 0.24123099446296692,
      "learning_rate": 0.00019034912969111581,
      "loss": 0.64,
      "step": 2403
    },
    {
      "epoch": 4.837022132796781,
      "grad_norm": 0.25305455923080444,
      "learning_rate": 0.00019034510514136233,
      "loss": 0.682,
      "step": 2404
    },
    {
      "epoch": 4.839034205231388,
      "grad_norm": 0.262697696685791,
      "learning_rate": 0.0001903410805916088,
      "loss": 0.6887,
      "step": 2405
    },
    {
      "epoch": 4.8410462776659955,
      "grad_norm": 0.24983297288417816,
      "learning_rate": 0.00019033705604185532,
      "loss": 0.672,
      "step": 2406
    },
    {
      "epoch": 4.843058350100604,
      "grad_norm": 0.2589597702026367,
      "learning_rate": 0.0001903330314921018,
      "loss": 0.723,
      "step": 2407
    },
    {
      "epoch": 4.845070422535211,
      "grad_norm": 0.2527557611465454,
      "learning_rate": 0.00019032900694234835,
      "loss": 0.6638,
      "step": 2408
    },
    {
      "epoch": 4.847082494969819,
      "grad_norm": 0.2365657091140747,
      "learning_rate": 0.00019032498239259483,
      "loss": 0.6756,
      "step": 2409
    },
    {
      "epoch": 4.849094567404427,
      "grad_norm": 0.2507765293121338,
      "learning_rate": 0.00019032095784284135,
      "loss": 0.6878,
      "step": 2410
    },
    {
      "epoch": 4.851106639839034,
      "grad_norm": 0.24580489099025726,
      "learning_rate": 0.00019031693329308783,
      "loss": 0.6844,
      "step": 2411
    },
    {
      "epoch": 4.853118712273642,
      "grad_norm": 0.23890531063079834,
      "learning_rate": 0.00019031290874333434,
      "loss": 0.7189,
      "step": 2412
    },
    {
      "epoch": 4.855130784708249,
      "grad_norm": 0.24327901005744934,
      "learning_rate": 0.00019030888419358086,
      "loss": 0.6748,
      "step": 2413
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.25410762429237366,
      "learning_rate": 0.00019030485964382737,
      "loss": 0.6501,
      "step": 2414
    },
    {
      "epoch": 4.859154929577465,
      "grad_norm": 0.2433546632528305,
      "learning_rate": 0.00019030083509407385,
      "loss": 0.6659,
      "step": 2415
    },
    {
      "epoch": 4.861167002012072,
      "grad_norm": 0.2505836486816406,
      "learning_rate": 0.00019029681054432036,
      "loss": 0.6647,
      "step": 2416
    },
    {
      "epoch": 4.8631790744466805,
      "grad_norm": 0.25522443652153015,
      "learning_rate": 0.00019029278599456685,
      "loss": 0.6684,
      "step": 2417
    },
    {
      "epoch": 4.865191146881288,
      "grad_norm": 0.2518933415412903,
      "learning_rate": 0.0001902887614448134,
      "loss": 0.6844,
      "step": 2418
    },
    {
      "epoch": 4.867203219315895,
      "grad_norm": 0.25341829657554626,
      "learning_rate": 0.00019028473689505987,
      "loss": 0.6584,
      "step": 2419
    },
    {
      "epoch": 4.869215291750503,
      "grad_norm": 0.24467600882053375,
      "learning_rate": 0.00019028071234530639,
      "loss": 0.7021,
      "step": 2420
    },
    {
      "epoch": 4.871227364185111,
      "grad_norm": 0.2524375021457672,
      "learning_rate": 0.00019027668779555287,
      "loss": 0.6691,
      "step": 2421
    },
    {
      "epoch": 4.873239436619718,
      "grad_norm": 0.2391015589237213,
      "learning_rate": 0.00019027266324579938,
      "loss": 0.7013,
      "step": 2422
    },
    {
      "epoch": 4.875251509054326,
      "grad_norm": 0.24796675145626068,
      "learning_rate": 0.0001902686386960459,
      "loss": 0.6954,
      "step": 2423
    },
    {
      "epoch": 4.877263581488934,
      "grad_norm": 0.24314482510089874,
      "learning_rate": 0.0001902646141462924,
      "loss": 0.6662,
      "step": 2424
    },
    {
      "epoch": 4.879275653923541,
      "grad_norm": 0.24629859626293182,
      "learning_rate": 0.0001902605895965389,
      "loss": 0.6504,
      "step": 2425
    },
    {
      "epoch": 4.881287726358149,
      "grad_norm": 0.2668483853340149,
      "learning_rate": 0.0001902565650467854,
      "loss": 0.6682,
      "step": 2426
    },
    {
      "epoch": 4.883299798792756,
      "grad_norm": 0.2493775337934494,
      "learning_rate": 0.0001902525404970319,
      "loss": 0.6511,
      "step": 2427
    },
    {
      "epoch": 4.885311871227364,
      "grad_norm": 0.2711859345436096,
      "learning_rate": 0.0001902485159472784,
      "loss": 0.7527,
      "step": 2428
    },
    {
      "epoch": 4.887323943661972,
      "grad_norm": 0.24676673114299774,
      "learning_rate": 0.00019024449139752492,
      "loss": 0.6712,
      "step": 2429
    },
    {
      "epoch": 4.889336016096579,
      "grad_norm": 0.24219699203968048,
      "learning_rate": 0.00019024046684777143,
      "loss": 0.664,
      "step": 2430
    },
    {
      "epoch": 4.891348088531187,
      "grad_norm": 0.25606247782707214,
      "learning_rate": 0.0001902364422980179,
      "loss": 0.6706,
      "step": 2431
    },
    {
      "epoch": 4.893360160965795,
      "grad_norm": 0.24677614867687225,
      "learning_rate": 0.00019023241774826442,
      "loss": 0.6839,
      "step": 2432
    },
    {
      "epoch": 4.895372233400402,
      "grad_norm": 0.24809986352920532,
      "learning_rate": 0.00019022839319851094,
      "loss": 0.6793,
      "step": 2433
    },
    {
      "epoch": 4.89738430583501,
      "grad_norm": 0.25192466378211975,
      "learning_rate": 0.00019022436864875742,
      "loss": 0.6917,
      "step": 2434
    },
    {
      "epoch": 4.899396378269618,
      "grad_norm": 0.2527700364589691,
      "learning_rate": 0.00019022034409900393,
      "loss": 0.6717,
      "step": 2435
    },
    {
      "epoch": 4.901408450704225,
      "grad_norm": 0.24188606441020966,
      "learning_rate": 0.00019021631954925042,
      "loss": 0.6364,
      "step": 2436
    },
    {
      "epoch": 4.903420523138833,
      "grad_norm": 0.2522927224636078,
      "learning_rate": 0.00019021229499949693,
      "loss": 0.6856,
      "step": 2437
    },
    {
      "epoch": 4.905432595573441,
      "grad_norm": 0.25540691614151,
      "learning_rate": 0.00019020827044974344,
      "loss": 0.689,
      "step": 2438
    },
    {
      "epoch": 4.907444668008048,
      "grad_norm": 0.23288899660110474,
      "learning_rate": 0.00019020424589998996,
      "loss": 0.6847,
      "step": 2439
    },
    {
      "epoch": 4.909456740442656,
      "grad_norm": 0.2510182559490204,
      "learning_rate": 0.00019020022135023644,
      "loss": 0.6656,
      "step": 2440
    },
    {
      "epoch": 4.9114688128772634,
      "grad_norm": 0.2645546793937683,
      "learning_rate": 0.00019019619680048295,
      "loss": 0.698,
      "step": 2441
    },
    {
      "epoch": 4.913480885311872,
      "grad_norm": 0.2533215284347534,
      "learning_rate": 0.00019019217225072944,
      "loss": 0.6885,
      "step": 2442
    },
    {
      "epoch": 4.915492957746479,
      "grad_norm": 0.24396447837352753,
      "learning_rate": 0.00019018814770097598,
      "loss": 0.6834,
      "step": 2443
    },
    {
      "epoch": 4.917505030181086,
      "grad_norm": 0.2421337068080902,
      "learning_rate": 0.00019018412315122246,
      "loss": 0.6611,
      "step": 2444
    },
    {
      "epoch": 4.9195171026156945,
      "grad_norm": 0.2437516152858734,
      "learning_rate": 0.00019018009860146898,
      "loss": 0.6899,
      "step": 2445
    },
    {
      "epoch": 4.921529175050302,
      "grad_norm": 0.24071024358272552,
      "learning_rate": 0.00019017607405171546,
      "loss": 0.6776,
      "step": 2446
    },
    {
      "epoch": 4.923541247484909,
      "grad_norm": 0.2565043568611145,
      "learning_rate": 0.00019017204950196197,
      "loss": 0.6909,
      "step": 2447
    },
    {
      "epoch": 4.925553319919517,
      "grad_norm": 0.2576991617679596,
      "learning_rate": 0.00019016802495220848,
      "loss": 0.7076,
      "step": 2448
    },
    {
      "epoch": 4.927565392354125,
      "grad_norm": 0.24717894196510315,
      "learning_rate": 0.000190164000402455,
      "loss": 0.6473,
      "step": 2449
    },
    {
      "epoch": 4.929577464788732,
      "grad_norm": 0.24796096980571747,
      "learning_rate": 0.00019015997585270148,
      "loss": 0.7143,
      "step": 2450
    },
    {
      "epoch": 4.93158953722334,
      "grad_norm": 0.24578653275966644,
      "learning_rate": 0.000190155951302948,
      "loss": 0.6236,
      "step": 2451
    },
    {
      "epoch": 4.933601609657948,
      "grad_norm": 0.26396694779396057,
      "learning_rate": 0.00019015192675319448,
      "loss": 0.6759,
      "step": 2452
    },
    {
      "epoch": 4.935613682092555,
      "grad_norm": 0.2338062822818756,
      "learning_rate": 0.00019014790220344102,
      "loss": 0.6549,
      "step": 2453
    },
    {
      "epoch": 4.937625754527163,
      "grad_norm": 0.25320184230804443,
      "learning_rate": 0.0001901438776536875,
      "loss": 0.6687,
      "step": 2454
    },
    {
      "epoch": 4.9396378269617705,
      "grad_norm": 0.24935419857501984,
      "learning_rate": 0.00019013985310393402,
      "loss": 0.6399,
      "step": 2455
    },
    {
      "epoch": 4.941649899396378,
      "grad_norm": 0.2581043541431427,
      "learning_rate": 0.0001901358285541805,
      "loss": 0.6621,
      "step": 2456
    },
    {
      "epoch": 4.943661971830986,
      "grad_norm": 0.23870807886123657,
      "learning_rate": 0.000190131804004427,
      "loss": 0.6226,
      "step": 2457
    },
    {
      "epoch": 4.945674044265593,
      "grad_norm": 0.243035227060318,
      "learning_rate": 0.00019012777945467353,
      "loss": 0.6698,
      "step": 2458
    },
    {
      "epoch": 4.9476861167002015,
      "grad_norm": 0.2628355026245117,
      "learning_rate": 0.00019012375490492004,
      "loss": 0.6219,
      "step": 2459
    },
    {
      "epoch": 4.949698189134809,
      "grad_norm": 0.24721777439117432,
      "learning_rate": 0.00019011973035516652,
      "loss": 0.6729,
      "step": 2460
    },
    {
      "epoch": 4.951710261569416,
      "grad_norm": 0.2529553771018982,
      "learning_rate": 0.00019011570580541303,
      "loss": 0.6668,
      "step": 2461
    },
    {
      "epoch": 4.953722334004024,
      "grad_norm": 0.2552398443222046,
      "learning_rate": 0.00019011168125565952,
      "loss": 0.6586,
      "step": 2462
    },
    {
      "epoch": 4.955734406438632,
      "grad_norm": 0.2489187866449356,
      "learning_rate": 0.00019010765670590603,
      "loss": 0.6879,
      "step": 2463
    },
    {
      "epoch": 4.957746478873239,
      "grad_norm": 0.23818418383598328,
      "learning_rate": 0.00019010363215615254,
      "loss": 0.6688,
      "step": 2464
    },
    {
      "epoch": 4.959758551307847,
      "grad_norm": 0.2513006329536438,
      "learning_rate": 0.00019009960760639906,
      "loss": 0.6533,
      "step": 2465
    },
    {
      "epoch": 4.961770623742455,
      "grad_norm": 0.23452399671077728,
      "learning_rate": 0.00019009558305664554,
      "loss": 0.6582,
      "step": 2466
    },
    {
      "epoch": 4.963782696177063,
      "grad_norm": 0.24977333843708038,
      "learning_rate": 0.00019009155850689205,
      "loss": 0.6604,
      "step": 2467
    },
    {
      "epoch": 4.96579476861167,
      "grad_norm": 0.2570830285549164,
      "learning_rate": 0.00019008753395713857,
      "loss": 0.714,
      "step": 2468
    },
    {
      "epoch": 4.9678068410462775,
      "grad_norm": 0.2413911074399948,
      "learning_rate": 0.00019008350940738505,
      "loss": 0.6519,
      "step": 2469
    },
    {
      "epoch": 4.969818913480886,
      "grad_norm": 0.2401355355978012,
      "learning_rate": 0.00019007948485763156,
      "loss": 0.6797,
      "step": 2470
    },
    {
      "epoch": 4.971830985915493,
      "grad_norm": 0.27349549531936646,
      "learning_rate": 0.00019007546030787805,
      "loss": 0.7139,
      "step": 2471
    },
    {
      "epoch": 4.9738430583501,
      "grad_norm": 0.25560134649276733,
      "learning_rate": 0.00019007143575812456,
      "loss": 0.6715,
      "step": 2472
    },
    {
      "epoch": 4.9758551307847085,
      "grad_norm": 0.23190544545650482,
      "learning_rate": 0.00019006741120837107,
      "loss": 0.6602,
      "step": 2473
    },
    {
      "epoch": 4.977867203219316,
      "grad_norm": 0.2401379495859146,
      "learning_rate": 0.00019006338665861759,
      "loss": 0.6543,
      "step": 2474
    },
    {
      "epoch": 4.979879275653923,
      "grad_norm": 0.23055499792099,
      "learning_rate": 0.00019005936210886407,
      "loss": 0.6947,
      "step": 2475
    },
    {
      "epoch": 4.981891348088531,
      "grad_norm": 0.2579190135002136,
      "learning_rate": 0.00019005533755911058,
      "loss": 0.6638,
      "step": 2476
    },
    {
      "epoch": 4.983903420523139,
      "grad_norm": 0.2429117113351822,
      "learning_rate": 0.00019005131300935707,
      "loss": 0.6439,
      "step": 2477
    },
    {
      "epoch": 4.985915492957746,
      "grad_norm": 0.24618960916996002,
      "learning_rate": 0.0001900472884596036,
      "loss": 0.6538,
      "step": 2478
    },
    {
      "epoch": 4.987927565392354,
      "grad_norm": 0.25968366861343384,
      "learning_rate": 0.0001900432639098501,
      "loss": 0.6754,
      "step": 2479
    },
    {
      "epoch": 4.989939637826962,
      "grad_norm": 0.26216673851013184,
      "learning_rate": 0.0001900392393600966,
      "loss": 0.7033,
      "step": 2480
    },
    {
      "epoch": 4.991951710261569,
      "grad_norm": 0.24038635194301605,
      "learning_rate": 0.0001900352148103431,
      "loss": 0.6816,
      "step": 2481
    },
    {
      "epoch": 4.993963782696177,
      "grad_norm": 0.24704338610172272,
      "learning_rate": 0.0001900311902605896,
      "loss": 0.6656,
      "step": 2482
    },
    {
      "epoch": 4.9959758551307845,
      "grad_norm": 0.24181953072547913,
      "learning_rate": 0.00019002716571083611,
      "loss": 0.6501,
      "step": 2483
    },
    {
      "epoch": 4.997987927565393,
      "grad_norm": 0.250110387802124,
      "learning_rate": 0.00019002314116108263,
      "loss": 0.6742,
      "step": 2484
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.24745984375476837,
      "learning_rate": 0.0001900191166113291,
      "loss": 0.6516,
      "step": 2485
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.7175166010856628,
      "eval_runtime": 49.8136,
      "eval_samples_per_second": 19.914,
      "eval_steps_per_second": 2.489,
      "step": 2485
    },
    {
      "epoch": 5.002012072434607,
      "grad_norm": 0.2430744171142578,
      "learning_rate": 0.00019001509206157562,
      "loss": 0.6664,
      "step": 2486
    },
    {
      "epoch": 5.0040241448692155,
      "grad_norm": 0.25776034593582153,
      "learning_rate": 0.0001900110675118221,
      "loss": 0.6514,
      "step": 2487
    },
    {
      "epoch": 5.006036217303823,
      "grad_norm": 0.2334631085395813,
      "learning_rate": 0.00019000704296206865,
      "loss": 0.6052,
      "step": 2488
    },
    {
      "epoch": 5.00804828973843,
      "grad_norm": 0.26757296919822693,
      "learning_rate": 0.00019000301841231513,
      "loss": 0.6419,
      "step": 2489
    },
    {
      "epoch": 5.010060362173038,
      "grad_norm": 0.24953626096248627,
      "learning_rate": 0.00018999899386256165,
      "loss": 0.6348,
      "step": 2490
    },
    {
      "epoch": 5.012072434607646,
      "grad_norm": 0.29378262162208557,
      "learning_rate": 0.00018999496931280813,
      "loss": 0.6068,
      "step": 2491
    },
    {
      "epoch": 5.014084507042254,
      "grad_norm": 0.26404088735580444,
      "learning_rate": 0.00018999094476305464,
      "loss": 0.635,
      "step": 2492
    },
    {
      "epoch": 5.016096579476861,
      "grad_norm": 0.25082358717918396,
      "learning_rate": 0.00018998692021330115,
      "loss": 0.5852,
      "step": 2493
    },
    {
      "epoch": 5.018108651911469,
      "grad_norm": 0.24761655926704407,
      "learning_rate": 0.00018998289566354767,
      "loss": 0.6449,
      "step": 2494
    },
    {
      "epoch": 5.020120724346077,
      "grad_norm": 0.2619706392288208,
      "learning_rate": 0.00018997887111379415,
      "loss": 0.6152,
      "step": 2495
    },
    {
      "epoch": 5.022132796780684,
      "grad_norm": 0.25836771726608276,
      "learning_rate": 0.00018997484656404066,
      "loss": 0.6372,
      "step": 2496
    },
    {
      "epoch": 5.0241448692152915,
      "grad_norm": 0.2580084502696991,
      "learning_rate": 0.00018997082201428715,
      "loss": 0.6235,
      "step": 2497
    },
    {
      "epoch": 5.0261569416499,
      "grad_norm": 0.26949867606163025,
      "learning_rate": 0.00018996679746453366,
      "loss": 0.644,
      "step": 2498
    },
    {
      "epoch": 5.028169014084507,
      "grad_norm": 0.26364946365356445,
      "learning_rate": 0.00018996277291478017,
      "loss": 0.6268,
      "step": 2499
    },
    {
      "epoch": 5.030181086519114,
      "grad_norm": 0.27386102080345154,
      "learning_rate": 0.00018995874836502669,
      "loss": 0.6743,
      "step": 2500
    },
    {
      "epoch": 5.0321931589537225,
      "grad_norm": 0.27057012915611267,
      "learning_rate": 0.00018995472381527317,
      "loss": 0.6127,
      "step": 2501
    },
    {
      "epoch": 5.03420523138833,
      "grad_norm": 0.2636951804161072,
      "learning_rate": 0.00018995069926551968,
      "loss": 0.6346,
      "step": 2502
    },
    {
      "epoch": 5.036217303822937,
      "grad_norm": 0.26181671023368835,
      "learning_rate": 0.0001899466747157662,
      "loss": 0.5999,
      "step": 2503
    },
    {
      "epoch": 5.038229376257545,
      "grad_norm": 0.2689956724643707,
      "learning_rate": 0.00018994265016601268,
      "loss": 0.6473,
      "step": 2504
    },
    {
      "epoch": 5.040241448692153,
      "grad_norm": 0.26402655243873596,
      "learning_rate": 0.0001899386256162592,
      "loss": 0.6187,
      "step": 2505
    },
    {
      "epoch": 5.042253521126761,
      "grad_norm": 0.25883153080940247,
      "learning_rate": 0.00018993460106650568,
      "loss": 0.6055,
      "step": 2506
    },
    {
      "epoch": 5.044265593561368,
      "grad_norm": 0.24401569366455078,
      "learning_rate": 0.0001899305765167522,
      "loss": 0.6722,
      "step": 2507
    },
    {
      "epoch": 5.046277665995976,
      "grad_norm": 0.252629816532135,
      "learning_rate": 0.0001899265519669987,
      "loss": 0.6041,
      "step": 2508
    },
    {
      "epoch": 5.048289738430584,
      "grad_norm": 0.2779274582862854,
      "learning_rate": 0.00018992252741724521,
      "loss": 0.6694,
      "step": 2509
    },
    {
      "epoch": 5.050301810865191,
      "grad_norm": 0.2670474052429199,
      "learning_rate": 0.0001899185028674917,
      "loss": 0.6612,
      "step": 2510
    },
    {
      "epoch": 5.0523138832997985,
      "grad_norm": 0.2724792957305908,
      "learning_rate": 0.0001899144783177382,
      "loss": 0.6735,
      "step": 2511
    },
    {
      "epoch": 5.054325955734407,
      "grad_norm": 0.27274367213249207,
      "learning_rate": 0.0001899104537679847,
      "loss": 0.6425,
      "step": 2512
    },
    {
      "epoch": 5.056338028169014,
      "grad_norm": 0.25531095266342163,
      "learning_rate": 0.00018990642921823124,
      "loss": 0.6404,
      "step": 2513
    },
    {
      "epoch": 5.058350100603621,
      "grad_norm": 0.2762371897697449,
      "learning_rate": 0.00018990240466847772,
      "loss": 0.6508,
      "step": 2514
    },
    {
      "epoch": 5.0603621730382295,
      "grad_norm": 0.26847201585769653,
      "learning_rate": 0.00018989838011872423,
      "loss": 0.6597,
      "step": 2515
    },
    {
      "epoch": 5.062374245472837,
      "grad_norm": 0.2685941755771637,
      "learning_rate": 0.00018989435556897072,
      "loss": 0.6207,
      "step": 2516
    },
    {
      "epoch": 5.064386317907445,
      "grad_norm": 0.2612045407295227,
      "learning_rate": 0.00018989033101921723,
      "loss": 0.6502,
      "step": 2517
    },
    {
      "epoch": 5.066398390342052,
      "grad_norm": 0.2622927129268646,
      "learning_rate": 0.00018988630646946374,
      "loss": 0.62,
      "step": 2518
    },
    {
      "epoch": 5.06841046277666,
      "grad_norm": 0.2636546492576599,
      "learning_rate": 0.00018988228191971026,
      "loss": 0.6387,
      "step": 2519
    },
    {
      "epoch": 5.070422535211268,
      "grad_norm": 0.259335994720459,
      "learning_rate": 0.00018987825736995674,
      "loss": 0.6408,
      "step": 2520
    },
    {
      "epoch": 5.072434607645875,
      "grad_norm": 0.2719300389289856,
      "learning_rate": 0.00018987423282020325,
      "loss": 0.6494,
      "step": 2521
    },
    {
      "epoch": 5.074446680080483,
      "grad_norm": 0.2537870705127716,
      "learning_rate": 0.00018987020827044974,
      "loss": 0.6549,
      "step": 2522
    },
    {
      "epoch": 5.076458752515091,
      "grad_norm": 0.26740241050720215,
      "learning_rate": 0.00018986618372069628,
      "loss": 0.6063,
      "step": 2523
    },
    {
      "epoch": 5.078470824949698,
      "grad_norm": 0.2696356177330017,
      "learning_rate": 0.00018986215917094276,
      "loss": 0.643,
      "step": 2524
    },
    {
      "epoch": 5.0804828973843055,
      "grad_norm": 0.2668154835700989,
      "learning_rate": 0.00018985813462118927,
      "loss": 0.6285,
      "step": 2525
    },
    {
      "epoch": 5.082494969818914,
      "grad_norm": 0.29732179641723633,
      "learning_rate": 0.00018985411007143576,
      "loss": 0.6824,
      "step": 2526
    },
    {
      "epoch": 5.084507042253521,
      "grad_norm": 0.27379336953163147,
      "learning_rate": 0.00018985008552168227,
      "loss": 0.6491,
      "step": 2527
    },
    {
      "epoch": 5.086519114688128,
      "grad_norm": 0.2540965974330902,
      "learning_rate": 0.00018984606097192878,
      "loss": 0.6231,
      "step": 2528
    },
    {
      "epoch": 5.0885311871227366,
      "grad_norm": 0.2538018226623535,
      "learning_rate": 0.0001898420364221753,
      "loss": 0.6435,
      "step": 2529
    },
    {
      "epoch": 5.090543259557344,
      "grad_norm": 0.28185969591140747,
      "learning_rate": 0.00018983801187242178,
      "loss": 0.6316,
      "step": 2530
    },
    {
      "epoch": 5.092555331991952,
      "grad_norm": 0.26817023754119873,
      "learning_rate": 0.0001898339873226683,
      "loss": 0.6252,
      "step": 2531
    },
    {
      "epoch": 5.094567404426559,
      "grad_norm": 0.2658981680870056,
      "learning_rate": 0.00018982996277291478,
      "loss": 0.6388,
      "step": 2532
    },
    {
      "epoch": 5.096579476861167,
      "grad_norm": 0.2669132351875305,
      "learning_rate": 0.0001898259382231613,
      "loss": 0.626,
      "step": 2533
    },
    {
      "epoch": 5.098591549295775,
      "grad_norm": 0.2695295214653015,
      "learning_rate": 0.0001898219136734078,
      "loss": 0.6726,
      "step": 2534
    },
    {
      "epoch": 5.100603621730382,
      "grad_norm": 0.27663636207580566,
      "learning_rate": 0.00018981788912365432,
      "loss": 0.6459,
      "step": 2535
    },
    {
      "epoch": 5.10261569416499,
      "grad_norm": 0.28536826372146606,
      "learning_rate": 0.0001898138645739008,
      "loss": 0.6705,
      "step": 2536
    },
    {
      "epoch": 5.104627766599598,
      "grad_norm": 0.2834741175174713,
      "learning_rate": 0.0001898098400241473,
      "loss": 0.6267,
      "step": 2537
    },
    {
      "epoch": 5.106639839034205,
      "grad_norm": 0.25740551948547363,
      "learning_rate": 0.00018980581547439383,
      "loss": 0.6255,
      "step": 2538
    },
    {
      "epoch": 5.1086519114688125,
      "grad_norm": 0.25717100501060486,
      "learning_rate": 0.0001898017909246403,
      "loss": 0.6377,
      "step": 2539
    },
    {
      "epoch": 5.110663983903421,
      "grad_norm": 0.2583122253417969,
      "learning_rate": 0.00018979776637488682,
      "loss": 0.668,
      "step": 2540
    },
    {
      "epoch": 5.112676056338028,
      "grad_norm": 0.2697280943393707,
      "learning_rate": 0.0001897937418251333,
      "loss": 0.6254,
      "step": 2541
    },
    {
      "epoch": 5.114688128772636,
      "grad_norm": 0.26289427280426025,
      "learning_rate": 0.00018978971727537982,
      "loss": 0.6132,
      "step": 2542
    },
    {
      "epoch": 5.116700201207244,
      "grad_norm": 0.29204797744750977,
      "learning_rate": 0.00018978569272562633,
      "loss": 0.6685,
      "step": 2543
    },
    {
      "epoch": 5.118712273641851,
      "grad_norm": 0.29852959513664246,
      "learning_rate": 0.00018978166817587284,
      "loss": 0.6621,
      "step": 2544
    },
    {
      "epoch": 5.120724346076459,
      "grad_norm": 0.30092012882232666,
      "learning_rate": 0.00018977764362611933,
      "loss": 0.6384,
      "step": 2545
    },
    {
      "epoch": 5.122736418511066,
      "grad_norm": 0.2730007469654083,
      "learning_rate": 0.00018977361907636584,
      "loss": 0.6574,
      "step": 2546
    },
    {
      "epoch": 5.124748490945674,
      "grad_norm": 0.27308598160743713,
      "learning_rate": 0.00018976959452661233,
      "loss": 0.6298,
      "step": 2547
    },
    {
      "epoch": 5.126760563380282,
      "grad_norm": 0.25545641779899597,
      "learning_rate": 0.00018976556997685887,
      "loss": 0.6174,
      "step": 2548
    },
    {
      "epoch": 5.128772635814889,
      "grad_norm": 0.256916880607605,
      "learning_rate": 0.00018976154542710535,
      "loss": 0.6376,
      "step": 2549
    },
    {
      "epoch": 5.130784708249497,
      "grad_norm": 0.2562265396118164,
      "learning_rate": 0.00018975752087735186,
      "loss": 0.6487,
      "step": 2550
    },
    {
      "epoch": 5.132796780684105,
      "grad_norm": 0.27081790566444397,
      "learning_rate": 0.00018975349632759835,
      "loss": 0.6394,
      "step": 2551
    },
    {
      "epoch": 5.134808853118712,
      "grad_norm": 0.25967803597450256,
      "learning_rate": 0.00018974947177784486,
      "loss": 0.6547,
      "step": 2552
    },
    {
      "epoch": 5.1368209255533195,
      "grad_norm": 0.2761690616607666,
      "learning_rate": 0.00018974544722809135,
      "loss": 0.6244,
      "step": 2553
    },
    {
      "epoch": 5.138832997987928,
      "grad_norm": 0.2677914500236511,
      "learning_rate": 0.00018974142267833789,
      "loss": 0.6631,
      "step": 2554
    },
    {
      "epoch": 5.140845070422535,
      "grad_norm": 0.2612059414386749,
      "learning_rate": 0.00018973739812858437,
      "loss": 0.6203,
      "step": 2555
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 0.27895694971084595,
      "learning_rate": 0.00018973337357883088,
      "loss": 0.6635,
      "step": 2556
    },
    {
      "epoch": 5.144869215291751,
      "grad_norm": 0.26269763708114624,
      "learning_rate": 0.00018972934902907737,
      "loss": 0.641,
      "step": 2557
    },
    {
      "epoch": 5.146881287726358,
      "grad_norm": 0.2517487406730652,
      "learning_rate": 0.00018972532447932388,
      "loss": 0.5914,
      "step": 2558
    },
    {
      "epoch": 5.148893360160966,
      "grad_norm": 0.2519668936729431,
      "learning_rate": 0.0001897212999295704,
      "loss": 0.6236,
      "step": 2559
    },
    {
      "epoch": 5.150905432595573,
      "grad_norm": 0.2590313255786896,
      "learning_rate": 0.0001897172753798169,
      "loss": 0.6533,
      "step": 2560
    },
    {
      "epoch": 5.152917505030181,
      "grad_norm": 0.2706417441368103,
      "learning_rate": 0.0001897132508300634,
      "loss": 0.6845,
      "step": 2561
    },
    {
      "epoch": 5.154929577464789,
      "grad_norm": 0.2584000825881958,
      "learning_rate": 0.0001897092262803099,
      "loss": 0.6476,
      "step": 2562
    },
    {
      "epoch": 5.156941649899396,
      "grad_norm": 0.253615140914917,
      "learning_rate": 0.0001897052017305564,
      "loss": 0.5968,
      "step": 2563
    },
    {
      "epoch": 5.158953722334004,
      "grad_norm": 0.2754031717777252,
      "learning_rate": 0.00018970117718080293,
      "loss": 0.6105,
      "step": 2564
    },
    {
      "epoch": 5.160965794768612,
      "grad_norm": 0.26222193241119385,
      "learning_rate": 0.0001896971526310494,
      "loss": 0.6019,
      "step": 2565
    },
    {
      "epoch": 5.162977867203219,
      "grad_norm": 0.2587164342403412,
      "learning_rate": 0.00018969312808129592,
      "loss": 0.613,
      "step": 2566
    },
    {
      "epoch": 5.164989939637827,
      "grad_norm": 0.27138957381248474,
      "learning_rate": 0.0001896891035315424,
      "loss": 0.6414,
      "step": 2567
    },
    {
      "epoch": 5.167002012072435,
      "grad_norm": 0.29934433102607727,
      "learning_rate": 0.00018968507898178892,
      "loss": 0.6027,
      "step": 2568
    },
    {
      "epoch": 5.169014084507042,
      "grad_norm": 0.26526904106140137,
      "learning_rate": 0.00018968105443203543,
      "loss": 0.6777,
      "step": 2569
    },
    {
      "epoch": 5.17102615694165,
      "grad_norm": 0.25974470376968384,
      "learning_rate": 0.00018967702988228192,
      "loss": 0.6384,
      "step": 2570
    },
    {
      "epoch": 5.173038229376258,
      "grad_norm": 0.2622644603252411,
      "learning_rate": 0.00018967300533252843,
      "loss": 0.6308,
      "step": 2571
    },
    {
      "epoch": 5.175050301810865,
      "grad_norm": 0.27169543504714966,
      "learning_rate": 0.00018966898078277494,
      "loss": 0.6742,
      "step": 2572
    },
    {
      "epoch": 5.177062374245473,
      "grad_norm": 0.264876127243042,
      "learning_rate": 0.00018966495623302143,
      "loss": 0.6467,
      "step": 2573
    },
    {
      "epoch": 5.17907444668008,
      "grad_norm": 0.2815327048301697,
      "learning_rate": 0.00018966093168326794,
      "loss": 0.6131,
      "step": 2574
    },
    {
      "epoch": 5.181086519114688,
      "grad_norm": 0.26040729880332947,
      "learning_rate": 0.00018965690713351445,
      "loss": 0.6741,
      "step": 2575
    },
    {
      "epoch": 5.183098591549296,
      "grad_norm": 0.26627209782600403,
      "learning_rate": 0.00018965288258376094,
      "loss": 0.6522,
      "step": 2576
    },
    {
      "epoch": 5.185110663983903,
      "grad_norm": 0.27125415205955505,
      "learning_rate": 0.00018964885803400745,
      "loss": 0.6099,
      "step": 2577
    },
    {
      "epoch": 5.187122736418511,
      "grad_norm": 0.2693953216075897,
      "learning_rate": 0.00018964483348425393,
      "loss": 0.6648,
      "step": 2578
    },
    {
      "epoch": 5.189134808853119,
      "grad_norm": 0.27869564294815063,
      "learning_rate": 0.00018964080893450047,
      "loss": 0.6259,
      "step": 2579
    },
    {
      "epoch": 5.191146881287726,
      "grad_norm": 0.2813829481601715,
      "learning_rate": 0.00018963678438474696,
      "loss": 0.6252,
      "step": 2580
    },
    {
      "epoch": 5.193158953722334,
      "grad_norm": 0.2608790099620819,
      "learning_rate": 0.00018963275983499347,
      "loss": 0.6451,
      "step": 2581
    },
    {
      "epoch": 5.195171026156942,
      "grad_norm": 0.2780724763870239,
      "learning_rate": 0.00018962873528523996,
      "loss": 0.6899,
      "step": 2582
    },
    {
      "epoch": 5.197183098591549,
      "grad_norm": 0.2829635739326477,
      "learning_rate": 0.00018962471073548647,
      "loss": 0.6663,
      "step": 2583
    },
    {
      "epoch": 5.199195171026157,
      "grad_norm": 0.2836062014102936,
      "learning_rate": 0.00018962068618573298,
      "loss": 0.6759,
      "step": 2584
    },
    {
      "epoch": 5.201207243460765,
      "grad_norm": 0.26677387952804565,
      "learning_rate": 0.0001896166616359795,
      "loss": 0.664,
      "step": 2585
    },
    {
      "epoch": 5.203219315895372,
      "grad_norm": 0.2720077335834503,
      "learning_rate": 0.00018961263708622598,
      "loss": 0.6243,
      "step": 2586
    },
    {
      "epoch": 5.20523138832998,
      "grad_norm": 0.26741576194763184,
      "learning_rate": 0.0001896086125364725,
      "loss": 0.6599,
      "step": 2587
    },
    {
      "epoch": 5.207243460764587,
      "grad_norm": 0.268777996301651,
      "learning_rate": 0.00018960458798671898,
      "loss": 0.6456,
      "step": 2588
    },
    {
      "epoch": 5.209255533199195,
      "grad_norm": 0.28686800599098206,
      "learning_rate": 0.00018960056343696551,
      "loss": 0.6582,
      "step": 2589
    },
    {
      "epoch": 5.211267605633803,
      "grad_norm": 0.28112730383872986,
      "learning_rate": 0.000189596538887212,
      "loss": 0.6488,
      "step": 2590
    },
    {
      "epoch": 5.21327967806841,
      "grad_norm": 0.2625673711299896,
      "learning_rate": 0.0001895925143374585,
      "loss": 0.6449,
      "step": 2591
    },
    {
      "epoch": 5.2152917505030185,
      "grad_norm": 0.26220402121543884,
      "learning_rate": 0.000189588489787705,
      "loss": 0.6567,
      "step": 2592
    },
    {
      "epoch": 5.217303822937626,
      "grad_norm": 0.26635080575942993,
      "learning_rate": 0.0001895844652379515,
      "loss": 0.6878,
      "step": 2593
    },
    {
      "epoch": 5.219315895372233,
      "grad_norm": 0.27550771832466125,
      "learning_rate": 0.00018958044068819802,
      "loss": 0.6554,
      "step": 2594
    },
    {
      "epoch": 5.221327967806841,
      "grad_norm": 0.25930255651474,
      "learning_rate": 0.00018957641613844453,
      "loss": 0.6479,
      "step": 2595
    },
    {
      "epoch": 5.223340040241449,
      "grad_norm": 0.2536938786506653,
      "learning_rate": 0.00018957239158869102,
      "loss": 0.6439,
      "step": 2596
    },
    {
      "epoch": 5.225352112676056,
      "grad_norm": 0.27519020438194275,
      "learning_rate": 0.00018956836703893753,
      "loss": 0.6522,
      "step": 2597
    },
    {
      "epoch": 5.227364185110664,
      "grad_norm": 0.27075591683387756,
      "learning_rate": 0.00018956434248918402,
      "loss": 0.6613,
      "step": 2598
    },
    {
      "epoch": 5.229376257545272,
      "grad_norm": 0.280238538980484,
      "learning_rate": 0.00018956031793943056,
      "loss": 0.6193,
      "step": 2599
    },
    {
      "epoch": 5.231388329979879,
      "grad_norm": 0.2730329930782318,
      "learning_rate": 0.00018955629338967704,
      "loss": 0.6424,
      "step": 2600
    },
    {
      "epoch": 5.233400402414487,
      "grad_norm": 0.27125146985054016,
      "learning_rate": 0.00018955226883992355,
      "loss": 0.6482,
      "step": 2601
    },
    {
      "epoch": 5.2354124748490944,
      "grad_norm": 0.2700479328632355,
      "learning_rate": 0.00018954824429017004,
      "loss": 0.6378,
      "step": 2602
    },
    {
      "epoch": 5.237424547283702,
      "grad_norm": 0.264117568731308,
      "learning_rate": 0.00018954421974041655,
      "loss": 0.6734,
      "step": 2603
    },
    {
      "epoch": 5.23943661971831,
      "grad_norm": 0.26527249813079834,
      "learning_rate": 0.00018954019519066306,
      "loss": 0.6176,
      "step": 2604
    },
    {
      "epoch": 5.241448692152917,
      "grad_norm": 0.2735840082168579,
      "learning_rate": 0.00018953617064090955,
      "loss": 0.6719,
      "step": 2605
    },
    {
      "epoch": 5.2434607645875255,
      "grad_norm": 0.27222296595573425,
      "learning_rate": 0.00018953214609115606,
      "loss": 0.6417,
      "step": 2606
    },
    {
      "epoch": 5.245472837022133,
      "grad_norm": 0.2687821686267853,
      "learning_rate": 0.00018952812154140257,
      "loss": 0.6388,
      "step": 2607
    },
    {
      "epoch": 5.24748490945674,
      "grad_norm": 0.2679801881313324,
      "learning_rate": 0.00018952409699164906,
      "loss": 0.6322,
      "step": 2608
    },
    {
      "epoch": 5.249496981891348,
      "grad_norm": 0.26710572838783264,
      "learning_rate": 0.00018952007244189557,
      "loss": 0.6295,
      "step": 2609
    },
    {
      "epoch": 5.251509054325956,
      "grad_norm": 0.271618515253067,
      "learning_rate": 0.00018951604789214208,
      "loss": 0.6525,
      "step": 2610
    },
    {
      "epoch": 5.253521126760563,
      "grad_norm": 0.2632712125778198,
      "learning_rate": 0.00018951202334238857,
      "loss": 0.6264,
      "step": 2611
    },
    {
      "epoch": 5.255533199195171,
      "grad_norm": 0.2635985016822815,
      "learning_rate": 0.00018950799879263508,
      "loss": 0.6259,
      "step": 2612
    },
    {
      "epoch": 5.257545271629779,
      "grad_norm": 0.26376277208328247,
      "learning_rate": 0.00018950397424288156,
      "loss": 0.6852,
      "step": 2613
    },
    {
      "epoch": 5.259557344064386,
      "grad_norm": 0.2818067967891693,
      "learning_rate": 0.0001894999496931281,
      "loss": 0.663,
      "step": 2614
    },
    {
      "epoch": 5.261569416498994,
      "grad_norm": 0.2826760709285736,
      "learning_rate": 0.0001894959251433746,
      "loss": 0.6324,
      "step": 2615
    },
    {
      "epoch": 5.2635814889336014,
      "grad_norm": 0.28126096725463867,
      "learning_rate": 0.0001894919005936211,
      "loss": 0.6442,
      "step": 2616
    },
    {
      "epoch": 5.26559356136821,
      "grad_norm": 0.25062915682792664,
      "learning_rate": 0.00018948787604386759,
      "loss": 0.6034,
      "step": 2617
    },
    {
      "epoch": 5.267605633802817,
      "grad_norm": 0.2719072699546814,
      "learning_rate": 0.0001894838514941141,
      "loss": 0.6491,
      "step": 2618
    },
    {
      "epoch": 5.269617706237424,
      "grad_norm": 0.2675890028476715,
      "learning_rate": 0.0001894798269443606,
      "loss": 0.6496,
      "step": 2619
    },
    {
      "epoch": 5.2716297786720325,
      "grad_norm": 0.26547563076019287,
      "learning_rate": 0.00018947580239460712,
      "loss": 0.6669,
      "step": 2620
    },
    {
      "epoch": 5.27364185110664,
      "grad_norm": 0.27818962931632996,
      "learning_rate": 0.0001894717778448536,
      "loss": 0.6526,
      "step": 2621
    },
    {
      "epoch": 5.275653923541247,
      "grad_norm": 0.2871329188346863,
      "learning_rate": 0.00018946775329510012,
      "loss": 0.6671,
      "step": 2622
    },
    {
      "epoch": 5.277665995975855,
      "grad_norm": 0.2745891511440277,
      "learning_rate": 0.0001894637287453466,
      "loss": 0.6186,
      "step": 2623
    },
    {
      "epoch": 5.279678068410463,
      "grad_norm": 0.27969640493392944,
      "learning_rate": 0.00018945970419559314,
      "loss": 0.6776,
      "step": 2624
    },
    {
      "epoch": 5.28169014084507,
      "grad_norm": 0.27406567335128784,
      "learning_rate": 0.00018945567964583963,
      "loss": 0.6291,
      "step": 2625
    },
    {
      "epoch": 5.283702213279678,
      "grad_norm": 0.2566035985946655,
      "learning_rate": 0.00018945165509608614,
      "loss": 0.6281,
      "step": 2626
    },
    {
      "epoch": 5.285714285714286,
      "grad_norm": 0.2603382170200348,
      "learning_rate": 0.00018944763054633263,
      "loss": 0.6566,
      "step": 2627
    },
    {
      "epoch": 5.287726358148893,
      "grad_norm": 0.27171340584754944,
      "learning_rate": 0.00018944360599657914,
      "loss": 0.6584,
      "step": 2628
    },
    {
      "epoch": 5.289738430583501,
      "grad_norm": 0.2629947364330292,
      "learning_rate": 0.00018943958144682565,
      "loss": 0.6178,
      "step": 2629
    },
    {
      "epoch": 5.2917505030181085,
      "grad_norm": 0.2745983898639679,
      "learning_rate": 0.00018943555689707216,
      "loss": 0.6702,
      "step": 2630
    },
    {
      "epoch": 5.293762575452717,
      "grad_norm": 0.28547656536102295,
      "learning_rate": 0.00018943153234731865,
      "loss": 0.6421,
      "step": 2631
    },
    {
      "epoch": 5.295774647887324,
      "grad_norm": 0.27536100149154663,
      "learning_rate": 0.00018942750779756516,
      "loss": 0.6509,
      "step": 2632
    },
    {
      "epoch": 5.297786720321931,
      "grad_norm": 0.28548097610473633,
      "learning_rate": 0.00018942348324781165,
      "loss": 0.6571,
      "step": 2633
    },
    {
      "epoch": 5.2997987927565395,
      "grad_norm": 0.269807904958725,
      "learning_rate": 0.00018941945869805818,
      "loss": 0.597,
      "step": 2634
    },
    {
      "epoch": 5.301810865191147,
      "grad_norm": 0.26674410700798035,
      "learning_rate": 0.00018941543414830467,
      "loss": 0.6237,
      "step": 2635
    },
    {
      "epoch": 5.303822937625754,
      "grad_norm": 0.2560076415538788,
      "learning_rate": 0.00018941140959855118,
      "loss": 0.6271,
      "step": 2636
    },
    {
      "epoch": 5.305835010060362,
      "grad_norm": 0.26934894919395447,
      "learning_rate": 0.00018940738504879767,
      "loss": 0.6292,
      "step": 2637
    },
    {
      "epoch": 5.30784708249497,
      "grad_norm": 0.28316834568977356,
      "learning_rate": 0.00018940336049904418,
      "loss": 0.666,
      "step": 2638
    },
    {
      "epoch": 5.309859154929577,
      "grad_norm": 0.2548958957195282,
      "learning_rate": 0.0001893993359492907,
      "loss": 0.6135,
      "step": 2639
    },
    {
      "epoch": 5.311871227364185,
      "grad_norm": 0.25352632999420166,
      "learning_rate": 0.00018939531139953718,
      "loss": 0.6114,
      "step": 2640
    },
    {
      "epoch": 5.313883299798793,
      "grad_norm": 0.27149608731269836,
      "learning_rate": 0.0001893912868497837,
      "loss": 0.6173,
      "step": 2641
    },
    {
      "epoch": 5.315895372233401,
      "grad_norm": 0.2881794273853302,
      "learning_rate": 0.0001893872623000302,
      "loss": 0.6541,
      "step": 2642
    },
    {
      "epoch": 5.317907444668008,
      "grad_norm": 0.28280243277549744,
      "learning_rate": 0.00018938323775027669,
      "loss": 0.6353,
      "step": 2643
    },
    {
      "epoch": 5.3199195171026155,
      "grad_norm": 0.27744248509407043,
      "learning_rate": 0.0001893792132005232,
      "loss": 0.668,
      "step": 2644
    },
    {
      "epoch": 5.321931589537224,
      "grad_norm": 0.26354292035102844,
      "learning_rate": 0.0001893751886507697,
      "loss": 0.6588,
      "step": 2645
    },
    {
      "epoch": 5.323943661971831,
      "grad_norm": 0.27004575729370117,
      "learning_rate": 0.0001893711641010162,
      "loss": 0.65,
      "step": 2646
    },
    {
      "epoch": 5.325955734406438,
      "grad_norm": 0.2536115348339081,
      "learning_rate": 0.0001893671395512627,
      "loss": 0.5931,
      "step": 2647
    },
    {
      "epoch": 5.3279678068410465,
      "grad_norm": 0.26013439893722534,
      "learning_rate": 0.0001893631150015092,
      "loss": 0.6571,
      "step": 2648
    },
    {
      "epoch": 5.329979879275654,
      "grad_norm": 0.2806578576564789,
      "learning_rate": 0.00018935909045175573,
      "loss": 0.6325,
      "step": 2649
    },
    {
      "epoch": 5.331991951710261,
      "grad_norm": 0.26826047897338867,
      "learning_rate": 0.00018935506590200222,
      "loss": 0.6542,
      "step": 2650
    },
    {
      "epoch": 5.334004024144869,
      "grad_norm": 0.2704671025276184,
      "learning_rate": 0.00018935104135224873,
      "loss": 0.6788,
      "step": 2651
    },
    {
      "epoch": 5.336016096579477,
      "grad_norm": 0.2709640562534332,
      "learning_rate": 0.00018934701680249521,
      "loss": 0.6458,
      "step": 2652
    },
    {
      "epoch": 5.338028169014084,
      "grad_norm": 0.28912127017974854,
      "learning_rate": 0.00018934299225274173,
      "loss": 0.6896,
      "step": 2653
    },
    {
      "epoch": 5.340040241448692,
      "grad_norm": 0.2779504954814911,
      "learning_rate": 0.00018933896770298824,
      "loss": 0.6334,
      "step": 2654
    },
    {
      "epoch": 5.3420523138833,
      "grad_norm": 0.2764737606048584,
      "learning_rate": 0.00018933494315323475,
      "loss": 0.6069,
      "step": 2655
    },
    {
      "epoch": 5.344064386317908,
      "grad_norm": 0.2650766968727112,
      "learning_rate": 0.00018933091860348124,
      "loss": 0.6075,
      "step": 2656
    },
    {
      "epoch": 5.346076458752515,
      "grad_norm": 0.2824741005897522,
      "learning_rate": 0.00018932689405372775,
      "loss": 0.702,
      "step": 2657
    },
    {
      "epoch": 5.3480885311871225,
      "grad_norm": 0.27721524238586426,
      "learning_rate": 0.00018932286950397423,
      "loss": 0.6655,
      "step": 2658
    },
    {
      "epoch": 5.350100603621731,
      "grad_norm": 0.2737990617752075,
      "learning_rate": 0.00018931884495422077,
      "loss": 0.6044,
      "step": 2659
    },
    {
      "epoch": 5.352112676056338,
      "grad_norm": 0.2749147117137909,
      "learning_rate": 0.00018931482040446726,
      "loss": 0.6755,
      "step": 2660
    },
    {
      "epoch": 5.354124748490945,
      "grad_norm": 0.3126353919506073,
      "learning_rate": 0.00018931079585471377,
      "loss": 0.6502,
      "step": 2661
    },
    {
      "epoch": 5.3561368209255535,
      "grad_norm": 0.25860705971717834,
      "learning_rate": 0.00018930677130496026,
      "loss": 0.6369,
      "step": 2662
    },
    {
      "epoch": 5.358148893360161,
      "grad_norm": 0.2832357883453369,
      "learning_rate": 0.00018930274675520677,
      "loss": 0.6199,
      "step": 2663
    },
    {
      "epoch": 5.360160965794769,
      "grad_norm": 0.2826072573661804,
      "learning_rate": 0.00018929872220545328,
      "loss": 0.6138,
      "step": 2664
    },
    {
      "epoch": 5.362173038229376,
      "grad_norm": 0.26680833101272583,
      "learning_rate": 0.0001892946976556998,
      "loss": 0.6025,
      "step": 2665
    },
    {
      "epoch": 5.364185110663984,
      "grad_norm": 0.2703198194503784,
      "learning_rate": 0.00018929067310594628,
      "loss": 0.626,
      "step": 2666
    },
    {
      "epoch": 5.366197183098592,
      "grad_norm": 0.2623455822467804,
      "learning_rate": 0.0001892866485561928,
      "loss": 0.6187,
      "step": 2667
    },
    {
      "epoch": 5.368209255533199,
      "grad_norm": 0.2741657495498657,
      "learning_rate": 0.00018928262400643927,
      "loss": 0.6186,
      "step": 2668
    },
    {
      "epoch": 5.370221327967807,
      "grad_norm": 0.26734498143196106,
      "learning_rate": 0.00018927859945668581,
      "loss": 0.6894,
      "step": 2669
    },
    {
      "epoch": 5.372233400402415,
      "grad_norm": 0.26490139961242676,
      "learning_rate": 0.0001892745749069323,
      "loss": 0.6393,
      "step": 2670
    },
    {
      "epoch": 5.374245472837022,
      "grad_norm": 0.2582879066467285,
      "learning_rate": 0.0001892705503571788,
      "loss": 0.6369,
      "step": 2671
    },
    {
      "epoch": 5.3762575452716295,
      "grad_norm": 0.27600714564323425,
      "learning_rate": 0.0001892665258074253,
      "loss": 0.6385,
      "step": 2672
    },
    {
      "epoch": 5.378269617706238,
      "grad_norm": 0.2857367694377899,
      "learning_rate": 0.0001892625012576718,
      "loss": 0.6313,
      "step": 2673
    },
    {
      "epoch": 5.380281690140845,
      "grad_norm": 0.27183642983436584,
      "learning_rate": 0.00018925847670791832,
      "loss": 0.6651,
      "step": 2674
    },
    {
      "epoch": 5.382293762575452,
      "grad_norm": 0.2648921012878418,
      "learning_rate": 0.0001892544521581648,
      "loss": 0.6355,
      "step": 2675
    },
    {
      "epoch": 5.3843058350100605,
      "grad_norm": 0.27937179803848267,
      "learning_rate": 0.00018925042760841132,
      "loss": 0.6734,
      "step": 2676
    },
    {
      "epoch": 5.386317907444668,
      "grad_norm": 0.2590011954307556,
      "learning_rate": 0.00018924640305865783,
      "loss": 0.6102,
      "step": 2677
    },
    {
      "epoch": 5.388329979879275,
      "grad_norm": 0.2672592103481293,
      "learning_rate": 0.00018924237850890432,
      "loss": 0.6486,
      "step": 2678
    },
    {
      "epoch": 5.390342052313883,
      "grad_norm": 0.26558077335357666,
      "learning_rate": 0.00018923835395915083,
      "loss": 0.6277,
      "step": 2679
    },
    {
      "epoch": 5.392354124748491,
      "grad_norm": 0.26510217785835266,
      "learning_rate": 0.00018923432940939734,
      "loss": 0.6427,
      "step": 2680
    },
    {
      "epoch": 5.394366197183099,
      "grad_norm": 0.25689613819122314,
      "learning_rate": 0.00018923030485964383,
      "loss": 0.6269,
      "step": 2681
    },
    {
      "epoch": 5.396378269617706,
      "grad_norm": 0.2508394718170166,
      "learning_rate": 0.00018922628030989034,
      "loss": 0.6208,
      "step": 2682
    },
    {
      "epoch": 5.398390342052314,
      "grad_norm": 0.27621719241142273,
      "learning_rate": 0.00018922225576013682,
      "loss": 0.6471,
      "step": 2683
    },
    {
      "epoch": 5.400402414486922,
      "grad_norm": 0.27801093459129333,
      "learning_rate": 0.00018921823121038336,
      "loss": 0.6641,
      "step": 2684
    },
    {
      "epoch": 5.402414486921529,
      "grad_norm": 0.2749769389629364,
      "learning_rate": 0.00018921420666062985,
      "loss": 0.6615,
      "step": 2685
    },
    {
      "epoch": 5.4044265593561365,
      "grad_norm": 0.2628626823425293,
      "learning_rate": 0.00018921018211087636,
      "loss": 0.6152,
      "step": 2686
    },
    {
      "epoch": 5.406438631790745,
      "grad_norm": 0.2713221609592438,
      "learning_rate": 0.00018920615756112284,
      "loss": 0.6456,
      "step": 2687
    },
    {
      "epoch": 5.408450704225352,
      "grad_norm": 0.269517183303833,
      "learning_rate": 0.00018920213301136936,
      "loss": 0.663,
      "step": 2688
    },
    {
      "epoch": 5.41046277665996,
      "grad_norm": 0.26583999395370483,
      "learning_rate": 0.00018919810846161587,
      "loss": 0.6377,
      "step": 2689
    },
    {
      "epoch": 5.4124748490945676,
      "grad_norm": 0.2687365710735321,
      "learning_rate": 0.00018919408391186238,
      "loss": 0.6702,
      "step": 2690
    },
    {
      "epoch": 5.414486921529175,
      "grad_norm": 0.27886515855789185,
      "learning_rate": 0.00018919005936210887,
      "loss": 0.6954,
      "step": 2691
    },
    {
      "epoch": 5.416498993963783,
      "grad_norm": 0.2884126603603363,
      "learning_rate": 0.00018918603481235538,
      "loss": 0.634,
      "step": 2692
    },
    {
      "epoch": 5.41851106639839,
      "grad_norm": 0.2821745276451111,
      "learning_rate": 0.00018918201026260186,
      "loss": 0.6365,
      "step": 2693
    },
    {
      "epoch": 5.420523138832998,
      "grad_norm": 0.2783926725387573,
      "learning_rate": 0.0001891779857128484,
      "loss": 0.6598,
      "step": 2694
    },
    {
      "epoch": 5.422535211267606,
      "grad_norm": 0.2806675136089325,
      "learning_rate": 0.0001891739611630949,
      "loss": 0.653,
      "step": 2695
    },
    {
      "epoch": 5.424547283702213,
      "grad_norm": 0.26065877079963684,
      "learning_rate": 0.0001891699366133414,
      "loss": 0.6214,
      "step": 2696
    },
    {
      "epoch": 5.426559356136821,
      "grad_norm": 0.27348792552948,
      "learning_rate": 0.00018916591206358789,
      "loss": 0.6324,
      "step": 2697
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 0.25545138120651245,
      "learning_rate": 0.0001891618875138344,
      "loss": 0.6338,
      "step": 2698
    },
    {
      "epoch": 5.430583501006036,
      "grad_norm": 0.2598606050014496,
      "learning_rate": 0.0001891578629640809,
      "loss": 0.6547,
      "step": 2699
    },
    {
      "epoch": 5.4325955734406435,
      "grad_norm": 0.2678517699241638,
      "learning_rate": 0.00018915383841432742,
      "loss": 0.6517,
      "step": 2700
    },
    {
      "epoch": 5.434607645875252,
      "grad_norm": 0.26328176259994507,
      "learning_rate": 0.0001891498138645739,
      "loss": 0.6413,
      "step": 2701
    },
    {
      "epoch": 5.436619718309859,
      "grad_norm": 0.27259325981140137,
      "learning_rate": 0.00018914578931482042,
      "loss": 0.6499,
      "step": 2702
    },
    {
      "epoch": 5.438631790744466,
      "grad_norm": 0.26830607652664185,
      "learning_rate": 0.0001891417647650669,
      "loss": 0.6586,
      "step": 2703
    },
    {
      "epoch": 5.440643863179075,
      "grad_norm": 0.2583838701248169,
      "learning_rate": 0.00018913774021531344,
      "loss": 0.6197,
      "step": 2704
    },
    {
      "epoch": 5.442655935613682,
      "grad_norm": 0.2801288962364197,
      "learning_rate": 0.00018913371566555993,
      "loss": 0.6843,
      "step": 2705
    },
    {
      "epoch": 5.44466800804829,
      "grad_norm": 0.26642048358917236,
      "learning_rate": 0.00018912969111580644,
      "loss": 0.6486,
      "step": 2706
    },
    {
      "epoch": 5.446680080482897,
      "grad_norm": 0.27157896757125854,
      "learning_rate": 0.00018912566656605293,
      "loss": 0.6459,
      "step": 2707
    },
    {
      "epoch": 5.448692152917505,
      "grad_norm": 0.28784170746803284,
      "learning_rate": 0.00018912164201629944,
      "loss": 0.6568,
      "step": 2708
    },
    {
      "epoch": 5.450704225352113,
      "grad_norm": 0.2834753096103668,
      "learning_rate": 0.00018911761746654595,
      "loss": 0.6236,
      "step": 2709
    },
    {
      "epoch": 5.45271629778672,
      "grad_norm": 0.26437392830848694,
      "learning_rate": 0.00018911359291679244,
      "loss": 0.6656,
      "step": 2710
    },
    {
      "epoch": 5.454728370221328,
      "grad_norm": 0.26080912351608276,
      "learning_rate": 0.00018910956836703895,
      "loss": 0.644,
      "step": 2711
    },
    {
      "epoch": 5.456740442655936,
      "grad_norm": 0.25966501235961914,
      "learning_rate": 0.00018910554381728546,
      "loss": 0.6224,
      "step": 2712
    },
    {
      "epoch": 5.458752515090543,
      "grad_norm": 0.31449154019355774,
      "learning_rate": 0.00018910151926753195,
      "loss": 0.6916,
      "step": 2713
    },
    {
      "epoch": 5.460764587525151,
      "grad_norm": 0.28966736793518066,
      "learning_rate": 0.00018909749471777846,
      "loss": 0.6155,
      "step": 2714
    },
    {
      "epoch": 5.462776659959759,
      "grad_norm": 0.25880327820777893,
      "learning_rate": 0.00018909347016802497,
      "loss": 0.6839,
      "step": 2715
    },
    {
      "epoch": 5.464788732394366,
      "grad_norm": 0.26511794328689575,
      "learning_rate": 0.00018908944561827145,
      "loss": 0.646,
      "step": 2716
    },
    {
      "epoch": 5.466800804828974,
      "grad_norm": 0.27111557126045227,
      "learning_rate": 0.00018908542106851797,
      "loss": 0.6288,
      "step": 2717
    },
    {
      "epoch": 5.468812877263582,
      "grad_norm": 0.2752942144870758,
      "learning_rate": 0.00018908139651876445,
      "loss": 0.6741,
      "step": 2718
    },
    {
      "epoch": 5.470824949698189,
      "grad_norm": 0.28256189823150635,
      "learning_rate": 0.000189077371969011,
      "loss": 0.6384,
      "step": 2719
    },
    {
      "epoch": 5.472837022132797,
      "grad_norm": 0.2867489755153656,
      "learning_rate": 0.00018907334741925748,
      "loss": 0.714,
      "step": 2720
    },
    {
      "epoch": 5.474849094567404,
      "grad_norm": 0.2762572169303894,
      "learning_rate": 0.000189069322869504,
      "loss": 0.6686,
      "step": 2721
    },
    {
      "epoch": 5.476861167002012,
      "grad_norm": 0.270311564207077,
      "learning_rate": 0.00018906529831975047,
      "loss": 0.6517,
      "step": 2722
    },
    {
      "epoch": 5.47887323943662,
      "grad_norm": 0.26608461141586304,
      "learning_rate": 0.00018906127376999699,
      "loss": 0.6758,
      "step": 2723
    },
    {
      "epoch": 5.480885311871227,
      "grad_norm": 0.2735450565814972,
      "learning_rate": 0.0001890572492202435,
      "loss": 0.6475,
      "step": 2724
    },
    {
      "epoch": 5.482897384305835,
      "grad_norm": 0.2631989121437073,
      "learning_rate": 0.00018905322467049,
      "loss": 0.6186,
      "step": 2725
    },
    {
      "epoch": 5.484909456740443,
      "grad_norm": 0.27581867575645447,
      "learning_rate": 0.0001890492001207365,
      "loss": 0.6749,
      "step": 2726
    },
    {
      "epoch": 5.48692152917505,
      "grad_norm": 0.26795658469200134,
      "learning_rate": 0.000189045175570983,
      "loss": 0.6615,
      "step": 2727
    },
    {
      "epoch": 5.4889336016096575,
      "grad_norm": 0.27431052923202515,
      "learning_rate": 0.0001890411510212295,
      "loss": 0.6527,
      "step": 2728
    },
    {
      "epoch": 5.490945674044266,
      "grad_norm": 0.2736332416534424,
      "learning_rate": 0.00018903712647147603,
      "loss": 0.6579,
      "step": 2729
    },
    {
      "epoch": 5.492957746478873,
      "grad_norm": 0.2692279815673828,
      "learning_rate": 0.00018903310192172252,
      "loss": 0.6636,
      "step": 2730
    },
    {
      "epoch": 5.494969818913481,
      "grad_norm": 0.265611857175827,
      "learning_rate": 0.00018902907737196903,
      "loss": 0.6505,
      "step": 2731
    },
    {
      "epoch": 5.496981891348089,
      "grad_norm": 0.25620168447494507,
      "learning_rate": 0.00018902505282221551,
      "loss": 0.6527,
      "step": 2732
    },
    {
      "epoch": 5.498993963782696,
      "grad_norm": 0.2931315004825592,
      "learning_rate": 0.00018902102827246203,
      "loss": 0.6369,
      "step": 2733
    },
    {
      "epoch": 5.501006036217304,
      "grad_norm": 0.2631898820400238,
      "learning_rate": 0.00018901700372270854,
      "loss": 0.6679,
      "step": 2734
    },
    {
      "epoch": 5.503018108651911,
      "grad_norm": 0.26675930619239807,
      "learning_rate": 0.00018901297917295505,
      "loss": 0.6423,
      "step": 2735
    },
    {
      "epoch": 5.505030181086519,
      "grad_norm": 0.2773146629333496,
      "learning_rate": 0.00018900895462320154,
      "loss": 0.6917,
      "step": 2736
    },
    {
      "epoch": 5.507042253521127,
      "grad_norm": 0.2611977756023407,
      "learning_rate": 0.00018900493007344805,
      "loss": 0.6652,
      "step": 2737
    },
    {
      "epoch": 5.509054325955734,
      "grad_norm": 0.2628273367881775,
      "learning_rate": 0.00018900090552369453,
      "loss": 0.66,
      "step": 2738
    },
    {
      "epoch": 5.5110663983903425,
      "grad_norm": 0.267062783241272,
      "learning_rate": 0.00018899688097394105,
      "loss": 0.6255,
      "step": 2739
    },
    {
      "epoch": 5.51307847082495,
      "grad_norm": 0.27048900723457336,
      "learning_rate": 0.00018899285642418756,
      "loss": 0.6371,
      "step": 2740
    },
    {
      "epoch": 5.515090543259557,
      "grad_norm": 0.2661522626876831,
      "learning_rate": 0.00018898883187443407,
      "loss": 0.6341,
      "step": 2741
    },
    {
      "epoch": 5.517102615694165,
      "grad_norm": 0.2564871311187744,
      "learning_rate": 0.00018898480732468056,
      "loss": 0.6382,
      "step": 2742
    },
    {
      "epoch": 5.519114688128773,
      "grad_norm": 0.2656586468219757,
      "learning_rate": 0.00018898078277492707,
      "loss": 0.6487,
      "step": 2743
    },
    {
      "epoch": 5.52112676056338,
      "grad_norm": 0.26445329189300537,
      "learning_rate": 0.00018897675822517358,
      "loss": 0.6796,
      "step": 2744
    },
    {
      "epoch": 5.523138832997988,
      "grad_norm": 0.26926225423812866,
      "learning_rate": 0.00018897273367542007,
      "loss": 0.6599,
      "step": 2745
    },
    {
      "epoch": 5.525150905432596,
      "grad_norm": 0.2772156894207001,
      "learning_rate": 0.00018896870912566658,
      "loss": 0.6742,
      "step": 2746
    },
    {
      "epoch": 5.527162977867203,
      "grad_norm": 0.28340163826942444,
      "learning_rate": 0.00018896468457591306,
      "loss": 0.6664,
      "step": 2747
    },
    {
      "epoch": 5.529175050301811,
      "grad_norm": 0.2863691449165344,
      "learning_rate": 0.00018896066002615957,
      "loss": 0.6576,
      "step": 2748
    },
    {
      "epoch": 5.531187122736418,
      "grad_norm": 0.26141831278800964,
      "learning_rate": 0.0001889566354764061,
      "loss": 0.665,
      "step": 2749
    },
    {
      "epoch": 5.533199195171026,
      "grad_norm": 0.2639279067516327,
      "learning_rate": 0.0001889526109266526,
      "loss": 0.6644,
      "step": 2750
    },
    {
      "epoch": 5.535211267605634,
      "grad_norm": 0.2584243416786194,
      "learning_rate": 0.00018894858637689908,
      "loss": 0.644,
      "step": 2751
    },
    {
      "epoch": 5.537223340040241,
      "grad_norm": 0.27497196197509766,
      "learning_rate": 0.0001889445618271456,
      "loss": 0.6751,
      "step": 2752
    },
    {
      "epoch": 5.539235412474849,
      "grad_norm": 0.2531399428844452,
      "learning_rate": 0.00018894053727739208,
      "loss": 0.6355,
      "step": 2753
    },
    {
      "epoch": 5.541247484909457,
      "grad_norm": 0.27194830775260925,
      "learning_rate": 0.00018893651272763862,
      "loss": 0.6674,
      "step": 2754
    },
    {
      "epoch": 5.543259557344064,
      "grad_norm": 0.2643773555755615,
      "learning_rate": 0.0001889324881778851,
      "loss": 0.6484,
      "step": 2755
    },
    {
      "epoch": 5.545271629778672,
      "grad_norm": 0.258373498916626,
      "learning_rate": 0.00018892846362813162,
      "loss": 0.6537,
      "step": 2756
    },
    {
      "epoch": 5.54728370221328,
      "grad_norm": 0.27687302231788635,
      "learning_rate": 0.0001889244390783781,
      "loss": 0.6804,
      "step": 2757
    },
    {
      "epoch": 5.549295774647887,
      "grad_norm": 0.2621976435184479,
      "learning_rate": 0.00018892041452862462,
      "loss": 0.6198,
      "step": 2758
    },
    {
      "epoch": 5.551307847082495,
      "grad_norm": 0.2726202607154846,
      "learning_rate": 0.00018891638997887113,
      "loss": 0.6641,
      "step": 2759
    },
    {
      "epoch": 5.553319919517103,
      "grad_norm": 0.2744455337524414,
      "learning_rate": 0.00018891236542911764,
      "loss": 0.6853,
      "step": 2760
    },
    {
      "epoch": 5.55533199195171,
      "grad_norm": 0.2628873288631439,
      "learning_rate": 0.00018890834087936413,
      "loss": 0.6763,
      "step": 2761
    },
    {
      "epoch": 5.557344064386318,
      "grad_norm": 0.27842238545417786,
      "learning_rate": 0.00018890431632961064,
      "loss": 0.6398,
      "step": 2762
    },
    {
      "epoch": 5.559356136820925,
      "grad_norm": 0.2594533860683441,
      "learning_rate": 0.00018890029177985712,
      "loss": 0.6131,
      "step": 2763
    },
    {
      "epoch": 5.561368209255534,
      "grad_norm": 0.26301419734954834,
      "learning_rate": 0.00018889626723010366,
      "loss": 0.676,
      "step": 2764
    },
    {
      "epoch": 5.563380281690141,
      "grad_norm": 0.24859778583049774,
      "learning_rate": 0.00018889224268035015,
      "loss": 0.6202,
      "step": 2765
    },
    {
      "epoch": 5.565392354124748,
      "grad_norm": 0.2752862274646759,
      "learning_rate": 0.00018888821813059666,
      "loss": 0.6717,
      "step": 2766
    },
    {
      "epoch": 5.5674044265593565,
      "grad_norm": 0.2822742760181427,
      "learning_rate": 0.00018888419358084314,
      "loss": 0.6516,
      "step": 2767
    },
    {
      "epoch": 5.569416498993964,
      "grad_norm": 0.2789616882801056,
      "learning_rate": 0.00018888016903108966,
      "loss": 0.6254,
      "step": 2768
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 0.2635176479816437,
      "learning_rate": 0.00018887614448133617,
      "loss": 0.6439,
      "step": 2769
    },
    {
      "epoch": 5.573440643863179,
      "grad_norm": 0.27919429540634155,
      "learning_rate": 0.00018887211993158268,
      "loss": 0.6674,
      "step": 2770
    },
    {
      "epoch": 5.575452716297787,
      "grad_norm": 0.26913100481033325,
      "learning_rate": 0.00018886809538182917,
      "loss": 0.641,
      "step": 2771
    },
    {
      "epoch": 5.577464788732394,
      "grad_norm": 0.2730187475681305,
      "learning_rate": 0.00018886407083207568,
      "loss": 0.6587,
      "step": 2772
    },
    {
      "epoch": 5.579476861167002,
      "grad_norm": 0.2682052254676819,
      "learning_rate": 0.00018886004628232216,
      "loss": 0.6428,
      "step": 2773
    },
    {
      "epoch": 5.58148893360161,
      "grad_norm": 0.26404038071632385,
      "learning_rate": 0.00018885602173256868,
      "loss": 0.6202,
      "step": 2774
    },
    {
      "epoch": 5.583501006036217,
      "grad_norm": 0.28030136227607727,
      "learning_rate": 0.0001888519971828152,
      "loss": 0.6698,
      "step": 2775
    },
    {
      "epoch": 5.585513078470825,
      "grad_norm": 0.2785491645336151,
      "learning_rate": 0.0001888479726330617,
      "loss": 0.6328,
      "step": 2776
    },
    {
      "epoch": 5.5875251509054324,
      "grad_norm": 0.28747662901878357,
      "learning_rate": 0.00018884394808330818,
      "loss": 0.6685,
      "step": 2777
    },
    {
      "epoch": 5.58953722334004,
      "grad_norm": 0.2553117871284485,
      "learning_rate": 0.0001888399235335547,
      "loss": 0.6401,
      "step": 2778
    },
    {
      "epoch": 5.591549295774648,
      "grad_norm": 0.2688484787940979,
      "learning_rate": 0.0001888358989838012,
      "loss": 0.6457,
      "step": 2779
    },
    {
      "epoch": 5.593561368209255,
      "grad_norm": 0.2704833149909973,
      "learning_rate": 0.0001888318744340477,
      "loss": 0.6628,
      "step": 2780
    },
    {
      "epoch": 5.5955734406438635,
      "grad_norm": 0.25818198919296265,
      "learning_rate": 0.0001888278498842942,
      "loss": 0.6436,
      "step": 2781
    },
    {
      "epoch": 5.597585513078471,
      "grad_norm": 0.2723888158798218,
      "learning_rate": 0.0001888238253345407,
      "loss": 0.6863,
      "step": 2782
    },
    {
      "epoch": 5.599597585513078,
      "grad_norm": 0.26530691981315613,
      "learning_rate": 0.0001888198007847872,
      "loss": 0.6391,
      "step": 2783
    },
    {
      "epoch": 5.601609657947686,
      "grad_norm": 0.27416491508483887,
      "learning_rate": 0.00018881577623503372,
      "loss": 0.6796,
      "step": 2784
    },
    {
      "epoch": 5.603621730382294,
      "grad_norm": 0.26089543104171753,
      "learning_rate": 0.00018881175168528023,
      "loss": 0.626,
      "step": 2785
    },
    {
      "epoch": 5.605633802816901,
      "grad_norm": 0.26261457800865173,
      "learning_rate": 0.00018880772713552671,
      "loss": 0.6327,
      "step": 2786
    },
    {
      "epoch": 5.607645875251509,
      "grad_norm": 0.2721834182739258,
      "learning_rate": 0.00018880370258577323,
      "loss": 0.5941,
      "step": 2787
    },
    {
      "epoch": 5.609657947686117,
      "grad_norm": 0.2742649018764496,
      "learning_rate": 0.0001887996780360197,
      "loss": 0.671,
      "step": 2788
    },
    {
      "epoch": 5.611670020120725,
      "grad_norm": 0.2640358805656433,
      "learning_rate": 0.00018879565348626625,
      "loss": 0.6018,
      "step": 2789
    },
    {
      "epoch": 5.613682092555332,
      "grad_norm": 0.2550469934940338,
      "learning_rate": 0.00018879162893651274,
      "loss": 0.6328,
      "step": 2790
    },
    {
      "epoch": 5.6156941649899395,
      "grad_norm": 0.29324615001678467,
      "learning_rate": 0.00018878760438675925,
      "loss": 0.6253,
      "step": 2791
    },
    {
      "epoch": 5.617706237424548,
      "grad_norm": 0.2769823968410492,
      "learning_rate": 0.00018878357983700573,
      "loss": 0.6858,
      "step": 2792
    },
    {
      "epoch": 5.619718309859155,
      "grad_norm": 0.2592722177505493,
      "learning_rate": 0.00018877955528725224,
      "loss": 0.6617,
      "step": 2793
    },
    {
      "epoch": 5.621730382293762,
      "grad_norm": 0.26770833134651184,
      "learning_rate": 0.00018877553073749873,
      "loss": 0.634,
      "step": 2794
    },
    {
      "epoch": 5.6237424547283705,
      "grad_norm": 0.27550092339515686,
      "learning_rate": 0.00018877150618774527,
      "loss": 0.6822,
      "step": 2795
    },
    {
      "epoch": 5.625754527162978,
      "grad_norm": 0.27369987964630127,
      "learning_rate": 0.00018876748163799175,
      "loss": 0.6659,
      "step": 2796
    },
    {
      "epoch": 5.627766599597585,
      "grad_norm": 0.27061599493026733,
      "learning_rate": 0.00018876345708823827,
      "loss": 0.6628,
      "step": 2797
    },
    {
      "epoch": 5.629778672032193,
      "grad_norm": 0.2639900743961334,
      "learning_rate": 0.00018875943253848475,
      "loss": 0.6475,
      "step": 2798
    },
    {
      "epoch": 5.631790744466801,
      "grad_norm": 0.26882869005203247,
      "learning_rate": 0.00018875540798873126,
      "loss": 0.653,
      "step": 2799
    },
    {
      "epoch": 5.633802816901408,
      "grad_norm": 0.2674033045768738,
      "learning_rate": 0.00018875138343897778,
      "loss": 0.6551,
      "step": 2800
    },
    {
      "epoch": 5.635814889336016,
      "grad_norm": 0.26604846119880676,
      "learning_rate": 0.0001887473588892243,
      "loss": 0.6383,
      "step": 2801
    },
    {
      "epoch": 5.637826961770624,
      "grad_norm": 0.27913445234298706,
      "learning_rate": 0.00018874333433947077,
      "loss": 0.6593,
      "step": 2802
    },
    {
      "epoch": 5.639839034205231,
      "grad_norm": 0.2760663628578186,
      "learning_rate": 0.00018873930978971729,
      "loss": 0.696,
      "step": 2803
    },
    {
      "epoch": 5.641851106639839,
      "grad_norm": 0.2749937176704407,
      "learning_rate": 0.00018873528523996377,
      "loss": 0.621,
      "step": 2804
    },
    {
      "epoch": 5.6438631790744465,
      "grad_norm": 0.27938607335090637,
      "learning_rate": 0.0001887312606902103,
      "loss": 0.6681,
      "step": 2805
    },
    {
      "epoch": 5.645875251509055,
      "grad_norm": 0.2798641622066498,
      "learning_rate": 0.0001887272361404568,
      "loss": 0.6709,
      "step": 2806
    },
    {
      "epoch": 5.647887323943662,
      "grad_norm": 0.2682272493839264,
      "learning_rate": 0.0001887232115907033,
      "loss": 0.665,
      "step": 2807
    },
    {
      "epoch": 5.649899396378269,
      "grad_norm": 0.2585066854953766,
      "learning_rate": 0.0001887191870409498,
      "loss": 0.6507,
      "step": 2808
    },
    {
      "epoch": 5.6519114688128775,
      "grad_norm": 0.27084627747535706,
      "learning_rate": 0.0001887151624911963,
      "loss": 0.7003,
      "step": 2809
    },
    {
      "epoch": 5.653923541247485,
      "grad_norm": 0.27274516224861145,
      "learning_rate": 0.00018871113794144282,
      "loss": 0.6722,
      "step": 2810
    },
    {
      "epoch": 5.655935613682092,
      "grad_norm": 0.2613006830215454,
      "learning_rate": 0.00018870711339168933,
      "loss": 0.6213,
      "step": 2811
    },
    {
      "epoch": 5.6579476861167,
      "grad_norm": 0.2580791413784027,
      "learning_rate": 0.00018870308884193581,
      "loss": 0.6296,
      "step": 2812
    },
    {
      "epoch": 5.659959758551308,
      "grad_norm": 0.26725393533706665,
      "learning_rate": 0.00018869906429218233,
      "loss": 0.6648,
      "step": 2813
    },
    {
      "epoch": 5.661971830985916,
      "grad_norm": 0.2618781328201294,
      "learning_rate": 0.0001886950397424288,
      "loss": 0.6606,
      "step": 2814
    },
    {
      "epoch": 5.663983903420523,
      "grad_norm": 0.26013559103012085,
      "learning_rate": 0.00018869101519267532,
      "loss": 0.6589,
      "step": 2815
    },
    {
      "epoch": 5.665995975855131,
      "grad_norm": 0.2772417962551117,
      "learning_rate": 0.00018868699064292184,
      "loss": 0.63,
      "step": 2816
    },
    {
      "epoch": 5.668008048289739,
      "grad_norm": 0.2702140808105469,
      "learning_rate": 0.00018868296609316832,
      "loss": 0.6629,
      "step": 2817
    },
    {
      "epoch": 5.670020120724346,
      "grad_norm": 0.26499006152153015,
      "learning_rate": 0.00018867894154341483,
      "loss": 0.665,
      "step": 2818
    },
    {
      "epoch": 5.6720321931589535,
      "grad_norm": 0.28096821904182434,
      "learning_rate": 0.00018867491699366135,
      "loss": 0.6313,
      "step": 2819
    },
    {
      "epoch": 5.674044265593562,
      "grad_norm": 0.2830004394054413,
      "learning_rate": 0.00018867089244390786,
      "loss": 0.6344,
      "step": 2820
    },
    {
      "epoch": 5.676056338028169,
      "grad_norm": 0.26814645528793335,
      "learning_rate": 0.00018866686789415434,
      "loss": 0.6757,
      "step": 2821
    },
    {
      "epoch": 5.678068410462776,
      "grad_norm": 0.2829509675502777,
      "learning_rate": 0.00018866284334440086,
      "loss": 0.6315,
      "step": 2822
    },
    {
      "epoch": 5.6800804828973845,
      "grad_norm": 0.27099674940109253,
      "learning_rate": 0.00018865881879464734,
      "loss": 0.6134,
      "step": 2823
    },
    {
      "epoch": 5.682092555331992,
      "grad_norm": 0.25141555070877075,
      "learning_rate": 0.00018865479424489385,
      "loss": 0.639,
      "step": 2824
    },
    {
      "epoch": 5.684104627766599,
      "grad_norm": 0.26964282989501953,
      "learning_rate": 0.00018865076969514036,
      "loss": 0.6359,
      "step": 2825
    },
    {
      "epoch": 5.686116700201207,
      "grad_norm": 0.27241379022598267,
      "learning_rate": 0.00018864674514538688,
      "loss": 0.6958,
      "step": 2826
    },
    {
      "epoch": 5.688128772635815,
      "grad_norm": 0.2715851068496704,
      "learning_rate": 0.00018864272059563336,
      "loss": 0.6645,
      "step": 2827
    },
    {
      "epoch": 5.690140845070422,
      "grad_norm": 0.2772862911224365,
      "learning_rate": 0.00018863869604587987,
      "loss": 0.6439,
      "step": 2828
    },
    {
      "epoch": 5.69215291750503,
      "grad_norm": 0.2720412015914917,
      "learning_rate": 0.00018863467149612636,
      "loss": 0.6043,
      "step": 2829
    },
    {
      "epoch": 5.694164989939638,
      "grad_norm": 0.2560216188430786,
      "learning_rate": 0.0001886306469463729,
      "loss": 0.669,
      "step": 2830
    },
    {
      "epoch": 5.696177062374246,
      "grad_norm": 0.26571276783943176,
      "learning_rate": 0.00018862662239661938,
      "loss": 0.6729,
      "step": 2831
    },
    {
      "epoch": 5.698189134808853,
      "grad_norm": 0.27329155802726746,
      "learning_rate": 0.0001886225978468659,
      "loss": 0.6677,
      "step": 2832
    },
    {
      "epoch": 5.7002012072434605,
      "grad_norm": 0.27189207077026367,
      "learning_rate": 0.00018861857329711238,
      "loss": 0.6595,
      "step": 2833
    },
    {
      "epoch": 5.702213279678069,
      "grad_norm": 0.25571945309638977,
      "learning_rate": 0.0001886145487473589,
      "loss": 0.6324,
      "step": 2834
    },
    {
      "epoch": 5.704225352112676,
      "grad_norm": 0.25738489627838135,
      "learning_rate": 0.0001886105241976054,
      "loss": 0.6385,
      "step": 2835
    },
    {
      "epoch": 5.706237424547283,
      "grad_norm": 0.2661890685558319,
      "learning_rate": 0.00018860649964785192,
      "loss": 0.6505,
      "step": 2836
    },
    {
      "epoch": 5.7082494969818915,
      "grad_norm": 0.26408982276916504,
      "learning_rate": 0.0001886024750980984,
      "loss": 0.6247,
      "step": 2837
    },
    {
      "epoch": 5.710261569416499,
      "grad_norm": 0.2564047873020172,
      "learning_rate": 0.00018859845054834492,
      "loss": 0.6578,
      "step": 2838
    },
    {
      "epoch": 5.712273641851107,
      "grad_norm": 0.2650635838508606,
      "learning_rate": 0.0001885944259985914,
      "loss": 0.6625,
      "step": 2839
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 0.27679958939552307,
      "learning_rate": 0.00018859040144883794,
      "loss": 0.6747,
      "step": 2840
    },
    {
      "epoch": 5.716297786720322,
      "grad_norm": 0.25724875926971436,
      "learning_rate": 0.00018858637689908442,
      "loss": 0.6564,
      "step": 2841
    },
    {
      "epoch": 5.71830985915493,
      "grad_norm": 0.2466447800397873,
      "learning_rate": 0.00018858235234933094,
      "loss": 0.6028,
      "step": 2842
    },
    {
      "epoch": 5.720321931589537,
      "grad_norm": 0.26937243342399597,
      "learning_rate": 0.00018857832779957742,
      "loss": 0.6449,
      "step": 2843
    },
    {
      "epoch": 5.722334004024145,
      "grad_norm": 0.26336976885795593,
      "learning_rate": 0.00018857430324982393,
      "loss": 0.6237,
      "step": 2844
    },
    {
      "epoch": 5.724346076458753,
      "grad_norm": 0.26685038208961487,
      "learning_rate": 0.00018857027870007045,
      "loss": 0.6345,
      "step": 2845
    },
    {
      "epoch": 5.72635814889336,
      "grad_norm": 0.28055569529533386,
      "learning_rate": 0.00018856625415031696,
      "loss": 0.6367,
      "step": 2846
    },
    {
      "epoch": 5.7283702213279675,
      "grad_norm": 0.2807612121105194,
      "learning_rate": 0.00018856222960056344,
      "loss": 0.6613,
      "step": 2847
    },
    {
      "epoch": 5.730382293762576,
      "grad_norm": 0.2699374854564667,
      "learning_rate": 0.00018855820505080996,
      "loss": 0.6641,
      "step": 2848
    },
    {
      "epoch": 5.732394366197183,
      "grad_norm": 0.26705077290534973,
      "learning_rate": 0.00018855418050105644,
      "loss": 0.6639,
      "step": 2849
    },
    {
      "epoch": 5.73440643863179,
      "grad_norm": 0.2536163628101349,
      "learning_rate": 0.00018855015595130295,
      "loss": 0.6326,
      "step": 2850
    },
    {
      "epoch": 5.7364185110663986,
      "grad_norm": 0.27404290437698364,
      "learning_rate": 0.00018854613140154947,
      "loss": 0.6589,
      "step": 2851
    },
    {
      "epoch": 5.738430583501006,
      "grad_norm": 0.26526156067848206,
      "learning_rate": 0.00018854210685179595,
      "loss": 0.6725,
      "step": 2852
    },
    {
      "epoch": 5.740442655935613,
      "grad_norm": 0.2689453065395355,
      "learning_rate": 0.00018853808230204246,
      "loss": 0.6056,
      "step": 2853
    },
    {
      "epoch": 5.742454728370221,
      "grad_norm": 0.2699264585971832,
      "learning_rate": 0.00018853405775228898,
      "loss": 0.655,
      "step": 2854
    },
    {
      "epoch": 5.744466800804829,
      "grad_norm": 0.2838594615459442,
      "learning_rate": 0.0001885300332025355,
      "loss": 0.6564,
      "step": 2855
    },
    {
      "epoch": 5.746478873239437,
      "grad_norm": 0.2572823762893677,
      "learning_rate": 0.00018852600865278197,
      "loss": 0.6577,
      "step": 2856
    },
    {
      "epoch": 5.748490945674044,
      "grad_norm": 0.27696868777275085,
      "learning_rate": 0.00018852198410302848,
      "loss": 0.6452,
      "step": 2857
    },
    {
      "epoch": 5.750503018108652,
      "grad_norm": 0.26944032311439514,
      "learning_rate": 0.00018851795955327497,
      "loss": 0.6371,
      "step": 2858
    },
    {
      "epoch": 5.75251509054326,
      "grad_norm": 0.2601065933704376,
      "learning_rate": 0.00018851393500352148,
      "loss": 0.6334,
      "step": 2859
    },
    {
      "epoch": 5.754527162977867,
      "grad_norm": 0.2757801413536072,
      "learning_rate": 0.000188509910453768,
      "loss": 0.6494,
      "step": 2860
    },
    {
      "epoch": 5.7565392354124745,
      "grad_norm": 0.2686181962490082,
      "learning_rate": 0.0001885058859040145,
      "loss": 0.6805,
      "step": 2861
    },
    {
      "epoch": 5.758551307847083,
      "grad_norm": 0.258156418800354,
      "learning_rate": 0.000188501861354261,
      "loss": 0.6213,
      "step": 2862
    },
    {
      "epoch": 5.76056338028169,
      "grad_norm": 0.26945993304252625,
      "learning_rate": 0.0001884978368045075,
      "loss": 0.6117,
      "step": 2863
    },
    {
      "epoch": 5.762575452716298,
      "grad_norm": 0.27253344655036926,
      "learning_rate": 0.000188493812254754,
      "loss": 0.6925,
      "step": 2864
    },
    {
      "epoch": 5.7645875251509056,
      "grad_norm": 0.2678373456001282,
      "learning_rate": 0.00018848978770500053,
      "loss": 0.6345,
      "step": 2865
    },
    {
      "epoch": 5.766599597585513,
      "grad_norm": 0.260393351316452,
      "learning_rate": 0.000188485763155247,
      "loss": 0.663,
      "step": 2866
    },
    {
      "epoch": 5.768611670020121,
      "grad_norm": 0.2613244652748108,
      "learning_rate": 0.00018848173860549353,
      "loss": 0.6354,
      "step": 2867
    },
    {
      "epoch": 5.770623742454728,
      "grad_norm": 0.2676010727882385,
      "learning_rate": 0.00018847771405574,
      "loss": 0.6657,
      "step": 2868
    },
    {
      "epoch": 5.772635814889336,
      "grad_norm": 0.2721961438655853,
      "learning_rate": 0.00018847368950598652,
      "loss": 0.6274,
      "step": 2869
    },
    {
      "epoch": 5.774647887323944,
      "grad_norm": 0.2626476585865021,
      "learning_rate": 0.00018846966495623304,
      "loss": 0.7082,
      "step": 2870
    },
    {
      "epoch": 5.776659959758551,
      "grad_norm": 0.2658696174621582,
      "learning_rate": 0.00018846564040647955,
      "loss": 0.6925,
      "step": 2871
    },
    {
      "epoch": 5.778672032193159,
      "grad_norm": 0.26762840151786804,
      "learning_rate": 0.00018846161585672603,
      "loss": 0.6537,
      "step": 2872
    },
    {
      "epoch": 5.780684104627767,
      "grad_norm": 0.2710340917110443,
      "learning_rate": 0.00018845759130697254,
      "loss": 0.6774,
      "step": 2873
    },
    {
      "epoch": 5.782696177062374,
      "grad_norm": 0.25305864214897156,
      "learning_rate": 0.00018845356675721903,
      "loss": 0.5991,
      "step": 2874
    },
    {
      "epoch": 5.7847082494969815,
      "grad_norm": 0.2615949809551239,
      "learning_rate": 0.00018844954220746557,
      "loss": 0.6688,
      "step": 2875
    },
    {
      "epoch": 5.78672032193159,
      "grad_norm": 0.2527913451194763,
      "learning_rate": 0.00018844551765771205,
      "loss": 0.6014,
      "step": 2876
    },
    {
      "epoch": 5.788732394366197,
      "grad_norm": 0.27358436584472656,
      "learning_rate": 0.00018844149310795857,
      "loss": 0.6676,
      "step": 2877
    },
    {
      "epoch": 5.790744466800804,
      "grad_norm": 0.2748027443885803,
      "learning_rate": 0.00018843746855820505,
      "loss": 0.681,
      "step": 2878
    },
    {
      "epoch": 5.792756539235413,
      "grad_norm": 0.2841557264328003,
      "learning_rate": 0.00018843344400845156,
      "loss": 0.7043,
      "step": 2879
    },
    {
      "epoch": 5.79476861167002,
      "grad_norm": 0.25285449624061584,
      "learning_rate": 0.00018842941945869808,
      "loss": 0.606,
      "step": 2880
    },
    {
      "epoch": 5.796780684104628,
      "grad_norm": 0.25439029932022095,
      "learning_rate": 0.0001884253949089446,
      "loss": 0.6571,
      "step": 2881
    },
    {
      "epoch": 5.798792756539235,
      "grad_norm": 0.266217976808548,
      "learning_rate": 0.00018842137035919107,
      "loss": 0.653,
      "step": 2882
    },
    {
      "epoch": 5.800804828973843,
      "grad_norm": 0.2731272578239441,
      "learning_rate": 0.00018841734580943759,
      "loss": 0.6487,
      "step": 2883
    },
    {
      "epoch": 5.802816901408451,
      "grad_norm": 0.2642285227775574,
      "learning_rate": 0.00018841332125968407,
      "loss": 0.6273,
      "step": 2884
    },
    {
      "epoch": 5.804828973843058,
      "grad_norm": 0.2697964012622833,
      "learning_rate": 0.00018840929670993058,
      "loss": 0.6393,
      "step": 2885
    },
    {
      "epoch": 5.806841046277666,
      "grad_norm": 0.2574930191040039,
      "learning_rate": 0.0001884052721601771,
      "loss": 0.6392,
      "step": 2886
    },
    {
      "epoch": 5.808853118712274,
      "grad_norm": 0.2478419691324234,
      "learning_rate": 0.00018840124761042358,
      "loss": 0.6133,
      "step": 2887
    },
    {
      "epoch": 5.810865191146881,
      "grad_norm": 0.2638835608959198,
      "learning_rate": 0.0001883972230606701,
      "loss": 0.6302,
      "step": 2888
    },
    {
      "epoch": 5.812877263581489,
      "grad_norm": 0.27635857462882996,
      "learning_rate": 0.00018839319851091658,
      "loss": 0.6395,
      "step": 2889
    },
    {
      "epoch": 5.814889336016097,
      "grad_norm": 0.27652400732040405,
      "learning_rate": 0.00018838917396116312,
      "loss": 0.6588,
      "step": 2890
    },
    {
      "epoch": 5.816901408450704,
      "grad_norm": 0.26875337958335876,
      "learning_rate": 0.0001883851494114096,
      "loss": 0.6744,
      "step": 2891
    },
    {
      "epoch": 5.818913480885312,
      "grad_norm": 0.2806858718395233,
      "learning_rate": 0.00018838112486165611,
      "loss": 0.6474,
      "step": 2892
    },
    {
      "epoch": 5.82092555331992,
      "grad_norm": 0.2631184458732605,
      "learning_rate": 0.0001883771003119026,
      "loss": 0.633,
      "step": 2893
    },
    {
      "epoch": 5.822937625754527,
      "grad_norm": 0.2582947909832001,
      "learning_rate": 0.0001883730757621491,
      "loss": 0.6743,
      "step": 2894
    },
    {
      "epoch": 5.824949698189135,
      "grad_norm": 0.263396680355072,
      "learning_rate": 0.00018836905121239562,
      "loss": 0.6808,
      "step": 2895
    },
    {
      "epoch": 5.826961770623742,
      "grad_norm": 0.26750901341438293,
      "learning_rate": 0.00018836502666264214,
      "loss": 0.6589,
      "step": 2896
    },
    {
      "epoch": 5.82897384305835,
      "grad_norm": 0.28406763076782227,
      "learning_rate": 0.00018836100211288862,
      "loss": 0.658,
      "step": 2897
    },
    {
      "epoch": 5.830985915492958,
      "grad_norm": 0.2582932412624359,
      "learning_rate": 0.00018835697756313513,
      "loss": 0.6419,
      "step": 2898
    },
    {
      "epoch": 5.832997987927565,
      "grad_norm": 0.27587637305259705,
      "learning_rate": 0.00018835295301338162,
      "loss": 0.6677,
      "step": 2899
    },
    {
      "epoch": 5.835010060362173,
      "grad_norm": 0.267081081867218,
      "learning_rate": 0.00018834892846362816,
      "loss": 0.667,
      "step": 2900
    },
    {
      "epoch": 5.837022132796781,
      "grad_norm": 0.2750610113143921,
      "learning_rate": 0.00018834490391387464,
      "loss": 0.7325,
      "step": 2901
    },
    {
      "epoch": 5.839034205231388,
      "grad_norm": 0.2451784312725067,
      "learning_rate": 0.00018834087936412115,
      "loss": 0.6032,
      "step": 2902
    },
    {
      "epoch": 5.8410462776659955,
      "grad_norm": 0.2691943645477295,
      "learning_rate": 0.00018833685481436764,
      "loss": 0.631,
      "step": 2903
    },
    {
      "epoch": 5.843058350100604,
      "grad_norm": 0.2834869623184204,
      "learning_rate": 0.00018833283026461415,
      "loss": 0.626,
      "step": 2904
    },
    {
      "epoch": 5.845070422535211,
      "grad_norm": 0.2737352252006531,
      "learning_rate": 0.00018832880571486066,
      "loss": 0.7062,
      "step": 2905
    },
    {
      "epoch": 5.847082494969819,
      "grad_norm": 0.2608701288700104,
      "learning_rate": 0.00018832478116510718,
      "loss": 0.6667,
      "step": 2906
    },
    {
      "epoch": 5.849094567404427,
      "grad_norm": 0.26675471663475037,
      "learning_rate": 0.00018832075661535366,
      "loss": 0.6474,
      "step": 2907
    },
    {
      "epoch": 5.851106639839034,
      "grad_norm": 0.3003569543361664,
      "learning_rate": 0.00018831673206560017,
      "loss": 0.6453,
      "step": 2908
    },
    {
      "epoch": 5.853118712273642,
      "grad_norm": 0.2572561502456665,
      "learning_rate": 0.00018831270751584666,
      "loss": 0.6349,
      "step": 2909
    },
    {
      "epoch": 5.855130784708249,
      "grad_norm": 0.2792206108570099,
      "learning_rate": 0.0001883086829660932,
      "loss": 0.6732,
      "step": 2910
    },
    {
      "epoch": 5.857142857142857,
      "grad_norm": 0.2680540382862091,
      "learning_rate": 0.00018830465841633968,
      "loss": 0.6186,
      "step": 2911
    },
    {
      "epoch": 5.859154929577465,
      "grad_norm": 0.24863003194332123,
      "learning_rate": 0.0001883006338665862,
      "loss": 0.6138,
      "step": 2912
    },
    {
      "epoch": 5.861167002012072,
      "grad_norm": 0.2626838982105255,
      "learning_rate": 0.00018829660931683268,
      "loss": 0.6296,
      "step": 2913
    },
    {
      "epoch": 5.8631790744466805,
      "grad_norm": 0.25601425766944885,
      "learning_rate": 0.0001882925847670792,
      "loss": 0.5939,
      "step": 2914
    },
    {
      "epoch": 5.865191146881288,
      "grad_norm": 0.2820679247379303,
      "learning_rate": 0.0001882885602173257,
      "loss": 0.6348,
      "step": 2915
    },
    {
      "epoch": 5.867203219315895,
      "grad_norm": 0.2736288905143738,
      "learning_rate": 0.0001882845356675722,
      "loss": 0.6543,
      "step": 2916
    },
    {
      "epoch": 5.869215291750503,
      "grad_norm": 0.25541412830352783,
      "learning_rate": 0.0001882805111178187,
      "loss": 0.6297,
      "step": 2917
    },
    {
      "epoch": 5.871227364185111,
      "grad_norm": 0.2707071006298065,
      "learning_rate": 0.00018827648656806521,
      "loss": 0.6941,
      "step": 2918
    },
    {
      "epoch": 5.873239436619718,
      "grad_norm": 0.2609599232673645,
      "learning_rate": 0.0001882724620183117,
      "loss": 0.6445,
      "step": 2919
    },
    {
      "epoch": 5.875251509054326,
      "grad_norm": 0.2750977575778961,
      "learning_rate": 0.0001882684374685582,
      "loss": 0.6686,
      "step": 2920
    },
    {
      "epoch": 5.877263581488934,
      "grad_norm": 0.2506524324417114,
      "learning_rate": 0.00018826441291880472,
      "loss": 0.6312,
      "step": 2921
    },
    {
      "epoch": 5.879275653923541,
      "grad_norm": 0.26350438594818115,
      "learning_rate": 0.0001882603883690512,
      "loss": 0.6147,
      "step": 2922
    },
    {
      "epoch": 5.881287726358149,
      "grad_norm": 0.2851364314556122,
      "learning_rate": 0.00018825636381929772,
      "loss": 0.6709,
      "step": 2923
    },
    {
      "epoch": 5.883299798792756,
      "grad_norm": 0.260095477104187,
      "learning_rate": 0.0001882523392695442,
      "loss": 0.6718,
      "step": 2924
    },
    {
      "epoch": 5.885311871227364,
      "grad_norm": 0.25214099884033203,
      "learning_rate": 0.00018824831471979075,
      "loss": 0.6326,
      "step": 2925
    },
    {
      "epoch": 5.887323943661972,
      "grad_norm": 0.26053810119628906,
      "learning_rate": 0.00018824429017003723,
      "loss": 0.6525,
      "step": 2926
    },
    {
      "epoch": 5.889336016096579,
      "grad_norm": 0.2611340582370758,
      "learning_rate": 0.00018824026562028374,
      "loss": 0.6156,
      "step": 2927
    },
    {
      "epoch": 5.891348088531187,
      "grad_norm": 0.2666439712047577,
      "learning_rate": 0.00018823624107053023,
      "loss": 0.6228,
      "step": 2928
    },
    {
      "epoch": 5.893360160965795,
      "grad_norm": 0.25237494707107544,
      "learning_rate": 0.00018823221652077674,
      "loss": 0.634,
      "step": 2929
    },
    {
      "epoch": 5.895372233400402,
      "grad_norm": 0.26477423310279846,
      "learning_rate": 0.00018822819197102325,
      "loss": 0.6391,
      "step": 2930
    },
    {
      "epoch": 5.89738430583501,
      "grad_norm": 0.258248507976532,
      "learning_rate": 0.00018822416742126977,
      "loss": 0.649,
      "step": 2931
    },
    {
      "epoch": 5.899396378269618,
      "grad_norm": 0.2541390061378479,
      "learning_rate": 0.00018822014287151625,
      "loss": 0.6331,
      "step": 2932
    },
    {
      "epoch": 5.901408450704225,
      "grad_norm": 0.2571428716182709,
      "learning_rate": 0.00018821611832176276,
      "loss": 0.7065,
      "step": 2933
    },
    {
      "epoch": 5.903420523138833,
      "grad_norm": 0.2615678012371063,
      "learning_rate": 0.00018821209377200925,
      "loss": 0.6607,
      "step": 2934
    },
    {
      "epoch": 5.905432595573441,
      "grad_norm": 0.2758655846118927,
      "learning_rate": 0.0001882080692222558,
      "loss": 0.6544,
      "step": 2935
    },
    {
      "epoch": 5.907444668008048,
      "grad_norm": 0.26053014397621155,
      "learning_rate": 0.00018820404467250227,
      "loss": 0.6692,
      "step": 2936
    },
    {
      "epoch": 5.909456740442656,
      "grad_norm": 0.2544717490673065,
      "learning_rate": 0.00018820002012274878,
      "loss": 0.6351,
      "step": 2937
    },
    {
      "epoch": 5.9114688128772634,
      "grad_norm": 0.26680129766464233,
      "learning_rate": 0.00018819599557299527,
      "loss": 0.6069,
      "step": 2938
    },
    {
      "epoch": 5.913480885311872,
      "grad_norm": 0.2520484924316406,
      "learning_rate": 0.00018819197102324178,
      "loss": 0.6463,
      "step": 2939
    },
    {
      "epoch": 5.915492957746479,
      "grad_norm": 0.28410688042640686,
      "learning_rate": 0.0001881879464734883,
      "loss": 0.6501,
      "step": 2940
    },
    {
      "epoch": 5.917505030181086,
      "grad_norm": 0.27441105246543884,
      "learning_rate": 0.0001881839219237348,
      "loss": 0.6453,
      "step": 2941
    },
    {
      "epoch": 5.9195171026156945,
      "grad_norm": 0.2567863464355469,
      "learning_rate": 0.0001881798973739813,
      "loss": 0.6405,
      "step": 2942
    },
    {
      "epoch": 5.921529175050302,
      "grad_norm": 0.2672419250011444,
      "learning_rate": 0.0001881758728242278,
      "loss": 0.693,
      "step": 2943
    },
    {
      "epoch": 5.923541247484909,
      "grad_norm": 0.25378814339637756,
      "learning_rate": 0.0001881718482744743,
      "loss": 0.6172,
      "step": 2944
    },
    {
      "epoch": 5.925553319919517,
      "grad_norm": 0.2695392370223999,
      "learning_rate": 0.00018816782372472083,
      "loss": 0.6674,
      "step": 2945
    },
    {
      "epoch": 5.927565392354125,
      "grad_norm": 0.25854045152664185,
      "learning_rate": 0.0001881637991749673,
      "loss": 0.6095,
      "step": 2946
    },
    {
      "epoch": 5.929577464788732,
      "grad_norm": 0.2527206242084503,
      "learning_rate": 0.00018815977462521383,
      "loss": 0.6738,
      "step": 2947
    },
    {
      "epoch": 5.93158953722334,
      "grad_norm": 0.27569088339805603,
      "learning_rate": 0.0001881557500754603,
      "loss": 0.6672,
      "step": 2948
    },
    {
      "epoch": 5.933601609657948,
      "grad_norm": 0.2549789547920227,
      "learning_rate": 0.00018815172552570682,
      "loss": 0.637,
      "step": 2949
    },
    {
      "epoch": 5.935613682092555,
      "grad_norm": 0.2698512673377991,
      "learning_rate": 0.00018814770097595333,
      "loss": 0.6628,
      "step": 2950
    },
    {
      "epoch": 5.937625754527163,
      "grad_norm": 0.2646556496620178,
      "learning_rate": 0.00018814367642619982,
      "loss": 0.627,
      "step": 2951
    },
    {
      "epoch": 5.9396378269617705,
      "grad_norm": 0.24482347071170807,
      "learning_rate": 0.00018813965187644633,
      "loss": 0.6668,
      "step": 2952
    },
    {
      "epoch": 5.941649899396378,
      "grad_norm": 0.2866670489311218,
      "learning_rate": 0.00018813562732669284,
      "loss": 0.6347,
      "step": 2953
    },
    {
      "epoch": 5.943661971830986,
      "grad_norm": 0.2497386336326599,
      "learning_rate": 0.00018813160277693933,
      "loss": 0.6545,
      "step": 2954
    },
    {
      "epoch": 5.945674044265593,
      "grad_norm": 0.26253068447113037,
      "learning_rate": 0.00018812757822718584,
      "loss": 0.6596,
      "step": 2955
    },
    {
      "epoch": 5.9476861167002015,
      "grad_norm": 0.25844693183898926,
      "learning_rate": 0.00018812355367743235,
      "loss": 0.6296,
      "step": 2956
    },
    {
      "epoch": 5.949698189134809,
      "grad_norm": 0.2764451801776886,
      "learning_rate": 0.00018811952912767884,
      "loss": 0.6817,
      "step": 2957
    },
    {
      "epoch": 5.951710261569416,
      "grad_norm": 0.2685864567756653,
      "learning_rate": 0.00018811550457792535,
      "loss": 0.6452,
      "step": 2958
    },
    {
      "epoch": 5.953722334004024,
      "grad_norm": 0.2711886465549469,
      "learning_rate": 0.00018811148002817184,
      "loss": 0.6503,
      "step": 2959
    },
    {
      "epoch": 5.955734406438632,
      "grad_norm": 0.2649580240249634,
      "learning_rate": 0.00018810745547841838,
      "loss": 0.6578,
      "step": 2960
    },
    {
      "epoch": 5.957746478873239,
      "grad_norm": 0.2642223536968231,
      "learning_rate": 0.00018810343092866486,
      "loss": 0.6263,
      "step": 2961
    },
    {
      "epoch": 5.959758551307847,
      "grad_norm": 0.26973187923431396,
      "learning_rate": 0.00018809940637891137,
      "loss": 0.6614,
      "step": 2962
    },
    {
      "epoch": 5.961770623742455,
      "grad_norm": 0.2568105161190033,
      "learning_rate": 0.00018809538182915786,
      "loss": 0.6478,
      "step": 2963
    },
    {
      "epoch": 5.963782696177063,
      "grad_norm": 0.26005950570106506,
      "learning_rate": 0.00018809135727940437,
      "loss": 0.6759,
      "step": 2964
    },
    {
      "epoch": 5.96579476861167,
      "grad_norm": 0.2515161335468292,
      "learning_rate": 0.00018808733272965088,
      "loss": 0.6199,
      "step": 2965
    },
    {
      "epoch": 5.9678068410462775,
      "grad_norm": 0.2540717124938965,
      "learning_rate": 0.0001880833081798974,
      "loss": 0.6625,
      "step": 2966
    },
    {
      "epoch": 5.969818913480886,
      "grad_norm": 0.24887245893478394,
      "learning_rate": 0.00018807928363014388,
      "loss": 0.6659,
      "step": 2967
    },
    {
      "epoch": 5.971830985915493,
      "grad_norm": 0.2531331181526184,
      "learning_rate": 0.0001880752590803904,
      "loss": 0.6588,
      "step": 2968
    },
    {
      "epoch": 5.9738430583501,
      "grad_norm": 0.24936561286449432,
      "learning_rate": 0.00018807123453063688,
      "loss": 0.6396,
      "step": 2969
    },
    {
      "epoch": 5.9758551307847085,
      "grad_norm": 0.2741866707801819,
      "learning_rate": 0.00018806720998088342,
      "loss": 0.6697,
      "step": 2970
    },
    {
      "epoch": 5.977867203219316,
      "grad_norm": 0.2709071934223175,
      "learning_rate": 0.0001880631854311299,
      "loss": 0.6656,
      "step": 2971
    },
    {
      "epoch": 5.979879275653923,
      "grad_norm": 0.2744315564632416,
      "learning_rate": 0.00018805916088137641,
      "loss": 0.6657,
      "step": 2972
    },
    {
      "epoch": 5.981891348088531,
      "grad_norm": 0.26182499527931213,
      "learning_rate": 0.0001880551363316229,
      "loss": 0.6318,
      "step": 2973
    },
    {
      "epoch": 5.983903420523139,
      "grad_norm": 0.2669580280780792,
      "learning_rate": 0.0001880511117818694,
      "loss": 0.611,
      "step": 2974
    },
    {
      "epoch": 5.985915492957746,
      "grad_norm": 0.26748642325401306,
      "learning_rate": 0.00018804708723211592,
      "loss": 0.6419,
      "step": 2975
    },
    {
      "epoch": 5.987927565392354,
      "grad_norm": 0.2531401216983795,
      "learning_rate": 0.00018804306268236244,
      "loss": 0.6516,
      "step": 2976
    },
    {
      "epoch": 5.989939637826962,
      "grad_norm": 0.25462067127227783,
      "learning_rate": 0.00018803903813260892,
      "loss": 0.6363,
      "step": 2977
    },
    {
      "epoch": 5.991951710261569,
      "grad_norm": 0.26170164346694946,
      "learning_rate": 0.00018803501358285543,
      "loss": 0.6443,
      "step": 2978
    },
    {
      "epoch": 5.993963782696177,
      "grad_norm": 0.28349170088768005,
      "learning_rate": 0.00018803098903310192,
      "loss": 0.6809,
      "step": 2979
    },
    {
      "epoch": 5.9959758551307845,
      "grad_norm": 0.2708267271518707,
      "learning_rate": 0.00018802696448334846,
      "loss": 0.6536,
      "step": 2980
    },
    {
      "epoch": 5.997987927565393,
      "grad_norm": 0.25495558977127075,
      "learning_rate": 0.00018802293993359494,
      "loss": 0.6289,
      "step": 2981
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.2581985592842102,
      "learning_rate": 0.00018801891538384145,
      "loss": 0.6436,
      "step": 2982
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.7194597125053406,
      "eval_runtime": 49.8126,
      "eval_samples_per_second": 19.915,
      "eval_steps_per_second": 2.489,
      "step": 2982
    },
    {
      "epoch": 6.002012072434607,
      "grad_norm": 0.27843356132507324,
      "learning_rate": 0.00018801489083408794,
      "loss": 0.6078,
      "step": 2983
    },
    {
      "epoch": 6.0040241448692155,
      "grad_norm": 0.2706563472747803,
      "learning_rate": 0.00018801086628433445,
      "loss": 0.6187,
      "step": 2984
    },
    {
      "epoch": 6.006036217303823,
      "grad_norm": 0.2561081349849701,
      "learning_rate": 0.00018800684173458096,
      "loss": 0.5891,
      "step": 2985
    },
    {
      "epoch": 6.00804828973843,
      "grad_norm": 0.28106290102005005,
      "learning_rate": 0.00018800281718482745,
      "loss": 0.5765,
      "step": 2986
    },
    {
      "epoch": 6.010060362173038,
      "grad_norm": 0.28542593121528625,
      "learning_rate": 0.00018799879263507396,
      "loss": 0.6463,
      "step": 2987
    },
    {
      "epoch": 6.012072434607646,
      "grad_norm": 0.3050227165222168,
      "learning_rate": 0.00018799476808532047,
      "loss": 0.635,
      "step": 2988
    },
    {
      "epoch": 6.014084507042254,
      "grad_norm": 0.3003861904144287,
      "learning_rate": 0.00018799074353556696,
      "loss": 0.6394,
      "step": 2989
    },
    {
      "epoch": 6.016096579476861,
      "grad_norm": 0.29686543345451355,
      "learning_rate": 0.00018798671898581347,
      "loss": 0.5961,
      "step": 2990
    },
    {
      "epoch": 6.018108651911469,
      "grad_norm": 0.27572792768478394,
      "learning_rate": 0.00018798269443605998,
      "loss": 0.6473,
      "step": 2991
    },
    {
      "epoch": 6.020120724346077,
      "grad_norm": 0.27621254324913025,
      "learning_rate": 0.00018797866988630647,
      "loss": 0.5905,
      "step": 2992
    },
    {
      "epoch": 6.022132796780684,
      "grad_norm": 0.2851243019104004,
      "learning_rate": 0.00018797464533655298,
      "loss": 0.5966,
      "step": 2993
    },
    {
      "epoch": 6.0241448692152915,
      "grad_norm": 0.29041239619255066,
      "learning_rate": 0.00018797062078679947,
      "loss": 0.6871,
      "step": 2994
    },
    {
      "epoch": 6.0261569416499,
      "grad_norm": 0.2811426520347595,
      "learning_rate": 0.000187966596237046,
      "loss": 0.6131,
      "step": 2995
    },
    {
      "epoch": 6.028169014084507,
      "grad_norm": 0.2695363163948059,
      "learning_rate": 0.0001879625716872925,
      "loss": 0.6113,
      "step": 2996
    },
    {
      "epoch": 6.030181086519114,
      "grad_norm": 0.29054370522499084,
      "learning_rate": 0.000187958547137539,
      "loss": 0.6238,
      "step": 2997
    },
    {
      "epoch": 6.0321931589537225,
      "grad_norm": 0.2818104028701782,
      "learning_rate": 0.0001879545225877855,
      "loss": 0.6066,
      "step": 2998
    },
    {
      "epoch": 6.03420523138833,
      "grad_norm": 0.2788974344730377,
      "learning_rate": 0.000187950498038032,
      "loss": 0.6312,
      "step": 2999
    },
    {
      "epoch": 6.036217303822937,
      "grad_norm": 0.2799672782421112,
      "learning_rate": 0.0001879464734882785,
      "loss": 0.5982,
      "step": 3000
    },
    {
      "epoch": 6.038229376257545,
      "grad_norm": 0.260831356048584,
      "learning_rate": 0.00018794244893852502,
      "loss": 0.6135,
      "step": 3001
    },
    {
      "epoch": 6.040241448692153,
      "grad_norm": 0.2839480936527252,
      "learning_rate": 0.0001879384243887715,
      "loss": 0.6032,
      "step": 3002
    },
    {
      "epoch": 6.042253521126761,
      "grad_norm": 0.291495144367218,
      "learning_rate": 0.00018793439983901802,
      "loss": 0.6033,
      "step": 3003
    },
    {
      "epoch": 6.044265593561368,
      "grad_norm": 0.27967679500579834,
      "learning_rate": 0.0001879303752892645,
      "loss": 0.5522,
      "step": 3004
    },
    {
      "epoch": 6.046277665995976,
      "grad_norm": 0.3108220398426056,
      "learning_rate": 0.00018792635073951105,
      "loss": 0.632,
      "step": 3005
    },
    {
      "epoch": 6.048289738430584,
      "grad_norm": 0.29037103056907654,
      "learning_rate": 0.00018792232618975753,
      "loss": 0.5712,
      "step": 3006
    },
    {
      "epoch": 6.050301810865191,
      "grad_norm": 0.290102481842041,
      "learning_rate": 0.00018791830164000404,
      "loss": 0.6728,
      "step": 3007
    },
    {
      "epoch": 6.0523138832997985,
      "grad_norm": 0.2795487940311432,
      "learning_rate": 0.00018791427709025053,
      "loss": 0.6292,
      "step": 3008
    },
    {
      "epoch": 6.054325955734407,
      "grad_norm": 0.2856253981590271,
      "learning_rate": 0.00018791025254049704,
      "loss": 0.5972,
      "step": 3009
    },
    {
      "epoch": 6.056338028169014,
      "grad_norm": 0.2806031107902527,
      "learning_rate": 0.00018790622799074355,
      "loss": 0.6126,
      "step": 3010
    },
    {
      "epoch": 6.058350100603621,
      "grad_norm": 0.29262715578079224,
      "learning_rate": 0.00018790220344099007,
      "loss": 0.5751,
      "step": 3011
    },
    {
      "epoch": 6.0603621730382295,
      "grad_norm": 0.29549142718315125,
      "learning_rate": 0.00018789817889123655,
      "loss": 0.578,
      "step": 3012
    },
    {
      "epoch": 6.062374245472837,
      "grad_norm": 0.3059971034526825,
      "learning_rate": 0.00018789415434148306,
      "loss": 0.6118,
      "step": 3013
    },
    {
      "epoch": 6.064386317907445,
      "grad_norm": 0.28895264863967896,
      "learning_rate": 0.00018789012979172955,
      "loss": 0.6669,
      "step": 3014
    },
    {
      "epoch": 6.066398390342052,
      "grad_norm": 0.30151233077049255,
      "learning_rate": 0.0001878861052419761,
      "loss": 0.6445,
      "step": 3015
    },
    {
      "epoch": 6.06841046277666,
      "grad_norm": 0.3057766258716583,
      "learning_rate": 0.00018788208069222257,
      "loss": 0.6536,
      "step": 3016
    },
    {
      "epoch": 6.070422535211268,
      "grad_norm": 0.2854677438735962,
      "learning_rate": 0.00018787805614246908,
      "loss": 0.6021,
      "step": 3017
    },
    {
      "epoch": 6.072434607645875,
      "grad_norm": 0.26129648089408875,
      "learning_rate": 0.00018787403159271557,
      "loss": 0.6001,
      "step": 3018
    },
    {
      "epoch": 6.074446680080483,
      "grad_norm": 0.2949959635734558,
      "learning_rate": 0.00018787000704296208,
      "loss": 0.6127,
      "step": 3019
    },
    {
      "epoch": 6.076458752515091,
      "grad_norm": 0.2896003723144531,
      "learning_rate": 0.0001878659824932086,
      "loss": 0.5924,
      "step": 3020
    },
    {
      "epoch": 6.078470824949698,
      "grad_norm": 0.27981072664260864,
      "learning_rate": 0.00018786195794345508,
      "loss": 0.627,
      "step": 3021
    },
    {
      "epoch": 6.0804828973843055,
      "grad_norm": 0.28556355834007263,
      "learning_rate": 0.0001878579333937016,
      "loss": 0.5908,
      "step": 3022
    },
    {
      "epoch": 6.082494969818914,
      "grad_norm": 0.29499971866607666,
      "learning_rate": 0.0001878539088439481,
      "loss": 0.6208,
      "step": 3023
    },
    {
      "epoch": 6.084507042253521,
      "grad_norm": 0.29257839918136597,
      "learning_rate": 0.0001878498842941946,
      "loss": 0.5909,
      "step": 3024
    },
    {
      "epoch": 6.086519114688128,
      "grad_norm": 0.28818279504776,
      "learning_rate": 0.0001878458597444411,
      "loss": 0.6407,
      "step": 3025
    },
    {
      "epoch": 6.0885311871227366,
      "grad_norm": 0.28746673464775085,
      "learning_rate": 0.0001878418351946876,
      "loss": 0.5839,
      "step": 3026
    },
    {
      "epoch": 6.090543259557344,
      "grad_norm": 0.2788259983062744,
      "learning_rate": 0.0001878378106449341,
      "loss": 0.5806,
      "step": 3027
    },
    {
      "epoch": 6.092555331991952,
      "grad_norm": 0.29889994859695435,
      "learning_rate": 0.0001878337860951806,
      "loss": 0.6077,
      "step": 3028
    },
    {
      "epoch": 6.094567404426559,
      "grad_norm": 0.2863650321960449,
      "learning_rate": 0.0001878297615454271,
      "loss": 0.5876,
      "step": 3029
    },
    {
      "epoch": 6.096579476861167,
      "grad_norm": 0.29677680134773254,
      "learning_rate": 0.00018782573699567363,
      "loss": 0.6411,
      "step": 3030
    },
    {
      "epoch": 6.098591549295775,
      "grad_norm": 0.2975207567214966,
      "learning_rate": 0.00018782171244592012,
      "loss": 0.6236,
      "step": 3031
    },
    {
      "epoch": 6.100603621730382,
      "grad_norm": 0.2847117781639099,
      "learning_rate": 0.00018781768789616663,
      "loss": 0.5851,
      "step": 3032
    },
    {
      "epoch": 6.10261569416499,
      "grad_norm": 0.28729110956192017,
      "learning_rate": 0.00018781366334641312,
      "loss": 0.5951,
      "step": 3033
    },
    {
      "epoch": 6.104627766599598,
      "grad_norm": 0.29327431321144104,
      "learning_rate": 0.00018780963879665963,
      "loss": 0.614,
      "step": 3034
    },
    {
      "epoch": 6.106639839034205,
      "grad_norm": 0.2984555661678314,
      "learning_rate": 0.00018780561424690614,
      "loss": 0.6369,
      "step": 3035
    },
    {
      "epoch": 6.1086519114688125,
      "grad_norm": 0.302564799785614,
      "learning_rate": 0.00018780158969715265,
      "loss": 0.592,
      "step": 3036
    },
    {
      "epoch": 6.110663983903421,
      "grad_norm": 0.2725556194782257,
      "learning_rate": 0.00018779756514739914,
      "loss": 0.594,
      "step": 3037
    },
    {
      "epoch": 6.112676056338028,
      "grad_norm": 0.2827705442905426,
      "learning_rate": 0.00018779354059764565,
      "loss": 0.6133,
      "step": 3038
    },
    {
      "epoch": 6.114688128772636,
      "grad_norm": 0.2778570055961609,
      "learning_rate": 0.00018778951604789214,
      "loss": 0.6209,
      "step": 3039
    },
    {
      "epoch": 6.116700201207244,
      "grad_norm": 0.28694698214530945,
      "learning_rate": 0.00018778549149813865,
      "loss": 0.6175,
      "step": 3040
    },
    {
      "epoch": 6.118712273641851,
      "grad_norm": 0.2978884279727936,
      "learning_rate": 0.00018778146694838516,
      "loss": 0.6245,
      "step": 3041
    },
    {
      "epoch": 6.120724346076459,
      "grad_norm": 0.27816590666770935,
      "learning_rate": 0.00018777744239863167,
      "loss": 0.5953,
      "step": 3042
    },
    {
      "epoch": 6.122736418511066,
      "grad_norm": 0.27729710936546326,
      "learning_rate": 0.00018777341784887816,
      "loss": 0.6177,
      "step": 3043
    },
    {
      "epoch": 6.124748490945674,
      "grad_norm": 0.2999322712421417,
      "learning_rate": 0.00018776939329912467,
      "loss": 0.65,
      "step": 3044
    },
    {
      "epoch": 6.126760563380282,
      "grad_norm": 0.29913291335105896,
      "learning_rate": 0.00018776536874937116,
      "loss": 0.6232,
      "step": 3045
    },
    {
      "epoch": 6.128772635814889,
      "grad_norm": 0.28064388036727905,
      "learning_rate": 0.0001877613441996177,
      "loss": 0.5916,
      "step": 3046
    },
    {
      "epoch": 6.130784708249497,
      "grad_norm": 0.2792418897151947,
      "learning_rate": 0.00018775731964986418,
      "loss": 0.5901,
      "step": 3047
    },
    {
      "epoch": 6.132796780684105,
      "grad_norm": 0.2882491946220398,
      "learning_rate": 0.0001877532951001107,
      "loss": 0.5957,
      "step": 3048
    },
    {
      "epoch": 6.134808853118712,
      "grad_norm": 0.29557791352272034,
      "learning_rate": 0.00018774927055035718,
      "loss": 0.6689,
      "step": 3049
    },
    {
      "epoch": 6.1368209255533195,
      "grad_norm": 0.28496861457824707,
      "learning_rate": 0.0001877452460006037,
      "loss": 0.5924,
      "step": 3050
    },
    {
      "epoch": 6.138832997987928,
      "grad_norm": 0.2765301764011383,
      "learning_rate": 0.0001877412214508502,
      "loss": 0.5955,
      "step": 3051
    },
    {
      "epoch": 6.140845070422535,
      "grad_norm": 0.2855939567089081,
      "learning_rate": 0.00018773719690109671,
      "loss": 0.6553,
      "step": 3052
    },
    {
      "epoch": 6.142857142857143,
      "grad_norm": 0.2960982024669647,
      "learning_rate": 0.0001877331723513432,
      "loss": 0.6285,
      "step": 3053
    },
    {
      "epoch": 6.144869215291751,
      "grad_norm": 0.2986789047718048,
      "learning_rate": 0.0001877291478015897,
      "loss": 0.6217,
      "step": 3054
    },
    {
      "epoch": 6.146881287726358,
      "grad_norm": 0.29439690709114075,
      "learning_rate": 0.0001877251232518362,
      "loss": 0.6611,
      "step": 3055
    },
    {
      "epoch": 6.148893360160966,
      "grad_norm": 0.27823323011398315,
      "learning_rate": 0.0001877210987020827,
      "loss": 0.6574,
      "step": 3056
    },
    {
      "epoch": 6.150905432595573,
      "grad_norm": 0.2922607660293579,
      "learning_rate": 0.00018771707415232922,
      "loss": 0.6232,
      "step": 3057
    },
    {
      "epoch": 6.152917505030181,
      "grad_norm": 0.27505454421043396,
      "learning_rate": 0.0001877130496025757,
      "loss": 0.5634,
      "step": 3058
    },
    {
      "epoch": 6.154929577464789,
      "grad_norm": 0.28721433877944946,
      "learning_rate": 0.00018770902505282222,
      "loss": 0.6285,
      "step": 3059
    },
    {
      "epoch": 6.156941649899396,
      "grad_norm": 0.3129710555076599,
      "learning_rate": 0.00018770500050306873,
      "loss": 0.6374,
      "step": 3060
    },
    {
      "epoch": 6.158953722334004,
      "grad_norm": 0.29061177372932434,
      "learning_rate": 0.00018770097595331524,
      "loss": 0.6319,
      "step": 3061
    },
    {
      "epoch": 6.160965794768612,
      "grad_norm": 0.2884877920150757,
      "learning_rate": 0.00018769695140356173,
      "loss": 0.631,
      "step": 3062
    },
    {
      "epoch": 6.162977867203219,
      "grad_norm": 0.2843751907348633,
      "learning_rate": 0.00018769292685380824,
      "loss": 0.624,
      "step": 3063
    },
    {
      "epoch": 6.164989939637827,
      "grad_norm": 0.31055352091789246,
      "learning_rate": 0.00018768890230405472,
      "loss": 0.6463,
      "step": 3064
    },
    {
      "epoch": 6.167002012072435,
      "grad_norm": 0.2858734428882599,
      "learning_rate": 0.00018768487775430124,
      "loss": 0.6384,
      "step": 3065
    },
    {
      "epoch": 6.169014084507042,
      "grad_norm": 0.28462326526641846,
      "learning_rate": 0.00018768085320454775,
      "loss": 0.6106,
      "step": 3066
    },
    {
      "epoch": 6.17102615694165,
      "grad_norm": 0.2934893071651459,
      "learning_rate": 0.00018767682865479426,
      "loss": 0.6223,
      "step": 3067
    },
    {
      "epoch": 6.173038229376258,
      "grad_norm": 0.29776155948638916,
      "learning_rate": 0.00018767280410504075,
      "loss": 0.6002,
      "step": 3068
    },
    {
      "epoch": 6.175050301810865,
      "grad_norm": 0.2817462980747223,
      "learning_rate": 0.00018766877955528726,
      "loss": 0.5842,
      "step": 3069
    },
    {
      "epoch": 6.177062374245473,
      "grad_norm": 0.30651241540908813,
      "learning_rate": 0.00018766475500553374,
      "loss": 0.6412,
      "step": 3070
    },
    {
      "epoch": 6.17907444668008,
      "grad_norm": 0.3014083206653595,
      "learning_rate": 0.00018766073045578028,
      "loss": 0.5954,
      "step": 3071
    },
    {
      "epoch": 6.181086519114688,
      "grad_norm": 0.29626619815826416,
      "learning_rate": 0.00018765670590602677,
      "loss": 0.6263,
      "step": 3072
    },
    {
      "epoch": 6.183098591549296,
      "grad_norm": 0.28062793612480164,
      "learning_rate": 0.00018765268135627328,
      "loss": 0.5891,
      "step": 3073
    },
    {
      "epoch": 6.185110663983903,
      "grad_norm": 0.3096192479133606,
      "learning_rate": 0.00018764865680651977,
      "loss": 0.6175,
      "step": 3074
    },
    {
      "epoch": 6.187122736418511,
      "grad_norm": 0.3052394390106201,
      "learning_rate": 0.00018764463225676628,
      "loss": 0.5917,
      "step": 3075
    },
    {
      "epoch": 6.189134808853119,
      "grad_norm": 0.2715795338153839,
      "learning_rate": 0.0001876406077070128,
      "loss": 0.5749,
      "step": 3076
    },
    {
      "epoch": 6.191146881287726,
      "grad_norm": 0.27166756987571716,
      "learning_rate": 0.0001876365831572593,
      "loss": 0.6192,
      "step": 3077
    },
    {
      "epoch": 6.193158953722334,
      "grad_norm": 0.27979159355163574,
      "learning_rate": 0.0001876325586075058,
      "loss": 0.6401,
      "step": 3078
    },
    {
      "epoch": 6.195171026156942,
      "grad_norm": 0.286331444978714,
      "learning_rate": 0.0001876285340577523,
      "loss": 0.6511,
      "step": 3079
    },
    {
      "epoch": 6.197183098591549,
      "grad_norm": 0.287963330745697,
      "learning_rate": 0.00018762450950799878,
      "loss": 0.6192,
      "step": 3080
    },
    {
      "epoch": 6.199195171026157,
      "grad_norm": 0.30129677057266235,
      "learning_rate": 0.00018762048495824532,
      "loss": 0.6336,
      "step": 3081
    },
    {
      "epoch": 6.201207243460765,
      "grad_norm": 0.2968980073928833,
      "learning_rate": 0.0001876164604084918,
      "loss": 0.6036,
      "step": 3082
    },
    {
      "epoch": 6.203219315895372,
      "grad_norm": 0.28601813316345215,
      "learning_rate": 0.00018761243585873832,
      "loss": 0.604,
      "step": 3083
    },
    {
      "epoch": 6.20523138832998,
      "grad_norm": 0.275495707988739,
      "learning_rate": 0.0001876084113089848,
      "loss": 0.6361,
      "step": 3084
    },
    {
      "epoch": 6.207243460764587,
      "grad_norm": 0.28149178624153137,
      "learning_rate": 0.00018760438675923132,
      "loss": 0.61,
      "step": 3085
    },
    {
      "epoch": 6.209255533199195,
      "grad_norm": 0.2715698182582855,
      "learning_rate": 0.00018760036220947783,
      "loss": 0.5867,
      "step": 3086
    },
    {
      "epoch": 6.211267605633803,
      "grad_norm": 0.27978113293647766,
      "learning_rate": 0.00018759633765972434,
      "loss": 0.6462,
      "step": 3087
    },
    {
      "epoch": 6.21327967806841,
      "grad_norm": 0.26782211661338806,
      "learning_rate": 0.00018759231310997083,
      "loss": 0.6139,
      "step": 3088
    },
    {
      "epoch": 6.2152917505030185,
      "grad_norm": 0.2845579981803894,
      "learning_rate": 0.00018758828856021734,
      "loss": 0.6181,
      "step": 3089
    },
    {
      "epoch": 6.217303822937626,
      "grad_norm": 0.2850625813007355,
      "learning_rate": 0.00018758426401046383,
      "loss": 0.6263,
      "step": 3090
    },
    {
      "epoch": 6.219315895372233,
      "grad_norm": 0.28287413716316223,
      "learning_rate": 0.00018758023946071034,
      "loss": 0.6165,
      "step": 3091
    },
    {
      "epoch": 6.221327967806841,
      "grad_norm": 0.2804449498653412,
      "learning_rate": 0.00018757621491095685,
      "loss": 0.6284,
      "step": 3092
    },
    {
      "epoch": 6.223340040241449,
      "grad_norm": 0.2745283246040344,
      "learning_rate": 0.00018757219036120334,
      "loss": 0.6399,
      "step": 3093
    },
    {
      "epoch": 6.225352112676056,
      "grad_norm": 0.29233095049858093,
      "learning_rate": 0.00018756816581144985,
      "loss": 0.6547,
      "step": 3094
    },
    {
      "epoch": 6.227364185110664,
      "grad_norm": 0.28685230016708374,
      "learning_rate": 0.00018756414126169636,
      "loss": 0.646,
      "step": 3095
    },
    {
      "epoch": 6.229376257545272,
      "grad_norm": 0.27475160360336304,
      "learning_rate": 0.00018756011671194287,
      "loss": 0.6215,
      "step": 3096
    },
    {
      "epoch": 6.231388329979879,
      "grad_norm": 0.27906090021133423,
      "learning_rate": 0.00018755609216218936,
      "loss": 0.6461,
      "step": 3097
    },
    {
      "epoch": 6.233400402414487,
      "grad_norm": 0.276718407869339,
      "learning_rate": 0.00018755206761243587,
      "loss": 0.6253,
      "step": 3098
    },
    {
      "epoch": 6.2354124748490944,
      "grad_norm": 0.281338095664978,
      "learning_rate": 0.00018754804306268235,
      "loss": 0.6029,
      "step": 3099
    },
    {
      "epoch": 6.237424547283702,
      "grad_norm": 0.28408655524253845,
      "learning_rate": 0.00018754401851292887,
      "loss": 0.6304,
      "step": 3100
    },
    {
      "epoch": 6.23943661971831,
      "grad_norm": 0.3033170700073242,
      "learning_rate": 0.00018753999396317538,
      "loss": 0.6539,
      "step": 3101
    },
    {
      "epoch": 6.241448692152917,
      "grad_norm": 0.2872968316078186,
      "learning_rate": 0.0001875359694134219,
      "loss": 0.6424,
      "step": 3102
    },
    {
      "epoch": 6.2434607645875255,
      "grad_norm": 0.28399360179901123,
      "learning_rate": 0.00018753194486366838,
      "loss": 0.6254,
      "step": 3103
    },
    {
      "epoch": 6.245472837022133,
      "grad_norm": 0.29552388191223145,
      "learning_rate": 0.0001875279203139149,
      "loss": 0.6117,
      "step": 3104
    },
    {
      "epoch": 6.24748490945674,
      "grad_norm": 0.296074777841568,
      "learning_rate": 0.00018752389576416137,
      "loss": 0.5971,
      "step": 3105
    },
    {
      "epoch": 6.249496981891348,
      "grad_norm": 0.2837512791156769,
      "learning_rate": 0.0001875198712144079,
      "loss": 0.624,
      "step": 3106
    },
    {
      "epoch": 6.251509054325956,
      "grad_norm": 0.30385589599609375,
      "learning_rate": 0.0001875158466646544,
      "loss": 0.6217,
      "step": 3107
    },
    {
      "epoch": 6.253521126760563,
      "grad_norm": 0.30025508999824524,
      "learning_rate": 0.0001875118221149009,
      "loss": 0.632,
      "step": 3108
    },
    {
      "epoch": 6.255533199195171,
      "grad_norm": 0.30813515186309814,
      "learning_rate": 0.0001875077975651474,
      "loss": 0.6289,
      "step": 3109
    },
    {
      "epoch": 6.257545271629779,
      "grad_norm": 0.28339430689811707,
      "learning_rate": 0.0001875037730153939,
      "loss": 0.6307,
      "step": 3110
    },
    {
      "epoch": 6.259557344064386,
      "grad_norm": 0.27726563811302185,
      "learning_rate": 0.00018749974846564042,
      "loss": 0.5918,
      "step": 3111
    },
    {
      "epoch": 6.261569416498994,
      "grad_norm": 0.28512078523635864,
      "learning_rate": 0.00018749572391588693,
      "loss": 0.6073,
      "step": 3112
    },
    {
      "epoch": 6.2635814889336014,
      "grad_norm": 0.27733367681503296,
      "learning_rate": 0.00018749169936613342,
      "loss": 0.6251,
      "step": 3113
    },
    {
      "epoch": 6.26559356136821,
      "grad_norm": 0.30534934997558594,
      "learning_rate": 0.00018748767481637993,
      "loss": 0.6486,
      "step": 3114
    },
    {
      "epoch": 6.267605633802817,
      "grad_norm": 0.285556435585022,
      "learning_rate": 0.00018748365026662641,
      "loss": 0.6376,
      "step": 3115
    },
    {
      "epoch": 6.269617706237424,
      "grad_norm": 0.28215593099594116,
      "learning_rate": 0.00018747962571687295,
      "loss": 0.608,
      "step": 3116
    },
    {
      "epoch": 6.2716297786720325,
      "grad_norm": 0.2800329029560089,
      "learning_rate": 0.00018747560116711944,
      "loss": 0.637,
      "step": 3117
    },
    {
      "epoch": 6.27364185110664,
      "grad_norm": 0.2845311164855957,
      "learning_rate": 0.00018747157661736595,
      "loss": 0.5867,
      "step": 3118
    },
    {
      "epoch": 6.275653923541247,
      "grad_norm": 0.3029472231864929,
      "learning_rate": 0.00018746755206761244,
      "loss": 0.6392,
      "step": 3119
    },
    {
      "epoch": 6.277665995975855,
      "grad_norm": 0.2902325987815857,
      "learning_rate": 0.00018746352751785895,
      "loss": 0.6473,
      "step": 3120
    },
    {
      "epoch": 6.279678068410463,
      "grad_norm": 0.3153904974460602,
      "learning_rate": 0.00018745950296810546,
      "loss": 0.6574,
      "step": 3121
    },
    {
      "epoch": 6.28169014084507,
      "grad_norm": 0.27993398904800415,
      "learning_rate": 0.00018745547841835197,
      "loss": 0.6277,
      "step": 3122
    },
    {
      "epoch": 6.283702213279678,
      "grad_norm": 0.2832745909690857,
      "learning_rate": 0.00018745145386859846,
      "loss": 0.6422,
      "step": 3123
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 0.28442949056625366,
      "learning_rate": 0.00018744742931884497,
      "loss": 0.634,
      "step": 3124
    },
    {
      "epoch": 6.287726358148893,
      "grad_norm": 0.2929171919822693,
      "learning_rate": 0.00018744340476909145,
      "loss": 0.6406,
      "step": 3125
    },
    {
      "epoch": 6.289738430583501,
      "grad_norm": 0.2833425998687744,
      "learning_rate": 0.00018743938021933797,
      "loss": 0.5911,
      "step": 3126
    },
    {
      "epoch": 6.2917505030181085,
      "grad_norm": 0.2888706624507904,
      "learning_rate": 0.00018743535566958448,
      "loss": 0.5618,
      "step": 3127
    },
    {
      "epoch": 6.293762575452717,
      "grad_norm": 0.31177225708961487,
      "learning_rate": 0.00018743133111983096,
      "loss": 0.6061,
      "step": 3128
    },
    {
      "epoch": 6.295774647887324,
      "grad_norm": 0.2943609356880188,
      "learning_rate": 0.00018742730657007748,
      "loss": 0.6512,
      "step": 3129
    },
    {
      "epoch": 6.297786720321931,
      "grad_norm": 0.3203284442424774,
      "learning_rate": 0.000187423282020324,
      "loss": 0.68,
      "step": 3130
    },
    {
      "epoch": 6.2997987927565395,
      "grad_norm": 0.2727661430835724,
      "learning_rate": 0.0001874192574705705,
      "loss": 0.6251,
      "step": 3131
    },
    {
      "epoch": 6.301810865191147,
      "grad_norm": 0.27839013934135437,
      "learning_rate": 0.00018741523292081699,
      "loss": 0.6107,
      "step": 3132
    },
    {
      "epoch": 6.303822937625754,
      "grad_norm": 0.2818228006362915,
      "learning_rate": 0.0001874112083710635,
      "loss": 0.5975,
      "step": 3133
    },
    {
      "epoch": 6.305835010060362,
      "grad_norm": 0.2890963554382324,
      "learning_rate": 0.00018740718382130998,
      "loss": 0.623,
      "step": 3134
    },
    {
      "epoch": 6.30784708249497,
      "grad_norm": 0.27615049481391907,
      "learning_rate": 0.0001874031592715565,
      "loss": 0.6124,
      "step": 3135
    },
    {
      "epoch": 6.309859154929577,
      "grad_norm": 0.28643521666526794,
      "learning_rate": 0.000187399134721803,
      "loss": 0.6687,
      "step": 3136
    },
    {
      "epoch": 6.311871227364185,
      "grad_norm": 0.2973787784576416,
      "learning_rate": 0.00018739511017204952,
      "loss": 0.6531,
      "step": 3137
    },
    {
      "epoch": 6.313883299798793,
      "grad_norm": 0.30164143443107605,
      "learning_rate": 0.000187391085622296,
      "loss": 0.6409,
      "step": 3138
    },
    {
      "epoch": 6.315895372233401,
      "grad_norm": 0.28285908699035645,
      "learning_rate": 0.00018738706107254252,
      "loss": 0.6363,
      "step": 3139
    },
    {
      "epoch": 6.317907444668008,
      "grad_norm": 0.28176915645599365,
      "learning_rate": 0.000187383036522789,
      "loss": 0.5781,
      "step": 3140
    },
    {
      "epoch": 6.3199195171026155,
      "grad_norm": 0.30610355734825134,
      "learning_rate": 0.00018737901197303554,
      "loss": 0.6601,
      "step": 3141
    },
    {
      "epoch": 6.321931589537224,
      "grad_norm": 0.2732545733451843,
      "learning_rate": 0.00018737498742328203,
      "loss": 0.5717,
      "step": 3142
    },
    {
      "epoch": 6.323943661971831,
      "grad_norm": 0.29323893785476685,
      "learning_rate": 0.00018737096287352854,
      "loss": 0.598,
      "step": 3143
    },
    {
      "epoch": 6.325955734406438,
      "grad_norm": 0.2818216383457184,
      "learning_rate": 0.00018736693832377502,
      "loss": 0.6286,
      "step": 3144
    },
    {
      "epoch": 6.3279678068410465,
      "grad_norm": 0.2717292010784149,
      "learning_rate": 0.00018736291377402154,
      "loss": 0.5923,
      "step": 3145
    },
    {
      "epoch": 6.329979879275654,
      "grad_norm": 0.2792191207408905,
      "learning_rate": 0.00018735888922426805,
      "loss": 0.5771,
      "step": 3146
    },
    {
      "epoch": 6.331991951710261,
      "grad_norm": 0.29480597376823425,
      "learning_rate": 0.00018735486467451456,
      "loss": 0.611,
      "step": 3147
    },
    {
      "epoch": 6.334004024144869,
      "grad_norm": 0.27298593521118164,
      "learning_rate": 0.00018735084012476105,
      "loss": 0.6364,
      "step": 3148
    },
    {
      "epoch": 6.336016096579477,
      "grad_norm": 0.28048276901245117,
      "learning_rate": 0.00018734681557500756,
      "loss": 0.604,
      "step": 3149
    },
    {
      "epoch": 6.338028169014084,
      "grad_norm": 0.2962583005428314,
      "learning_rate": 0.00018734279102525404,
      "loss": 0.6162,
      "step": 3150
    },
    {
      "epoch": 6.340040241448692,
      "grad_norm": 0.2890494465827942,
      "learning_rate": 0.00018733876647550058,
      "loss": 0.6137,
      "step": 3151
    },
    {
      "epoch": 6.3420523138833,
      "grad_norm": 0.2830573320388794,
      "learning_rate": 0.00018733474192574707,
      "loss": 0.6111,
      "step": 3152
    },
    {
      "epoch": 6.344064386317908,
      "grad_norm": 0.29076096415519714,
      "learning_rate": 0.00018733071737599358,
      "loss": 0.5836,
      "step": 3153
    },
    {
      "epoch": 6.346076458752515,
      "grad_norm": 0.3016425669193268,
      "learning_rate": 0.00018732669282624007,
      "loss": 0.641,
      "step": 3154
    },
    {
      "epoch": 6.3480885311871225,
      "grad_norm": 0.29125696420669556,
      "learning_rate": 0.00018732266827648658,
      "loss": 0.6388,
      "step": 3155
    },
    {
      "epoch": 6.350100603621731,
      "grad_norm": 0.2828425168991089,
      "learning_rate": 0.0001873186437267331,
      "loss": 0.6146,
      "step": 3156
    },
    {
      "epoch": 6.352112676056338,
      "grad_norm": 0.2870238721370697,
      "learning_rate": 0.0001873146191769796,
      "loss": 0.6092,
      "step": 3157
    },
    {
      "epoch": 6.354124748490945,
      "grad_norm": 0.27551600337028503,
      "learning_rate": 0.0001873105946272261,
      "loss": 0.5951,
      "step": 3158
    },
    {
      "epoch": 6.3561368209255535,
      "grad_norm": 0.2857276499271393,
      "learning_rate": 0.0001873065700774726,
      "loss": 0.6089,
      "step": 3159
    },
    {
      "epoch": 6.358148893360161,
      "grad_norm": 0.27864348888397217,
      "learning_rate": 0.00018730254552771908,
      "loss": 0.6039,
      "step": 3160
    },
    {
      "epoch": 6.360160965794769,
      "grad_norm": 0.2981797158718109,
      "learning_rate": 0.0001872985209779656,
      "loss": 0.6109,
      "step": 3161
    },
    {
      "epoch": 6.362173038229376,
      "grad_norm": 0.2834793031215668,
      "learning_rate": 0.0001872944964282121,
      "loss": 0.609,
      "step": 3162
    },
    {
      "epoch": 6.364185110663984,
      "grad_norm": 0.2738797962665558,
      "learning_rate": 0.0001872904718784586,
      "loss": 0.6221,
      "step": 3163
    },
    {
      "epoch": 6.366197183098592,
      "grad_norm": 0.2788841426372528,
      "learning_rate": 0.0001872864473287051,
      "loss": 0.611,
      "step": 3164
    },
    {
      "epoch": 6.368209255533199,
      "grad_norm": 0.2898672819137573,
      "learning_rate": 0.00018728242277895162,
      "loss": 0.6189,
      "step": 3165
    },
    {
      "epoch": 6.370221327967807,
      "grad_norm": 0.2939891219139099,
      "learning_rate": 0.00018727839822919813,
      "loss": 0.6381,
      "step": 3166
    },
    {
      "epoch": 6.372233400402415,
      "grad_norm": 0.28176000714302063,
      "learning_rate": 0.00018727437367944462,
      "loss": 0.5988,
      "step": 3167
    },
    {
      "epoch": 6.374245472837022,
      "grad_norm": 0.3049333095550537,
      "learning_rate": 0.00018727034912969113,
      "loss": 0.6228,
      "step": 3168
    },
    {
      "epoch": 6.3762575452716295,
      "grad_norm": 0.287383496761322,
      "learning_rate": 0.0001872663245799376,
      "loss": 0.6181,
      "step": 3169
    },
    {
      "epoch": 6.378269617706238,
      "grad_norm": 0.2869010269641876,
      "learning_rate": 0.00018726230003018413,
      "loss": 0.6281,
      "step": 3170
    },
    {
      "epoch": 6.380281690140845,
      "grad_norm": 0.2989436388015747,
      "learning_rate": 0.00018725827548043064,
      "loss": 0.6027,
      "step": 3171
    },
    {
      "epoch": 6.382293762575452,
      "grad_norm": 0.27838200330734253,
      "learning_rate": 0.00018725425093067715,
      "loss": 0.594,
      "step": 3172
    },
    {
      "epoch": 6.3843058350100605,
      "grad_norm": 0.27153557538986206,
      "learning_rate": 0.00018725022638092363,
      "loss": 0.5736,
      "step": 3173
    },
    {
      "epoch": 6.386317907444668,
      "grad_norm": 0.30511465668678284,
      "learning_rate": 0.00018724620183117015,
      "loss": 0.6344,
      "step": 3174
    },
    {
      "epoch": 6.388329979879275,
      "grad_norm": 0.2982047498226166,
      "learning_rate": 0.00018724217728141663,
      "loss": 0.6089,
      "step": 3175
    },
    {
      "epoch": 6.390342052313883,
      "grad_norm": 0.281432181596756,
      "learning_rate": 0.00018723815273166317,
      "loss": 0.5759,
      "step": 3176
    },
    {
      "epoch": 6.392354124748491,
      "grad_norm": 0.28279316425323486,
      "learning_rate": 0.00018723412818190966,
      "loss": 0.5993,
      "step": 3177
    },
    {
      "epoch": 6.394366197183099,
      "grad_norm": 0.2831626534461975,
      "learning_rate": 0.00018723010363215617,
      "loss": 0.6072,
      "step": 3178
    },
    {
      "epoch": 6.396378269617706,
      "grad_norm": 0.28381410241127014,
      "learning_rate": 0.00018722607908240265,
      "loss": 0.6615,
      "step": 3179
    },
    {
      "epoch": 6.398390342052314,
      "grad_norm": 0.2765849232673645,
      "learning_rate": 0.00018722205453264917,
      "loss": 0.605,
      "step": 3180
    },
    {
      "epoch": 6.400402414486922,
      "grad_norm": 0.28997039794921875,
      "learning_rate": 0.00018721802998289568,
      "loss": 0.5792,
      "step": 3181
    },
    {
      "epoch": 6.402414486921529,
      "grad_norm": 0.28015536069869995,
      "learning_rate": 0.0001872140054331422,
      "loss": 0.5698,
      "step": 3182
    },
    {
      "epoch": 6.4044265593561365,
      "grad_norm": 0.278652548789978,
      "learning_rate": 0.00018720998088338868,
      "loss": 0.6071,
      "step": 3183
    },
    {
      "epoch": 6.406438631790745,
      "grad_norm": 0.2659895122051239,
      "learning_rate": 0.0001872059563336352,
      "loss": 0.5871,
      "step": 3184
    },
    {
      "epoch": 6.408450704225352,
      "grad_norm": 0.28714609146118164,
      "learning_rate": 0.00018720193178388167,
      "loss": 0.5911,
      "step": 3185
    },
    {
      "epoch": 6.41046277665996,
      "grad_norm": 0.2749687135219574,
      "learning_rate": 0.0001871979072341282,
      "loss": 0.616,
      "step": 3186
    },
    {
      "epoch": 6.4124748490945676,
      "grad_norm": 0.29269862174987793,
      "learning_rate": 0.0001871938826843747,
      "loss": 0.6492,
      "step": 3187
    },
    {
      "epoch": 6.414486921529175,
      "grad_norm": 0.2971166968345642,
      "learning_rate": 0.0001871898581346212,
      "loss": 0.6406,
      "step": 3188
    },
    {
      "epoch": 6.416498993963783,
      "grad_norm": 0.27148768305778503,
      "learning_rate": 0.0001871858335848677,
      "loss": 0.6469,
      "step": 3189
    },
    {
      "epoch": 6.41851106639839,
      "grad_norm": 0.27952998876571655,
      "learning_rate": 0.0001871818090351142,
      "loss": 0.6319,
      "step": 3190
    },
    {
      "epoch": 6.420523138832998,
      "grad_norm": 0.29641473293304443,
      "learning_rate": 0.00018717778448536072,
      "loss": 0.6521,
      "step": 3191
    },
    {
      "epoch": 6.422535211267606,
      "grad_norm": 0.2878665030002594,
      "learning_rate": 0.00018717375993560723,
      "loss": 0.5801,
      "step": 3192
    },
    {
      "epoch": 6.424547283702213,
      "grad_norm": 0.285332590341568,
      "learning_rate": 0.00018716973538585372,
      "loss": 0.6032,
      "step": 3193
    },
    {
      "epoch": 6.426559356136821,
      "grad_norm": 0.30445632338523865,
      "learning_rate": 0.00018716571083610023,
      "loss": 0.6444,
      "step": 3194
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 0.2942982017993927,
      "learning_rate": 0.00018716168628634671,
      "loss": 0.633,
      "step": 3195
    },
    {
      "epoch": 6.430583501006036,
      "grad_norm": 0.29123425483703613,
      "learning_rate": 0.00018715766173659323,
      "loss": 0.6298,
      "step": 3196
    },
    {
      "epoch": 6.4325955734406435,
      "grad_norm": 0.29829835891723633,
      "learning_rate": 0.00018715363718683974,
      "loss": 0.6543,
      "step": 3197
    },
    {
      "epoch": 6.434607645875252,
      "grad_norm": 0.27303168177604675,
      "learning_rate": 0.00018714961263708622,
      "loss": 0.6157,
      "step": 3198
    },
    {
      "epoch": 6.436619718309859,
      "grad_norm": 0.28708934783935547,
      "learning_rate": 0.00018714558808733274,
      "loss": 0.6519,
      "step": 3199
    },
    {
      "epoch": 6.438631790744466,
      "grad_norm": 0.28468891978263855,
      "learning_rate": 0.00018714156353757925,
      "loss": 0.6053,
      "step": 3200
    },
    {
      "epoch": 6.440643863179075,
      "grad_norm": 0.2774926722049713,
      "learning_rate": 0.00018713753898782576,
      "loss": 0.5911,
      "step": 3201
    },
    {
      "epoch": 6.442655935613682,
      "grad_norm": 0.3006489872932434,
      "learning_rate": 0.00018713351443807225,
      "loss": 0.6381,
      "step": 3202
    },
    {
      "epoch": 6.44466800804829,
      "grad_norm": 0.27617278695106506,
      "learning_rate": 0.00018712948988831876,
      "loss": 0.6448,
      "step": 3203
    },
    {
      "epoch": 6.446680080482897,
      "grad_norm": 0.30543652176856995,
      "learning_rate": 0.00018712546533856524,
      "loss": 0.5864,
      "step": 3204
    },
    {
      "epoch": 6.448692152917505,
      "grad_norm": 0.2820427119731903,
      "learning_rate": 0.00018712144078881175,
      "loss": 0.6006,
      "step": 3205
    },
    {
      "epoch": 6.450704225352113,
      "grad_norm": 0.3266999423503876,
      "learning_rate": 0.00018711741623905827,
      "loss": 0.6631,
      "step": 3206
    },
    {
      "epoch": 6.45271629778672,
      "grad_norm": 0.2834579050540924,
      "learning_rate": 0.00018711339168930478,
      "loss": 0.6273,
      "step": 3207
    },
    {
      "epoch": 6.454728370221328,
      "grad_norm": 0.2790171205997467,
      "learning_rate": 0.00018710936713955126,
      "loss": 0.5839,
      "step": 3208
    },
    {
      "epoch": 6.456740442655936,
      "grad_norm": 0.2910121977329254,
      "learning_rate": 0.00018710534258979778,
      "loss": 0.6403,
      "step": 3209
    },
    {
      "epoch": 6.458752515090543,
      "grad_norm": 0.2947225272655487,
      "learning_rate": 0.00018710131804004426,
      "loss": 0.6295,
      "step": 3210
    },
    {
      "epoch": 6.460764587525151,
      "grad_norm": 0.27582666277885437,
      "learning_rate": 0.0001870972934902908,
      "loss": 0.6169,
      "step": 3211
    },
    {
      "epoch": 6.462776659959759,
      "grad_norm": 0.28274455666542053,
      "learning_rate": 0.00018709326894053729,
      "loss": 0.6095,
      "step": 3212
    },
    {
      "epoch": 6.464788732394366,
      "grad_norm": 0.3101743161678314,
      "learning_rate": 0.0001870892443907838,
      "loss": 0.6565,
      "step": 3213
    },
    {
      "epoch": 6.466800804828974,
      "grad_norm": 0.2966960370540619,
      "learning_rate": 0.00018708521984103028,
      "loss": 0.5857,
      "step": 3214
    },
    {
      "epoch": 6.468812877263582,
      "grad_norm": 0.29385805130004883,
      "learning_rate": 0.0001870811952912768,
      "loss": 0.6498,
      "step": 3215
    },
    {
      "epoch": 6.470824949698189,
      "grad_norm": 0.2935580313205719,
      "learning_rate": 0.0001870771707415233,
      "loss": 0.6212,
      "step": 3216
    },
    {
      "epoch": 6.472837022132797,
      "grad_norm": 0.2977200150489807,
      "learning_rate": 0.00018707314619176982,
      "loss": 0.6275,
      "step": 3217
    },
    {
      "epoch": 6.474849094567404,
      "grad_norm": 0.28493914008140564,
      "learning_rate": 0.0001870691216420163,
      "loss": 0.6443,
      "step": 3218
    },
    {
      "epoch": 6.476861167002012,
      "grad_norm": 0.2837647795677185,
      "learning_rate": 0.00018706509709226282,
      "loss": 0.6604,
      "step": 3219
    },
    {
      "epoch": 6.47887323943662,
      "grad_norm": 0.28047946095466614,
      "learning_rate": 0.0001870610725425093,
      "loss": 0.6191,
      "step": 3220
    },
    {
      "epoch": 6.480885311871227,
      "grad_norm": 0.2860097289085388,
      "learning_rate": 0.00018705704799275584,
      "loss": 0.6671,
      "step": 3221
    },
    {
      "epoch": 6.482897384305835,
      "grad_norm": 0.3167630732059479,
      "learning_rate": 0.00018705302344300233,
      "loss": 0.6324,
      "step": 3222
    },
    {
      "epoch": 6.484909456740443,
      "grad_norm": 0.281125545501709,
      "learning_rate": 0.00018704899889324884,
      "loss": 0.6593,
      "step": 3223
    },
    {
      "epoch": 6.48692152917505,
      "grad_norm": 0.2861597537994385,
      "learning_rate": 0.00018704497434349532,
      "loss": 0.6348,
      "step": 3224
    },
    {
      "epoch": 6.4889336016096575,
      "grad_norm": 0.28705328702926636,
      "learning_rate": 0.00018704094979374184,
      "loss": 0.6769,
      "step": 3225
    },
    {
      "epoch": 6.490945674044266,
      "grad_norm": 0.2814541459083557,
      "learning_rate": 0.00018703692524398835,
      "loss": 0.6208,
      "step": 3226
    },
    {
      "epoch": 6.492957746478873,
      "grad_norm": 0.29266324639320374,
      "learning_rate": 0.00018703290069423483,
      "loss": 0.6233,
      "step": 3227
    },
    {
      "epoch": 6.494969818913481,
      "grad_norm": 0.27271324396133423,
      "learning_rate": 0.00018702887614448135,
      "loss": 0.6203,
      "step": 3228
    },
    {
      "epoch": 6.496981891348089,
      "grad_norm": 0.27986741065979004,
      "learning_rate": 0.00018702485159472786,
      "loss": 0.6422,
      "step": 3229
    },
    {
      "epoch": 6.498993963782696,
      "grad_norm": 0.26296618580818176,
      "learning_rate": 0.00018702082704497434,
      "loss": 0.5937,
      "step": 3230
    },
    {
      "epoch": 6.501006036217304,
      "grad_norm": 0.2932393550872803,
      "learning_rate": 0.00018701680249522086,
      "loss": 0.6012,
      "step": 3231
    },
    {
      "epoch": 6.503018108651911,
      "grad_norm": 0.2921523153781891,
      "learning_rate": 0.00018701277794546737,
      "loss": 0.6181,
      "step": 3232
    },
    {
      "epoch": 6.505030181086519,
      "grad_norm": 0.3041001558303833,
      "learning_rate": 0.00018700875339571385,
      "loss": 0.6008,
      "step": 3233
    },
    {
      "epoch": 6.507042253521127,
      "grad_norm": 0.28677940368652344,
      "learning_rate": 0.00018700472884596036,
      "loss": 0.5842,
      "step": 3234
    },
    {
      "epoch": 6.509054325955734,
      "grad_norm": 0.29706043004989624,
      "learning_rate": 0.00018700070429620685,
      "loss": 0.6405,
      "step": 3235
    },
    {
      "epoch": 6.5110663983903425,
      "grad_norm": 0.29666006565093994,
      "learning_rate": 0.0001869966797464534,
      "loss": 0.6665,
      "step": 3236
    },
    {
      "epoch": 6.51307847082495,
      "grad_norm": 0.2854311466217041,
      "learning_rate": 0.00018699265519669987,
      "loss": 0.613,
      "step": 3237
    },
    {
      "epoch": 6.515090543259557,
      "grad_norm": 0.2820456922054291,
      "learning_rate": 0.0001869886306469464,
      "loss": 0.6322,
      "step": 3238
    },
    {
      "epoch": 6.517102615694165,
      "grad_norm": 0.2842826843261719,
      "learning_rate": 0.00018698460609719287,
      "loss": 0.5794,
      "step": 3239
    },
    {
      "epoch": 6.519114688128773,
      "grad_norm": 0.3037363290786743,
      "learning_rate": 0.00018698058154743938,
      "loss": 0.6771,
      "step": 3240
    },
    {
      "epoch": 6.52112676056338,
      "grad_norm": 0.27291595935821533,
      "learning_rate": 0.0001869765569976859,
      "loss": 0.6215,
      "step": 3241
    },
    {
      "epoch": 6.523138832997988,
      "grad_norm": 0.27691489458084106,
      "learning_rate": 0.0001869725324479324,
      "loss": 0.6064,
      "step": 3242
    },
    {
      "epoch": 6.525150905432596,
      "grad_norm": 0.28090420365333557,
      "learning_rate": 0.0001869685078981789,
      "loss": 0.6421,
      "step": 3243
    },
    {
      "epoch": 6.527162977867203,
      "grad_norm": 0.2871590852737427,
      "learning_rate": 0.0001869644833484254,
      "loss": 0.6287,
      "step": 3244
    },
    {
      "epoch": 6.529175050301811,
      "grad_norm": 0.2901114821434021,
      "learning_rate": 0.0001869604587986719,
      "loss": 0.6423,
      "step": 3245
    },
    {
      "epoch": 6.531187122736418,
      "grad_norm": 0.27612802386283875,
      "learning_rate": 0.00018695643424891843,
      "loss": 0.6284,
      "step": 3246
    },
    {
      "epoch": 6.533199195171026,
      "grad_norm": 0.28213661909103394,
      "learning_rate": 0.00018695240969916492,
      "loss": 0.6386,
      "step": 3247
    },
    {
      "epoch": 6.535211267605634,
      "grad_norm": 0.30863964557647705,
      "learning_rate": 0.00018694838514941143,
      "loss": 0.6663,
      "step": 3248
    },
    {
      "epoch": 6.537223340040241,
      "grad_norm": 0.2925754189491272,
      "learning_rate": 0.0001869443605996579,
      "loss": 0.6522,
      "step": 3249
    },
    {
      "epoch": 6.539235412474849,
      "grad_norm": 0.2929423153400421,
      "learning_rate": 0.00018694033604990442,
      "loss": 0.6065,
      "step": 3250
    },
    {
      "epoch": 6.541247484909457,
      "grad_norm": 0.3055776059627533,
      "learning_rate": 0.00018693631150015094,
      "loss": 0.6254,
      "step": 3251
    },
    {
      "epoch": 6.543259557344064,
      "grad_norm": 0.2846772074699402,
      "learning_rate": 0.00018693228695039745,
      "loss": 0.6078,
      "step": 3252
    },
    {
      "epoch": 6.545271629778672,
      "grad_norm": 0.29730746150016785,
      "learning_rate": 0.00018692826240064393,
      "loss": 0.6507,
      "step": 3253
    },
    {
      "epoch": 6.54728370221328,
      "grad_norm": 0.30001893639564514,
      "learning_rate": 0.00018692423785089045,
      "loss": 0.6621,
      "step": 3254
    },
    {
      "epoch": 6.549295774647887,
      "grad_norm": 0.28414973616600037,
      "learning_rate": 0.00018692021330113693,
      "loss": 0.6191,
      "step": 3255
    },
    {
      "epoch": 6.551307847082495,
      "grad_norm": 0.2978362441062927,
      "learning_rate": 0.00018691618875138347,
      "loss": 0.6305,
      "step": 3256
    },
    {
      "epoch": 6.553319919517103,
      "grad_norm": 0.311535507440567,
      "learning_rate": 0.00018691216420162996,
      "loss": 0.6421,
      "step": 3257
    },
    {
      "epoch": 6.55533199195171,
      "grad_norm": 0.296424925327301,
      "learning_rate": 0.00018690813965187647,
      "loss": 0.6395,
      "step": 3258
    },
    {
      "epoch": 6.557344064386318,
      "grad_norm": 0.27578118443489075,
      "learning_rate": 0.00018690411510212295,
      "loss": 0.6467,
      "step": 3259
    },
    {
      "epoch": 6.559356136820925,
      "grad_norm": 0.28745344281196594,
      "learning_rate": 0.00018690009055236947,
      "loss": 0.6355,
      "step": 3260
    },
    {
      "epoch": 6.561368209255534,
      "grad_norm": 0.2879126965999603,
      "learning_rate": 0.00018689606600261598,
      "loss": 0.6344,
      "step": 3261
    },
    {
      "epoch": 6.563380281690141,
      "grad_norm": 0.2904759645462036,
      "learning_rate": 0.00018689204145286246,
      "loss": 0.592,
      "step": 3262
    },
    {
      "epoch": 6.565392354124748,
      "grad_norm": 0.2925150394439697,
      "learning_rate": 0.00018688801690310898,
      "loss": 0.6239,
      "step": 3263
    },
    {
      "epoch": 6.5674044265593565,
      "grad_norm": 0.2854691445827484,
      "learning_rate": 0.0001868839923533555,
      "loss": 0.6157,
      "step": 3264
    },
    {
      "epoch": 6.569416498993964,
      "grad_norm": 0.29230043292045593,
      "learning_rate": 0.00018687996780360197,
      "loss": 0.6592,
      "step": 3265
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 0.29435834288597107,
      "learning_rate": 0.00018687594325384848,
      "loss": 0.6255,
      "step": 3266
    },
    {
      "epoch": 6.573440643863179,
      "grad_norm": 0.28273123502731323,
      "learning_rate": 0.000186871918704095,
      "loss": 0.6303,
      "step": 3267
    },
    {
      "epoch": 6.575452716297787,
      "grad_norm": 0.2838943898677826,
      "learning_rate": 0.00018686789415434148,
      "loss": 0.6083,
      "step": 3268
    },
    {
      "epoch": 6.577464788732394,
      "grad_norm": 0.29869213700294495,
      "learning_rate": 0.000186863869604588,
      "loss": 0.6736,
      "step": 3269
    },
    {
      "epoch": 6.579476861167002,
      "grad_norm": 0.29015451669692993,
      "learning_rate": 0.00018685984505483448,
      "loss": 0.6153,
      "step": 3270
    },
    {
      "epoch": 6.58148893360161,
      "grad_norm": 0.28659588098526,
      "learning_rate": 0.00018685582050508102,
      "loss": 0.6051,
      "step": 3271
    },
    {
      "epoch": 6.583501006036217,
      "grad_norm": 0.2759647071361542,
      "learning_rate": 0.0001868517959553275,
      "loss": 0.6157,
      "step": 3272
    },
    {
      "epoch": 6.585513078470825,
      "grad_norm": 0.2843998074531555,
      "learning_rate": 0.00018684777140557402,
      "loss": 0.6115,
      "step": 3273
    },
    {
      "epoch": 6.5875251509054324,
      "grad_norm": 0.28715088963508606,
      "learning_rate": 0.0001868437468558205,
      "loss": 0.6606,
      "step": 3274
    },
    {
      "epoch": 6.58953722334004,
      "grad_norm": 0.2755161225795746,
      "learning_rate": 0.000186839722306067,
      "loss": 0.6201,
      "step": 3275
    },
    {
      "epoch": 6.591549295774648,
      "grad_norm": 0.29389968514442444,
      "learning_rate": 0.00018683569775631353,
      "loss": 0.6028,
      "step": 3276
    },
    {
      "epoch": 6.593561368209255,
      "grad_norm": 0.2950966954231262,
      "learning_rate": 0.00018683167320656004,
      "loss": 0.6353,
      "step": 3277
    },
    {
      "epoch": 6.5955734406438635,
      "grad_norm": 0.27720460295677185,
      "learning_rate": 0.00018682764865680652,
      "loss": 0.637,
      "step": 3278
    },
    {
      "epoch": 6.597585513078471,
      "grad_norm": 0.28648272156715393,
      "learning_rate": 0.00018682362410705304,
      "loss": 0.6364,
      "step": 3279
    },
    {
      "epoch": 6.599597585513078,
      "grad_norm": 0.28642621636390686,
      "learning_rate": 0.00018681959955729952,
      "loss": 0.6145,
      "step": 3280
    },
    {
      "epoch": 6.601609657947686,
      "grad_norm": 0.29321980476379395,
      "learning_rate": 0.00018681557500754603,
      "loss": 0.6429,
      "step": 3281
    },
    {
      "epoch": 6.603621730382294,
      "grad_norm": 0.2942180633544922,
      "learning_rate": 0.00018681155045779254,
      "loss": 0.6227,
      "step": 3282
    },
    {
      "epoch": 6.605633802816901,
      "grad_norm": 0.28430667519569397,
      "learning_rate": 0.00018680752590803906,
      "loss": 0.6264,
      "step": 3283
    },
    {
      "epoch": 6.607645875251509,
      "grad_norm": 0.2798367738723755,
      "learning_rate": 0.00018680350135828554,
      "loss": 0.6051,
      "step": 3284
    },
    {
      "epoch": 6.609657947686117,
      "grad_norm": 0.27878400683403015,
      "learning_rate": 0.00018679947680853205,
      "loss": 0.6588,
      "step": 3285
    },
    {
      "epoch": 6.611670020120725,
      "grad_norm": 0.29434144496917725,
      "learning_rate": 0.00018679545225877854,
      "loss": 0.6367,
      "step": 3286
    },
    {
      "epoch": 6.613682092555332,
      "grad_norm": 0.2712564468383789,
      "learning_rate": 0.00018679142770902508,
      "loss": 0.646,
      "step": 3287
    },
    {
      "epoch": 6.6156941649899395,
      "grad_norm": 0.2850381135940552,
      "learning_rate": 0.00018678740315927156,
      "loss": 0.6727,
      "step": 3288
    },
    {
      "epoch": 6.617706237424548,
      "grad_norm": 0.27271315455436707,
      "learning_rate": 0.00018678337860951808,
      "loss": 0.6422,
      "step": 3289
    },
    {
      "epoch": 6.619718309859155,
      "grad_norm": 0.28918465971946716,
      "learning_rate": 0.00018677935405976456,
      "loss": 0.6299,
      "step": 3290
    },
    {
      "epoch": 6.621730382293762,
      "grad_norm": 0.2902395725250244,
      "learning_rate": 0.00018677532951001107,
      "loss": 0.5957,
      "step": 3291
    },
    {
      "epoch": 6.6237424547283705,
      "grad_norm": 0.2875104248523712,
      "learning_rate": 0.00018677130496025759,
      "loss": 0.6371,
      "step": 3292
    },
    {
      "epoch": 6.625754527162978,
      "grad_norm": 0.3035053312778473,
      "learning_rate": 0.0001867672804105041,
      "loss": 0.6615,
      "step": 3293
    },
    {
      "epoch": 6.627766599597585,
      "grad_norm": 0.2879751920700073,
      "learning_rate": 0.00018676325586075058,
      "loss": 0.6227,
      "step": 3294
    },
    {
      "epoch": 6.629778672032193,
      "grad_norm": 0.2845781147480011,
      "learning_rate": 0.0001867592313109971,
      "loss": 0.5961,
      "step": 3295
    },
    {
      "epoch": 6.631790744466801,
      "grad_norm": 0.2848626673221588,
      "learning_rate": 0.00018675520676124358,
      "loss": 0.5789,
      "step": 3296
    },
    {
      "epoch": 6.633802816901408,
      "grad_norm": 0.28561264276504517,
      "learning_rate": 0.0001867511822114901,
      "loss": 0.6606,
      "step": 3297
    },
    {
      "epoch": 6.635814889336016,
      "grad_norm": 0.28785890340805054,
      "learning_rate": 0.0001867471576617366,
      "loss": 0.6284,
      "step": 3298
    },
    {
      "epoch": 6.637826961770624,
      "grad_norm": 0.28871485590934753,
      "learning_rate": 0.00018674313311198312,
      "loss": 0.5969,
      "step": 3299
    },
    {
      "epoch": 6.639839034205231,
      "grad_norm": 0.27902287244796753,
      "learning_rate": 0.0001867391085622296,
      "loss": 0.6229,
      "step": 3300
    },
    {
      "epoch": 6.641851106639839,
      "grad_norm": 0.29525652527809143,
      "learning_rate": 0.00018673508401247611,
      "loss": 0.6127,
      "step": 3301
    },
    {
      "epoch": 6.6438631790744465,
      "grad_norm": 0.2803886830806732,
      "learning_rate": 0.00018673105946272263,
      "loss": 0.6071,
      "step": 3302
    },
    {
      "epoch": 6.645875251509055,
      "grad_norm": 0.28312569856643677,
      "learning_rate": 0.0001867270349129691,
      "loss": 0.6341,
      "step": 3303
    },
    {
      "epoch": 6.647887323943662,
      "grad_norm": 0.30053970217704773,
      "learning_rate": 0.00018672301036321562,
      "loss": 0.6725,
      "step": 3304
    },
    {
      "epoch": 6.649899396378269,
      "grad_norm": 0.27114900946617126,
      "learning_rate": 0.0001867189858134621,
      "loss": 0.6291,
      "step": 3305
    },
    {
      "epoch": 6.6519114688128775,
      "grad_norm": 0.30176523327827454,
      "learning_rate": 0.00018671496126370862,
      "loss": 0.6012,
      "step": 3306
    },
    {
      "epoch": 6.653923541247485,
      "grad_norm": 0.2785022556781769,
      "learning_rate": 0.00018671093671395513,
      "loss": 0.6478,
      "step": 3307
    },
    {
      "epoch": 6.655935613682092,
      "grad_norm": 0.2858012914657593,
      "learning_rate": 0.00018670691216420165,
      "loss": 0.6232,
      "step": 3308
    },
    {
      "epoch": 6.6579476861167,
      "grad_norm": 0.29933425784111023,
      "learning_rate": 0.00018670288761444813,
      "loss": 0.601,
      "step": 3309
    },
    {
      "epoch": 6.659959758551308,
      "grad_norm": 0.28730738162994385,
      "learning_rate": 0.00018669886306469464,
      "loss": 0.5889,
      "step": 3310
    },
    {
      "epoch": 6.661971830985916,
      "grad_norm": 0.30787020921707153,
      "learning_rate": 0.00018669483851494113,
      "loss": 0.6127,
      "step": 3311
    },
    {
      "epoch": 6.663983903420523,
      "grad_norm": 0.3109688460826874,
      "learning_rate": 0.00018669081396518767,
      "loss": 0.6486,
      "step": 3312
    },
    {
      "epoch": 6.665995975855131,
      "grad_norm": 0.3100365698337555,
      "learning_rate": 0.00018668678941543415,
      "loss": 0.6252,
      "step": 3313
    },
    {
      "epoch": 6.668008048289739,
      "grad_norm": 0.27846482396125793,
      "learning_rate": 0.00018668276486568066,
      "loss": 0.6176,
      "step": 3314
    },
    {
      "epoch": 6.670020120724346,
      "grad_norm": 0.28472405672073364,
      "learning_rate": 0.00018667874031592715,
      "loss": 0.6457,
      "step": 3315
    },
    {
      "epoch": 6.6720321931589535,
      "grad_norm": 0.29859721660614014,
      "learning_rate": 0.00018667471576617366,
      "loss": 0.6546,
      "step": 3316
    },
    {
      "epoch": 6.674044265593562,
      "grad_norm": 0.29196983575820923,
      "learning_rate": 0.00018667069121642017,
      "loss": 0.6066,
      "step": 3317
    },
    {
      "epoch": 6.676056338028169,
      "grad_norm": 0.28201204538345337,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.6416,
      "step": 3318
    },
    {
      "epoch": 6.678068410462776,
      "grad_norm": 0.2853355407714844,
      "learning_rate": 0.00018666264211691317,
      "loss": 0.6013,
      "step": 3319
    },
    {
      "epoch": 6.6800804828973845,
      "grad_norm": 0.30878710746765137,
      "learning_rate": 0.00018665861756715968,
      "loss": 0.6659,
      "step": 3320
    },
    {
      "epoch": 6.682092555331992,
      "grad_norm": 0.3054582178592682,
      "learning_rate": 0.00018665459301740617,
      "loss": 0.6428,
      "step": 3321
    },
    {
      "epoch": 6.684104627766599,
      "grad_norm": 0.30063608288764954,
      "learning_rate": 0.0001866505684676527,
      "loss": 0.6176,
      "step": 3322
    },
    {
      "epoch": 6.686116700201207,
      "grad_norm": 0.2866707146167755,
      "learning_rate": 0.0001866465439178992,
      "loss": 0.6286,
      "step": 3323
    },
    {
      "epoch": 6.688128772635815,
      "grad_norm": 0.29475194215774536,
      "learning_rate": 0.0001866425193681457,
      "loss": 0.6357,
      "step": 3324
    },
    {
      "epoch": 6.690140845070422,
      "grad_norm": 0.29189929366111755,
      "learning_rate": 0.0001866384948183922,
      "loss": 0.6297,
      "step": 3325
    },
    {
      "epoch": 6.69215291750503,
      "grad_norm": 0.2925298511981964,
      "learning_rate": 0.0001866344702686387,
      "loss": 0.6647,
      "step": 3326
    },
    {
      "epoch": 6.694164989939638,
      "grad_norm": 0.28234270215034485,
      "learning_rate": 0.00018663044571888522,
      "loss": 0.6372,
      "step": 3327
    },
    {
      "epoch": 6.696177062374246,
      "grad_norm": 0.2807905673980713,
      "learning_rate": 0.00018662642116913173,
      "loss": 0.6331,
      "step": 3328
    },
    {
      "epoch": 6.698189134808853,
      "grad_norm": 0.28312838077545166,
      "learning_rate": 0.0001866223966193782,
      "loss": 0.6704,
      "step": 3329
    },
    {
      "epoch": 6.7002012072434605,
      "grad_norm": 0.27901479601860046,
      "learning_rate": 0.00018661837206962472,
      "loss": 0.5963,
      "step": 3330
    },
    {
      "epoch": 6.702213279678069,
      "grad_norm": 0.28873029351234436,
      "learning_rate": 0.0001866143475198712,
      "loss": 0.6377,
      "step": 3331
    },
    {
      "epoch": 6.704225352112676,
      "grad_norm": 0.2859089970588684,
      "learning_rate": 0.00018661032297011772,
      "loss": 0.6693,
      "step": 3332
    },
    {
      "epoch": 6.706237424547283,
      "grad_norm": 0.2975480854511261,
      "learning_rate": 0.00018660629842036423,
      "loss": 0.6255,
      "step": 3333
    },
    {
      "epoch": 6.7082494969818915,
      "grad_norm": 0.28717130422592163,
      "learning_rate": 0.00018660227387061075,
      "loss": 0.6299,
      "step": 3334
    },
    {
      "epoch": 6.710261569416499,
      "grad_norm": 0.30042505264282227,
      "learning_rate": 0.00018659824932085723,
      "loss": 0.6466,
      "step": 3335
    },
    {
      "epoch": 6.712273641851107,
      "grad_norm": 0.2811654806137085,
      "learning_rate": 0.00018659422477110374,
      "loss": 0.64,
      "step": 3336
    },
    {
      "epoch": 6.714285714285714,
      "grad_norm": 0.2937040627002716,
      "learning_rate": 0.00018659020022135026,
      "loss": 0.6611,
      "step": 3337
    },
    {
      "epoch": 6.716297786720322,
      "grad_norm": 0.2900322675704956,
      "learning_rate": 0.00018658617567159674,
      "loss": 0.5817,
      "step": 3338
    },
    {
      "epoch": 6.71830985915493,
      "grad_norm": 0.27571430802345276,
      "learning_rate": 0.00018658215112184325,
      "loss": 0.6069,
      "step": 3339
    },
    {
      "epoch": 6.720321931589537,
      "grad_norm": 0.290199339389801,
      "learning_rate": 0.00018657812657208974,
      "loss": 0.6356,
      "step": 3340
    },
    {
      "epoch": 6.722334004024145,
      "grad_norm": 0.2890339493751526,
      "learning_rate": 0.00018657410202233625,
      "loss": 0.6329,
      "step": 3341
    },
    {
      "epoch": 6.724346076458753,
      "grad_norm": 0.2929695248603821,
      "learning_rate": 0.00018657007747258276,
      "loss": 0.6449,
      "step": 3342
    },
    {
      "epoch": 6.72635814889336,
      "grad_norm": 0.301442414522171,
      "learning_rate": 0.00018656605292282928,
      "loss": 0.6207,
      "step": 3343
    },
    {
      "epoch": 6.7283702213279675,
      "grad_norm": 0.2794228494167328,
      "learning_rate": 0.00018656202837307576,
      "loss": 0.6038,
      "step": 3344
    },
    {
      "epoch": 6.730382293762576,
      "grad_norm": 0.2810283303260803,
      "learning_rate": 0.00018655800382332227,
      "loss": 0.6146,
      "step": 3345
    },
    {
      "epoch": 6.732394366197183,
      "grad_norm": 0.26786184310913086,
      "learning_rate": 0.00018655397927356876,
      "loss": 0.6383,
      "step": 3346
    },
    {
      "epoch": 6.73440643863179,
      "grad_norm": 0.297162801027298,
      "learning_rate": 0.0001865499547238153,
      "loss": 0.5966,
      "step": 3347
    },
    {
      "epoch": 6.7364185110663986,
      "grad_norm": 0.30820250511169434,
      "learning_rate": 0.00018654593017406178,
      "loss": 0.6256,
      "step": 3348
    },
    {
      "epoch": 6.738430583501006,
      "grad_norm": 0.2872162461280823,
      "learning_rate": 0.0001865419056243083,
      "loss": 0.6395,
      "step": 3349
    },
    {
      "epoch": 6.740442655935613,
      "grad_norm": 0.302860289812088,
      "learning_rate": 0.00018653788107455478,
      "loss": 0.5867,
      "step": 3350
    },
    {
      "epoch": 6.742454728370221,
      "grad_norm": 0.29904457926750183,
      "learning_rate": 0.0001865338565248013,
      "loss": 0.624,
      "step": 3351
    },
    {
      "epoch": 6.744466800804829,
      "grad_norm": 0.29921746253967285,
      "learning_rate": 0.0001865298319750478,
      "loss": 0.6219,
      "step": 3352
    },
    {
      "epoch": 6.746478873239437,
      "grad_norm": 0.2836242616176605,
      "learning_rate": 0.00018652580742529432,
      "loss": 0.6052,
      "step": 3353
    },
    {
      "epoch": 6.748490945674044,
      "grad_norm": 0.2938081920146942,
      "learning_rate": 0.0001865217828755408,
      "loss": 0.6286,
      "step": 3354
    },
    {
      "epoch": 6.750503018108652,
      "grad_norm": 0.3081355392932892,
      "learning_rate": 0.0001865177583257873,
      "loss": 0.6325,
      "step": 3355
    },
    {
      "epoch": 6.75251509054326,
      "grad_norm": 0.2861347794532776,
      "learning_rate": 0.0001865137337760338,
      "loss": 0.678,
      "step": 3356
    },
    {
      "epoch": 6.754527162977867,
      "grad_norm": 0.28739893436431885,
      "learning_rate": 0.00018650970922628034,
      "loss": 0.578,
      "step": 3357
    },
    {
      "epoch": 6.7565392354124745,
      "grad_norm": 0.29866352677345276,
      "learning_rate": 0.00018650568467652682,
      "loss": 0.6362,
      "step": 3358
    },
    {
      "epoch": 6.758551307847083,
      "grad_norm": 0.2859703302383423,
      "learning_rate": 0.00018650166012677333,
      "loss": 0.6545,
      "step": 3359
    },
    {
      "epoch": 6.76056338028169,
      "grad_norm": 0.2779940962791443,
      "learning_rate": 0.00018649763557701982,
      "loss": 0.6352,
      "step": 3360
    },
    {
      "epoch": 6.762575452716298,
      "grad_norm": 0.27890828251838684,
      "learning_rate": 0.00018649361102726633,
      "loss": 0.6111,
      "step": 3361
    },
    {
      "epoch": 6.7645875251509056,
      "grad_norm": 0.2928635776042938,
      "learning_rate": 0.00018648958647751284,
      "loss": 0.6259,
      "step": 3362
    },
    {
      "epoch": 6.766599597585513,
      "grad_norm": 0.27525803446769714,
      "learning_rate": 0.00018648556192775936,
      "loss": 0.6542,
      "step": 3363
    },
    {
      "epoch": 6.768611670020121,
      "grad_norm": 0.28086262941360474,
      "learning_rate": 0.00018648153737800584,
      "loss": 0.634,
      "step": 3364
    },
    {
      "epoch": 6.770623742454728,
      "grad_norm": 0.29486551880836487,
      "learning_rate": 0.00018647751282825235,
      "loss": 0.641,
      "step": 3365
    },
    {
      "epoch": 6.772635814889336,
      "grad_norm": 0.2907952070236206,
      "learning_rate": 0.00018647348827849884,
      "loss": 0.6307,
      "step": 3366
    },
    {
      "epoch": 6.774647887323944,
      "grad_norm": 0.2792438268661499,
      "learning_rate": 0.00018646946372874535,
      "loss": 0.642,
      "step": 3367
    },
    {
      "epoch": 6.776659959758551,
      "grad_norm": 0.27423006296157837,
      "learning_rate": 0.00018646543917899186,
      "loss": 0.5991,
      "step": 3368
    },
    {
      "epoch": 6.778672032193159,
      "grad_norm": 0.26846396923065186,
      "learning_rate": 0.00018646141462923838,
      "loss": 0.5986,
      "step": 3369
    },
    {
      "epoch": 6.780684104627767,
      "grad_norm": 0.296074777841568,
      "learning_rate": 0.00018645739007948486,
      "loss": 0.6644,
      "step": 3370
    },
    {
      "epoch": 6.782696177062374,
      "grad_norm": 0.2909877896308899,
      "learning_rate": 0.00018645336552973137,
      "loss": 0.6643,
      "step": 3371
    },
    {
      "epoch": 6.7847082494969815,
      "grad_norm": 0.2931733727455139,
      "learning_rate": 0.00018644934097997789,
      "loss": 0.5849,
      "step": 3372
    },
    {
      "epoch": 6.78672032193159,
      "grad_norm": 0.28499171137809753,
      "learning_rate": 0.00018644531643022437,
      "loss": 0.6075,
      "step": 3373
    },
    {
      "epoch": 6.788732394366197,
      "grad_norm": 0.29628413915634155,
      "learning_rate": 0.00018644129188047088,
      "loss": 0.6516,
      "step": 3374
    },
    {
      "epoch": 6.790744466800804,
      "grad_norm": 0.266171395778656,
      "learning_rate": 0.00018643726733071737,
      "loss": 0.5708,
      "step": 3375
    },
    {
      "epoch": 6.792756539235413,
      "grad_norm": 0.2727988064289093,
      "learning_rate": 0.00018643324278096388,
      "loss": 0.6062,
      "step": 3376
    },
    {
      "epoch": 6.79476861167002,
      "grad_norm": 0.2901458442211151,
      "learning_rate": 0.0001864292182312104,
      "loss": 0.6446,
      "step": 3377
    },
    {
      "epoch": 6.796780684104628,
      "grad_norm": 0.2982393503189087,
      "learning_rate": 0.0001864251936814569,
      "loss": 0.6368,
      "step": 3378
    },
    {
      "epoch": 6.798792756539235,
      "grad_norm": 0.3072792887687683,
      "learning_rate": 0.0001864211691317034,
      "loss": 0.6174,
      "step": 3379
    },
    {
      "epoch": 6.800804828973843,
      "grad_norm": 0.2939176857471466,
      "learning_rate": 0.0001864171445819499,
      "loss": 0.6457,
      "step": 3380
    },
    {
      "epoch": 6.802816901408451,
      "grad_norm": 0.29315614700317383,
      "learning_rate": 0.0001864131200321964,
      "loss": 0.6177,
      "step": 3381
    },
    {
      "epoch": 6.804828973843058,
      "grad_norm": 0.27288302779197693,
      "learning_rate": 0.00018640909548244293,
      "loss": 0.5811,
      "step": 3382
    },
    {
      "epoch": 6.806841046277666,
      "grad_norm": 0.2985702455043793,
      "learning_rate": 0.0001864050709326894,
      "loss": 0.6392,
      "step": 3383
    },
    {
      "epoch": 6.808853118712274,
      "grad_norm": 0.28694406151771545,
      "learning_rate": 0.00018640104638293592,
      "loss": 0.6251,
      "step": 3384
    },
    {
      "epoch": 6.810865191146881,
      "grad_norm": 0.2684195339679718,
      "learning_rate": 0.0001863970218331824,
      "loss": 0.5919,
      "step": 3385
    },
    {
      "epoch": 6.812877263581489,
      "grad_norm": 0.2858060598373413,
      "learning_rate": 0.00018639299728342892,
      "loss": 0.6223,
      "step": 3386
    },
    {
      "epoch": 6.814889336016097,
      "grad_norm": 0.2781445384025574,
      "learning_rate": 0.00018638897273367543,
      "loss": 0.6232,
      "step": 3387
    },
    {
      "epoch": 6.816901408450704,
      "grad_norm": 0.29670771956443787,
      "learning_rate": 0.00018638494818392195,
      "loss": 0.6665,
      "step": 3388
    },
    {
      "epoch": 6.818913480885312,
      "grad_norm": 0.2853619456291199,
      "learning_rate": 0.00018638092363416843,
      "loss": 0.6343,
      "step": 3389
    },
    {
      "epoch": 6.82092555331992,
      "grad_norm": 0.28880980610847473,
      "learning_rate": 0.00018637689908441494,
      "loss": 0.6139,
      "step": 3390
    },
    {
      "epoch": 6.822937625754527,
      "grad_norm": 0.26739537715911865,
      "learning_rate": 0.00018637287453466143,
      "loss": 0.6011,
      "step": 3391
    },
    {
      "epoch": 6.824949698189135,
      "grad_norm": 0.3097425699234009,
      "learning_rate": 0.00018636884998490797,
      "loss": 0.6188,
      "step": 3392
    },
    {
      "epoch": 6.826961770623742,
      "grad_norm": 0.2852710485458374,
      "learning_rate": 0.00018636482543515445,
      "loss": 0.6354,
      "step": 3393
    },
    {
      "epoch": 6.82897384305835,
      "grad_norm": 0.27468541264533997,
      "learning_rate": 0.00018636080088540096,
      "loss": 0.6051,
      "step": 3394
    },
    {
      "epoch": 6.830985915492958,
      "grad_norm": 0.2873722016811371,
      "learning_rate": 0.00018635677633564745,
      "loss": 0.6423,
      "step": 3395
    },
    {
      "epoch": 6.832997987927565,
      "grad_norm": 0.2685464024543762,
      "learning_rate": 0.00018635275178589396,
      "loss": 0.6682,
      "step": 3396
    },
    {
      "epoch": 6.835010060362173,
      "grad_norm": 0.2758699357509613,
      "learning_rate": 0.00018634872723614047,
      "loss": 0.6187,
      "step": 3397
    },
    {
      "epoch": 6.837022132796781,
      "grad_norm": 0.2769030034542084,
      "learning_rate": 0.00018634470268638699,
      "loss": 0.6114,
      "step": 3398
    },
    {
      "epoch": 6.839034205231388,
      "grad_norm": 0.2845545709133148,
      "learning_rate": 0.00018634067813663347,
      "loss": 0.6865,
      "step": 3399
    },
    {
      "epoch": 6.8410462776659955,
      "grad_norm": 0.2834662199020386,
      "learning_rate": 0.00018633665358687998,
      "loss": 0.6152,
      "step": 3400
    },
    {
      "epoch": 6.843058350100604,
      "grad_norm": 0.280387818813324,
      "learning_rate": 0.00018633262903712647,
      "loss": 0.6146,
      "step": 3401
    },
    {
      "epoch": 6.845070422535211,
      "grad_norm": 0.30159106850624084,
      "learning_rate": 0.00018632860448737298,
      "loss": 0.5994,
      "step": 3402
    },
    {
      "epoch": 6.847082494969819,
      "grad_norm": 0.2702931761741638,
      "learning_rate": 0.0001863245799376195,
      "loss": 0.6489,
      "step": 3403
    },
    {
      "epoch": 6.849094567404427,
      "grad_norm": 0.27820831537246704,
      "learning_rate": 0.00018632055538786598,
      "loss": 0.665,
      "step": 3404
    },
    {
      "epoch": 6.851106639839034,
      "grad_norm": 0.28738704323768616,
      "learning_rate": 0.0001863165308381125,
      "loss": 0.6504,
      "step": 3405
    },
    {
      "epoch": 6.853118712273642,
      "grad_norm": 0.26218777894973755,
      "learning_rate": 0.000186312506288359,
      "loss": 0.6009,
      "step": 3406
    },
    {
      "epoch": 6.855130784708249,
      "grad_norm": 0.2878638803958893,
      "learning_rate": 0.00018630848173860551,
      "loss": 0.6488,
      "step": 3407
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 0.2716931700706482,
      "learning_rate": 0.000186304457188852,
      "loss": 0.628,
      "step": 3408
    },
    {
      "epoch": 6.859154929577465,
      "grad_norm": 0.27445173263549805,
      "learning_rate": 0.0001863004326390985,
      "loss": 0.6322,
      "step": 3409
    },
    {
      "epoch": 6.861167002012072,
      "grad_norm": 0.285008043050766,
      "learning_rate": 0.000186296408089345,
      "loss": 0.6403,
      "step": 3410
    },
    {
      "epoch": 6.8631790744466805,
      "grad_norm": 0.2863132953643799,
      "learning_rate": 0.0001862923835395915,
      "loss": 0.6204,
      "step": 3411
    },
    {
      "epoch": 6.865191146881288,
      "grad_norm": 0.2798987627029419,
      "learning_rate": 0.00018628835898983802,
      "loss": 0.6624,
      "step": 3412
    },
    {
      "epoch": 6.867203219315895,
      "grad_norm": 0.29643136262893677,
      "learning_rate": 0.00018628433444008453,
      "loss": 0.6377,
      "step": 3413
    },
    {
      "epoch": 6.869215291750503,
      "grad_norm": 0.3134288787841797,
      "learning_rate": 0.00018628030989033102,
      "loss": 0.6607,
      "step": 3414
    },
    {
      "epoch": 6.871227364185111,
      "grad_norm": 0.2862496078014374,
      "learning_rate": 0.00018627628534057753,
      "loss": 0.6283,
      "step": 3415
    },
    {
      "epoch": 6.873239436619718,
      "grad_norm": 0.2623264789581299,
      "learning_rate": 0.00018627226079082402,
      "loss": 0.6078,
      "step": 3416
    },
    {
      "epoch": 6.875251509054326,
      "grad_norm": 0.2725905478000641,
      "learning_rate": 0.00018626823624107056,
      "loss": 0.5897,
      "step": 3417
    },
    {
      "epoch": 6.877263581488934,
      "grad_norm": 0.2706768810749054,
      "learning_rate": 0.00018626421169131704,
      "loss": 0.5931,
      "step": 3418
    },
    {
      "epoch": 6.879275653923541,
      "grad_norm": 0.28803274035453796,
      "learning_rate": 0.00018626018714156355,
      "loss": 0.6589,
      "step": 3419
    },
    {
      "epoch": 6.881287726358149,
      "grad_norm": 0.2824598252773285,
      "learning_rate": 0.00018625616259181004,
      "loss": 0.6618,
      "step": 3420
    },
    {
      "epoch": 6.883299798792756,
      "grad_norm": 0.28779953718185425,
      "learning_rate": 0.00018625213804205655,
      "loss": 0.6145,
      "step": 3421
    },
    {
      "epoch": 6.885311871227364,
      "grad_norm": 0.2812255024909973,
      "learning_rate": 0.00018624811349230306,
      "loss": 0.6027,
      "step": 3422
    },
    {
      "epoch": 6.887323943661972,
      "grad_norm": 0.2784578204154968,
      "learning_rate": 0.00018624408894254957,
      "loss": 0.6225,
      "step": 3423
    },
    {
      "epoch": 6.889336016096579,
      "grad_norm": 0.2906556725502014,
      "learning_rate": 0.00018624006439279606,
      "loss": 0.6369,
      "step": 3424
    },
    {
      "epoch": 6.891348088531187,
      "grad_norm": 0.29412713646888733,
      "learning_rate": 0.00018623603984304257,
      "loss": 0.6266,
      "step": 3425
    },
    {
      "epoch": 6.893360160965795,
      "grad_norm": 0.2825610935688019,
      "learning_rate": 0.00018623201529328906,
      "loss": 0.6051,
      "step": 3426
    },
    {
      "epoch": 6.895372233400402,
      "grad_norm": 0.26279544830322266,
      "learning_rate": 0.0001862279907435356,
      "loss": 0.6407,
      "step": 3427
    },
    {
      "epoch": 6.89738430583501,
      "grad_norm": 0.29292812943458557,
      "learning_rate": 0.00018622396619378208,
      "loss": 0.62,
      "step": 3428
    },
    {
      "epoch": 6.899396378269618,
      "grad_norm": 0.28406527638435364,
      "learning_rate": 0.0001862199416440286,
      "loss": 0.6445,
      "step": 3429
    },
    {
      "epoch": 6.901408450704225,
      "grad_norm": 0.26695480942726135,
      "learning_rate": 0.00018621591709427508,
      "loss": 0.6185,
      "step": 3430
    },
    {
      "epoch": 6.903420523138833,
      "grad_norm": 0.28700846433639526,
      "learning_rate": 0.0001862118925445216,
      "loss": 0.6293,
      "step": 3431
    },
    {
      "epoch": 6.905432595573441,
      "grad_norm": 0.28822585940361023,
      "learning_rate": 0.0001862078679947681,
      "loss": 0.6386,
      "step": 3432
    },
    {
      "epoch": 6.907444668008048,
      "grad_norm": 0.28369736671447754,
      "learning_rate": 0.00018620384344501462,
      "loss": 0.6496,
      "step": 3433
    },
    {
      "epoch": 6.909456740442656,
      "grad_norm": 0.2815188467502594,
      "learning_rate": 0.0001861998188952611,
      "loss": 0.5971,
      "step": 3434
    },
    {
      "epoch": 6.9114688128772634,
      "grad_norm": 0.27904078364372253,
      "learning_rate": 0.0001861957943455076,
      "loss": 0.6571,
      "step": 3435
    },
    {
      "epoch": 6.913480885311872,
      "grad_norm": 0.2935665547847748,
      "learning_rate": 0.0001861917697957541,
      "loss": 0.5846,
      "step": 3436
    },
    {
      "epoch": 6.915492957746479,
      "grad_norm": 0.2935965061187744,
      "learning_rate": 0.0001861877452460006,
      "loss": 0.6736,
      "step": 3437
    },
    {
      "epoch": 6.917505030181086,
      "grad_norm": 0.2782779335975647,
      "learning_rate": 0.00018618372069624712,
      "loss": 0.6167,
      "step": 3438
    },
    {
      "epoch": 6.9195171026156945,
      "grad_norm": 0.2934304177761078,
      "learning_rate": 0.0001861796961464936,
      "loss": 0.6192,
      "step": 3439
    },
    {
      "epoch": 6.921529175050302,
      "grad_norm": 0.27650266885757446,
      "learning_rate": 0.00018617567159674012,
      "loss": 0.5815,
      "step": 3440
    },
    {
      "epoch": 6.923541247484909,
      "grad_norm": 0.2783285677433014,
      "learning_rate": 0.00018617164704698663,
      "loss": 0.6383,
      "step": 3441
    },
    {
      "epoch": 6.925553319919517,
      "grad_norm": 0.2660670876502991,
      "learning_rate": 0.00018616762249723314,
      "loss": 0.601,
      "step": 3442
    },
    {
      "epoch": 6.927565392354125,
      "grad_norm": 0.2748003900051117,
      "learning_rate": 0.00018616359794747963,
      "loss": 0.6355,
      "step": 3443
    },
    {
      "epoch": 6.929577464788732,
      "grad_norm": 0.278365820646286,
      "learning_rate": 0.00018615957339772614,
      "loss": 0.6311,
      "step": 3444
    },
    {
      "epoch": 6.93158953722334,
      "grad_norm": 0.27748340368270874,
      "learning_rate": 0.00018615554884797263,
      "loss": 0.6092,
      "step": 3445
    },
    {
      "epoch": 6.933601609657948,
      "grad_norm": 0.2749761939048767,
      "learning_rate": 0.00018615152429821914,
      "loss": 0.5822,
      "step": 3446
    },
    {
      "epoch": 6.935613682092555,
      "grad_norm": 0.28898927569389343,
      "learning_rate": 0.00018614749974846565,
      "loss": 0.6792,
      "step": 3447
    },
    {
      "epoch": 6.937625754527163,
      "grad_norm": 0.2822638750076294,
      "learning_rate": 0.00018614347519871216,
      "loss": 0.6306,
      "step": 3448
    },
    {
      "epoch": 6.9396378269617705,
      "grad_norm": 0.29317331314086914,
      "learning_rate": 0.00018613945064895865,
      "loss": 0.6513,
      "step": 3449
    },
    {
      "epoch": 6.941649899396378,
      "grad_norm": 0.28896835446357727,
      "learning_rate": 0.00018613542609920516,
      "loss": 0.6227,
      "step": 3450
    },
    {
      "epoch": 6.943661971830986,
      "grad_norm": 0.2739420235157013,
      "learning_rate": 0.00018613140154945165,
      "loss": 0.6473,
      "step": 3451
    },
    {
      "epoch": 6.945674044265593,
      "grad_norm": 0.2835499942302704,
      "learning_rate": 0.00018612737699969819,
      "loss": 0.6339,
      "step": 3452
    },
    {
      "epoch": 6.9476861167002015,
      "grad_norm": 0.29894402623176575,
      "learning_rate": 0.00018612335244994467,
      "loss": 0.6527,
      "step": 3453
    },
    {
      "epoch": 6.949698189134809,
      "grad_norm": 0.2920985519886017,
      "learning_rate": 0.00018611932790019118,
      "loss": 0.6295,
      "step": 3454
    },
    {
      "epoch": 6.951710261569416,
      "grad_norm": 0.2919997274875641,
      "learning_rate": 0.00018611530335043767,
      "loss": 0.7248,
      "step": 3455
    },
    {
      "epoch": 6.953722334004024,
      "grad_norm": 0.29661065340042114,
      "learning_rate": 0.00018611127880068418,
      "loss": 0.646,
      "step": 3456
    },
    {
      "epoch": 6.955734406438632,
      "grad_norm": 0.278226763010025,
      "learning_rate": 0.0001861072542509307,
      "loss": 0.6686,
      "step": 3457
    },
    {
      "epoch": 6.957746478873239,
      "grad_norm": 0.2754010260105133,
      "learning_rate": 0.0001861032297011772,
      "loss": 0.6538,
      "step": 3458
    },
    {
      "epoch": 6.959758551307847,
      "grad_norm": 0.2792157530784607,
      "learning_rate": 0.0001860992051514237,
      "loss": 0.6428,
      "step": 3459
    },
    {
      "epoch": 6.961770623742455,
      "grad_norm": 0.27266839146614075,
      "learning_rate": 0.0001860951806016702,
      "loss": 0.6297,
      "step": 3460
    },
    {
      "epoch": 6.963782696177063,
      "grad_norm": 0.26288649439811707,
      "learning_rate": 0.0001860911560519167,
      "loss": 0.6016,
      "step": 3461
    },
    {
      "epoch": 6.96579476861167,
      "grad_norm": 0.27814632654190063,
      "learning_rate": 0.00018608713150216323,
      "loss": 0.6114,
      "step": 3462
    },
    {
      "epoch": 6.9678068410462775,
      "grad_norm": 0.2744019627571106,
      "learning_rate": 0.0001860831069524097,
      "loss": 0.6271,
      "step": 3463
    },
    {
      "epoch": 6.969818913480886,
      "grad_norm": 0.28863435983657837,
      "learning_rate": 0.00018607908240265622,
      "loss": 0.647,
      "step": 3464
    },
    {
      "epoch": 6.971830985915493,
      "grad_norm": 0.27949458360671997,
      "learning_rate": 0.0001860750578529027,
      "loss": 0.6382,
      "step": 3465
    },
    {
      "epoch": 6.9738430583501,
      "grad_norm": 0.27484312653541565,
      "learning_rate": 0.00018607103330314922,
      "loss": 0.6487,
      "step": 3466
    },
    {
      "epoch": 6.9758551307847085,
      "grad_norm": 0.2816930115222931,
      "learning_rate": 0.00018606700875339573,
      "loss": 0.6625,
      "step": 3467
    },
    {
      "epoch": 6.977867203219316,
      "grad_norm": 0.2776351571083069,
      "learning_rate": 0.00018606298420364225,
      "loss": 0.6411,
      "step": 3468
    },
    {
      "epoch": 6.979879275653923,
      "grad_norm": 0.2905101180076599,
      "learning_rate": 0.00018605895965388873,
      "loss": 0.6058,
      "step": 3469
    },
    {
      "epoch": 6.981891348088531,
      "grad_norm": 0.27381977438926697,
      "learning_rate": 0.00018605493510413524,
      "loss": 0.684,
      "step": 3470
    },
    {
      "epoch": 6.983903420523139,
      "grad_norm": 0.2755391001701355,
      "learning_rate": 0.00018605091055438173,
      "loss": 0.6264,
      "step": 3471
    },
    {
      "epoch": 6.985915492957746,
      "grad_norm": 0.2831551134586334,
      "learning_rate": 0.00018604688600462824,
      "loss": 0.6156,
      "step": 3472
    },
    {
      "epoch": 6.987927565392354,
      "grad_norm": 0.293159157037735,
      "learning_rate": 0.00018604286145487475,
      "loss": 0.5976,
      "step": 3473
    },
    {
      "epoch": 6.989939637826962,
      "grad_norm": 0.2811547815799713,
      "learning_rate": 0.00018603883690512124,
      "loss": 0.6221,
      "step": 3474
    },
    {
      "epoch": 6.991951710261569,
      "grad_norm": 0.27707207202911377,
      "learning_rate": 0.00018603481235536775,
      "loss": 0.6521,
      "step": 3475
    },
    {
      "epoch": 6.993963782696177,
      "grad_norm": 0.2681272029876709,
      "learning_rate": 0.00018603078780561426,
      "loss": 0.6322,
      "step": 3476
    },
    {
      "epoch": 6.9959758551307845,
      "grad_norm": 0.29417234659194946,
      "learning_rate": 0.00018602676325586077,
      "loss": 0.6625,
      "step": 3477
    },
    {
      "epoch": 6.997987927565393,
      "grad_norm": 0.28106948733329773,
      "learning_rate": 0.00018602273870610726,
      "loss": 0.6249,
      "step": 3478
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.28117743134498596,
      "learning_rate": 0.00018601871415635377,
      "loss": 0.6314,
      "step": 3479
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.7213904857635498,
      "eval_runtime": 49.8029,
      "eval_samples_per_second": 19.919,
      "eval_steps_per_second": 2.49,
      "step": 3479
    },
    {
      "epoch": 7.002012072434607,
      "grad_norm": 0.2755378484725952,
      "learning_rate": 0.00018601468960660026,
      "loss": 0.5632,
      "step": 3480
    },
    {
      "epoch": 7.0040241448692155,
      "grad_norm": 0.2994983494281769,
      "learning_rate": 0.00018601066505684677,
      "loss": 0.6142,
      "step": 3481
    },
    {
      "epoch": 7.006036217303823,
      "grad_norm": 0.28346315026283264,
      "learning_rate": 0.00018600664050709328,
      "loss": 0.5917,
      "step": 3482
    },
    {
      "epoch": 7.00804828973843,
      "grad_norm": 0.2862197458744049,
      "learning_rate": 0.0001860026159573398,
      "loss": 0.5846,
      "step": 3483
    },
    {
      "epoch": 7.010060362173038,
      "grad_norm": 0.3183804154396057,
      "learning_rate": 0.00018599859140758628,
      "loss": 0.6142,
      "step": 3484
    },
    {
      "epoch": 7.012072434607646,
      "grad_norm": 0.32437771558761597,
      "learning_rate": 0.0001859945668578328,
      "loss": 0.5591,
      "step": 3485
    },
    {
      "epoch": 7.014084507042254,
      "grad_norm": 0.3094824552536011,
      "learning_rate": 0.00018599054230807928,
      "loss": 0.6148,
      "step": 3486
    },
    {
      "epoch": 7.016096579476861,
      "grad_norm": 0.2896202504634857,
      "learning_rate": 0.00018598651775832581,
      "loss": 0.5755,
      "step": 3487
    },
    {
      "epoch": 7.018108651911469,
      "grad_norm": 0.2829195559024811,
      "learning_rate": 0.0001859824932085723,
      "loss": 0.6113,
      "step": 3488
    },
    {
      "epoch": 7.020120724346077,
      "grad_norm": 0.2850130796432495,
      "learning_rate": 0.0001859784686588188,
      "loss": 0.5935,
      "step": 3489
    },
    {
      "epoch": 7.022132796780684,
      "grad_norm": 0.3001333475112915,
      "learning_rate": 0.0001859744441090653,
      "loss": 0.6304,
      "step": 3490
    },
    {
      "epoch": 7.0241448692152915,
      "grad_norm": 0.28411149978637695,
      "learning_rate": 0.0001859704195593118,
      "loss": 0.5533,
      "step": 3491
    },
    {
      "epoch": 7.0261569416499,
      "grad_norm": 0.31712377071380615,
      "learning_rate": 0.00018596639500955832,
      "loss": 0.6388,
      "step": 3492
    },
    {
      "epoch": 7.028169014084507,
      "grad_norm": 0.311500608921051,
      "learning_rate": 0.00018596237045980483,
      "loss": 0.6137,
      "step": 3493
    },
    {
      "epoch": 7.030181086519114,
      "grad_norm": 0.3368820250034332,
      "learning_rate": 0.00018595834591005132,
      "loss": 0.5822,
      "step": 3494
    },
    {
      "epoch": 7.0321931589537225,
      "grad_norm": 0.31910020112991333,
      "learning_rate": 0.00018595432136029783,
      "loss": 0.6199,
      "step": 3495
    },
    {
      "epoch": 7.03420523138833,
      "grad_norm": 0.30304446816444397,
      "learning_rate": 0.00018595029681054432,
      "loss": 0.6083,
      "step": 3496
    },
    {
      "epoch": 7.036217303822937,
      "grad_norm": 0.2952718734741211,
      "learning_rate": 0.00018594627226079086,
      "loss": 0.5647,
      "step": 3497
    },
    {
      "epoch": 7.038229376257545,
      "grad_norm": 0.29842305183410645,
      "learning_rate": 0.00018594224771103734,
      "loss": 0.5736,
      "step": 3498
    },
    {
      "epoch": 7.040241448692153,
      "grad_norm": 0.3351109027862549,
      "learning_rate": 0.00018593822316128385,
      "loss": 0.5647,
      "step": 3499
    },
    {
      "epoch": 7.042253521126761,
      "grad_norm": 0.29156801104545593,
      "learning_rate": 0.00018593419861153034,
      "loss": 0.5982,
      "step": 3500
    },
    {
      "epoch": 7.044265593561368,
      "grad_norm": 0.28348344564437866,
      "learning_rate": 0.00018593017406177685,
      "loss": 0.5563,
      "step": 3501
    },
    {
      "epoch": 7.046277665995976,
      "grad_norm": 0.312356561422348,
      "learning_rate": 0.00018592614951202336,
      "loss": 0.5668,
      "step": 3502
    },
    {
      "epoch": 7.048289738430584,
      "grad_norm": 0.3072909116744995,
      "learning_rate": 0.00018592212496226987,
      "loss": 0.5894,
      "step": 3503
    },
    {
      "epoch": 7.050301810865191,
      "grad_norm": 0.29753515124320984,
      "learning_rate": 0.00018591810041251636,
      "loss": 0.571,
      "step": 3504
    },
    {
      "epoch": 7.0523138832997985,
      "grad_norm": 0.3287130296230316,
      "learning_rate": 0.00018591407586276287,
      "loss": 0.617,
      "step": 3505
    },
    {
      "epoch": 7.054325955734407,
      "grad_norm": 0.32413578033447266,
      "learning_rate": 0.00018591005131300936,
      "loss": 0.5747,
      "step": 3506
    },
    {
      "epoch": 7.056338028169014,
      "grad_norm": 0.30566272139549255,
      "learning_rate": 0.00018590602676325587,
      "loss": 0.5769,
      "step": 3507
    },
    {
      "epoch": 7.058350100603621,
      "grad_norm": 0.28216663002967834,
      "learning_rate": 0.00018590200221350238,
      "loss": 0.5589,
      "step": 3508
    },
    {
      "epoch": 7.0603621730382295,
      "grad_norm": 0.2972755432128906,
      "learning_rate": 0.00018589797766374887,
      "loss": 0.5665,
      "step": 3509
    },
    {
      "epoch": 7.062374245472837,
      "grad_norm": 0.2900603115558624,
      "learning_rate": 0.00018589395311399538,
      "loss": 0.6063,
      "step": 3510
    },
    {
      "epoch": 7.064386317907445,
      "grad_norm": 0.298333078622818,
      "learning_rate": 0.0001858899285642419,
      "loss": 0.5782,
      "step": 3511
    },
    {
      "epoch": 7.066398390342052,
      "grad_norm": 0.294350266456604,
      "learning_rate": 0.0001858859040144884,
      "loss": 0.5578,
      "step": 3512
    },
    {
      "epoch": 7.06841046277666,
      "grad_norm": 0.2961312234401703,
      "learning_rate": 0.0001858818794647349,
      "loss": 0.6097,
      "step": 3513
    },
    {
      "epoch": 7.070422535211268,
      "grad_norm": 0.29654762148857117,
      "learning_rate": 0.0001858778549149814,
      "loss": 0.5593,
      "step": 3514
    },
    {
      "epoch": 7.072434607645875,
      "grad_norm": 0.2958713173866272,
      "learning_rate": 0.00018587383036522789,
      "loss": 0.573,
      "step": 3515
    },
    {
      "epoch": 7.074446680080483,
      "grad_norm": 0.3012859523296356,
      "learning_rate": 0.0001858698058154744,
      "loss": 0.6187,
      "step": 3516
    },
    {
      "epoch": 7.076458752515091,
      "grad_norm": 0.32164451479911804,
      "learning_rate": 0.0001858657812657209,
      "loss": 0.5925,
      "step": 3517
    },
    {
      "epoch": 7.078470824949698,
      "grad_norm": 0.3062233626842499,
      "learning_rate": 0.00018586175671596742,
      "loss": 0.5998,
      "step": 3518
    },
    {
      "epoch": 7.0804828973843055,
      "grad_norm": 0.30869707465171814,
      "learning_rate": 0.0001858577321662139,
      "loss": 0.6261,
      "step": 3519
    },
    {
      "epoch": 7.082494969818914,
      "grad_norm": 0.30281296372413635,
      "learning_rate": 0.00018585370761646042,
      "loss": 0.603,
      "step": 3520
    },
    {
      "epoch": 7.084507042253521,
      "grad_norm": 0.3188812732696533,
      "learning_rate": 0.0001858496830667069,
      "loss": 0.5394,
      "step": 3521
    },
    {
      "epoch": 7.086519114688128,
      "grad_norm": 0.2853640019893646,
      "learning_rate": 0.00018584565851695342,
      "loss": 0.5724,
      "step": 3522
    },
    {
      "epoch": 7.0885311871227366,
      "grad_norm": 0.31091949343681335,
      "learning_rate": 0.00018584163396719993,
      "loss": 0.6091,
      "step": 3523
    },
    {
      "epoch": 7.090543259557344,
      "grad_norm": 0.3018338084220886,
      "learning_rate": 0.00018583760941744644,
      "loss": 0.5792,
      "step": 3524
    },
    {
      "epoch": 7.092555331991952,
      "grad_norm": 0.2988145649433136,
      "learning_rate": 0.00018583358486769293,
      "loss": 0.6106,
      "step": 3525
    },
    {
      "epoch": 7.094567404426559,
      "grad_norm": 0.3140573501586914,
      "learning_rate": 0.00018582956031793944,
      "loss": 0.5605,
      "step": 3526
    },
    {
      "epoch": 7.096579476861167,
      "grad_norm": 0.31426307559013367,
      "learning_rate": 0.00018582553576818592,
      "loss": 0.6191,
      "step": 3527
    },
    {
      "epoch": 7.098591549295775,
      "grad_norm": 0.30062341690063477,
      "learning_rate": 0.00018582151121843246,
      "loss": 0.6317,
      "step": 3528
    },
    {
      "epoch": 7.100603621730382,
      "grad_norm": 0.29713964462280273,
      "learning_rate": 0.00018581748666867895,
      "loss": 0.5831,
      "step": 3529
    },
    {
      "epoch": 7.10261569416499,
      "grad_norm": 0.30192631483078003,
      "learning_rate": 0.00018581346211892546,
      "loss": 0.6173,
      "step": 3530
    },
    {
      "epoch": 7.104627766599598,
      "grad_norm": 0.30765077471733093,
      "learning_rate": 0.00018580943756917195,
      "loss": 0.61,
      "step": 3531
    },
    {
      "epoch": 7.106639839034205,
      "grad_norm": 0.3067636489868164,
      "learning_rate": 0.00018580541301941846,
      "loss": 0.5812,
      "step": 3532
    },
    {
      "epoch": 7.1086519114688125,
      "grad_norm": 0.28897643089294434,
      "learning_rate": 0.00018580138846966497,
      "loss": 0.5633,
      "step": 3533
    },
    {
      "epoch": 7.110663983903421,
      "grad_norm": 0.31154441833496094,
      "learning_rate": 0.00018579736391991148,
      "loss": 0.6038,
      "step": 3534
    },
    {
      "epoch": 7.112676056338028,
      "grad_norm": 0.3178783655166626,
      "learning_rate": 0.00018579333937015797,
      "loss": 0.6365,
      "step": 3535
    },
    {
      "epoch": 7.114688128772636,
      "grad_norm": 0.30873000621795654,
      "learning_rate": 0.00018578931482040448,
      "loss": 0.5881,
      "step": 3536
    },
    {
      "epoch": 7.116700201207244,
      "grad_norm": 0.3030356466770172,
      "learning_rate": 0.00018578529027065096,
      "loss": 0.5966,
      "step": 3537
    },
    {
      "epoch": 7.118712273641851,
      "grad_norm": 0.31704744696617126,
      "learning_rate": 0.0001857812657208975,
      "loss": 0.6329,
      "step": 3538
    },
    {
      "epoch": 7.120724346076459,
      "grad_norm": 0.3013467490673065,
      "learning_rate": 0.000185777241171144,
      "loss": 0.5908,
      "step": 3539
    },
    {
      "epoch": 7.122736418511066,
      "grad_norm": 0.29717540740966797,
      "learning_rate": 0.0001857732166213905,
      "loss": 0.5737,
      "step": 3540
    },
    {
      "epoch": 7.124748490945674,
      "grad_norm": 0.3097764551639557,
      "learning_rate": 0.00018576919207163699,
      "loss": 0.6006,
      "step": 3541
    },
    {
      "epoch": 7.126760563380282,
      "grad_norm": 0.2917981445789337,
      "learning_rate": 0.0001857651675218835,
      "loss": 0.5806,
      "step": 3542
    },
    {
      "epoch": 7.128772635814889,
      "grad_norm": 0.3055417835712433,
      "learning_rate": 0.00018576114297213,
      "loss": 0.6183,
      "step": 3543
    },
    {
      "epoch": 7.130784708249497,
      "grad_norm": 0.3151664733886719,
      "learning_rate": 0.0001857571184223765,
      "loss": 0.572,
      "step": 3544
    },
    {
      "epoch": 7.132796780684105,
      "grad_norm": 0.30489829182624817,
      "learning_rate": 0.000185753093872623,
      "loss": 0.5571,
      "step": 3545
    },
    {
      "epoch": 7.134808853118712,
      "grad_norm": 0.29087725281715393,
      "learning_rate": 0.0001857490693228695,
      "loss": 0.5838,
      "step": 3546
    },
    {
      "epoch": 7.1368209255533195,
      "grad_norm": 0.29449835419654846,
      "learning_rate": 0.000185745044773116,
      "loss": 0.5825,
      "step": 3547
    },
    {
      "epoch": 7.138832997987928,
      "grad_norm": 0.30075356364250183,
      "learning_rate": 0.00018574102022336252,
      "loss": 0.6103,
      "step": 3548
    },
    {
      "epoch": 7.140845070422535,
      "grad_norm": 0.31003284454345703,
      "learning_rate": 0.00018573699567360903,
      "loss": 0.592,
      "step": 3549
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 0.3251141905784607,
      "learning_rate": 0.00018573297112385551,
      "loss": 0.5929,
      "step": 3550
    },
    {
      "epoch": 7.144869215291751,
      "grad_norm": 0.3237297534942627,
      "learning_rate": 0.00018572894657410203,
      "loss": 0.5923,
      "step": 3551
    },
    {
      "epoch": 7.146881287726358,
      "grad_norm": 0.3038310110569,
      "learning_rate": 0.0001857249220243485,
      "loss": 0.5823,
      "step": 3552
    },
    {
      "epoch": 7.148893360160966,
      "grad_norm": 0.3116050660610199,
      "learning_rate": 0.00018572089747459505,
      "loss": 0.5659,
      "step": 3553
    },
    {
      "epoch": 7.150905432595573,
      "grad_norm": 0.3209916651248932,
      "learning_rate": 0.00018571687292484154,
      "loss": 0.6121,
      "step": 3554
    },
    {
      "epoch": 7.152917505030181,
      "grad_norm": 0.3181271553039551,
      "learning_rate": 0.00018571284837508805,
      "loss": 0.6072,
      "step": 3555
    },
    {
      "epoch": 7.154929577464789,
      "grad_norm": 0.30335673689842224,
      "learning_rate": 0.00018570882382533453,
      "loss": 0.5677,
      "step": 3556
    },
    {
      "epoch": 7.156941649899396,
      "grad_norm": 0.2976473569869995,
      "learning_rate": 0.00018570479927558105,
      "loss": 0.5842,
      "step": 3557
    },
    {
      "epoch": 7.158953722334004,
      "grad_norm": 0.31759387254714966,
      "learning_rate": 0.00018570077472582756,
      "loss": 0.6366,
      "step": 3558
    },
    {
      "epoch": 7.160965794768612,
      "grad_norm": 0.3068994879722595,
      "learning_rate": 0.00018569675017607407,
      "loss": 0.6397,
      "step": 3559
    },
    {
      "epoch": 7.162977867203219,
      "grad_norm": 0.29318201541900635,
      "learning_rate": 0.00018569272562632056,
      "loss": 0.5573,
      "step": 3560
    },
    {
      "epoch": 7.164989939637827,
      "grad_norm": 0.3214964270591736,
      "learning_rate": 0.00018568870107656707,
      "loss": 0.582,
      "step": 3561
    },
    {
      "epoch": 7.167002012072435,
      "grad_norm": 0.3244374692440033,
      "learning_rate": 0.00018568467652681355,
      "loss": 0.6075,
      "step": 3562
    },
    {
      "epoch": 7.169014084507042,
      "grad_norm": 0.32280853390693665,
      "learning_rate": 0.0001856806519770601,
      "loss": 0.5625,
      "step": 3563
    },
    {
      "epoch": 7.17102615694165,
      "grad_norm": 0.3238777816295624,
      "learning_rate": 0.00018567662742730658,
      "loss": 0.5998,
      "step": 3564
    },
    {
      "epoch": 7.173038229376258,
      "grad_norm": 0.29955512285232544,
      "learning_rate": 0.0001856726028775531,
      "loss": 0.5657,
      "step": 3565
    },
    {
      "epoch": 7.175050301810865,
      "grad_norm": 0.30949535965919495,
      "learning_rate": 0.00018566857832779957,
      "loss": 0.5763,
      "step": 3566
    },
    {
      "epoch": 7.177062374245473,
      "grad_norm": 0.2994414269924164,
      "learning_rate": 0.0001856645537780461,
      "loss": 0.5835,
      "step": 3567
    },
    {
      "epoch": 7.17907444668008,
      "grad_norm": 0.31802141666412354,
      "learning_rate": 0.0001856605292282926,
      "loss": 0.6189,
      "step": 3568
    },
    {
      "epoch": 7.181086519114688,
      "grad_norm": 0.29060566425323486,
      "learning_rate": 0.0001856565046785391,
      "loss": 0.6145,
      "step": 3569
    },
    {
      "epoch": 7.183098591549296,
      "grad_norm": 0.3126339912414551,
      "learning_rate": 0.0001856524801287856,
      "loss": 0.5772,
      "step": 3570
    },
    {
      "epoch": 7.185110663983903,
      "grad_norm": 0.3051607012748718,
      "learning_rate": 0.0001856484555790321,
      "loss": 0.6087,
      "step": 3571
    },
    {
      "epoch": 7.187122736418511,
      "grad_norm": 0.3188849985599518,
      "learning_rate": 0.0001856444310292786,
      "loss": 0.5863,
      "step": 3572
    },
    {
      "epoch": 7.189134808853119,
      "grad_norm": 0.28922054171562195,
      "learning_rate": 0.0001856404064795251,
      "loss": 0.6027,
      "step": 3573
    },
    {
      "epoch": 7.191146881287726,
      "grad_norm": 0.30322617292404175,
      "learning_rate": 0.00018563638192977162,
      "loss": 0.5757,
      "step": 3574
    },
    {
      "epoch": 7.193158953722334,
      "grad_norm": 0.30608615279197693,
      "learning_rate": 0.00018563235738001813,
      "loss": 0.6179,
      "step": 3575
    },
    {
      "epoch": 7.195171026156942,
      "grad_norm": 0.31006723642349243,
      "learning_rate": 0.00018562833283026462,
      "loss": 0.6299,
      "step": 3576
    },
    {
      "epoch": 7.197183098591549,
      "grad_norm": 0.3180522620677948,
      "learning_rate": 0.00018562430828051113,
      "loss": 0.5987,
      "step": 3577
    },
    {
      "epoch": 7.199195171026157,
      "grad_norm": 0.3103138506412506,
      "learning_rate": 0.00018562028373075764,
      "loss": 0.5718,
      "step": 3578
    },
    {
      "epoch": 7.201207243460765,
      "grad_norm": 0.31406304240226746,
      "learning_rate": 0.00018561625918100413,
      "loss": 0.6379,
      "step": 3579
    },
    {
      "epoch": 7.203219315895372,
      "grad_norm": 0.3179028630256653,
      "learning_rate": 0.00018561223463125064,
      "loss": 0.5935,
      "step": 3580
    },
    {
      "epoch": 7.20523138832998,
      "grad_norm": 0.3192217946052551,
      "learning_rate": 0.00018560821008149712,
      "loss": 0.5891,
      "step": 3581
    },
    {
      "epoch": 7.207243460764587,
      "grad_norm": 0.3100817799568176,
      "learning_rate": 0.00018560418553174363,
      "loss": 0.5981,
      "step": 3582
    },
    {
      "epoch": 7.209255533199195,
      "grad_norm": 0.31743961572647095,
      "learning_rate": 0.00018560016098199015,
      "loss": 0.5973,
      "step": 3583
    },
    {
      "epoch": 7.211267605633803,
      "grad_norm": 0.3016660809516907,
      "learning_rate": 0.00018559613643223666,
      "loss": 0.6002,
      "step": 3584
    },
    {
      "epoch": 7.21327967806841,
      "grad_norm": 0.2981850504875183,
      "learning_rate": 0.00018559211188248314,
      "loss": 0.5485,
      "step": 3585
    },
    {
      "epoch": 7.2152917505030185,
      "grad_norm": 0.3057992160320282,
      "learning_rate": 0.00018558808733272966,
      "loss": 0.5692,
      "step": 3586
    },
    {
      "epoch": 7.217303822937626,
      "grad_norm": 0.31647148728370667,
      "learning_rate": 0.00018558406278297614,
      "loss": 0.5976,
      "step": 3587
    },
    {
      "epoch": 7.219315895372233,
      "grad_norm": 0.3304251730442047,
      "learning_rate": 0.00018558003823322268,
      "loss": 0.5966,
      "step": 3588
    },
    {
      "epoch": 7.221327967806841,
      "grad_norm": 0.31526288390159607,
      "learning_rate": 0.00018557601368346917,
      "loss": 0.6277,
      "step": 3589
    },
    {
      "epoch": 7.223340040241449,
      "grad_norm": 0.3030240535736084,
      "learning_rate": 0.00018557198913371568,
      "loss": 0.6019,
      "step": 3590
    },
    {
      "epoch": 7.225352112676056,
      "grad_norm": 0.30899304151535034,
      "learning_rate": 0.00018556796458396216,
      "loss": 0.6047,
      "step": 3591
    },
    {
      "epoch": 7.227364185110664,
      "grad_norm": 0.31657397747039795,
      "learning_rate": 0.00018556394003420868,
      "loss": 0.5999,
      "step": 3592
    },
    {
      "epoch": 7.229376257545272,
      "grad_norm": 0.3375987112522125,
      "learning_rate": 0.0001855599154844552,
      "loss": 0.6144,
      "step": 3593
    },
    {
      "epoch": 7.231388329979879,
      "grad_norm": 0.32427576184272766,
      "learning_rate": 0.0001855558909347017,
      "loss": 0.6298,
      "step": 3594
    },
    {
      "epoch": 7.233400402414487,
      "grad_norm": 0.3226393163204193,
      "learning_rate": 0.00018555186638494819,
      "loss": 0.5488,
      "step": 3595
    },
    {
      "epoch": 7.2354124748490944,
      "grad_norm": 0.3213765323162079,
      "learning_rate": 0.0001855478418351947,
      "loss": 0.5683,
      "step": 3596
    },
    {
      "epoch": 7.237424547283702,
      "grad_norm": 0.33895835280418396,
      "learning_rate": 0.00018554381728544118,
      "loss": 0.6287,
      "step": 3597
    },
    {
      "epoch": 7.23943661971831,
      "grad_norm": 0.3156062066555023,
      "learning_rate": 0.00018553979273568772,
      "loss": 0.6134,
      "step": 3598
    },
    {
      "epoch": 7.241448692152917,
      "grad_norm": 0.31462767720222473,
      "learning_rate": 0.0001855357681859342,
      "loss": 0.5519,
      "step": 3599
    },
    {
      "epoch": 7.2434607645875255,
      "grad_norm": 0.30509626865386963,
      "learning_rate": 0.00018553174363618072,
      "loss": 0.5779,
      "step": 3600
    },
    {
      "epoch": 7.245472837022133,
      "grad_norm": 0.3131030797958374,
      "learning_rate": 0.0001855277190864272,
      "loss": 0.6014,
      "step": 3601
    },
    {
      "epoch": 7.24748490945674,
      "grad_norm": 0.31615129113197327,
      "learning_rate": 0.00018552369453667372,
      "loss": 0.5582,
      "step": 3602
    },
    {
      "epoch": 7.249496981891348,
      "grad_norm": 0.3125949501991272,
      "learning_rate": 0.00018551966998692023,
      "loss": 0.6357,
      "step": 3603
    },
    {
      "epoch": 7.251509054325956,
      "grad_norm": 0.3197545111179352,
      "learning_rate": 0.00018551564543716674,
      "loss": 0.6114,
      "step": 3604
    },
    {
      "epoch": 7.253521126760563,
      "grad_norm": 0.33213087916374207,
      "learning_rate": 0.00018551162088741323,
      "loss": 0.608,
      "step": 3605
    },
    {
      "epoch": 7.255533199195171,
      "grad_norm": 0.30987685918807983,
      "learning_rate": 0.00018550759633765974,
      "loss": 0.5446,
      "step": 3606
    },
    {
      "epoch": 7.257545271629779,
      "grad_norm": 0.30508604645729065,
      "learning_rate": 0.00018550357178790622,
      "loss": 0.5985,
      "step": 3607
    },
    {
      "epoch": 7.259557344064386,
      "grad_norm": 0.30664584040641785,
      "learning_rate": 0.00018549954723815274,
      "loss": 0.6453,
      "step": 3608
    },
    {
      "epoch": 7.261569416498994,
      "grad_norm": 0.3043754994869232,
      "learning_rate": 0.00018549552268839925,
      "loss": 0.5851,
      "step": 3609
    },
    {
      "epoch": 7.2635814889336014,
      "grad_norm": 0.29899126291275024,
      "learning_rate": 0.00018549149813864576,
      "loss": 0.5827,
      "step": 3610
    },
    {
      "epoch": 7.26559356136821,
      "grad_norm": 0.325501024723053,
      "learning_rate": 0.00018548747358889225,
      "loss": 0.5752,
      "step": 3611
    },
    {
      "epoch": 7.267605633802817,
      "grad_norm": 0.31774240732192993,
      "learning_rate": 0.00018548344903913876,
      "loss": 0.6303,
      "step": 3612
    },
    {
      "epoch": 7.269617706237424,
      "grad_norm": 0.3338095545768738,
      "learning_rate": 0.00018547942448938527,
      "loss": 0.6651,
      "step": 3613
    },
    {
      "epoch": 7.2716297786720325,
      "grad_norm": 0.3096151351928711,
      "learning_rate": 0.00018547539993963175,
      "loss": 0.6063,
      "step": 3614
    },
    {
      "epoch": 7.27364185110664,
      "grad_norm": 0.32777249813079834,
      "learning_rate": 0.00018547137538987827,
      "loss": 0.6208,
      "step": 3615
    },
    {
      "epoch": 7.275653923541247,
      "grad_norm": 0.31639963388442993,
      "learning_rate": 0.00018546735084012475,
      "loss": 0.5934,
      "step": 3616
    },
    {
      "epoch": 7.277665995975855,
      "grad_norm": 0.313258558511734,
      "learning_rate": 0.00018546332629037126,
      "loss": 0.6025,
      "step": 3617
    },
    {
      "epoch": 7.279678068410463,
      "grad_norm": 0.31757962703704834,
      "learning_rate": 0.00018545930174061778,
      "loss": 0.6219,
      "step": 3618
    },
    {
      "epoch": 7.28169014084507,
      "grad_norm": 0.31188780069351196,
      "learning_rate": 0.0001854552771908643,
      "loss": 0.5646,
      "step": 3619
    },
    {
      "epoch": 7.283702213279678,
      "grad_norm": 0.3097171485424042,
      "learning_rate": 0.00018545125264111077,
      "loss": 0.5511,
      "step": 3620
    },
    {
      "epoch": 7.285714285714286,
      "grad_norm": 0.30300474166870117,
      "learning_rate": 0.00018544722809135729,
      "loss": 0.6094,
      "step": 3621
    },
    {
      "epoch": 7.287726358148893,
      "grad_norm": 0.3085879981517792,
      "learning_rate": 0.00018544320354160377,
      "loss": 0.6295,
      "step": 3622
    },
    {
      "epoch": 7.289738430583501,
      "grad_norm": 0.310690701007843,
      "learning_rate": 0.0001854391789918503,
      "loss": 0.5869,
      "step": 3623
    },
    {
      "epoch": 7.2917505030181085,
      "grad_norm": 0.3162490427494049,
      "learning_rate": 0.0001854351544420968,
      "loss": 0.6479,
      "step": 3624
    },
    {
      "epoch": 7.293762575452717,
      "grad_norm": 0.3119962215423584,
      "learning_rate": 0.0001854311298923433,
      "loss": 0.6207,
      "step": 3625
    },
    {
      "epoch": 7.295774647887324,
      "grad_norm": 0.31281158328056335,
      "learning_rate": 0.0001854271053425898,
      "loss": 0.6111,
      "step": 3626
    },
    {
      "epoch": 7.297786720321931,
      "grad_norm": 0.32169467210769653,
      "learning_rate": 0.0001854230807928363,
      "loss": 0.6315,
      "step": 3627
    },
    {
      "epoch": 7.2997987927565395,
      "grad_norm": 0.3095742464065552,
      "learning_rate": 0.00018541905624308282,
      "loss": 0.6133,
      "step": 3628
    },
    {
      "epoch": 7.301810865191147,
      "grad_norm": 0.30522891879081726,
      "learning_rate": 0.00018541503169332933,
      "loss": 0.581,
      "step": 3629
    },
    {
      "epoch": 7.303822937625754,
      "grad_norm": 0.3047066628932953,
      "learning_rate": 0.00018541100714357581,
      "loss": 0.5981,
      "step": 3630
    },
    {
      "epoch": 7.305835010060362,
      "grad_norm": 0.3207870423793793,
      "learning_rate": 0.00018540698259382233,
      "loss": 0.6182,
      "step": 3631
    },
    {
      "epoch": 7.30784708249497,
      "grad_norm": 0.31819549202919006,
      "learning_rate": 0.0001854029580440688,
      "loss": 0.5905,
      "step": 3632
    },
    {
      "epoch": 7.309859154929577,
      "grad_norm": 0.31202828884124756,
      "learning_rate": 0.00018539893349431535,
      "loss": 0.5788,
      "step": 3633
    },
    {
      "epoch": 7.311871227364185,
      "grad_norm": 0.3182344138622284,
      "learning_rate": 0.00018539490894456184,
      "loss": 0.6213,
      "step": 3634
    },
    {
      "epoch": 7.313883299798793,
      "grad_norm": 0.3355540633201599,
      "learning_rate": 0.00018539088439480835,
      "loss": 0.6156,
      "step": 3635
    },
    {
      "epoch": 7.315895372233401,
      "grad_norm": 0.3131635785102844,
      "learning_rate": 0.00018538685984505483,
      "loss": 0.5663,
      "step": 3636
    },
    {
      "epoch": 7.317907444668008,
      "grad_norm": 0.30752623081207275,
      "learning_rate": 0.00018538283529530135,
      "loss": 0.5839,
      "step": 3637
    },
    {
      "epoch": 7.3199195171026155,
      "grad_norm": 0.32994943857192993,
      "learning_rate": 0.00018537881074554786,
      "loss": 0.6312,
      "step": 3638
    },
    {
      "epoch": 7.321931589537224,
      "grad_norm": 0.2944413721561432,
      "learning_rate": 0.00018537478619579437,
      "loss": 0.5971,
      "step": 3639
    },
    {
      "epoch": 7.323943661971831,
      "grad_norm": 0.3170987367630005,
      "learning_rate": 0.00018537076164604086,
      "loss": 0.6181,
      "step": 3640
    },
    {
      "epoch": 7.325955734406438,
      "grad_norm": 0.3754635453224182,
      "learning_rate": 0.00018536673709628737,
      "loss": 0.6188,
      "step": 3641
    },
    {
      "epoch": 7.3279678068410465,
      "grad_norm": 0.3029501140117645,
      "learning_rate": 0.00018536271254653385,
      "loss": 0.5425,
      "step": 3642
    },
    {
      "epoch": 7.329979879275654,
      "grad_norm": 0.31119632720947266,
      "learning_rate": 0.00018535868799678037,
      "loss": 0.5888,
      "step": 3643
    },
    {
      "epoch": 7.331991951710261,
      "grad_norm": 0.3216603994369507,
      "learning_rate": 0.00018535466344702688,
      "loss": 0.6239,
      "step": 3644
    },
    {
      "epoch": 7.334004024144869,
      "grad_norm": 0.3143276274204254,
      "learning_rate": 0.0001853506388972734,
      "loss": 0.5829,
      "step": 3645
    },
    {
      "epoch": 7.336016096579477,
      "grad_norm": 0.3050835430622101,
      "learning_rate": 0.00018534661434751987,
      "loss": 0.6027,
      "step": 3646
    },
    {
      "epoch": 7.338028169014084,
      "grad_norm": 0.3059162199497223,
      "learning_rate": 0.0001853425897977664,
      "loss": 0.5638,
      "step": 3647
    },
    {
      "epoch": 7.340040241448692,
      "grad_norm": 0.29395467042922974,
      "learning_rate": 0.0001853385652480129,
      "loss": 0.5804,
      "step": 3648
    },
    {
      "epoch": 7.3420523138833,
      "grad_norm": 0.3070265054702759,
      "learning_rate": 0.00018533454069825938,
      "loss": 0.5722,
      "step": 3649
    },
    {
      "epoch": 7.344064386317908,
      "grad_norm": 0.33088210225105286,
      "learning_rate": 0.0001853305161485059,
      "loss": 0.6022,
      "step": 3650
    },
    {
      "epoch": 7.346076458752515,
      "grad_norm": 0.309235155582428,
      "learning_rate": 0.00018532649159875238,
      "loss": 0.5805,
      "step": 3651
    },
    {
      "epoch": 7.3480885311871225,
      "grad_norm": 0.31286025047302246,
      "learning_rate": 0.0001853224670489989,
      "loss": 0.5848,
      "step": 3652
    },
    {
      "epoch": 7.350100603621731,
      "grad_norm": 0.3169751465320587,
      "learning_rate": 0.0001853184424992454,
      "loss": 0.5846,
      "step": 3653
    },
    {
      "epoch": 7.352112676056338,
      "grad_norm": 0.3105457127094269,
      "learning_rate": 0.00018531441794949192,
      "loss": 0.613,
      "step": 3654
    },
    {
      "epoch": 7.354124748490945,
      "grad_norm": 0.31349292397499084,
      "learning_rate": 0.0001853103933997384,
      "loss": 0.6401,
      "step": 3655
    },
    {
      "epoch": 7.3561368209255535,
      "grad_norm": 0.33467134833335876,
      "learning_rate": 0.00018530636884998492,
      "loss": 0.6161,
      "step": 3656
    },
    {
      "epoch": 7.358148893360161,
      "grad_norm": 0.3205617666244507,
      "learning_rate": 0.0001853023443002314,
      "loss": 0.5469,
      "step": 3657
    },
    {
      "epoch": 7.360160965794769,
      "grad_norm": 0.31025612354278564,
      "learning_rate": 0.00018529831975047794,
      "loss": 0.6008,
      "step": 3658
    },
    {
      "epoch": 7.362173038229376,
      "grad_norm": 0.3029836118221283,
      "learning_rate": 0.00018529429520072443,
      "loss": 0.6055,
      "step": 3659
    },
    {
      "epoch": 7.364185110663984,
      "grad_norm": 0.2924966812133789,
      "learning_rate": 0.00018529027065097094,
      "loss": 0.5579,
      "step": 3660
    },
    {
      "epoch": 7.366197183098592,
      "grad_norm": 0.3078261613845825,
      "learning_rate": 0.00018528624610121742,
      "loss": 0.6031,
      "step": 3661
    },
    {
      "epoch": 7.368209255533199,
      "grad_norm": 0.3174267113208771,
      "learning_rate": 0.00018528222155146393,
      "loss": 0.5689,
      "step": 3662
    },
    {
      "epoch": 7.370221327967807,
      "grad_norm": 0.3195587992668152,
      "learning_rate": 0.00018527819700171045,
      "loss": 0.6244,
      "step": 3663
    },
    {
      "epoch": 7.372233400402415,
      "grad_norm": 0.328822523355484,
      "learning_rate": 0.00018527417245195696,
      "loss": 0.6258,
      "step": 3664
    },
    {
      "epoch": 7.374245472837022,
      "grad_norm": 0.3063860535621643,
      "learning_rate": 0.00018527014790220344,
      "loss": 0.5824,
      "step": 3665
    },
    {
      "epoch": 7.3762575452716295,
      "grad_norm": 0.323131263256073,
      "learning_rate": 0.00018526612335244996,
      "loss": 0.6339,
      "step": 3666
    },
    {
      "epoch": 7.378269617706238,
      "grad_norm": 0.3130348026752472,
      "learning_rate": 0.00018526209880269644,
      "loss": 0.5745,
      "step": 3667
    },
    {
      "epoch": 7.380281690140845,
      "grad_norm": 0.3018139898777008,
      "learning_rate": 0.00018525807425294298,
      "loss": 0.6223,
      "step": 3668
    },
    {
      "epoch": 7.382293762575452,
      "grad_norm": 0.3031691610813141,
      "learning_rate": 0.00018525404970318947,
      "loss": 0.5668,
      "step": 3669
    },
    {
      "epoch": 7.3843058350100605,
      "grad_norm": 0.3405672311782837,
      "learning_rate": 0.00018525002515343598,
      "loss": 0.621,
      "step": 3670
    },
    {
      "epoch": 7.386317907444668,
      "grad_norm": 0.31484296917915344,
      "learning_rate": 0.00018524600060368246,
      "loss": 0.5925,
      "step": 3671
    },
    {
      "epoch": 7.388329979879275,
      "grad_norm": 0.2991406321525574,
      "learning_rate": 0.00018524197605392898,
      "loss": 0.5662,
      "step": 3672
    },
    {
      "epoch": 7.390342052313883,
      "grad_norm": 0.3080996572971344,
      "learning_rate": 0.0001852379515041755,
      "loss": 0.6109,
      "step": 3673
    },
    {
      "epoch": 7.392354124748491,
      "grad_norm": 0.30283477902412415,
      "learning_rate": 0.000185233926954422,
      "loss": 0.5709,
      "step": 3674
    },
    {
      "epoch": 7.394366197183099,
      "grad_norm": 0.2990976870059967,
      "learning_rate": 0.00018522990240466849,
      "loss": 0.5895,
      "step": 3675
    },
    {
      "epoch": 7.396378269617706,
      "grad_norm": 0.30931931734085083,
      "learning_rate": 0.000185225877854915,
      "loss": 0.5815,
      "step": 3676
    },
    {
      "epoch": 7.398390342052314,
      "grad_norm": 0.317484974861145,
      "learning_rate": 0.00018522185330516148,
      "loss": 0.5832,
      "step": 3677
    },
    {
      "epoch": 7.400402414486922,
      "grad_norm": 0.32943716645240784,
      "learning_rate": 0.000185217828755408,
      "loss": 0.6092,
      "step": 3678
    },
    {
      "epoch": 7.402414486921529,
      "grad_norm": 0.2965677082538605,
      "learning_rate": 0.0001852138042056545,
      "loss": 0.5987,
      "step": 3679
    },
    {
      "epoch": 7.4044265593561365,
      "grad_norm": 0.3045043349266052,
      "learning_rate": 0.00018520977965590102,
      "loss": 0.5967,
      "step": 3680
    },
    {
      "epoch": 7.406438631790745,
      "grad_norm": 0.3061509430408478,
      "learning_rate": 0.0001852057551061475,
      "loss": 0.6023,
      "step": 3681
    },
    {
      "epoch": 7.408450704225352,
      "grad_norm": 0.3191155791282654,
      "learning_rate": 0.00018520173055639402,
      "loss": 0.6522,
      "step": 3682
    },
    {
      "epoch": 7.41046277665996,
      "grad_norm": 0.3140910267829895,
      "learning_rate": 0.00018519770600664053,
      "loss": 0.614,
      "step": 3683
    },
    {
      "epoch": 7.4124748490945676,
      "grad_norm": 0.32629916071891785,
      "learning_rate": 0.00018519368145688701,
      "loss": 0.6205,
      "step": 3684
    },
    {
      "epoch": 7.414486921529175,
      "grad_norm": 0.3089849054813385,
      "learning_rate": 0.00018518965690713353,
      "loss": 0.6148,
      "step": 3685
    },
    {
      "epoch": 7.416498993963783,
      "grad_norm": 0.30768483877182007,
      "learning_rate": 0.00018518563235738,
      "loss": 0.61,
      "step": 3686
    },
    {
      "epoch": 7.41851106639839,
      "grad_norm": 0.29670703411102295,
      "learning_rate": 0.00018518160780762652,
      "loss": 0.6061,
      "step": 3687
    },
    {
      "epoch": 7.420523138832998,
      "grad_norm": 0.30671074986457825,
      "learning_rate": 0.00018517758325787304,
      "loss": 0.6051,
      "step": 3688
    },
    {
      "epoch": 7.422535211267606,
      "grad_norm": 0.31137824058532715,
      "learning_rate": 0.00018517355870811955,
      "loss": 0.6438,
      "step": 3689
    },
    {
      "epoch": 7.424547283702213,
      "grad_norm": 0.308694988489151,
      "learning_rate": 0.00018516953415836603,
      "loss": 0.5875,
      "step": 3690
    },
    {
      "epoch": 7.426559356136821,
      "grad_norm": 0.31494414806365967,
      "learning_rate": 0.00018516550960861254,
      "loss": 0.6088,
      "step": 3691
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 0.31623387336730957,
      "learning_rate": 0.00018516148505885903,
      "loss": 0.6484,
      "step": 3692
    },
    {
      "epoch": 7.430583501006036,
      "grad_norm": 0.2905539274215698,
      "learning_rate": 0.00018515746050910557,
      "loss": 0.5842,
      "step": 3693
    },
    {
      "epoch": 7.4325955734406435,
      "grad_norm": 0.29963940382003784,
      "learning_rate": 0.00018515343595935205,
      "loss": 0.599,
      "step": 3694
    },
    {
      "epoch": 7.434607645875252,
      "grad_norm": 0.29341286420822144,
      "learning_rate": 0.00018514941140959857,
      "loss": 0.6224,
      "step": 3695
    },
    {
      "epoch": 7.436619718309859,
      "grad_norm": 0.31163978576660156,
      "learning_rate": 0.00018514538685984505,
      "loss": 0.5981,
      "step": 3696
    },
    {
      "epoch": 7.438631790744466,
      "grad_norm": 0.320159912109375,
      "learning_rate": 0.00018514136231009156,
      "loss": 0.5572,
      "step": 3697
    },
    {
      "epoch": 7.440643863179075,
      "grad_norm": 0.3103996217250824,
      "learning_rate": 0.00018513733776033808,
      "loss": 0.6064,
      "step": 3698
    },
    {
      "epoch": 7.442655935613682,
      "grad_norm": 0.32216405868530273,
      "learning_rate": 0.0001851333132105846,
      "loss": 0.6045,
      "step": 3699
    },
    {
      "epoch": 7.44466800804829,
      "grad_norm": 0.3145750164985657,
      "learning_rate": 0.00018512928866083107,
      "loss": 0.6143,
      "step": 3700
    },
    {
      "epoch": 7.446680080482897,
      "grad_norm": 0.29488322138786316,
      "learning_rate": 0.00018512526411107759,
      "loss": 0.6042,
      "step": 3701
    },
    {
      "epoch": 7.448692152917505,
      "grad_norm": 0.30552178621292114,
      "learning_rate": 0.00018512123956132407,
      "loss": 0.6153,
      "step": 3702
    },
    {
      "epoch": 7.450704225352113,
      "grad_norm": 0.2893763482570648,
      "learning_rate": 0.0001851172150115706,
      "loss": 0.6006,
      "step": 3703
    },
    {
      "epoch": 7.45271629778672,
      "grad_norm": 0.30322614312171936,
      "learning_rate": 0.0001851131904618171,
      "loss": 0.6137,
      "step": 3704
    },
    {
      "epoch": 7.454728370221328,
      "grad_norm": 0.30575454235076904,
      "learning_rate": 0.0001851091659120636,
      "loss": 0.5785,
      "step": 3705
    },
    {
      "epoch": 7.456740442655936,
      "grad_norm": 0.32902082800865173,
      "learning_rate": 0.0001851051413623101,
      "loss": 0.5836,
      "step": 3706
    },
    {
      "epoch": 7.458752515090543,
      "grad_norm": 0.3100363314151764,
      "learning_rate": 0.0001851011168125566,
      "loss": 0.6247,
      "step": 3707
    },
    {
      "epoch": 7.460764587525151,
      "grad_norm": 0.31233638525009155,
      "learning_rate": 0.00018509709226280312,
      "loss": 0.5905,
      "step": 3708
    },
    {
      "epoch": 7.462776659959759,
      "grad_norm": 0.3284430205821991,
      "learning_rate": 0.00018509306771304963,
      "loss": 0.5776,
      "step": 3709
    },
    {
      "epoch": 7.464788732394366,
      "grad_norm": 0.3178752660751343,
      "learning_rate": 0.00018508904316329611,
      "loss": 0.6028,
      "step": 3710
    },
    {
      "epoch": 7.466800804828974,
      "grad_norm": 0.3028256595134735,
      "learning_rate": 0.00018508501861354263,
      "loss": 0.6304,
      "step": 3711
    },
    {
      "epoch": 7.468812877263582,
      "grad_norm": 0.29589998722076416,
      "learning_rate": 0.0001850809940637891,
      "loss": 0.61,
      "step": 3712
    },
    {
      "epoch": 7.470824949698189,
      "grad_norm": 0.3145149350166321,
      "learning_rate": 0.00018507696951403562,
      "loss": 0.6186,
      "step": 3713
    },
    {
      "epoch": 7.472837022132797,
      "grad_norm": 0.2825372815132141,
      "learning_rate": 0.00018507294496428214,
      "loss": 0.6113,
      "step": 3714
    },
    {
      "epoch": 7.474849094567404,
      "grad_norm": 0.3108330965042114,
      "learning_rate": 0.00018506892041452862,
      "loss": 0.563,
      "step": 3715
    },
    {
      "epoch": 7.476861167002012,
      "grad_norm": 0.31627556681632996,
      "learning_rate": 0.00018506489586477513,
      "loss": 0.6224,
      "step": 3716
    },
    {
      "epoch": 7.47887323943662,
      "grad_norm": 0.3111422657966614,
      "learning_rate": 0.00018506087131502165,
      "loss": 0.6311,
      "step": 3717
    },
    {
      "epoch": 7.480885311871227,
      "grad_norm": 0.3032503128051758,
      "learning_rate": 0.00018505684676526816,
      "loss": 0.6197,
      "step": 3718
    },
    {
      "epoch": 7.482897384305835,
      "grad_norm": 0.30385062098503113,
      "learning_rate": 0.00018505282221551464,
      "loss": 0.6035,
      "step": 3719
    },
    {
      "epoch": 7.484909456740443,
      "grad_norm": 0.29800736904144287,
      "learning_rate": 0.00018504879766576116,
      "loss": 0.5658,
      "step": 3720
    },
    {
      "epoch": 7.48692152917505,
      "grad_norm": 0.31701213121414185,
      "learning_rate": 0.00018504477311600764,
      "loss": 0.6243,
      "step": 3721
    },
    {
      "epoch": 7.4889336016096575,
      "grad_norm": 0.31170305609703064,
      "learning_rate": 0.00018504074856625415,
      "loss": 0.6041,
      "step": 3722
    },
    {
      "epoch": 7.490945674044266,
      "grad_norm": 0.2827877402305603,
      "learning_rate": 0.00018503672401650066,
      "loss": 0.5457,
      "step": 3723
    },
    {
      "epoch": 7.492957746478873,
      "grad_norm": 0.3166639506816864,
      "learning_rate": 0.00018503269946674718,
      "loss": 0.6148,
      "step": 3724
    },
    {
      "epoch": 7.494969818913481,
      "grad_norm": 0.2937287986278534,
      "learning_rate": 0.00018502867491699366,
      "loss": 0.5657,
      "step": 3725
    },
    {
      "epoch": 7.496981891348089,
      "grad_norm": 0.2997215986251831,
      "learning_rate": 0.00018502465036724017,
      "loss": 0.5833,
      "step": 3726
    },
    {
      "epoch": 7.498993963782696,
      "grad_norm": 0.2907858192920685,
      "learning_rate": 0.00018502062581748666,
      "loss": 0.5956,
      "step": 3727
    },
    {
      "epoch": 7.501006036217304,
      "grad_norm": 0.3203009366989136,
      "learning_rate": 0.0001850166012677332,
      "loss": 0.6033,
      "step": 3728
    },
    {
      "epoch": 7.503018108651911,
      "grad_norm": 0.3079999089241028,
      "learning_rate": 0.00018501257671797968,
      "loss": 0.5998,
      "step": 3729
    },
    {
      "epoch": 7.505030181086519,
      "grad_norm": 0.31454432010650635,
      "learning_rate": 0.0001850085521682262,
      "loss": 0.6218,
      "step": 3730
    },
    {
      "epoch": 7.507042253521127,
      "grad_norm": 0.29743698239326477,
      "learning_rate": 0.00018500452761847268,
      "loss": 0.6307,
      "step": 3731
    },
    {
      "epoch": 7.509054325955734,
      "grad_norm": 0.31757238507270813,
      "learning_rate": 0.0001850005030687192,
      "loss": 0.5999,
      "step": 3732
    },
    {
      "epoch": 7.5110663983903425,
      "grad_norm": 0.29978030920028687,
      "learning_rate": 0.0001849964785189657,
      "loss": 0.6112,
      "step": 3733
    },
    {
      "epoch": 7.51307847082495,
      "grad_norm": 0.30810728669166565,
      "learning_rate": 0.00018499245396921222,
      "loss": 0.6108,
      "step": 3734
    },
    {
      "epoch": 7.515090543259557,
      "grad_norm": 0.3141598105430603,
      "learning_rate": 0.0001849884294194587,
      "loss": 0.6278,
      "step": 3735
    },
    {
      "epoch": 7.517102615694165,
      "grad_norm": 0.3030368685722351,
      "learning_rate": 0.00018498440486970522,
      "loss": 0.6272,
      "step": 3736
    },
    {
      "epoch": 7.519114688128773,
      "grad_norm": 0.30941125750541687,
      "learning_rate": 0.0001849803803199517,
      "loss": 0.6189,
      "step": 3737
    },
    {
      "epoch": 7.52112676056338,
      "grad_norm": 0.3073003590106964,
      "learning_rate": 0.00018497635577019824,
      "loss": 0.6311,
      "step": 3738
    },
    {
      "epoch": 7.523138832997988,
      "grad_norm": 0.31619349122047424,
      "learning_rate": 0.00018497233122044472,
      "loss": 0.5969,
      "step": 3739
    },
    {
      "epoch": 7.525150905432596,
      "grad_norm": 0.3119567930698395,
      "learning_rate": 0.00018496830667069124,
      "loss": 0.6014,
      "step": 3740
    },
    {
      "epoch": 7.527162977867203,
      "grad_norm": 0.31226134300231934,
      "learning_rate": 0.00018496428212093772,
      "loss": 0.6165,
      "step": 3741
    },
    {
      "epoch": 7.529175050301811,
      "grad_norm": 0.3125346601009369,
      "learning_rate": 0.00018496025757118423,
      "loss": 0.5827,
      "step": 3742
    },
    {
      "epoch": 7.531187122736418,
      "grad_norm": 0.31739145517349243,
      "learning_rate": 0.00018495623302143075,
      "loss": 0.6119,
      "step": 3743
    },
    {
      "epoch": 7.533199195171026,
      "grad_norm": 0.3061307668685913,
      "learning_rate": 0.00018495220847167726,
      "loss": 0.6277,
      "step": 3744
    },
    {
      "epoch": 7.535211267605634,
      "grad_norm": 0.30328112840652466,
      "learning_rate": 0.00018494818392192374,
      "loss": 0.6478,
      "step": 3745
    },
    {
      "epoch": 7.537223340040241,
      "grad_norm": 0.31624072790145874,
      "learning_rate": 0.00018494415937217026,
      "loss": 0.6233,
      "step": 3746
    },
    {
      "epoch": 7.539235412474849,
      "grad_norm": 0.3067547678947449,
      "learning_rate": 0.00018494013482241674,
      "loss": 0.5628,
      "step": 3747
    },
    {
      "epoch": 7.541247484909457,
      "grad_norm": 0.3098922073841095,
      "learning_rate": 0.00018493611027266325,
      "loss": 0.5715,
      "step": 3748
    },
    {
      "epoch": 7.543259557344064,
      "grad_norm": 0.31505224108695984,
      "learning_rate": 0.00018493208572290977,
      "loss": 0.6277,
      "step": 3749
    },
    {
      "epoch": 7.545271629778672,
      "grad_norm": 0.33177071809768677,
      "learning_rate": 0.00018492806117315625,
      "loss": 0.636,
      "step": 3750
    },
    {
      "epoch": 7.54728370221328,
      "grad_norm": 0.3057185709476471,
      "learning_rate": 0.00018492403662340276,
      "loss": 0.6156,
      "step": 3751
    },
    {
      "epoch": 7.549295774647887,
      "grad_norm": 0.31637516617774963,
      "learning_rate": 0.00018492001207364928,
      "loss": 0.619,
      "step": 3752
    },
    {
      "epoch": 7.551307847082495,
      "grad_norm": 0.3241708278656006,
      "learning_rate": 0.0001849159875238958,
      "loss": 0.6876,
      "step": 3753
    },
    {
      "epoch": 7.553319919517103,
      "grad_norm": 0.3225438892841339,
      "learning_rate": 0.00018491196297414227,
      "loss": 0.6122,
      "step": 3754
    },
    {
      "epoch": 7.55533199195171,
      "grad_norm": 0.31011494994163513,
      "learning_rate": 0.00018490793842438878,
      "loss": 0.6076,
      "step": 3755
    },
    {
      "epoch": 7.557344064386318,
      "grad_norm": 0.3094121813774109,
      "learning_rate": 0.00018490391387463527,
      "loss": 0.5829,
      "step": 3756
    },
    {
      "epoch": 7.559356136820925,
      "grad_norm": 0.29512813687324524,
      "learning_rate": 0.00018489988932488178,
      "loss": 0.5967,
      "step": 3757
    },
    {
      "epoch": 7.561368209255534,
      "grad_norm": 0.3025163412094116,
      "learning_rate": 0.0001848958647751283,
      "loss": 0.5862,
      "step": 3758
    },
    {
      "epoch": 7.563380281690141,
      "grad_norm": 0.30898022651672363,
      "learning_rate": 0.0001848918402253748,
      "loss": 0.5974,
      "step": 3759
    },
    {
      "epoch": 7.565392354124748,
      "grad_norm": 0.3155287802219391,
      "learning_rate": 0.0001848878156756213,
      "loss": 0.5949,
      "step": 3760
    },
    {
      "epoch": 7.5674044265593565,
      "grad_norm": 0.3045366108417511,
      "learning_rate": 0.0001848837911258678,
      "loss": 0.6057,
      "step": 3761
    },
    {
      "epoch": 7.569416498993964,
      "grad_norm": 0.3256875276565552,
      "learning_rate": 0.0001848797665761143,
      "loss": 0.5779,
      "step": 3762
    },
    {
      "epoch": 7.571428571428571,
      "grad_norm": 0.309438019990921,
      "learning_rate": 0.0001848757420263608,
      "loss": 0.5974,
      "step": 3763
    },
    {
      "epoch": 7.573440643863179,
      "grad_norm": 0.29814794659614563,
      "learning_rate": 0.0001848717174766073,
      "loss": 0.6164,
      "step": 3764
    },
    {
      "epoch": 7.575452716297787,
      "grad_norm": 0.3063430190086365,
      "learning_rate": 0.00018486769292685383,
      "loss": 0.6111,
      "step": 3765
    },
    {
      "epoch": 7.577464788732394,
      "grad_norm": 0.30213630199432373,
      "learning_rate": 0.0001848636683771003,
      "loss": 0.587,
      "step": 3766
    },
    {
      "epoch": 7.579476861167002,
      "grad_norm": 0.2941371202468872,
      "learning_rate": 0.00018485964382734682,
      "loss": 0.6376,
      "step": 3767
    },
    {
      "epoch": 7.58148893360161,
      "grad_norm": 0.30413419008255005,
      "learning_rate": 0.0001848556192775933,
      "loss": 0.5989,
      "step": 3768
    },
    {
      "epoch": 7.583501006036217,
      "grad_norm": 0.32285481691360474,
      "learning_rate": 0.00018485159472783985,
      "loss": 0.6253,
      "step": 3769
    },
    {
      "epoch": 7.585513078470825,
      "grad_norm": 0.33988192677497864,
      "learning_rate": 0.00018484757017808633,
      "loss": 0.6317,
      "step": 3770
    },
    {
      "epoch": 7.5875251509054324,
      "grad_norm": 0.2946437895298004,
      "learning_rate": 0.00018484354562833284,
      "loss": 0.5696,
      "step": 3771
    },
    {
      "epoch": 7.58953722334004,
      "grad_norm": 0.3191382586956024,
      "learning_rate": 0.00018483952107857933,
      "loss": 0.6133,
      "step": 3772
    },
    {
      "epoch": 7.591549295774648,
      "grad_norm": 0.30472248792648315,
      "learning_rate": 0.00018483549652882584,
      "loss": 0.6153,
      "step": 3773
    },
    {
      "epoch": 7.593561368209255,
      "grad_norm": 0.2984521687030792,
      "learning_rate": 0.00018483147197907235,
      "loss": 0.6203,
      "step": 3774
    },
    {
      "epoch": 7.5955734406438635,
      "grad_norm": 0.29370009899139404,
      "learning_rate": 0.00018482744742931887,
      "loss": 0.6375,
      "step": 3775
    },
    {
      "epoch": 7.597585513078471,
      "grad_norm": 0.3104649782180786,
      "learning_rate": 0.00018482342287956535,
      "loss": 0.6298,
      "step": 3776
    },
    {
      "epoch": 7.599597585513078,
      "grad_norm": 0.29014790058135986,
      "learning_rate": 0.00018481939832981186,
      "loss": 0.5895,
      "step": 3777
    },
    {
      "epoch": 7.601609657947686,
      "grad_norm": 0.3034825921058655,
      "learning_rate": 0.00018481537378005835,
      "loss": 0.6257,
      "step": 3778
    },
    {
      "epoch": 7.603621730382294,
      "grad_norm": 0.3370126187801361,
      "learning_rate": 0.0001848113492303049,
      "loss": 0.6004,
      "step": 3779
    },
    {
      "epoch": 7.605633802816901,
      "grad_norm": 0.28959426283836365,
      "learning_rate": 0.00018480732468055137,
      "loss": 0.6034,
      "step": 3780
    },
    {
      "epoch": 7.607645875251509,
      "grad_norm": 0.3087228536605835,
      "learning_rate": 0.00018480330013079789,
      "loss": 0.5867,
      "step": 3781
    },
    {
      "epoch": 7.609657947686117,
      "grad_norm": 0.31705304980278015,
      "learning_rate": 0.00018479927558104437,
      "loss": 0.5375,
      "step": 3782
    },
    {
      "epoch": 7.611670020120725,
      "grad_norm": 0.31348568201065063,
      "learning_rate": 0.00018479525103129088,
      "loss": 0.6199,
      "step": 3783
    },
    {
      "epoch": 7.613682092555332,
      "grad_norm": 0.3760371506214142,
      "learning_rate": 0.0001847912264815374,
      "loss": 0.5698,
      "step": 3784
    },
    {
      "epoch": 7.6156941649899395,
      "grad_norm": 0.3372495770454407,
      "learning_rate": 0.00018478720193178388,
      "loss": 0.6033,
      "step": 3785
    },
    {
      "epoch": 7.617706237424548,
      "grad_norm": 0.3097424805164337,
      "learning_rate": 0.0001847831773820304,
      "loss": 0.5986,
      "step": 3786
    },
    {
      "epoch": 7.619718309859155,
      "grad_norm": 0.31618034839630127,
      "learning_rate": 0.0001847791528322769,
      "loss": 0.6335,
      "step": 3787
    },
    {
      "epoch": 7.621730382293762,
      "grad_norm": 0.29891395568847656,
      "learning_rate": 0.0001847751282825234,
      "loss": 0.6504,
      "step": 3788
    },
    {
      "epoch": 7.6237424547283705,
      "grad_norm": 0.3100728690624237,
      "learning_rate": 0.0001847711037327699,
      "loss": 0.5955,
      "step": 3789
    },
    {
      "epoch": 7.625754527162978,
      "grad_norm": 0.3035891056060791,
      "learning_rate": 0.00018476707918301641,
      "loss": 0.6421,
      "step": 3790
    },
    {
      "epoch": 7.627766599597585,
      "grad_norm": 0.318809449672699,
      "learning_rate": 0.0001847630546332629,
      "loss": 0.6422,
      "step": 3791
    },
    {
      "epoch": 7.629778672032193,
      "grad_norm": 0.30698204040527344,
      "learning_rate": 0.0001847590300835094,
      "loss": 0.5975,
      "step": 3792
    },
    {
      "epoch": 7.631790744466801,
      "grad_norm": 0.30463290214538574,
      "learning_rate": 0.0001847550055337559,
      "loss": 0.5766,
      "step": 3793
    },
    {
      "epoch": 7.633802816901408,
      "grad_norm": 0.33142706751823425,
      "learning_rate": 0.00018475098098400244,
      "loss": 0.5922,
      "step": 3794
    },
    {
      "epoch": 7.635814889336016,
      "grad_norm": 0.3049880862236023,
      "learning_rate": 0.00018474695643424892,
      "loss": 0.6089,
      "step": 3795
    },
    {
      "epoch": 7.637826961770624,
      "grad_norm": 0.3185148537158966,
      "learning_rate": 0.00018474293188449543,
      "loss": 0.6107,
      "step": 3796
    },
    {
      "epoch": 7.639839034205231,
      "grad_norm": 0.3209283947944641,
      "learning_rate": 0.00018473890733474192,
      "loss": 0.6229,
      "step": 3797
    },
    {
      "epoch": 7.641851106639839,
      "grad_norm": 0.3018540143966675,
      "learning_rate": 0.00018473488278498843,
      "loss": 0.5892,
      "step": 3798
    },
    {
      "epoch": 7.6438631790744465,
      "grad_norm": 0.33160093426704407,
      "learning_rate": 0.00018473085823523494,
      "loss": 0.6209,
      "step": 3799
    },
    {
      "epoch": 7.645875251509055,
      "grad_norm": 0.3130689561367035,
      "learning_rate": 0.00018472683368548146,
      "loss": 0.6391,
      "step": 3800
    },
    {
      "epoch": 7.647887323943662,
      "grad_norm": 0.3048609793186188,
      "learning_rate": 0.00018472280913572794,
      "loss": 0.6146,
      "step": 3801
    },
    {
      "epoch": 7.649899396378269,
      "grad_norm": 0.29449978470802307,
      "learning_rate": 0.00018471878458597445,
      "loss": 0.6088,
      "step": 3802
    },
    {
      "epoch": 7.6519114688128775,
      "grad_norm": 0.32287096977233887,
      "learning_rate": 0.00018471476003622094,
      "loss": 0.6152,
      "step": 3803
    },
    {
      "epoch": 7.653923541247485,
      "grad_norm": 0.3058958053588867,
      "learning_rate": 0.00018471073548646748,
      "loss": 0.6052,
      "step": 3804
    },
    {
      "epoch": 7.655935613682092,
      "grad_norm": 0.2995564639568329,
      "learning_rate": 0.00018470671093671396,
      "loss": 0.5796,
      "step": 3805
    },
    {
      "epoch": 7.6579476861167,
      "grad_norm": 0.3245334327220917,
      "learning_rate": 0.00018470268638696047,
      "loss": 0.6071,
      "step": 3806
    },
    {
      "epoch": 7.659959758551308,
      "grad_norm": 0.30783843994140625,
      "learning_rate": 0.00018469866183720696,
      "loss": 0.5848,
      "step": 3807
    },
    {
      "epoch": 7.661971830985916,
      "grad_norm": 0.3183729946613312,
      "learning_rate": 0.00018469463728745347,
      "loss": 0.6071,
      "step": 3808
    },
    {
      "epoch": 7.663983903420523,
      "grad_norm": 0.2956922650337219,
      "learning_rate": 0.00018469061273769998,
      "loss": 0.579,
      "step": 3809
    },
    {
      "epoch": 7.665995975855131,
      "grad_norm": 0.3171554505825043,
      "learning_rate": 0.0001846865881879465,
      "loss": 0.6216,
      "step": 3810
    },
    {
      "epoch": 7.668008048289739,
      "grad_norm": 0.3004923164844513,
      "learning_rate": 0.00018468256363819298,
      "loss": 0.6442,
      "step": 3811
    },
    {
      "epoch": 7.670020120724346,
      "grad_norm": 0.29210659861564636,
      "learning_rate": 0.0001846785390884395,
      "loss": 0.6194,
      "step": 3812
    },
    {
      "epoch": 7.6720321931589535,
      "grad_norm": 0.3065038323402405,
      "learning_rate": 0.00018467451453868598,
      "loss": 0.5853,
      "step": 3813
    },
    {
      "epoch": 7.674044265593562,
      "grad_norm": 0.32544323801994324,
      "learning_rate": 0.00018467048998893252,
      "loss": 0.6372,
      "step": 3814
    },
    {
      "epoch": 7.676056338028169,
      "grad_norm": 0.3103438913822174,
      "learning_rate": 0.000184666465439179,
      "loss": 0.583,
      "step": 3815
    },
    {
      "epoch": 7.678068410462776,
      "grad_norm": 0.29887014627456665,
      "learning_rate": 0.00018466244088942551,
      "loss": 0.6098,
      "step": 3816
    },
    {
      "epoch": 7.6800804828973845,
      "grad_norm": 0.2967297434806824,
      "learning_rate": 0.000184658416339672,
      "loss": 0.6159,
      "step": 3817
    },
    {
      "epoch": 7.682092555331992,
      "grad_norm": 0.29158931970596313,
      "learning_rate": 0.0001846543917899185,
      "loss": 0.6046,
      "step": 3818
    },
    {
      "epoch": 7.684104627766599,
      "grad_norm": 0.3157629668712616,
      "learning_rate": 0.00018465036724016502,
      "loss": 0.5999,
      "step": 3819
    },
    {
      "epoch": 7.686116700201207,
      "grad_norm": 0.3150407373905182,
      "learning_rate": 0.0001846463426904115,
      "loss": 0.6266,
      "step": 3820
    },
    {
      "epoch": 7.688128772635815,
      "grad_norm": 0.29711684584617615,
      "learning_rate": 0.00018464231814065802,
      "loss": 0.5936,
      "step": 3821
    },
    {
      "epoch": 7.690140845070422,
      "grad_norm": 0.323294073343277,
      "learning_rate": 0.00018463829359090453,
      "loss": 0.5862,
      "step": 3822
    },
    {
      "epoch": 7.69215291750503,
      "grad_norm": 0.29782968759536743,
      "learning_rate": 0.00018463426904115102,
      "loss": 0.623,
      "step": 3823
    },
    {
      "epoch": 7.694164989939638,
      "grad_norm": 0.2891417443752289,
      "learning_rate": 0.00018463024449139753,
      "loss": 0.608,
      "step": 3824
    },
    {
      "epoch": 7.696177062374246,
      "grad_norm": 0.30106034874916077,
      "learning_rate": 0.00018462621994164404,
      "loss": 0.5874,
      "step": 3825
    },
    {
      "epoch": 7.698189134808853,
      "grad_norm": 0.28658124804496765,
      "learning_rate": 0.00018462219539189053,
      "loss": 0.6008,
      "step": 3826
    },
    {
      "epoch": 7.7002012072434605,
      "grad_norm": 0.3051539659500122,
      "learning_rate": 0.00018461817084213704,
      "loss": 0.6017,
      "step": 3827
    },
    {
      "epoch": 7.702213279678069,
      "grad_norm": 0.2982518672943115,
      "learning_rate": 0.00018461414629238353,
      "loss": 0.6405,
      "step": 3828
    },
    {
      "epoch": 7.704225352112676,
      "grad_norm": 0.3033792972564697,
      "learning_rate": 0.00018461012174263007,
      "loss": 0.6169,
      "step": 3829
    },
    {
      "epoch": 7.706237424547283,
      "grad_norm": 0.3115798234939575,
      "learning_rate": 0.00018460609719287655,
      "loss": 0.6582,
      "step": 3830
    },
    {
      "epoch": 7.7082494969818915,
      "grad_norm": 0.3177725374698639,
      "learning_rate": 0.00018460207264312306,
      "loss": 0.6076,
      "step": 3831
    },
    {
      "epoch": 7.710261569416499,
      "grad_norm": 0.29749876260757446,
      "learning_rate": 0.00018459804809336955,
      "loss": 0.5841,
      "step": 3832
    },
    {
      "epoch": 7.712273641851107,
      "grad_norm": 0.2985267639160156,
      "learning_rate": 0.00018459402354361606,
      "loss": 0.5916,
      "step": 3833
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 0.29938459396362305,
      "learning_rate": 0.00018458999899386257,
      "loss": 0.5919,
      "step": 3834
    },
    {
      "epoch": 7.716297786720322,
      "grad_norm": 0.31000688672065735,
      "learning_rate": 0.00018458597444410908,
      "loss": 0.5979,
      "step": 3835
    },
    {
      "epoch": 7.71830985915493,
      "grad_norm": 0.2962483763694763,
      "learning_rate": 0.00018458194989435557,
      "loss": 0.6088,
      "step": 3836
    },
    {
      "epoch": 7.720321931589537,
      "grad_norm": 0.29113999009132385,
      "learning_rate": 0.00018457792534460208,
      "loss": 0.5843,
      "step": 3837
    },
    {
      "epoch": 7.722334004024145,
      "grad_norm": 0.3006965219974518,
      "learning_rate": 0.00018457390079484857,
      "loss": 0.5892,
      "step": 3838
    },
    {
      "epoch": 7.724346076458753,
      "grad_norm": 0.3187740445137024,
      "learning_rate": 0.0001845698762450951,
      "loss": 0.6379,
      "step": 3839
    },
    {
      "epoch": 7.72635814889336,
      "grad_norm": 0.30241507291793823,
      "learning_rate": 0.0001845658516953416,
      "loss": 0.5848,
      "step": 3840
    },
    {
      "epoch": 7.7283702213279675,
      "grad_norm": 0.29618924856185913,
      "learning_rate": 0.0001845618271455881,
      "loss": 0.586,
      "step": 3841
    },
    {
      "epoch": 7.730382293762576,
      "grad_norm": 0.33295193314552307,
      "learning_rate": 0.0001845578025958346,
      "loss": 0.6086,
      "step": 3842
    },
    {
      "epoch": 7.732394366197183,
      "grad_norm": 0.3045293986797333,
      "learning_rate": 0.0001845537780460811,
      "loss": 0.5797,
      "step": 3843
    },
    {
      "epoch": 7.73440643863179,
      "grad_norm": 0.2895336449146271,
      "learning_rate": 0.0001845497534963276,
      "loss": 0.6246,
      "step": 3844
    },
    {
      "epoch": 7.7364185110663986,
      "grad_norm": 0.3006502389907837,
      "learning_rate": 0.00018454572894657413,
      "loss": 0.5983,
      "step": 3845
    },
    {
      "epoch": 7.738430583501006,
      "grad_norm": 0.30821409821510315,
      "learning_rate": 0.0001845417043968206,
      "loss": 0.6416,
      "step": 3846
    },
    {
      "epoch": 7.740442655935613,
      "grad_norm": 0.3055034875869751,
      "learning_rate": 0.00018453767984706712,
      "loss": 0.5976,
      "step": 3847
    },
    {
      "epoch": 7.742454728370221,
      "grad_norm": 0.3025270998477936,
      "learning_rate": 0.0001845336552973136,
      "loss": 0.6237,
      "step": 3848
    },
    {
      "epoch": 7.744466800804829,
      "grad_norm": 0.29805946350097656,
      "learning_rate": 0.00018452963074756015,
      "loss": 0.6067,
      "step": 3849
    },
    {
      "epoch": 7.746478873239437,
      "grad_norm": 0.2951424717903137,
      "learning_rate": 0.00018452560619780663,
      "loss": 0.5954,
      "step": 3850
    },
    {
      "epoch": 7.748490945674044,
      "grad_norm": 0.3267522156238556,
      "learning_rate": 0.00018452158164805314,
      "loss": 0.5602,
      "step": 3851
    },
    {
      "epoch": 7.750503018108652,
      "grad_norm": 0.30726391077041626,
      "learning_rate": 0.00018451755709829963,
      "loss": 0.5797,
      "step": 3852
    },
    {
      "epoch": 7.75251509054326,
      "grad_norm": 0.28427359461784363,
      "learning_rate": 0.00018451353254854614,
      "loss": 0.6051,
      "step": 3853
    },
    {
      "epoch": 7.754527162977867,
      "grad_norm": 0.3137834072113037,
      "learning_rate": 0.00018450950799879265,
      "loss": 0.626,
      "step": 3854
    },
    {
      "epoch": 7.7565392354124745,
      "grad_norm": 0.3153478503227234,
      "learning_rate": 0.00018450548344903914,
      "loss": 0.5937,
      "step": 3855
    },
    {
      "epoch": 7.758551307847083,
      "grad_norm": 0.3076239228248596,
      "learning_rate": 0.00018450145889928565,
      "loss": 0.5938,
      "step": 3856
    },
    {
      "epoch": 7.76056338028169,
      "grad_norm": 0.3034018576145172,
      "learning_rate": 0.00018449743434953216,
      "loss": 0.6076,
      "step": 3857
    },
    {
      "epoch": 7.762575452716298,
      "grad_norm": 0.30966684222221375,
      "learning_rate": 0.00018449340979977865,
      "loss": 0.6055,
      "step": 3858
    },
    {
      "epoch": 7.7645875251509056,
      "grad_norm": 0.30199623107910156,
      "learning_rate": 0.00018448938525002516,
      "loss": 0.604,
      "step": 3859
    },
    {
      "epoch": 7.766599597585513,
      "grad_norm": 0.30763909220695496,
      "learning_rate": 0.00018448536070027167,
      "loss": 0.6061,
      "step": 3860
    },
    {
      "epoch": 7.768611670020121,
      "grad_norm": 0.2927539348602295,
      "learning_rate": 0.00018448133615051816,
      "loss": 0.5648,
      "step": 3861
    },
    {
      "epoch": 7.770623742454728,
      "grad_norm": 0.31993359327316284,
      "learning_rate": 0.00018447731160076467,
      "loss": 0.5895,
      "step": 3862
    },
    {
      "epoch": 7.772635814889336,
      "grad_norm": 0.3058805465698242,
      "learning_rate": 0.00018447328705101116,
      "loss": 0.5744,
      "step": 3863
    },
    {
      "epoch": 7.774647887323944,
      "grad_norm": 0.3033157289028168,
      "learning_rate": 0.0001844692625012577,
      "loss": 0.5845,
      "step": 3864
    },
    {
      "epoch": 7.776659959758551,
      "grad_norm": 0.31646403670310974,
      "learning_rate": 0.00018446523795150418,
      "loss": 0.6064,
      "step": 3865
    },
    {
      "epoch": 7.778672032193159,
      "grad_norm": 0.3073490858078003,
      "learning_rate": 0.0001844612134017507,
      "loss": 0.6201,
      "step": 3866
    },
    {
      "epoch": 7.780684104627767,
      "grad_norm": 0.31874319911003113,
      "learning_rate": 0.00018445718885199718,
      "loss": 0.6088,
      "step": 3867
    },
    {
      "epoch": 7.782696177062374,
      "grad_norm": 0.3179212212562561,
      "learning_rate": 0.0001844531643022437,
      "loss": 0.5996,
      "step": 3868
    },
    {
      "epoch": 7.7847082494969815,
      "grad_norm": 0.3078339695930481,
      "learning_rate": 0.0001844491397524902,
      "loss": 0.5771,
      "step": 3869
    },
    {
      "epoch": 7.78672032193159,
      "grad_norm": 0.31527388095855713,
      "learning_rate": 0.00018444511520273671,
      "loss": 0.5844,
      "step": 3870
    },
    {
      "epoch": 7.788732394366197,
      "grad_norm": 0.2983766496181488,
      "learning_rate": 0.0001844410906529832,
      "loss": 0.6285,
      "step": 3871
    },
    {
      "epoch": 7.790744466800804,
      "grad_norm": 0.2874498665332794,
      "learning_rate": 0.0001844370661032297,
      "loss": 0.6068,
      "step": 3872
    },
    {
      "epoch": 7.792756539235413,
      "grad_norm": 0.3134203851222992,
      "learning_rate": 0.0001844330415534762,
      "loss": 0.5665,
      "step": 3873
    },
    {
      "epoch": 7.79476861167002,
      "grad_norm": 0.3193458616733551,
      "learning_rate": 0.00018442901700372274,
      "loss": 0.6051,
      "step": 3874
    },
    {
      "epoch": 7.796780684104628,
      "grad_norm": 0.31186404824256897,
      "learning_rate": 0.00018442499245396922,
      "loss": 0.6186,
      "step": 3875
    },
    {
      "epoch": 7.798792756539235,
      "grad_norm": 0.2889246940612793,
      "learning_rate": 0.00018442096790421573,
      "loss": 0.6088,
      "step": 3876
    },
    {
      "epoch": 7.800804828973843,
      "grad_norm": 0.31454554200172424,
      "learning_rate": 0.00018441694335446222,
      "loss": 0.6102,
      "step": 3877
    },
    {
      "epoch": 7.802816901408451,
      "grad_norm": 0.308538556098938,
      "learning_rate": 0.00018441291880470873,
      "loss": 0.6292,
      "step": 3878
    },
    {
      "epoch": 7.804828973843058,
      "grad_norm": 0.300546795129776,
      "learning_rate": 0.00018440889425495524,
      "loss": 0.6268,
      "step": 3879
    },
    {
      "epoch": 7.806841046277666,
      "grad_norm": 0.3059558868408203,
      "learning_rate": 0.00018440486970520175,
      "loss": 0.6569,
      "step": 3880
    },
    {
      "epoch": 7.808853118712274,
      "grad_norm": 0.30540063977241516,
      "learning_rate": 0.00018440084515544824,
      "loss": 0.5943,
      "step": 3881
    },
    {
      "epoch": 7.810865191146881,
      "grad_norm": 0.2966700494289398,
      "learning_rate": 0.00018439682060569475,
      "loss": 0.6281,
      "step": 3882
    },
    {
      "epoch": 7.812877263581489,
      "grad_norm": 0.30185118317604065,
      "learning_rate": 0.00018439279605594124,
      "loss": 0.5745,
      "step": 3883
    },
    {
      "epoch": 7.814889336016097,
      "grad_norm": 0.3176279664039612,
      "learning_rate": 0.00018438877150618778,
      "loss": 0.6033,
      "step": 3884
    },
    {
      "epoch": 7.816901408450704,
      "grad_norm": 0.30931204557418823,
      "learning_rate": 0.00018438474695643426,
      "loss": 0.577,
      "step": 3885
    },
    {
      "epoch": 7.818913480885312,
      "grad_norm": 0.3351427912712097,
      "learning_rate": 0.00018438072240668077,
      "loss": 0.6169,
      "step": 3886
    },
    {
      "epoch": 7.82092555331992,
      "grad_norm": 0.3340681791305542,
      "learning_rate": 0.00018437669785692726,
      "loss": 0.6341,
      "step": 3887
    },
    {
      "epoch": 7.822937625754527,
      "grad_norm": 0.35345467925071716,
      "learning_rate": 0.00018437267330717377,
      "loss": 0.6069,
      "step": 3888
    },
    {
      "epoch": 7.824949698189135,
      "grad_norm": 0.2983127236366272,
      "learning_rate": 0.00018436864875742028,
      "loss": 0.6312,
      "step": 3889
    },
    {
      "epoch": 7.826961770623742,
      "grad_norm": 0.32854360342025757,
      "learning_rate": 0.00018436462420766677,
      "loss": 0.6071,
      "step": 3890
    },
    {
      "epoch": 7.82897384305835,
      "grad_norm": 0.299530565738678,
      "learning_rate": 0.00018436059965791328,
      "loss": 0.587,
      "step": 3891
    },
    {
      "epoch": 7.830985915492958,
      "grad_norm": 0.3345330059528351,
      "learning_rate": 0.00018435657510815977,
      "loss": 0.5707,
      "step": 3892
    },
    {
      "epoch": 7.832997987927565,
      "grad_norm": 0.2916523218154907,
      "learning_rate": 0.00018435255055840628,
      "loss": 0.6474,
      "step": 3893
    },
    {
      "epoch": 7.835010060362173,
      "grad_norm": 0.3229615092277527,
      "learning_rate": 0.0001843485260086528,
      "loss": 0.6083,
      "step": 3894
    },
    {
      "epoch": 7.837022132796781,
      "grad_norm": 0.3057841360569,
      "learning_rate": 0.0001843445014588993,
      "loss": 0.6078,
      "step": 3895
    },
    {
      "epoch": 7.839034205231388,
      "grad_norm": 0.3012442886829376,
      "learning_rate": 0.0001843404769091458,
      "loss": 0.6266,
      "step": 3896
    },
    {
      "epoch": 7.8410462776659955,
      "grad_norm": 0.30102667212486267,
      "learning_rate": 0.0001843364523593923,
      "loss": 0.6089,
      "step": 3897
    },
    {
      "epoch": 7.843058350100604,
      "grad_norm": 0.29749757051467896,
      "learning_rate": 0.00018433242780963878,
      "loss": 0.5754,
      "step": 3898
    },
    {
      "epoch": 7.845070422535211,
      "grad_norm": 0.30591434240341187,
      "learning_rate": 0.00018432840325988532,
      "loss": 0.5916,
      "step": 3899
    },
    {
      "epoch": 7.847082494969819,
      "grad_norm": 0.30515334010124207,
      "learning_rate": 0.0001843243787101318,
      "loss": 0.6305,
      "step": 3900
    },
    {
      "epoch": 7.849094567404427,
      "grad_norm": 0.3054271936416626,
      "learning_rate": 0.00018432035416037832,
      "loss": 0.6015,
      "step": 3901
    },
    {
      "epoch": 7.851106639839034,
      "grad_norm": 0.3039834201335907,
      "learning_rate": 0.0001843163296106248,
      "loss": 0.6031,
      "step": 3902
    },
    {
      "epoch": 7.853118712273642,
      "grad_norm": 0.30724355578422546,
      "learning_rate": 0.00018431230506087132,
      "loss": 0.5882,
      "step": 3903
    },
    {
      "epoch": 7.855130784708249,
      "grad_norm": 0.2871743440628052,
      "learning_rate": 0.00018430828051111783,
      "loss": 0.563,
      "step": 3904
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 0.3040030002593994,
      "learning_rate": 0.00018430425596136434,
      "loss": 0.5582,
      "step": 3905
    },
    {
      "epoch": 7.859154929577465,
      "grad_norm": 0.30457666516304016,
      "learning_rate": 0.00018430023141161083,
      "loss": 0.6106,
      "step": 3906
    },
    {
      "epoch": 7.861167002012072,
      "grad_norm": 0.30312055349349976,
      "learning_rate": 0.00018429620686185734,
      "loss": 0.5829,
      "step": 3907
    },
    {
      "epoch": 7.8631790744466805,
      "grad_norm": 0.30906298756599426,
      "learning_rate": 0.00018429218231210383,
      "loss": 0.5726,
      "step": 3908
    },
    {
      "epoch": 7.865191146881288,
      "grad_norm": 0.2931908667087555,
      "learning_rate": 0.00018428815776235037,
      "loss": 0.6469,
      "step": 3909
    },
    {
      "epoch": 7.867203219315895,
      "grad_norm": 0.3092467188835144,
      "learning_rate": 0.00018428413321259685,
      "loss": 0.628,
      "step": 3910
    },
    {
      "epoch": 7.869215291750503,
      "grad_norm": 0.30116286873817444,
      "learning_rate": 0.00018428010866284336,
      "loss": 0.6003,
      "step": 3911
    },
    {
      "epoch": 7.871227364185111,
      "grad_norm": 0.30102184414863586,
      "learning_rate": 0.00018427608411308985,
      "loss": 0.6199,
      "step": 3912
    },
    {
      "epoch": 7.873239436619718,
      "grad_norm": 0.3024139702320099,
      "learning_rate": 0.00018427205956333636,
      "loss": 0.6212,
      "step": 3913
    },
    {
      "epoch": 7.875251509054326,
      "grad_norm": 0.3010469377040863,
      "learning_rate": 0.00018426803501358287,
      "loss": 0.5772,
      "step": 3914
    },
    {
      "epoch": 7.877263581488934,
      "grad_norm": 0.32403501868247986,
      "learning_rate": 0.00018426401046382938,
      "loss": 0.6278,
      "step": 3915
    },
    {
      "epoch": 7.879275653923541,
      "grad_norm": 0.3098800778388977,
      "learning_rate": 0.00018425998591407587,
      "loss": 0.5983,
      "step": 3916
    },
    {
      "epoch": 7.881287726358149,
      "grad_norm": 0.30536797642707825,
      "learning_rate": 0.00018425596136432238,
      "loss": 0.6037,
      "step": 3917
    },
    {
      "epoch": 7.883299798792756,
      "grad_norm": 0.2884826362133026,
      "learning_rate": 0.00018425193681456887,
      "loss": 0.5844,
      "step": 3918
    },
    {
      "epoch": 7.885311871227364,
      "grad_norm": 0.2951485216617584,
      "learning_rate": 0.00018424791226481538,
      "loss": 0.5954,
      "step": 3919
    },
    {
      "epoch": 7.887323943661972,
      "grad_norm": 0.31456294655799866,
      "learning_rate": 0.0001842438877150619,
      "loss": 0.601,
      "step": 3920
    },
    {
      "epoch": 7.889336016096579,
      "grad_norm": 0.3122164011001587,
      "learning_rate": 0.0001842398631653084,
      "loss": 0.633,
      "step": 3921
    },
    {
      "epoch": 7.891348088531187,
      "grad_norm": 0.30980589985847473,
      "learning_rate": 0.0001842358386155549,
      "loss": 0.6648,
      "step": 3922
    },
    {
      "epoch": 7.893360160965795,
      "grad_norm": 0.3023490011692047,
      "learning_rate": 0.0001842318140658014,
      "loss": 0.6023,
      "step": 3923
    },
    {
      "epoch": 7.895372233400402,
      "grad_norm": 0.3173123896121979,
      "learning_rate": 0.0001842277895160479,
      "loss": 0.6192,
      "step": 3924
    },
    {
      "epoch": 7.89738430583501,
      "grad_norm": 0.29579028487205505,
      "learning_rate": 0.0001842237649662944,
      "loss": 0.5924,
      "step": 3925
    },
    {
      "epoch": 7.899396378269618,
      "grad_norm": 0.31720438599586487,
      "learning_rate": 0.0001842197404165409,
      "loss": 0.5993,
      "step": 3926
    },
    {
      "epoch": 7.901408450704225,
      "grad_norm": 0.3202791213989258,
      "learning_rate": 0.0001842157158667874,
      "loss": 0.6397,
      "step": 3927
    },
    {
      "epoch": 7.903420523138833,
      "grad_norm": 0.31884101033210754,
      "learning_rate": 0.0001842116913170339,
      "loss": 0.6022,
      "step": 3928
    },
    {
      "epoch": 7.905432595573441,
      "grad_norm": 0.31356728076934814,
      "learning_rate": 0.00018420766676728042,
      "loss": 0.6062,
      "step": 3929
    },
    {
      "epoch": 7.907444668008048,
      "grad_norm": 0.3137269616127014,
      "learning_rate": 0.00018420364221752693,
      "loss": 0.6389,
      "step": 3930
    },
    {
      "epoch": 7.909456740442656,
      "grad_norm": 0.3497375249862671,
      "learning_rate": 0.00018419961766777342,
      "loss": 0.6569,
      "step": 3931
    },
    {
      "epoch": 7.9114688128772634,
      "grad_norm": 0.3159787952899933,
      "learning_rate": 0.00018419559311801993,
      "loss": 0.6316,
      "step": 3932
    },
    {
      "epoch": 7.913480885311872,
      "grad_norm": 0.3136949837207794,
      "learning_rate": 0.00018419156856826641,
      "loss": 0.6211,
      "step": 3933
    },
    {
      "epoch": 7.915492957746479,
      "grad_norm": 0.3001473546028137,
      "learning_rate": 0.00018418754401851295,
      "loss": 0.6031,
      "step": 3934
    },
    {
      "epoch": 7.917505030181086,
      "grad_norm": 0.3040471076965332,
      "learning_rate": 0.00018418351946875944,
      "loss": 0.6004,
      "step": 3935
    },
    {
      "epoch": 7.9195171026156945,
      "grad_norm": 0.31230345368385315,
      "learning_rate": 0.00018417949491900595,
      "loss": 0.6117,
      "step": 3936
    },
    {
      "epoch": 7.921529175050302,
      "grad_norm": 0.290977418422699,
      "learning_rate": 0.00018417547036925244,
      "loss": 0.6398,
      "step": 3937
    },
    {
      "epoch": 7.923541247484909,
      "grad_norm": 0.29945486783981323,
      "learning_rate": 0.00018417144581949895,
      "loss": 0.6067,
      "step": 3938
    },
    {
      "epoch": 7.925553319919517,
      "grad_norm": 0.28768324851989746,
      "learning_rate": 0.00018416742126974546,
      "loss": 0.6115,
      "step": 3939
    },
    {
      "epoch": 7.927565392354125,
      "grad_norm": 0.3256227374076843,
      "learning_rate": 0.00018416339671999197,
      "loss": 0.6092,
      "step": 3940
    },
    {
      "epoch": 7.929577464788732,
      "grad_norm": 0.2978951930999756,
      "learning_rate": 0.00018415937217023846,
      "loss": 0.5888,
      "step": 3941
    },
    {
      "epoch": 7.93158953722334,
      "grad_norm": 0.29469433426856995,
      "learning_rate": 0.00018415534762048497,
      "loss": 0.6048,
      "step": 3942
    },
    {
      "epoch": 7.933601609657948,
      "grad_norm": 0.2985520362854004,
      "learning_rate": 0.00018415132307073146,
      "loss": 0.6139,
      "step": 3943
    },
    {
      "epoch": 7.935613682092555,
      "grad_norm": 0.3135456442832947,
      "learning_rate": 0.000184147298520978,
      "loss": 0.6208,
      "step": 3944
    },
    {
      "epoch": 7.937625754527163,
      "grad_norm": 0.3154447674751282,
      "learning_rate": 0.00018414327397122448,
      "loss": 0.6046,
      "step": 3945
    },
    {
      "epoch": 7.9396378269617705,
      "grad_norm": 0.3276667594909668,
      "learning_rate": 0.000184139249421471,
      "loss": 0.6045,
      "step": 3946
    },
    {
      "epoch": 7.941649899396378,
      "grad_norm": 0.3211591839790344,
      "learning_rate": 0.00018413522487171748,
      "loss": 0.6245,
      "step": 3947
    },
    {
      "epoch": 7.943661971830986,
      "grad_norm": 0.30211660265922546,
      "learning_rate": 0.000184131200321964,
      "loss": 0.5906,
      "step": 3948
    },
    {
      "epoch": 7.945674044265593,
      "grad_norm": 0.3083512783050537,
      "learning_rate": 0.0001841271757722105,
      "loss": 0.5908,
      "step": 3949
    },
    {
      "epoch": 7.9476861167002015,
      "grad_norm": 0.3038877248764038,
      "learning_rate": 0.00018412315122245701,
      "loss": 0.6358,
      "step": 3950
    },
    {
      "epoch": 7.949698189134809,
      "grad_norm": 0.3078017830848694,
      "learning_rate": 0.0001841191266727035,
      "loss": 0.6322,
      "step": 3951
    },
    {
      "epoch": 7.951710261569416,
      "grad_norm": 0.2916727662086487,
      "learning_rate": 0.00018411510212295,
      "loss": 0.5698,
      "step": 3952
    },
    {
      "epoch": 7.953722334004024,
      "grad_norm": 0.2949788272380829,
      "learning_rate": 0.0001841110775731965,
      "loss": 0.619,
      "step": 3953
    },
    {
      "epoch": 7.955734406438632,
      "grad_norm": 0.29714247584342957,
      "learning_rate": 0.000184107053023443,
      "loss": 0.609,
      "step": 3954
    },
    {
      "epoch": 7.957746478873239,
      "grad_norm": 0.30426064133644104,
      "learning_rate": 0.00018410302847368952,
      "loss": 0.595,
      "step": 3955
    },
    {
      "epoch": 7.959758551307847,
      "grad_norm": 0.31266075372695923,
      "learning_rate": 0.00018409900392393603,
      "loss": 0.5906,
      "step": 3956
    },
    {
      "epoch": 7.961770623742455,
      "grad_norm": 0.321473091840744,
      "learning_rate": 0.00018409497937418252,
      "loss": 0.6247,
      "step": 3957
    },
    {
      "epoch": 7.963782696177063,
      "grad_norm": 0.30204111337661743,
      "learning_rate": 0.00018409095482442903,
      "loss": 0.6334,
      "step": 3958
    },
    {
      "epoch": 7.96579476861167,
      "grad_norm": 0.3152894079685211,
      "learning_rate": 0.00018408693027467554,
      "loss": 0.6161,
      "step": 3959
    },
    {
      "epoch": 7.9678068410462775,
      "grad_norm": 0.3098990321159363,
      "learning_rate": 0.00018408290572492203,
      "loss": 0.6536,
      "step": 3960
    },
    {
      "epoch": 7.969818913480886,
      "grad_norm": 0.31830504536628723,
      "learning_rate": 0.00018407888117516854,
      "loss": 0.5741,
      "step": 3961
    },
    {
      "epoch": 7.971830985915493,
      "grad_norm": 0.3043207824230194,
      "learning_rate": 0.00018407485662541502,
      "loss": 0.6279,
      "step": 3962
    },
    {
      "epoch": 7.9738430583501,
      "grad_norm": 0.2873075008392334,
      "learning_rate": 0.00018407083207566154,
      "loss": 0.6214,
      "step": 3963
    },
    {
      "epoch": 7.9758551307847085,
      "grad_norm": 0.3192310631275177,
      "learning_rate": 0.00018406680752590805,
      "loss": 0.6232,
      "step": 3964
    },
    {
      "epoch": 7.977867203219316,
      "grad_norm": 0.31391799449920654,
      "learning_rate": 0.00018406278297615456,
      "loss": 0.6361,
      "step": 3965
    },
    {
      "epoch": 7.979879275653923,
      "grad_norm": 0.3137444853782654,
      "learning_rate": 0.00018405875842640105,
      "loss": 0.5994,
      "step": 3966
    },
    {
      "epoch": 7.981891348088531,
      "grad_norm": 0.3014979064464569,
      "learning_rate": 0.00018405473387664756,
      "loss": 0.6078,
      "step": 3967
    },
    {
      "epoch": 7.983903420523139,
      "grad_norm": 0.3044586777687073,
      "learning_rate": 0.00018405070932689404,
      "loss": 0.5848,
      "step": 3968
    },
    {
      "epoch": 7.985915492957746,
      "grad_norm": 0.304306298494339,
      "learning_rate": 0.00018404668477714058,
      "loss": 0.6484,
      "step": 3969
    },
    {
      "epoch": 7.987927565392354,
      "grad_norm": 0.3215855658054352,
      "learning_rate": 0.00018404266022738707,
      "loss": 0.5845,
      "step": 3970
    },
    {
      "epoch": 7.989939637826962,
      "grad_norm": 0.30474960803985596,
      "learning_rate": 0.00018403863567763358,
      "loss": 0.603,
      "step": 3971
    },
    {
      "epoch": 7.991951710261569,
      "grad_norm": 0.29758134484291077,
      "learning_rate": 0.00018403461112788007,
      "loss": 0.603,
      "step": 3972
    },
    {
      "epoch": 7.993963782696177,
      "grad_norm": 0.30814090371131897,
      "learning_rate": 0.00018403058657812658,
      "loss": 0.6272,
      "step": 3973
    },
    {
      "epoch": 7.9959758551307845,
      "grad_norm": 0.2987096607685089,
      "learning_rate": 0.0001840265620283731,
      "loss": 0.6067,
      "step": 3974
    },
    {
      "epoch": 7.997987927565393,
      "grad_norm": 0.3193155825138092,
      "learning_rate": 0.0001840225374786196,
      "loss": 0.6556,
      "step": 3975
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.3066348135471344,
      "learning_rate": 0.0001840185129288661,
      "loss": 0.6246,
      "step": 3976
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.7283226847648621,
      "eval_runtime": 49.8379,
      "eval_samples_per_second": 19.905,
      "eval_steps_per_second": 2.488,
      "step": 3976
    },
    {
      "epoch": 8.002012072434608,
      "grad_norm": 0.2977255880832672,
      "learning_rate": 0.0001840144883791126,
      "loss": 0.6057,
      "step": 3977
    },
    {
      "epoch": 8.004024144869215,
      "grad_norm": 0.2862827181816101,
      "learning_rate": 0.00018401046382935908,
      "loss": 0.573,
      "step": 3978
    },
    {
      "epoch": 8.006036217303823,
      "grad_norm": 0.30256935954093933,
      "learning_rate": 0.00018400643927960562,
      "loss": 0.5272,
      "step": 3979
    },
    {
      "epoch": 8.008048289738431,
      "grad_norm": 0.30204901099205017,
      "learning_rate": 0.0001840024147298521,
      "loss": 0.5681,
      "step": 3980
    },
    {
      "epoch": 8.010060362173038,
      "grad_norm": 0.3286653459072113,
      "learning_rate": 0.00018399839018009862,
      "loss": 0.605,
      "step": 3981
    },
    {
      "epoch": 8.012072434607646,
      "grad_norm": 0.33922430872917175,
      "learning_rate": 0.0001839943656303451,
      "loss": 0.5683,
      "step": 3982
    },
    {
      "epoch": 8.014084507042254,
      "grad_norm": 0.3188481628894806,
      "learning_rate": 0.00018399034108059162,
      "loss": 0.5541,
      "step": 3983
    },
    {
      "epoch": 8.01609657947686,
      "grad_norm": 0.31280645728111267,
      "learning_rate": 0.00018398631653083813,
      "loss": 0.5721,
      "step": 3984
    },
    {
      "epoch": 8.018108651911469,
      "grad_norm": 0.31341642141342163,
      "learning_rate": 0.00018398229198108464,
      "loss": 0.5676,
      "step": 3985
    },
    {
      "epoch": 8.020120724346077,
      "grad_norm": 0.31148025393486023,
      "learning_rate": 0.00018397826743133113,
      "loss": 0.5758,
      "step": 3986
    },
    {
      "epoch": 8.022132796780683,
      "grad_norm": 0.3254360854625702,
      "learning_rate": 0.00018397424288157764,
      "loss": 0.5383,
      "step": 3987
    },
    {
      "epoch": 8.024144869215291,
      "grad_norm": 0.3253933787345886,
      "learning_rate": 0.00018397021833182413,
      "loss": 0.5594,
      "step": 3988
    },
    {
      "epoch": 8.0261569416499,
      "grad_norm": 0.3186008930206299,
      "learning_rate": 0.00018396619378207064,
      "loss": 0.5347,
      "step": 3989
    },
    {
      "epoch": 8.028169014084508,
      "grad_norm": 0.3417876958847046,
      "learning_rate": 0.00018396216923231715,
      "loss": 0.6027,
      "step": 3990
    },
    {
      "epoch": 8.030181086519114,
      "grad_norm": 0.3354881703853607,
      "learning_rate": 0.00018395814468256366,
      "loss": 0.5611,
      "step": 3991
    },
    {
      "epoch": 8.032193158953723,
      "grad_norm": 0.31179624795913696,
      "learning_rate": 0.00018395412013281015,
      "loss": 0.5831,
      "step": 3992
    },
    {
      "epoch": 8.03420523138833,
      "grad_norm": 0.31532806158065796,
      "learning_rate": 0.00018395009558305666,
      "loss": 0.5724,
      "step": 3993
    },
    {
      "epoch": 8.036217303822937,
      "grad_norm": 0.32174786925315857,
      "learning_rate": 0.00018394607103330317,
      "loss": 0.5319,
      "step": 3994
    },
    {
      "epoch": 8.038229376257545,
      "grad_norm": 0.3204638659954071,
      "learning_rate": 0.00018394204648354966,
      "loss": 0.5833,
      "step": 3995
    },
    {
      "epoch": 8.040241448692154,
      "grad_norm": 0.32712599635124207,
      "learning_rate": 0.00018393802193379617,
      "loss": 0.5828,
      "step": 3996
    },
    {
      "epoch": 8.04225352112676,
      "grad_norm": 0.32431039214134216,
      "learning_rate": 0.00018393399738404265,
      "loss": 0.5995,
      "step": 3997
    },
    {
      "epoch": 8.044265593561368,
      "grad_norm": 0.3516981899738312,
      "learning_rate": 0.00018392997283428917,
      "loss": 0.5651,
      "step": 3998
    },
    {
      "epoch": 8.046277665995976,
      "grad_norm": 0.3099937438964844,
      "learning_rate": 0.00018392594828453568,
      "loss": 0.5569,
      "step": 3999
    },
    {
      "epoch": 8.048289738430583,
      "grad_norm": 0.36071404814720154,
      "learning_rate": 0.0001839219237347822,
      "loss": 0.557,
      "step": 4000
    },
    {
      "epoch": 8.050301810865191,
      "grad_norm": 0.3276440501213074,
      "learning_rate": 0.00018391789918502868,
      "loss": 0.5827,
      "step": 4001
    },
    {
      "epoch": 8.0523138832998,
      "grad_norm": 0.33440491557121277,
      "learning_rate": 0.0001839138746352752,
      "loss": 0.5653,
      "step": 4002
    },
    {
      "epoch": 8.054325955734406,
      "grad_norm": 0.33185410499572754,
      "learning_rate": 0.00018390985008552167,
      "loss": 0.5834,
      "step": 4003
    },
    {
      "epoch": 8.056338028169014,
      "grad_norm": 0.3251165747642517,
      "learning_rate": 0.0001839058255357682,
      "loss": 0.5632,
      "step": 4004
    },
    {
      "epoch": 8.058350100603622,
      "grad_norm": 0.3223230838775635,
      "learning_rate": 0.0001839018009860147,
      "loss": 0.5409,
      "step": 4005
    },
    {
      "epoch": 8.060362173038229,
      "grad_norm": 0.3363652527332306,
      "learning_rate": 0.0001838977764362612,
      "loss": 0.5848,
      "step": 4006
    },
    {
      "epoch": 8.062374245472837,
      "grad_norm": 0.3100391924381256,
      "learning_rate": 0.0001838937518865077,
      "loss": 0.5705,
      "step": 4007
    },
    {
      "epoch": 8.064386317907445,
      "grad_norm": 0.3185870349407196,
      "learning_rate": 0.0001838897273367542,
      "loss": 0.5581,
      "step": 4008
    },
    {
      "epoch": 8.066398390342052,
      "grad_norm": 0.3353888988494873,
      "learning_rate": 0.0001838857027870007,
      "loss": 0.5881,
      "step": 4009
    },
    {
      "epoch": 8.06841046277666,
      "grad_norm": 0.317975252866745,
      "learning_rate": 0.00018388167823724723,
      "loss": 0.5324,
      "step": 4010
    },
    {
      "epoch": 8.070422535211268,
      "grad_norm": 0.3191041946411133,
      "learning_rate": 0.00018387765368749372,
      "loss": 0.5071,
      "step": 4011
    },
    {
      "epoch": 8.072434607645874,
      "grad_norm": 0.32183006405830383,
      "learning_rate": 0.00018387362913774023,
      "loss": 0.5472,
      "step": 4012
    },
    {
      "epoch": 8.074446680080483,
      "grad_norm": 0.34144678711891174,
      "learning_rate": 0.00018386960458798671,
      "loss": 0.6142,
      "step": 4013
    },
    {
      "epoch": 8.07645875251509,
      "grad_norm": 0.31763383746147156,
      "learning_rate": 0.00018386558003823323,
      "loss": 0.5427,
      "step": 4014
    },
    {
      "epoch": 8.078470824949699,
      "grad_norm": 0.34155938029289246,
      "learning_rate": 0.00018386155548847974,
      "loss": 0.559,
      "step": 4015
    },
    {
      "epoch": 8.080482897384305,
      "grad_norm": 0.33313870429992676,
      "learning_rate": 0.00018385753093872625,
      "loss": 0.5504,
      "step": 4016
    },
    {
      "epoch": 8.082494969818914,
      "grad_norm": 0.32398784160614014,
      "learning_rate": 0.00018385350638897274,
      "loss": 0.5595,
      "step": 4017
    },
    {
      "epoch": 8.084507042253522,
      "grad_norm": 0.33784258365631104,
      "learning_rate": 0.00018384948183921925,
      "loss": 0.5636,
      "step": 4018
    },
    {
      "epoch": 8.086519114688128,
      "grad_norm": 0.3259287476539612,
      "learning_rate": 0.00018384545728946573,
      "loss": 0.5687,
      "step": 4019
    },
    {
      "epoch": 8.088531187122737,
      "grad_norm": 0.32259705662727356,
      "learning_rate": 0.00018384143273971227,
      "loss": 0.5387,
      "step": 4020
    },
    {
      "epoch": 8.090543259557345,
      "grad_norm": 0.32119929790496826,
      "learning_rate": 0.00018383740818995876,
      "loss": 0.554,
      "step": 4021
    },
    {
      "epoch": 8.092555331991951,
      "grad_norm": 0.30540552735328674,
      "learning_rate": 0.00018383338364020527,
      "loss": 0.5474,
      "step": 4022
    },
    {
      "epoch": 8.09456740442656,
      "grad_norm": 0.3243469297885895,
      "learning_rate": 0.00018382935909045175,
      "loss": 0.5703,
      "step": 4023
    },
    {
      "epoch": 8.096579476861168,
      "grad_norm": 0.3327174484729767,
      "learning_rate": 0.00018382533454069827,
      "loss": 0.6015,
      "step": 4024
    },
    {
      "epoch": 8.098591549295774,
      "grad_norm": 0.3214159905910492,
      "learning_rate": 0.00018382130999094478,
      "loss": 0.541,
      "step": 4025
    },
    {
      "epoch": 8.100603621730382,
      "grad_norm": 0.3246605098247528,
      "learning_rate": 0.0001838172854411913,
      "loss": 0.5218,
      "step": 4026
    },
    {
      "epoch": 8.10261569416499,
      "grad_norm": 0.3254704177379608,
      "learning_rate": 0.00018381326089143778,
      "loss": 0.5508,
      "step": 4027
    },
    {
      "epoch": 8.104627766599597,
      "grad_norm": 0.335767537355423,
      "learning_rate": 0.0001838092363416843,
      "loss": 0.5695,
      "step": 4028
    },
    {
      "epoch": 8.106639839034205,
      "grad_norm": 0.3655865490436554,
      "learning_rate": 0.00018380521179193077,
      "loss": 0.5803,
      "step": 4029
    },
    {
      "epoch": 8.108651911468813,
      "grad_norm": 0.32967546582221985,
      "learning_rate": 0.00018380118724217729,
      "loss": 0.5521,
      "step": 4030
    },
    {
      "epoch": 8.11066398390342,
      "grad_norm": 0.35822540521621704,
      "learning_rate": 0.0001837971626924238,
      "loss": 0.5527,
      "step": 4031
    },
    {
      "epoch": 8.112676056338028,
      "grad_norm": 0.3261033296585083,
      "learning_rate": 0.00018379313814267028,
      "loss": 0.597,
      "step": 4032
    },
    {
      "epoch": 8.114688128772636,
      "grad_norm": 0.3208509683609009,
      "learning_rate": 0.0001837891135929168,
      "loss": 0.5707,
      "step": 4033
    },
    {
      "epoch": 8.116700201207243,
      "grad_norm": 0.33075621724128723,
      "learning_rate": 0.00018378508904316328,
      "loss": 0.5409,
      "step": 4034
    },
    {
      "epoch": 8.11871227364185,
      "grad_norm": 0.3152954578399658,
      "learning_rate": 0.00018378106449340982,
      "loss": 0.5715,
      "step": 4035
    },
    {
      "epoch": 8.120724346076459,
      "grad_norm": 0.36238881945610046,
      "learning_rate": 0.0001837770399436563,
      "loss": 0.6159,
      "step": 4036
    },
    {
      "epoch": 8.122736418511066,
      "grad_norm": 0.3375072777271271,
      "learning_rate": 0.00018377301539390282,
      "loss": 0.5865,
      "step": 4037
    },
    {
      "epoch": 8.124748490945674,
      "grad_norm": 0.32663124799728394,
      "learning_rate": 0.0001837689908441493,
      "loss": 0.5784,
      "step": 4038
    },
    {
      "epoch": 8.126760563380282,
      "grad_norm": 0.32755813002586365,
      "learning_rate": 0.00018376496629439581,
      "loss": 0.5765,
      "step": 4039
    },
    {
      "epoch": 8.12877263581489,
      "grad_norm": 0.3341127932071686,
      "learning_rate": 0.00018376094174464233,
      "loss": 0.5598,
      "step": 4040
    },
    {
      "epoch": 8.130784708249497,
      "grad_norm": 0.3202272057533264,
      "learning_rate": 0.00018375691719488884,
      "loss": 0.5716,
      "step": 4041
    },
    {
      "epoch": 8.132796780684105,
      "grad_norm": 0.3448027968406677,
      "learning_rate": 0.00018375289264513532,
      "loss": 0.5958,
      "step": 4042
    },
    {
      "epoch": 8.134808853118713,
      "grad_norm": 0.3374834358692169,
      "learning_rate": 0.00018374886809538184,
      "loss": 0.5591,
      "step": 4043
    },
    {
      "epoch": 8.13682092555332,
      "grad_norm": 0.33349019289016724,
      "learning_rate": 0.00018374484354562832,
      "loss": 0.5671,
      "step": 4044
    },
    {
      "epoch": 8.138832997987928,
      "grad_norm": 0.3262869715690613,
      "learning_rate": 0.00018374081899587486,
      "loss": 0.5789,
      "step": 4045
    },
    {
      "epoch": 8.140845070422536,
      "grad_norm": 0.340437650680542,
      "learning_rate": 0.00018373679444612135,
      "loss": 0.5561,
      "step": 4046
    },
    {
      "epoch": 8.142857142857142,
      "grad_norm": 0.32993143796920776,
      "learning_rate": 0.00018373276989636786,
      "loss": 0.5392,
      "step": 4047
    },
    {
      "epoch": 8.14486921529175,
      "grad_norm": 0.3336632251739502,
      "learning_rate": 0.00018372874534661434,
      "loss": 0.5729,
      "step": 4048
    },
    {
      "epoch": 8.146881287726359,
      "grad_norm": 0.33594274520874023,
      "learning_rate": 0.00018372472079686086,
      "loss": 0.5596,
      "step": 4049
    },
    {
      "epoch": 8.148893360160965,
      "grad_norm": 0.340178519487381,
      "learning_rate": 0.00018372069624710737,
      "loss": 0.5971,
      "step": 4050
    },
    {
      "epoch": 8.150905432595573,
      "grad_norm": 0.3186802864074707,
      "learning_rate": 0.00018371667169735388,
      "loss": 0.5436,
      "step": 4051
    },
    {
      "epoch": 8.152917505030182,
      "grad_norm": 0.3283087909221649,
      "learning_rate": 0.00018371264714760037,
      "loss": 0.5866,
      "step": 4052
    },
    {
      "epoch": 8.154929577464788,
      "grad_norm": 0.33488813042640686,
      "learning_rate": 0.00018370862259784688,
      "loss": 0.5491,
      "step": 4053
    },
    {
      "epoch": 8.156941649899396,
      "grad_norm": 0.31816357374191284,
      "learning_rate": 0.00018370459804809336,
      "loss": 0.5932,
      "step": 4054
    },
    {
      "epoch": 8.158953722334005,
      "grad_norm": 0.3415926694869995,
      "learning_rate": 0.0001837005734983399,
      "loss": 0.5913,
      "step": 4055
    },
    {
      "epoch": 8.160965794768611,
      "grad_norm": 0.3399929106235504,
      "learning_rate": 0.0001836965489485864,
      "loss": 0.6423,
      "step": 4056
    },
    {
      "epoch": 8.16297786720322,
      "grad_norm": 0.3255767524242401,
      "learning_rate": 0.0001836925243988329,
      "loss": 0.5295,
      "step": 4057
    },
    {
      "epoch": 8.164989939637827,
      "grad_norm": 0.34336069226264954,
      "learning_rate": 0.00018368849984907938,
      "loss": 0.5566,
      "step": 4058
    },
    {
      "epoch": 8.167002012072434,
      "grad_norm": 0.3299746811389923,
      "learning_rate": 0.0001836844752993259,
      "loss": 0.5192,
      "step": 4059
    },
    {
      "epoch": 8.169014084507042,
      "grad_norm": 0.3196984827518463,
      "learning_rate": 0.0001836804507495724,
      "loss": 0.5811,
      "step": 4060
    },
    {
      "epoch": 8.17102615694165,
      "grad_norm": 0.3430146872997284,
      "learning_rate": 0.0001836764261998189,
      "loss": 0.5614,
      "step": 4061
    },
    {
      "epoch": 8.173038229376257,
      "grad_norm": 0.31386351585388184,
      "learning_rate": 0.0001836724016500654,
      "loss": 0.5441,
      "step": 4062
    },
    {
      "epoch": 8.175050301810865,
      "grad_norm": 0.32892856001853943,
      "learning_rate": 0.00018366837710031192,
      "loss": 0.5746,
      "step": 4063
    },
    {
      "epoch": 8.177062374245473,
      "grad_norm": 0.3302125334739685,
      "learning_rate": 0.0001836643525505584,
      "loss": 0.5356,
      "step": 4064
    },
    {
      "epoch": 8.179074446680081,
      "grad_norm": 0.344033420085907,
      "learning_rate": 0.00018366032800080492,
      "loss": 0.5576,
      "step": 4065
    },
    {
      "epoch": 8.181086519114688,
      "grad_norm": 0.3479980528354645,
      "learning_rate": 0.00018365630345105143,
      "loss": 0.5772,
      "step": 4066
    },
    {
      "epoch": 8.183098591549296,
      "grad_norm": 0.31869396567344666,
      "learning_rate": 0.0001836522789012979,
      "loss": 0.5558,
      "step": 4067
    },
    {
      "epoch": 8.185110663983904,
      "grad_norm": 0.3347221314907074,
      "learning_rate": 0.00018364825435154443,
      "loss": 0.5901,
      "step": 4068
    },
    {
      "epoch": 8.18712273641851,
      "grad_norm": 0.3180975317955017,
      "learning_rate": 0.0001836442298017909,
      "loss": 0.5477,
      "step": 4069
    },
    {
      "epoch": 8.189134808853119,
      "grad_norm": 0.34088242053985596,
      "learning_rate": 0.00018364020525203745,
      "loss": 0.5988,
      "step": 4070
    },
    {
      "epoch": 8.191146881287727,
      "grad_norm": 0.341092050075531,
      "learning_rate": 0.00018363618070228393,
      "loss": 0.6251,
      "step": 4071
    },
    {
      "epoch": 8.193158953722333,
      "grad_norm": 0.3483004570007324,
      "learning_rate": 0.00018363215615253045,
      "loss": 0.5694,
      "step": 4072
    },
    {
      "epoch": 8.195171026156942,
      "grad_norm": 0.34151577949523926,
      "learning_rate": 0.00018362813160277693,
      "loss": 0.5469,
      "step": 4073
    },
    {
      "epoch": 8.19718309859155,
      "grad_norm": 0.33661866188049316,
      "learning_rate": 0.00018362410705302344,
      "loss": 0.5939,
      "step": 4074
    },
    {
      "epoch": 8.199195171026156,
      "grad_norm": 0.34149208664894104,
      "learning_rate": 0.00018362008250326996,
      "loss": 0.5501,
      "step": 4075
    },
    {
      "epoch": 8.201207243460765,
      "grad_norm": 0.3060658872127533,
      "learning_rate": 0.00018361605795351647,
      "loss": 0.5611,
      "step": 4076
    },
    {
      "epoch": 8.203219315895373,
      "grad_norm": 0.3305854797363281,
      "learning_rate": 0.00018361203340376295,
      "loss": 0.5849,
      "step": 4077
    },
    {
      "epoch": 8.20523138832998,
      "grad_norm": 0.3230293393135071,
      "learning_rate": 0.00018360800885400947,
      "loss": 0.6113,
      "step": 4078
    },
    {
      "epoch": 8.207243460764587,
      "grad_norm": 0.32549095153808594,
      "learning_rate": 0.00018360398430425595,
      "loss": 0.5513,
      "step": 4079
    },
    {
      "epoch": 8.209255533199196,
      "grad_norm": 0.35020989179611206,
      "learning_rate": 0.0001835999597545025,
      "loss": 0.5961,
      "step": 4080
    },
    {
      "epoch": 8.211267605633802,
      "grad_norm": 0.3776947855949402,
      "learning_rate": 0.00018359593520474898,
      "loss": 0.5668,
      "step": 4081
    },
    {
      "epoch": 8.21327967806841,
      "grad_norm": 0.3225150406360626,
      "learning_rate": 0.0001835919106549955,
      "loss": 0.5606,
      "step": 4082
    },
    {
      "epoch": 8.215291750503019,
      "grad_norm": 0.3646746873855591,
      "learning_rate": 0.00018358788610524197,
      "loss": 0.616,
      "step": 4083
    },
    {
      "epoch": 8.217303822937625,
      "grad_norm": 0.3467578589916229,
      "learning_rate": 0.00018358386155548849,
      "loss": 0.5818,
      "step": 4084
    },
    {
      "epoch": 8.219315895372233,
      "grad_norm": 0.35624316334724426,
      "learning_rate": 0.000183579837005735,
      "loss": 0.5439,
      "step": 4085
    },
    {
      "epoch": 8.221327967806841,
      "grad_norm": 0.3169761300086975,
      "learning_rate": 0.0001835758124559815,
      "loss": 0.541,
      "step": 4086
    },
    {
      "epoch": 8.223340040241448,
      "grad_norm": 0.3467971980571747,
      "learning_rate": 0.000183571787906228,
      "loss": 0.5629,
      "step": 4087
    },
    {
      "epoch": 8.225352112676056,
      "grad_norm": 0.31272092461586,
      "learning_rate": 0.0001835677633564745,
      "loss": 0.5703,
      "step": 4088
    },
    {
      "epoch": 8.227364185110664,
      "grad_norm": 0.3232373297214508,
      "learning_rate": 0.000183563738806721,
      "loss": 0.5883,
      "step": 4089
    },
    {
      "epoch": 8.229376257545272,
      "grad_norm": 0.35212865471839905,
      "learning_rate": 0.00018355971425696753,
      "loss": 0.5476,
      "step": 4090
    },
    {
      "epoch": 8.231388329979879,
      "grad_norm": 0.34603962302207947,
      "learning_rate": 0.00018355568970721402,
      "loss": 0.5937,
      "step": 4091
    },
    {
      "epoch": 8.233400402414487,
      "grad_norm": 0.3726692199707031,
      "learning_rate": 0.00018355166515746053,
      "loss": 0.5834,
      "step": 4092
    },
    {
      "epoch": 8.235412474849095,
      "grad_norm": 0.3133936822414398,
      "learning_rate": 0.00018354764060770701,
      "loss": 0.5908,
      "step": 4093
    },
    {
      "epoch": 8.237424547283702,
      "grad_norm": 0.34316110610961914,
      "learning_rate": 0.00018354361605795353,
      "loss": 0.5566,
      "step": 4094
    },
    {
      "epoch": 8.23943661971831,
      "grad_norm": 0.33006903529167175,
      "learning_rate": 0.00018353959150820004,
      "loss": 0.5719,
      "step": 4095
    },
    {
      "epoch": 8.241448692152918,
      "grad_norm": 0.33276015520095825,
      "learning_rate": 0.00018353556695844652,
      "loss": 0.5828,
      "step": 4096
    },
    {
      "epoch": 8.243460764587525,
      "grad_norm": 0.3150896430015564,
      "learning_rate": 0.00018353154240869304,
      "loss": 0.5882,
      "step": 4097
    },
    {
      "epoch": 8.245472837022133,
      "grad_norm": 0.3336569368839264,
      "learning_rate": 0.00018352751785893955,
      "loss": 0.5496,
      "step": 4098
    },
    {
      "epoch": 8.247484909456741,
      "grad_norm": 0.3368958532810211,
      "learning_rate": 0.00018352349330918603,
      "loss": 0.5898,
      "step": 4099
    },
    {
      "epoch": 8.249496981891348,
      "grad_norm": 0.3463659882545471,
      "learning_rate": 0.00018351946875943255,
      "loss": 0.5913,
      "step": 4100
    },
    {
      "epoch": 8.251509054325956,
      "grad_norm": 0.32593750953674316,
      "learning_rate": 0.00018351544420967906,
      "loss": 0.5585,
      "step": 4101
    },
    {
      "epoch": 8.253521126760564,
      "grad_norm": 0.3211762309074402,
      "learning_rate": 0.00018351141965992554,
      "loss": 0.59,
      "step": 4102
    },
    {
      "epoch": 8.25553319919517,
      "grad_norm": 0.33370286226272583,
      "learning_rate": 0.00018350739511017205,
      "loss": 0.5379,
      "step": 4103
    },
    {
      "epoch": 8.257545271629779,
      "grad_norm": 0.3322366774082184,
      "learning_rate": 0.00018350337056041854,
      "loss": 0.616,
      "step": 4104
    },
    {
      "epoch": 8.259557344064387,
      "grad_norm": 0.32377171516418457,
      "learning_rate": 0.00018349934601066508,
      "loss": 0.5439,
      "step": 4105
    },
    {
      "epoch": 8.261569416498993,
      "grad_norm": 0.3347240686416626,
      "learning_rate": 0.00018349532146091156,
      "loss": 0.5852,
      "step": 4106
    },
    {
      "epoch": 8.263581488933601,
      "grad_norm": 0.3279971480369568,
      "learning_rate": 0.00018349129691115808,
      "loss": 0.5937,
      "step": 4107
    },
    {
      "epoch": 8.26559356136821,
      "grad_norm": 0.32784849405288696,
      "learning_rate": 0.00018348727236140456,
      "loss": 0.571,
      "step": 4108
    },
    {
      "epoch": 8.267605633802816,
      "grad_norm": 0.34968438744544983,
      "learning_rate": 0.00018348324781165107,
      "loss": 0.5947,
      "step": 4109
    },
    {
      "epoch": 8.269617706237424,
      "grad_norm": 0.3435806632041931,
      "learning_rate": 0.00018347922326189759,
      "loss": 0.6409,
      "step": 4110
    },
    {
      "epoch": 8.271629778672033,
      "grad_norm": 0.33476075530052185,
      "learning_rate": 0.0001834751987121441,
      "loss": 0.5682,
      "step": 4111
    },
    {
      "epoch": 8.273641851106639,
      "grad_norm": 0.3435288369655609,
      "learning_rate": 0.00018347117416239058,
      "loss": 0.5479,
      "step": 4112
    },
    {
      "epoch": 8.275653923541247,
      "grad_norm": 0.3266744911670685,
      "learning_rate": 0.0001834671496126371,
      "loss": 0.5365,
      "step": 4113
    },
    {
      "epoch": 8.277665995975855,
      "grad_norm": 0.3193713426589966,
      "learning_rate": 0.00018346312506288358,
      "loss": 0.5542,
      "step": 4114
    },
    {
      "epoch": 8.279678068410464,
      "grad_norm": 0.3246534466743469,
      "learning_rate": 0.00018345910051313012,
      "loss": 0.5704,
      "step": 4115
    },
    {
      "epoch": 8.28169014084507,
      "grad_norm": 0.3308528959751129,
      "learning_rate": 0.0001834550759633766,
      "loss": 0.5916,
      "step": 4116
    },
    {
      "epoch": 8.283702213279678,
      "grad_norm": 0.3370896577835083,
      "learning_rate": 0.00018345105141362312,
      "loss": 0.5977,
      "step": 4117
    },
    {
      "epoch": 8.285714285714286,
      "grad_norm": 0.3267458379268646,
      "learning_rate": 0.0001834470268638696,
      "loss": 0.564,
      "step": 4118
    },
    {
      "epoch": 8.287726358148893,
      "grad_norm": 0.32186374068260193,
      "learning_rate": 0.00018344300231411611,
      "loss": 0.6023,
      "step": 4119
    },
    {
      "epoch": 8.289738430583501,
      "grad_norm": 0.3158879280090332,
      "learning_rate": 0.00018343897776436263,
      "loss": 0.5366,
      "step": 4120
    },
    {
      "epoch": 8.29175050301811,
      "grad_norm": 0.35905617475509644,
      "learning_rate": 0.00018343495321460914,
      "loss": 0.5993,
      "step": 4121
    },
    {
      "epoch": 8.293762575452716,
      "grad_norm": 0.3341173827648163,
      "learning_rate": 0.00018343092866485562,
      "loss": 0.5915,
      "step": 4122
    },
    {
      "epoch": 8.295774647887324,
      "grad_norm": 0.3589267134666443,
      "learning_rate": 0.00018342690411510214,
      "loss": 0.5946,
      "step": 4123
    },
    {
      "epoch": 8.297786720321932,
      "grad_norm": 0.33833786845207214,
      "learning_rate": 0.00018342287956534862,
      "loss": 0.6022,
      "step": 4124
    },
    {
      "epoch": 8.299798792756539,
      "grad_norm": 0.3473764955997467,
      "learning_rate": 0.00018341885501559516,
      "loss": 0.5713,
      "step": 4125
    },
    {
      "epoch": 8.301810865191147,
      "grad_norm": 0.33781784772872925,
      "learning_rate": 0.00018341483046584165,
      "loss": 0.5332,
      "step": 4126
    },
    {
      "epoch": 8.303822937625755,
      "grad_norm": 0.3190464377403259,
      "learning_rate": 0.00018341080591608816,
      "loss": 0.585,
      "step": 4127
    },
    {
      "epoch": 8.305835010060362,
      "grad_norm": 0.3363647162914276,
      "learning_rate": 0.00018340678136633464,
      "loss": 0.5819,
      "step": 4128
    },
    {
      "epoch": 8.30784708249497,
      "grad_norm": 0.3572331368923187,
      "learning_rate": 0.00018340275681658116,
      "loss": 0.5869,
      "step": 4129
    },
    {
      "epoch": 8.309859154929578,
      "grad_norm": 0.3276376724243164,
      "learning_rate": 0.00018339873226682767,
      "loss": 0.5731,
      "step": 4130
    },
    {
      "epoch": 8.311871227364184,
      "grad_norm": 0.32877442240715027,
      "learning_rate": 0.00018339470771707415,
      "loss": 0.5914,
      "step": 4131
    },
    {
      "epoch": 8.313883299798793,
      "grad_norm": 0.3307737112045288,
      "learning_rate": 0.00018339068316732067,
      "loss": 0.5569,
      "step": 4132
    },
    {
      "epoch": 8.3158953722334,
      "grad_norm": 0.3403491973876953,
      "learning_rate": 0.00018338665861756718,
      "loss": 0.5551,
      "step": 4133
    },
    {
      "epoch": 8.317907444668007,
      "grad_norm": 0.3507951498031616,
      "learning_rate": 0.00018338263406781366,
      "loss": 0.5775,
      "step": 4134
    },
    {
      "epoch": 8.319919517102615,
      "grad_norm": 0.3201413154602051,
      "learning_rate": 0.00018337860951806017,
      "loss": 0.5792,
      "step": 4135
    },
    {
      "epoch": 8.321931589537224,
      "grad_norm": 0.309959352016449,
      "learning_rate": 0.0001833745849683067,
      "loss": 0.5582,
      "step": 4136
    },
    {
      "epoch": 8.323943661971832,
      "grad_norm": 0.32455262541770935,
      "learning_rate": 0.00018337056041855317,
      "loss": 0.5887,
      "step": 4137
    },
    {
      "epoch": 8.325955734406438,
      "grad_norm": 0.33119460940361023,
      "learning_rate": 0.00018336653586879968,
      "loss": 0.5772,
      "step": 4138
    },
    {
      "epoch": 8.327967806841047,
      "grad_norm": 0.3445654511451721,
      "learning_rate": 0.00018336251131904617,
      "loss": 0.5719,
      "step": 4139
    },
    {
      "epoch": 8.329979879275655,
      "grad_norm": 0.32876357436180115,
      "learning_rate": 0.0001833584867692927,
      "loss": 0.5539,
      "step": 4140
    },
    {
      "epoch": 8.331991951710261,
      "grad_norm": 0.35475611686706543,
      "learning_rate": 0.0001833544622195392,
      "loss": 0.5702,
      "step": 4141
    },
    {
      "epoch": 8.33400402414487,
      "grad_norm": 0.32837703824043274,
      "learning_rate": 0.0001833504376697857,
      "loss": 0.5631,
      "step": 4142
    },
    {
      "epoch": 8.336016096579478,
      "grad_norm": 0.33608779311180115,
      "learning_rate": 0.0001833464131200322,
      "loss": 0.5598,
      "step": 4143
    },
    {
      "epoch": 8.338028169014084,
      "grad_norm": 0.319367915391922,
      "learning_rate": 0.0001833423885702787,
      "loss": 0.5551,
      "step": 4144
    },
    {
      "epoch": 8.340040241448692,
      "grad_norm": 0.3125264048576355,
      "learning_rate": 0.00018333836402052522,
      "loss": 0.5724,
      "step": 4145
    },
    {
      "epoch": 8.3420523138833,
      "grad_norm": 0.32927218079566956,
      "learning_rate": 0.00018333433947077173,
      "loss": 0.5728,
      "step": 4146
    },
    {
      "epoch": 8.344064386317907,
      "grad_norm": 0.3397599160671234,
      "learning_rate": 0.0001833303149210182,
      "loss": 0.5734,
      "step": 4147
    },
    {
      "epoch": 8.346076458752515,
      "grad_norm": 0.32229477167129517,
      "learning_rate": 0.00018332629037126472,
      "loss": 0.5904,
      "step": 4148
    },
    {
      "epoch": 8.348088531187123,
      "grad_norm": 0.34966161847114563,
      "learning_rate": 0.0001833222658215112,
      "loss": 0.6141,
      "step": 4149
    },
    {
      "epoch": 8.35010060362173,
      "grad_norm": 0.3158787786960602,
      "learning_rate": 0.00018331824127175775,
      "loss": 0.5513,
      "step": 4150
    },
    {
      "epoch": 8.352112676056338,
      "grad_norm": 0.3555399775505066,
      "learning_rate": 0.00018331421672200423,
      "loss": 0.5497,
      "step": 4151
    },
    {
      "epoch": 8.354124748490946,
      "grad_norm": 0.3415384888648987,
      "learning_rate": 0.00018331019217225075,
      "loss": 0.578,
      "step": 4152
    },
    {
      "epoch": 8.356136820925553,
      "grad_norm": 0.3411322236061096,
      "learning_rate": 0.00018330616762249723,
      "loss": 0.5906,
      "step": 4153
    },
    {
      "epoch": 8.35814889336016,
      "grad_norm": 0.3305804133415222,
      "learning_rate": 0.00018330214307274374,
      "loss": 0.5805,
      "step": 4154
    },
    {
      "epoch": 8.360160965794769,
      "grad_norm": 0.31593549251556396,
      "learning_rate": 0.00018329811852299026,
      "loss": 0.5765,
      "step": 4155
    },
    {
      "epoch": 8.362173038229376,
      "grad_norm": 0.33426719903945923,
      "learning_rate": 0.00018329409397323677,
      "loss": 0.6238,
      "step": 4156
    },
    {
      "epoch": 8.364185110663984,
      "grad_norm": 0.31446918845176697,
      "learning_rate": 0.00018329006942348325,
      "loss": 0.5631,
      "step": 4157
    },
    {
      "epoch": 8.366197183098592,
      "grad_norm": 0.31291061639785767,
      "learning_rate": 0.00018328604487372977,
      "loss": 0.5751,
      "step": 4158
    },
    {
      "epoch": 8.368209255533198,
      "grad_norm": 0.325452983379364,
      "learning_rate": 0.00018328202032397625,
      "loss": 0.6054,
      "step": 4159
    },
    {
      "epoch": 8.370221327967807,
      "grad_norm": 0.329195111989975,
      "learning_rate": 0.0001832779957742228,
      "loss": 0.5817,
      "step": 4160
    },
    {
      "epoch": 8.372233400402415,
      "grad_norm": 0.3289010226726532,
      "learning_rate": 0.00018327397122446928,
      "loss": 0.616,
      "step": 4161
    },
    {
      "epoch": 8.374245472837021,
      "grad_norm": 0.33495238423347473,
      "learning_rate": 0.0001832699466747158,
      "loss": 0.589,
      "step": 4162
    },
    {
      "epoch": 8.37625754527163,
      "grad_norm": 0.3372180461883545,
      "learning_rate": 0.00018326592212496227,
      "loss": 0.5383,
      "step": 4163
    },
    {
      "epoch": 8.378269617706238,
      "grad_norm": 0.32793471217155457,
      "learning_rate": 0.00018326189757520878,
      "loss": 0.5498,
      "step": 4164
    },
    {
      "epoch": 8.380281690140846,
      "grad_norm": 0.31656911969184875,
      "learning_rate": 0.0001832578730254553,
      "loss": 0.6017,
      "step": 4165
    },
    {
      "epoch": 8.382293762575452,
      "grad_norm": 0.32206031680107117,
      "learning_rate": 0.00018325384847570178,
      "loss": 0.6087,
      "step": 4166
    },
    {
      "epoch": 8.38430583501006,
      "grad_norm": 0.3303263485431671,
      "learning_rate": 0.0001832498239259483,
      "loss": 0.592,
      "step": 4167
    },
    {
      "epoch": 8.386317907444669,
      "grad_norm": 0.341470330953598,
      "learning_rate": 0.0001832457993761948,
      "loss": 0.5556,
      "step": 4168
    },
    {
      "epoch": 8.388329979879275,
      "grad_norm": 0.32233551144599915,
      "learning_rate": 0.0001832417748264413,
      "loss": 0.5541,
      "step": 4169
    },
    {
      "epoch": 8.390342052313883,
      "grad_norm": 0.32152804732322693,
      "learning_rate": 0.0001832377502766878,
      "loss": 0.5632,
      "step": 4170
    },
    {
      "epoch": 8.392354124748492,
      "grad_norm": 0.33249202370643616,
      "learning_rate": 0.00018323372572693432,
      "loss": 0.6066,
      "step": 4171
    },
    {
      "epoch": 8.394366197183098,
      "grad_norm": 0.35282135009765625,
      "learning_rate": 0.0001832297011771808,
      "loss": 0.6002,
      "step": 4172
    },
    {
      "epoch": 8.396378269617706,
      "grad_norm": 0.3455875813961029,
      "learning_rate": 0.0001832256766274273,
      "loss": 0.5792,
      "step": 4173
    },
    {
      "epoch": 8.398390342052314,
      "grad_norm": 0.3306972086429596,
      "learning_rate": 0.0001832216520776738,
      "loss": 0.5798,
      "step": 4174
    },
    {
      "epoch": 8.400402414486921,
      "grad_norm": 0.33141160011291504,
      "learning_rate": 0.00018321762752792034,
      "loss": 0.5735,
      "step": 4175
    },
    {
      "epoch": 8.40241448692153,
      "grad_norm": 0.33560359477996826,
      "learning_rate": 0.00018321360297816682,
      "loss": 0.5919,
      "step": 4176
    },
    {
      "epoch": 8.404426559356137,
      "grad_norm": 0.3316914439201355,
      "learning_rate": 0.00018320957842841334,
      "loss": 0.5788,
      "step": 4177
    },
    {
      "epoch": 8.406438631790744,
      "grad_norm": 0.3322809636592865,
      "learning_rate": 0.00018320555387865982,
      "loss": 0.5858,
      "step": 4178
    },
    {
      "epoch": 8.408450704225352,
      "grad_norm": 0.34196603298187256,
      "learning_rate": 0.00018320152932890633,
      "loss": 0.5798,
      "step": 4179
    },
    {
      "epoch": 8.41046277665996,
      "grad_norm": 0.33215248584747314,
      "learning_rate": 0.00018319750477915284,
      "loss": 0.6118,
      "step": 4180
    },
    {
      "epoch": 8.412474849094567,
      "grad_norm": 0.34289032220840454,
      "learning_rate": 0.00018319348022939936,
      "loss": 0.6008,
      "step": 4181
    },
    {
      "epoch": 8.414486921529175,
      "grad_norm": 0.3400332033634186,
      "learning_rate": 0.00018318945567964584,
      "loss": 0.5938,
      "step": 4182
    },
    {
      "epoch": 8.416498993963783,
      "grad_norm": 0.32139846682548523,
      "learning_rate": 0.00018318543112989235,
      "loss": 0.5661,
      "step": 4183
    },
    {
      "epoch": 8.41851106639839,
      "grad_norm": 0.3236444294452667,
      "learning_rate": 0.00018318140658013884,
      "loss": 0.5714,
      "step": 4184
    },
    {
      "epoch": 8.420523138832998,
      "grad_norm": 0.33304864168167114,
      "learning_rate": 0.00018317738203038538,
      "loss": 0.6035,
      "step": 4185
    },
    {
      "epoch": 8.422535211267606,
      "grad_norm": 0.33899635076522827,
      "learning_rate": 0.00018317335748063186,
      "loss": 0.5719,
      "step": 4186
    },
    {
      "epoch": 8.424547283702214,
      "grad_norm": 0.3269365131855011,
      "learning_rate": 0.00018316933293087838,
      "loss": 0.5419,
      "step": 4187
    },
    {
      "epoch": 8.42655935613682,
      "grad_norm": 0.35060369968414307,
      "learning_rate": 0.00018316530838112486,
      "loss": 0.5858,
      "step": 4188
    },
    {
      "epoch": 8.428571428571429,
      "grad_norm": 0.34090715646743774,
      "learning_rate": 0.00018316128383137137,
      "loss": 0.5291,
      "step": 4189
    },
    {
      "epoch": 8.430583501006037,
      "grad_norm": 0.3428179621696472,
      "learning_rate": 0.00018315725928161789,
      "loss": 0.5803,
      "step": 4190
    },
    {
      "epoch": 8.432595573440643,
      "grad_norm": 0.3427894413471222,
      "learning_rate": 0.0001831532347318644,
      "loss": 0.6041,
      "step": 4191
    },
    {
      "epoch": 8.434607645875252,
      "grad_norm": 0.3110508620738983,
      "learning_rate": 0.00018314921018211088,
      "loss": 0.553,
      "step": 4192
    },
    {
      "epoch": 8.43661971830986,
      "grad_norm": 0.3415348529815674,
      "learning_rate": 0.0001831451856323574,
      "loss": 0.6082,
      "step": 4193
    },
    {
      "epoch": 8.438631790744466,
      "grad_norm": 0.3212357759475708,
      "learning_rate": 0.00018314116108260388,
      "loss": 0.5618,
      "step": 4194
    },
    {
      "epoch": 8.440643863179075,
      "grad_norm": 0.32091444730758667,
      "learning_rate": 0.00018313713653285042,
      "loss": 0.5772,
      "step": 4195
    },
    {
      "epoch": 8.442655935613683,
      "grad_norm": 0.32797592878341675,
      "learning_rate": 0.0001831331119830969,
      "loss": 0.584,
      "step": 4196
    },
    {
      "epoch": 8.44466800804829,
      "grad_norm": 0.3172262907028198,
      "learning_rate": 0.00018312908743334342,
      "loss": 0.5711,
      "step": 4197
    },
    {
      "epoch": 8.446680080482897,
      "grad_norm": 0.3532048761844635,
      "learning_rate": 0.0001831250628835899,
      "loss": 0.5716,
      "step": 4198
    },
    {
      "epoch": 8.448692152917506,
      "grad_norm": 0.33860301971435547,
      "learning_rate": 0.00018312103833383641,
      "loss": 0.5592,
      "step": 4199
    },
    {
      "epoch": 8.450704225352112,
      "grad_norm": 0.3394852876663208,
      "learning_rate": 0.00018311701378408293,
      "loss": 0.5722,
      "step": 4200
    },
    {
      "epoch": 8.45271629778672,
      "grad_norm": 0.33484557271003723,
      "learning_rate": 0.0001831129892343294,
      "loss": 0.5671,
      "step": 4201
    },
    {
      "epoch": 8.454728370221329,
      "grad_norm": 0.3473385274410248,
      "learning_rate": 0.00018310896468457592,
      "loss": 0.5639,
      "step": 4202
    },
    {
      "epoch": 8.456740442655935,
      "grad_norm": 0.32231226563453674,
      "learning_rate": 0.0001831049401348224,
      "loss": 0.5517,
      "step": 4203
    },
    {
      "epoch": 8.458752515090543,
      "grad_norm": 0.31527164578437805,
      "learning_rate": 0.00018310091558506892,
      "loss": 0.6164,
      "step": 4204
    },
    {
      "epoch": 8.460764587525151,
      "grad_norm": 0.33620592951774597,
      "learning_rate": 0.00018309689103531543,
      "loss": 0.6034,
      "step": 4205
    },
    {
      "epoch": 8.462776659959758,
      "grad_norm": 0.32830682396888733,
      "learning_rate": 0.00018309286648556195,
      "loss": 0.5617,
      "step": 4206
    },
    {
      "epoch": 8.464788732394366,
      "grad_norm": 0.323954313993454,
      "learning_rate": 0.00018308884193580843,
      "loss": 0.596,
      "step": 4207
    },
    {
      "epoch": 8.466800804828974,
      "grad_norm": 0.33457428216934204,
      "learning_rate": 0.00018308481738605494,
      "loss": 0.5966,
      "step": 4208
    },
    {
      "epoch": 8.46881287726358,
      "grad_norm": 0.33196336030960083,
      "learning_rate": 0.00018308079283630143,
      "loss": 0.5807,
      "step": 4209
    },
    {
      "epoch": 8.470824949698189,
      "grad_norm": 0.32661664485931396,
      "learning_rate": 0.00018307676828654797,
      "loss": 0.574,
      "step": 4210
    },
    {
      "epoch": 8.472837022132797,
      "grad_norm": 0.3412211239337921,
      "learning_rate": 0.00018307274373679445,
      "loss": 0.5654,
      "step": 4211
    },
    {
      "epoch": 8.474849094567404,
      "grad_norm": 0.3307575285434723,
      "learning_rate": 0.00018306871918704096,
      "loss": 0.5558,
      "step": 4212
    },
    {
      "epoch": 8.476861167002012,
      "grad_norm": 0.3259197473526001,
      "learning_rate": 0.00018306469463728745,
      "loss": 0.5733,
      "step": 4213
    },
    {
      "epoch": 8.47887323943662,
      "grad_norm": 0.3389723300933838,
      "learning_rate": 0.00018306067008753396,
      "loss": 0.5911,
      "step": 4214
    },
    {
      "epoch": 8.480885311871228,
      "grad_norm": 0.33796626329421997,
      "learning_rate": 0.00018305664553778047,
      "loss": 0.6016,
      "step": 4215
    },
    {
      "epoch": 8.482897384305835,
      "grad_norm": 0.3573395311832428,
      "learning_rate": 0.000183052620988027,
      "loss": 0.5851,
      "step": 4216
    },
    {
      "epoch": 8.484909456740443,
      "grad_norm": 0.3189626932144165,
      "learning_rate": 0.00018304859643827347,
      "loss": 0.5849,
      "step": 4217
    },
    {
      "epoch": 8.486921529175051,
      "grad_norm": 0.3510195314884186,
      "learning_rate": 0.00018304457188851998,
      "loss": 0.5858,
      "step": 4218
    },
    {
      "epoch": 8.488933601609657,
      "grad_norm": 0.35362881422042847,
      "learning_rate": 0.00018304054733876647,
      "loss": 0.6187,
      "step": 4219
    },
    {
      "epoch": 8.490945674044266,
      "grad_norm": 0.3410976231098175,
      "learning_rate": 0.000183036522789013,
      "loss": 0.6199,
      "step": 4220
    },
    {
      "epoch": 8.492957746478874,
      "grad_norm": 0.3085171580314636,
      "learning_rate": 0.0001830324982392595,
      "loss": 0.5613,
      "step": 4221
    },
    {
      "epoch": 8.49496981891348,
      "grad_norm": 0.3396012783050537,
      "learning_rate": 0.000183028473689506,
      "loss": 0.578,
      "step": 4222
    },
    {
      "epoch": 8.496981891348089,
      "grad_norm": 0.34212812781333923,
      "learning_rate": 0.0001830244491397525,
      "loss": 0.6429,
      "step": 4223
    },
    {
      "epoch": 8.498993963782697,
      "grad_norm": 0.338344007730484,
      "learning_rate": 0.000183020424589999,
      "loss": 0.5747,
      "step": 4224
    },
    {
      "epoch": 8.501006036217303,
      "grad_norm": 0.3231712579727173,
      "learning_rate": 0.00018301640004024552,
      "loss": 0.569,
      "step": 4225
    },
    {
      "epoch": 8.503018108651911,
      "grad_norm": 0.36244645714759827,
      "learning_rate": 0.00018301237549049203,
      "loss": 0.5783,
      "step": 4226
    },
    {
      "epoch": 8.50503018108652,
      "grad_norm": 0.3306887745857239,
      "learning_rate": 0.0001830083509407385,
      "loss": 0.6503,
      "step": 4227
    },
    {
      "epoch": 8.507042253521126,
      "grad_norm": 0.31858840584754944,
      "learning_rate": 0.00018300432639098502,
      "loss": 0.5717,
      "step": 4228
    },
    {
      "epoch": 8.509054325955734,
      "grad_norm": 0.33140116930007935,
      "learning_rate": 0.0001830003018412315,
      "loss": 0.5985,
      "step": 4229
    },
    {
      "epoch": 8.511066398390343,
      "grad_norm": 0.32291799783706665,
      "learning_rate": 0.00018299627729147802,
      "loss": 0.5724,
      "step": 4230
    },
    {
      "epoch": 8.513078470824949,
      "grad_norm": 0.3359142541885376,
      "learning_rate": 0.00018299225274172453,
      "loss": 0.5512,
      "step": 4231
    },
    {
      "epoch": 8.515090543259557,
      "grad_norm": 0.32507219910621643,
      "learning_rate": 0.00018298822819197105,
      "loss": 0.6019,
      "step": 4232
    },
    {
      "epoch": 8.517102615694165,
      "grad_norm": 0.3424624800682068,
      "learning_rate": 0.00018298420364221753,
      "loss": 0.584,
      "step": 4233
    },
    {
      "epoch": 8.519114688128772,
      "grad_norm": 0.33007997274398804,
      "learning_rate": 0.00018298017909246404,
      "loss": 0.571,
      "step": 4234
    },
    {
      "epoch": 8.52112676056338,
      "grad_norm": 0.33369943499565125,
      "learning_rate": 0.00018297615454271056,
      "loss": 0.5836,
      "step": 4235
    },
    {
      "epoch": 8.523138832997988,
      "grad_norm": 0.33214080333709717,
      "learning_rate": 0.00018297212999295704,
      "loss": 0.5451,
      "step": 4236
    },
    {
      "epoch": 8.525150905432596,
      "grad_norm": 0.32484593987464905,
      "learning_rate": 0.00018296810544320355,
      "loss": 0.5719,
      "step": 4237
    },
    {
      "epoch": 8.527162977867203,
      "grad_norm": 0.3201076090335846,
      "learning_rate": 0.00018296408089345004,
      "loss": 0.5748,
      "step": 4238
    },
    {
      "epoch": 8.529175050301811,
      "grad_norm": 0.31843721866607666,
      "learning_rate": 0.00018296005634369655,
      "loss": 0.5633,
      "step": 4239
    },
    {
      "epoch": 8.53118712273642,
      "grad_norm": 0.3113594949245453,
      "learning_rate": 0.00018295603179394306,
      "loss": 0.5624,
      "step": 4240
    },
    {
      "epoch": 8.533199195171026,
      "grad_norm": 0.3374987542629242,
      "learning_rate": 0.00018295200724418958,
      "loss": 0.5983,
      "step": 4241
    },
    {
      "epoch": 8.535211267605634,
      "grad_norm": 0.33648544549942017,
      "learning_rate": 0.00018294798269443606,
      "loss": 0.5796,
      "step": 4242
    },
    {
      "epoch": 8.537223340040242,
      "grad_norm": 0.34968477487564087,
      "learning_rate": 0.00018294395814468257,
      "loss": 0.6043,
      "step": 4243
    },
    {
      "epoch": 8.539235412474849,
      "grad_norm": 0.32273146510124207,
      "learning_rate": 0.00018293993359492906,
      "loss": 0.573,
      "step": 4244
    },
    {
      "epoch": 8.541247484909457,
      "grad_norm": 0.31850194931030273,
      "learning_rate": 0.0001829359090451756,
      "loss": 0.5807,
      "step": 4245
    },
    {
      "epoch": 8.543259557344065,
      "grad_norm": 0.34314388036727905,
      "learning_rate": 0.00018293188449542208,
      "loss": 0.6171,
      "step": 4246
    },
    {
      "epoch": 8.545271629778671,
      "grad_norm": 0.3202800154685974,
      "learning_rate": 0.0001829278599456686,
      "loss": 0.5889,
      "step": 4247
    },
    {
      "epoch": 8.54728370221328,
      "grad_norm": 0.32486820220947266,
      "learning_rate": 0.00018292383539591508,
      "loss": 0.6305,
      "step": 4248
    },
    {
      "epoch": 8.549295774647888,
      "grad_norm": 0.33468273282051086,
      "learning_rate": 0.0001829198108461616,
      "loss": 0.5685,
      "step": 4249
    },
    {
      "epoch": 8.551307847082494,
      "grad_norm": 0.3254976272583008,
      "learning_rate": 0.00018291578629640808,
      "loss": 0.6307,
      "step": 4250
    },
    {
      "epoch": 8.553319919517103,
      "grad_norm": 0.3188943564891815,
      "learning_rate": 0.00018291176174665462,
      "loss": 0.619,
      "step": 4251
    },
    {
      "epoch": 8.55533199195171,
      "grad_norm": 0.3216911256313324,
      "learning_rate": 0.0001829077371969011,
      "loss": 0.5889,
      "step": 4252
    },
    {
      "epoch": 8.557344064386317,
      "grad_norm": 0.3284384608268738,
      "learning_rate": 0.0001829037126471476,
      "loss": 0.6034,
      "step": 4253
    },
    {
      "epoch": 8.559356136820925,
      "grad_norm": 0.322936087846756,
      "learning_rate": 0.0001828996880973941,
      "loss": 0.5682,
      "step": 4254
    },
    {
      "epoch": 8.561368209255534,
      "grad_norm": 0.3438686430454254,
      "learning_rate": 0.0001828956635476406,
      "loss": 0.597,
      "step": 4255
    },
    {
      "epoch": 8.56338028169014,
      "grad_norm": 0.32074952125549316,
      "learning_rate": 0.00018289163899788712,
      "loss": 0.5605,
      "step": 4256
    },
    {
      "epoch": 8.565392354124748,
      "grad_norm": 0.3099704384803772,
      "learning_rate": 0.00018288761444813364,
      "loss": 0.569,
      "step": 4257
    },
    {
      "epoch": 8.567404426559357,
      "grad_norm": 0.3374677300453186,
      "learning_rate": 0.00018288358989838012,
      "loss": 0.6023,
      "step": 4258
    },
    {
      "epoch": 8.569416498993963,
      "grad_norm": 0.3248615562915802,
      "learning_rate": 0.00018287956534862663,
      "loss": 0.5745,
      "step": 4259
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 0.3351181447505951,
      "learning_rate": 0.00018287554079887312,
      "loss": 0.5793,
      "step": 4260
    },
    {
      "epoch": 8.57344064386318,
      "grad_norm": 0.32469046115875244,
      "learning_rate": 0.00018287151624911966,
      "loss": 0.5836,
      "step": 4261
    },
    {
      "epoch": 8.575452716297786,
      "grad_norm": 0.32876861095428467,
      "learning_rate": 0.00018286749169936614,
      "loss": 0.5709,
      "step": 4262
    },
    {
      "epoch": 8.577464788732394,
      "grad_norm": 0.32456403970718384,
      "learning_rate": 0.00018286346714961265,
      "loss": 0.5614,
      "step": 4263
    },
    {
      "epoch": 8.579476861167002,
      "grad_norm": 0.30717018246650696,
      "learning_rate": 0.00018285944259985914,
      "loss": 0.5492,
      "step": 4264
    },
    {
      "epoch": 8.58148893360161,
      "grad_norm": 0.31463494896888733,
      "learning_rate": 0.00018285541805010565,
      "loss": 0.5538,
      "step": 4265
    },
    {
      "epoch": 8.583501006036217,
      "grad_norm": 0.3096868693828583,
      "learning_rate": 0.00018285139350035216,
      "loss": 0.5723,
      "step": 4266
    },
    {
      "epoch": 8.585513078470825,
      "grad_norm": 0.3353797495365143,
      "learning_rate": 0.00018284736895059868,
      "loss": 0.5858,
      "step": 4267
    },
    {
      "epoch": 8.587525150905433,
      "grad_norm": 0.33209675550460815,
      "learning_rate": 0.00018284334440084516,
      "loss": 0.5971,
      "step": 4268
    },
    {
      "epoch": 8.58953722334004,
      "grad_norm": 0.3272773325443268,
      "learning_rate": 0.00018283931985109167,
      "loss": 0.5496,
      "step": 4269
    },
    {
      "epoch": 8.591549295774648,
      "grad_norm": 0.3236989378929138,
      "learning_rate": 0.00018283529530133816,
      "loss": 0.5812,
      "step": 4270
    },
    {
      "epoch": 8.593561368209256,
      "grad_norm": 0.3356207311153412,
      "learning_rate": 0.00018283127075158467,
      "loss": 0.6191,
      "step": 4271
    },
    {
      "epoch": 8.595573440643863,
      "grad_norm": 0.3214264214038849,
      "learning_rate": 0.00018282724620183118,
      "loss": 0.516,
      "step": 4272
    },
    {
      "epoch": 8.59758551307847,
      "grad_norm": 0.3287356197834015,
      "learning_rate": 0.00018282322165207767,
      "loss": 0.5873,
      "step": 4273
    },
    {
      "epoch": 8.599597585513079,
      "grad_norm": 0.33482372760772705,
      "learning_rate": 0.00018281919710232418,
      "loss": 0.5984,
      "step": 4274
    },
    {
      "epoch": 8.601609657947686,
      "grad_norm": 0.3153732120990753,
      "learning_rate": 0.0001828151725525707,
      "loss": 0.5765,
      "step": 4275
    },
    {
      "epoch": 8.603621730382294,
      "grad_norm": 0.32328689098358154,
      "learning_rate": 0.0001828111480028172,
      "loss": 0.5649,
      "step": 4276
    },
    {
      "epoch": 8.605633802816902,
      "grad_norm": 0.34677696228027344,
      "learning_rate": 0.0001828071234530637,
      "loss": 0.6096,
      "step": 4277
    },
    {
      "epoch": 8.607645875251508,
      "grad_norm": 0.3160472810268402,
      "learning_rate": 0.0001828030989033102,
      "loss": 0.5634,
      "step": 4278
    },
    {
      "epoch": 8.609657947686117,
      "grad_norm": 0.3358705937862396,
      "learning_rate": 0.0001827990743535567,
      "loss": 0.5735,
      "step": 4279
    },
    {
      "epoch": 8.611670020120725,
      "grad_norm": 0.331444650888443,
      "learning_rate": 0.0001827950498038032,
      "loss": 0.6317,
      "step": 4280
    },
    {
      "epoch": 8.613682092555331,
      "grad_norm": 0.3271447718143463,
      "learning_rate": 0.0001827910252540497,
      "loss": 0.6031,
      "step": 4281
    },
    {
      "epoch": 8.61569416498994,
      "grad_norm": 0.3177720010280609,
      "learning_rate": 0.00018278700070429622,
      "loss": 0.5699,
      "step": 4282
    },
    {
      "epoch": 8.617706237424548,
      "grad_norm": 0.31152626872062683,
      "learning_rate": 0.0001827829761545427,
      "loss": 0.5717,
      "step": 4283
    },
    {
      "epoch": 8.619718309859154,
      "grad_norm": 0.30956631898880005,
      "learning_rate": 0.00018277895160478922,
      "loss": 0.5686,
      "step": 4284
    },
    {
      "epoch": 8.621730382293762,
      "grad_norm": 0.32879406213760376,
      "learning_rate": 0.0001827749270550357,
      "loss": 0.5879,
      "step": 4285
    },
    {
      "epoch": 8.62374245472837,
      "grad_norm": 0.32597461342811584,
      "learning_rate": 0.00018277090250528225,
      "loss": 0.6042,
      "step": 4286
    },
    {
      "epoch": 8.625754527162979,
      "grad_norm": 0.3203426003456116,
      "learning_rate": 0.00018276687795552873,
      "loss": 0.5957,
      "step": 4287
    },
    {
      "epoch": 8.627766599597585,
      "grad_norm": 0.34161749482154846,
      "learning_rate": 0.00018276285340577524,
      "loss": 0.6237,
      "step": 4288
    },
    {
      "epoch": 8.629778672032193,
      "grad_norm": 0.3227345943450928,
      "learning_rate": 0.00018275882885602173,
      "loss": 0.5789,
      "step": 4289
    },
    {
      "epoch": 8.631790744466802,
      "grad_norm": 0.31891945004463196,
      "learning_rate": 0.00018275480430626824,
      "loss": 0.5718,
      "step": 4290
    },
    {
      "epoch": 8.633802816901408,
      "grad_norm": 0.31970691680908203,
      "learning_rate": 0.00018275077975651475,
      "loss": 0.5926,
      "step": 4291
    },
    {
      "epoch": 8.635814889336016,
      "grad_norm": 0.33059239387512207,
      "learning_rate": 0.00018274675520676126,
      "loss": 0.6042,
      "step": 4292
    },
    {
      "epoch": 8.637826961770624,
      "grad_norm": 0.33174043893814087,
      "learning_rate": 0.00018274273065700775,
      "loss": 0.5946,
      "step": 4293
    },
    {
      "epoch": 8.639839034205231,
      "grad_norm": 0.3267751634120941,
      "learning_rate": 0.00018273870610725426,
      "loss": 0.5814,
      "step": 4294
    },
    {
      "epoch": 8.64185110663984,
      "grad_norm": 0.33479490876197815,
      "learning_rate": 0.00018273468155750075,
      "loss": 0.6077,
      "step": 4295
    },
    {
      "epoch": 8.643863179074447,
      "grad_norm": 0.3096420168876648,
      "learning_rate": 0.00018273065700774729,
      "loss": 0.569,
      "step": 4296
    },
    {
      "epoch": 8.645875251509054,
      "grad_norm": 0.3200443387031555,
      "learning_rate": 0.00018272663245799377,
      "loss": 0.5567,
      "step": 4297
    },
    {
      "epoch": 8.647887323943662,
      "grad_norm": 0.31723129749298096,
      "learning_rate": 0.00018272260790824028,
      "loss": 0.5969,
      "step": 4298
    },
    {
      "epoch": 8.64989939637827,
      "grad_norm": 0.34144124388694763,
      "learning_rate": 0.00018271858335848677,
      "loss": 0.5909,
      "step": 4299
    },
    {
      "epoch": 8.651911468812877,
      "grad_norm": 0.3305197060108185,
      "learning_rate": 0.00018271455880873328,
      "loss": 0.5759,
      "step": 4300
    },
    {
      "epoch": 8.653923541247485,
      "grad_norm": 0.3361116349697113,
      "learning_rate": 0.0001827105342589798,
      "loss": 0.5672,
      "step": 4301
    },
    {
      "epoch": 8.655935613682093,
      "grad_norm": 0.3233480155467987,
      "learning_rate": 0.0001827065097092263,
      "loss": 0.5672,
      "step": 4302
    },
    {
      "epoch": 8.6579476861167,
      "grad_norm": 0.3374873399734497,
      "learning_rate": 0.0001827024851594728,
      "loss": 0.57,
      "step": 4303
    },
    {
      "epoch": 8.659959758551308,
      "grad_norm": 0.3452494740486145,
      "learning_rate": 0.0001826984606097193,
      "loss": 0.6098,
      "step": 4304
    },
    {
      "epoch": 8.661971830985916,
      "grad_norm": 0.3268430829048157,
      "learning_rate": 0.0001826944360599658,
      "loss": 0.5652,
      "step": 4305
    },
    {
      "epoch": 8.663983903420522,
      "grad_norm": 0.33458712697029114,
      "learning_rate": 0.0001826904115102123,
      "loss": 0.6012,
      "step": 4306
    },
    {
      "epoch": 8.66599597585513,
      "grad_norm": 0.31828129291534424,
      "learning_rate": 0.0001826863869604588,
      "loss": 0.6126,
      "step": 4307
    },
    {
      "epoch": 8.668008048289739,
      "grad_norm": 0.3320760130882263,
      "learning_rate": 0.0001826823624107053,
      "loss": 0.5427,
      "step": 4308
    },
    {
      "epoch": 8.670020120724345,
      "grad_norm": 0.3368097245693207,
      "learning_rate": 0.0001826783378609518,
      "loss": 0.6011,
      "step": 4309
    },
    {
      "epoch": 8.672032193158953,
      "grad_norm": 0.341793417930603,
      "learning_rate": 0.00018267431331119832,
      "loss": 0.5959,
      "step": 4310
    },
    {
      "epoch": 8.674044265593562,
      "grad_norm": 0.3229448199272156,
      "learning_rate": 0.00018267028876144483,
      "loss": 0.5699,
      "step": 4311
    },
    {
      "epoch": 8.676056338028168,
      "grad_norm": 0.33539196848869324,
      "learning_rate": 0.00018266626421169132,
      "loss": 0.6175,
      "step": 4312
    },
    {
      "epoch": 8.678068410462776,
      "grad_norm": 0.33240023255348206,
      "learning_rate": 0.00018266223966193783,
      "loss": 0.5594,
      "step": 4313
    },
    {
      "epoch": 8.680080482897385,
      "grad_norm": 0.3226797580718994,
      "learning_rate": 0.00018265821511218432,
      "loss": 0.5926,
      "step": 4314
    },
    {
      "epoch": 8.682092555331993,
      "grad_norm": 0.3523479700088501,
      "learning_rate": 0.00018265419056243083,
      "loss": 0.6395,
      "step": 4315
    },
    {
      "epoch": 8.6841046277666,
      "grad_norm": 0.31946951150894165,
      "learning_rate": 0.00018265016601267734,
      "loss": 0.5806,
      "step": 4316
    },
    {
      "epoch": 8.686116700201207,
      "grad_norm": 0.32418620586395264,
      "learning_rate": 0.00018264614146292385,
      "loss": 0.5982,
      "step": 4317
    },
    {
      "epoch": 8.688128772635816,
      "grad_norm": 0.3164699077606201,
      "learning_rate": 0.00018264211691317034,
      "loss": 0.5632,
      "step": 4318
    },
    {
      "epoch": 8.690140845070422,
      "grad_norm": 0.33014753460884094,
      "learning_rate": 0.00018263809236341685,
      "loss": 0.5996,
      "step": 4319
    },
    {
      "epoch": 8.69215291750503,
      "grad_norm": 0.34651702642440796,
      "learning_rate": 0.00018263406781366334,
      "loss": 0.5743,
      "step": 4320
    },
    {
      "epoch": 8.694164989939638,
      "grad_norm": 0.32120567560195923,
      "learning_rate": 0.00018263004326390987,
      "loss": 0.5568,
      "step": 4321
    },
    {
      "epoch": 8.696177062374245,
      "grad_norm": 0.33469080924987793,
      "learning_rate": 0.00018262601871415636,
      "loss": 0.6136,
      "step": 4322
    },
    {
      "epoch": 8.698189134808853,
      "grad_norm": 0.3806546926498413,
      "learning_rate": 0.00018262199416440287,
      "loss": 0.5805,
      "step": 4323
    },
    {
      "epoch": 8.700201207243461,
      "grad_norm": 0.33797216415405273,
      "learning_rate": 0.00018261796961464936,
      "loss": 0.5935,
      "step": 4324
    },
    {
      "epoch": 8.702213279678068,
      "grad_norm": 0.3182049095630646,
      "learning_rate": 0.00018261394506489587,
      "loss": 0.5982,
      "step": 4325
    },
    {
      "epoch": 8.704225352112676,
      "grad_norm": 0.3183179497718811,
      "learning_rate": 0.00018260992051514238,
      "loss": 0.5666,
      "step": 4326
    },
    {
      "epoch": 8.706237424547284,
      "grad_norm": 0.3448978662490845,
      "learning_rate": 0.0001826058959653889,
      "loss": 0.6086,
      "step": 4327
    },
    {
      "epoch": 8.70824949698189,
      "grad_norm": 0.3113442659378052,
      "learning_rate": 0.00018260187141563538,
      "loss": 0.578,
      "step": 4328
    },
    {
      "epoch": 8.710261569416499,
      "grad_norm": 0.33305442333221436,
      "learning_rate": 0.0001825978468658819,
      "loss": 0.6002,
      "step": 4329
    },
    {
      "epoch": 8.712273641851107,
      "grad_norm": 0.31285709142684937,
      "learning_rate": 0.00018259382231612838,
      "loss": 0.5892,
      "step": 4330
    },
    {
      "epoch": 8.714285714285714,
      "grad_norm": 0.3172411322593689,
      "learning_rate": 0.00018258979776637492,
      "loss": 0.5992,
      "step": 4331
    },
    {
      "epoch": 8.716297786720322,
      "grad_norm": 0.3521444797515869,
      "learning_rate": 0.0001825857732166214,
      "loss": 0.6132,
      "step": 4332
    },
    {
      "epoch": 8.71830985915493,
      "grad_norm": 0.3431682884693146,
      "learning_rate": 0.0001825817486668679,
      "loss": 0.6346,
      "step": 4333
    },
    {
      "epoch": 8.720321931589538,
      "grad_norm": 0.32056787610054016,
      "learning_rate": 0.0001825777241171144,
      "loss": 0.5977,
      "step": 4334
    },
    {
      "epoch": 8.722334004024145,
      "grad_norm": 0.34052783250808716,
      "learning_rate": 0.0001825736995673609,
      "loss": 0.6029,
      "step": 4335
    },
    {
      "epoch": 8.724346076458753,
      "grad_norm": 0.3276316523551941,
      "learning_rate": 0.00018256967501760742,
      "loss": 0.5945,
      "step": 4336
    },
    {
      "epoch": 8.726358148893361,
      "grad_norm": 0.32150694727897644,
      "learning_rate": 0.00018256565046785393,
      "loss": 0.6008,
      "step": 4337
    },
    {
      "epoch": 8.728370221327967,
      "grad_norm": 0.32412028312683105,
      "learning_rate": 0.00018256162591810042,
      "loss": 0.5905,
      "step": 4338
    },
    {
      "epoch": 8.730382293762576,
      "grad_norm": 0.3232932388782501,
      "learning_rate": 0.00018255760136834693,
      "loss": 0.5745,
      "step": 4339
    },
    {
      "epoch": 8.732394366197184,
      "grad_norm": 0.31905224919319153,
      "learning_rate": 0.00018255357681859342,
      "loss": 0.5782,
      "step": 4340
    },
    {
      "epoch": 8.73440643863179,
      "grad_norm": 0.3231624364852905,
      "learning_rate": 0.00018254955226883993,
      "loss": 0.5962,
      "step": 4341
    },
    {
      "epoch": 8.736418511066399,
      "grad_norm": 0.3528333008289337,
      "learning_rate": 0.00018254552771908644,
      "loss": 0.6002,
      "step": 4342
    },
    {
      "epoch": 8.738430583501007,
      "grad_norm": 0.34645435214042664,
      "learning_rate": 0.00018254150316933293,
      "loss": 0.5962,
      "step": 4343
    },
    {
      "epoch": 8.740442655935613,
      "grad_norm": 0.3337412178516388,
      "learning_rate": 0.00018253747861957944,
      "loss": 0.5908,
      "step": 4344
    },
    {
      "epoch": 8.742454728370221,
      "grad_norm": 0.3264424204826355,
      "learning_rate": 0.00018253345406982595,
      "loss": 0.5776,
      "step": 4345
    },
    {
      "epoch": 8.74446680080483,
      "grad_norm": 0.3222390413284302,
      "learning_rate": 0.00018252942952007246,
      "loss": 0.6416,
      "step": 4346
    },
    {
      "epoch": 8.746478873239436,
      "grad_norm": 0.3216545879840851,
      "learning_rate": 0.00018252540497031895,
      "loss": 0.5502,
      "step": 4347
    },
    {
      "epoch": 8.748490945674044,
      "grad_norm": 0.31582966446876526,
      "learning_rate": 0.00018252138042056546,
      "loss": 0.6037,
      "step": 4348
    },
    {
      "epoch": 8.750503018108652,
      "grad_norm": 0.31664222478866577,
      "learning_rate": 0.00018251735587081195,
      "loss": 0.6016,
      "step": 4349
    },
    {
      "epoch": 8.752515090543259,
      "grad_norm": 0.32112154364585876,
      "learning_rate": 0.00018251333132105846,
      "loss": 0.6161,
      "step": 4350
    },
    {
      "epoch": 8.754527162977867,
      "grad_norm": 0.3319356143474579,
      "learning_rate": 0.00018250930677130497,
      "loss": 0.613,
      "step": 4351
    },
    {
      "epoch": 8.756539235412475,
      "grad_norm": 0.33276450634002686,
      "learning_rate": 0.00018250528222155148,
      "loss": 0.5985,
      "step": 4352
    },
    {
      "epoch": 8.758551307847082,
      "grad_norm": 0.35497549176216125,
      "learning_rate": 0.00018250125767179797,
      "loss": 0.5475,
      "step": 4353
    },
    {
      "epoch": 8.76056338028169,
      "grad_norm": 0.3166711926460266,
      "learning_rate": 0.00018249723312204448,
      "loss": 0.6031,
      "step": 4354
    },
    {
      "epoch": 8.762575452716298,
      "grad_norm": 0.3205239772796631,
      "learning_rate": 0.00018249320857229096,
      "loss": 0.5666,
      "step": 4355
    },
    {
      "epoch": 8.764587525150905,
      "grad_norm": 0.31796005368232727,
      "learning_rate": 0.0001824891840225375,
      "loss": 0.5836,
      "step": 4356
    },
    {
      "epoch": 8.766599597585513,
      "grad_norm": 0.3257794678211212,
      "learning_rate": 0.000182485159472784,
      "loss": 0.6037,
      "step": 4357
    },
    {
      "epoch": 8.768611670020121,
      "grad_norm": 0.30893880128860474,
      "learning_rate": 0.0001824811349230305,
      "loss": 0.5567,
      "step": 4358
    },
    {
      "epoch": 8.770623742454728,
      "grad_norm": 0.3451729714870453,
      "learning_rate": 0.000182477110373277,
      "loss": 0.5638,
      "step": 4359
    },
    {
      "epoch": 8.772635814889336,
      "grad_norm": 0.3375260531902313,
      "learning_rate": 0.0001824730858235235,
      "loss": 0.6062,
      "step": 4360
    },
    {
      "epoch": 8.774647887323944,
      "grad_norm": 0.3266104757785797,
      "learning_rate": 0.00018246906127377,
      "loss": 0.5915,
      "step": 4361
    },
    {
      "epoch": 8.77665995975855,
      "grad_norm": 0.337966650724411,
      "learning_rate": 0.00018246503672401652,
      "loss": 0.573,
      "step": 4362
    },
    {
      "epoch": 8.778672032193159,
      "grad_norm": 0.328692764043808,
      "learning_rate": 0.000182461012174263,
      "loss": 0.5884,
      "step": 4363
    },
    {
      "epoch": 8.780684104627767,
      "grad_norm": 0.33746138215065,
      "learning_rate": 0.00018245698762450952,
      "loss": 0.6126,
      "step": 4364
    },
    {
      "epoch": 8.782696177062375,
      "grad_norm": 0.34401869773864746,
      "learning_rate": 0.000182452963074756,
      "loss": 0.6294,
      "step": 4365
    },
    {
      "epoch": 8.784708249496981,
      "grad_norm": 0.32713356614112854,
      "learning_rate": 0.00018244893852500255,
      "loss": 0.6125,
      "step": 4366
    },
    {
      "epoch": 8.78672032193159,
      "grad_norm": 0.3222235441207886,
      "learning_rate": 0.00018244491397524903,
      "loss": 0.595,
      "step": 4367
    },
    {
      "epoch": 8.788732394366198,
      "grad_norm": 0.33407822251319885,
      "learning_rate": 0.00018244088942549554,
      "loss": 0.5963,
      "step": 4368
    },
    {
      "epoch": 8.790744466800804,
      "grad_norm": 0.3183322846889496,
      "learning_rate": 0.00018243686487574203,
      "loss": 0.5786,
      "step": 4369
    },
    {
      "epoch": 8.792756539235413,
      "grad_norm": 0.3299352526664734,
      "learning_rate": 0.00018243284032598854,
      "loss": 0.5909,
      "step": 4370
    },
    {
      "epoch": 8.79476861167002,
      "grad_norm": 0.335679829120636,
      "learning_rate": 0.00018242881577623505,
      "loss": 0.6115,
      "step": 4371
    },
    {
      "epoch": 8.796780684104627,
      "grad_norm": 0.3422590494155884,
      "learning_rate": 0.00018242479122648156,
      "loss": 0.5914,
      "step": 4372
    },
    {
      "epoch": 8.798792756539235,
      "grad_norm": 0.31722939014434814,
      "learning_rate": 0.00018242076667672805,
      "loss": 0.5768,
      "step": 4373
    },
    {
      "epoch": 8.800804828973844,
      "grad_norm": 0.3460966646671295,
      "learning_rate": 0.00018241674212697456,
      "loss": 0.6056,
      "step": 4374
    },
    {
      "epoch": 8.80281690140845,
      "grad_norm": 0.3395211696624756,
      "learning_rate": 0.00018241271757722105,
      "loss": 0.6578,
      "step": 4375
    },
    {
      "epoch": 8.804828973843058,
      "grad_norm": 0.32654011249542236,
      "learning_rate": 0.00018240869302746756,
      "loss": 0.5753,
      "step": 4376
    },
    {
      "epoch": 8.806841046277667,
      "grad_norm": 0.32401320338249207,
      "learning_rate": 0.00018240466847771407,
      "loss": 0.5628,
      "step": 4377
    },
    {
      "epoch": 8.808853118712273,
      "grad_norm": 0.33120784163475037,
      "learning_rate": 0.00018240064392796056,
      "loss": 0.5685,
      "step": 4378
    },
    {
      "epoch": 8.810865191146881,
      "grad_norm": 0.33318260312080383,
      "learning_rate": 0.00018239661937820707,
      "loss": 0.5924,
      "step": 4379
    },
    {
      "epoch": 8.81287726358149,
      "grad_norm": 0.3420269787311554,
      "learning_rate": 0.00018239259482845355,
      "loss": 0.6327,
      "step": 4380
    },
    {
      "epoch": 8.814889336016096,
      "grad_norm": 0.32481151819229126,
      "learning_rate": 0.0001823885702787001,
      "loss": 0.5934,
      "step": 4381
    },
    {
      "epoch": 8.816901408450704,
      "grad_norm": 0.3276706635951996,
      "learning_rate": 0.00018238454572894658,
      "loss": 0.6278,
      "step": 4382
    },
    {
      "epoch": 8.818913480885312,
      "grad_norm": 0.3457902669906616,
      "learning_rate": 0.0001823805211791931,
      "loss": 0.5758,
      "step": 4383
    },
    {
      "epoch": 8.82092555331992,
      "grad_norm": 0.33127355575561523,
      "learning_rate": 0.00018237649662943958,
      "loss": 0.645,
      "step": 4384
    },
    {
      "epoch": 8.822937625754527,
      "grad_norm": 0.3609013259410858,
      "learning_rate": 0.0001823724720796861,
      "loss": 0.6059,
      "step": 4385
    },
    {
      "epoch": 8.824949698189135,
      "grad_norm": 0.33889108896255493,
      "learning_rate": 0.0001823684475299326,
      "loss": 0.613,
      "step": 4386
    },
    {
      "epoch": 8.826961770623743,
      "grad_norm": 0.3137572109699249,
      "learning_rate": 0.0001823644229801791,
      "loss": 0.5492,
      "step": 4387
    },
    {
      "epoch": 8.82897384305835,
      "grad_norm": 0.3310171663761139,
      "learning_rate": 0.0001823603984304256,
      "loss": 0.5727,
      "step": 4388
    },
    {
      "epoch": 8.830985915492958,
      "grad_norm": 0.3380938768386841,
      "learning_rate": 0.0001823563738806721,
      "loss": 0.6162,
      "step": 4389
    },
    {
      "epoch": 8.832997987927566,
      "grad_norm": 0.33241671323776245,
      "learning_rate": 0.0001823523493309186,
      "loss": 0.6226,
      "step": 4390
    },
    {
      "epoch": 8.835010060362173,
      "grad_norm": 0.33403280377388,
      "learning_rate": 0.00018234832478116513,
      "loss": 0.5521,
      "step": 4391
    },
    {
      "epoch": 8.83702213279678,
      "grad_norm": 0.3480355441570282,
      "learning_rate": 0.00018234430023141162,
      "loss": 0.6189,
      "step": 4392
    },
    {
      "epoch": 8.839034205231389,
      "grad_norm": 0.3095870018005371,
      "learning_rate": 0.00018234027568165813,
      "loss": 0.5642,
      "step": 4393
    },
    {
      "epoch": 8.841046277665995,
      "grad_norm": 0.32084372639656067,
      "learning_rate": 0.00018233625113190462,
      "loss": 0.5701,
      "step": 4394
    },
    {
      "epoch": 8.843058350100604,
      "grad_norm": 0.3246077597141266,
      "learning_rate": 0.00018233222658215113,
      "loss": 0.6188,
      "step": 4395
    },
    {
      "epoch": 8.845070422535212,
      "grad_norm": 0.3232371211051941,
      "learning_rate": 0.00018232820203239764,
      "loss": 0.5562,
      "step": 4396
    },
    {
      "epoch": 8.847082494969818,
      "grad_norm": 0.3123041093349457,
      "learning_rate": 0.00018232417748264415,
      "loss": 0.6052,
      "step": 4397
    },
    {
      "epoch": 8.849094567404427,
      "grad_norm": 0.325676292181015,
      "learning_rate": 0.00018232015293289064,
      "loss": 0.6239,
      "step": 4398
    },
    {
      "epoch": 8.851106639839035,
      "grad_norm": 0.31973570585250854,
      "learning_rate": 0.00018231612838313715,
      "loss": 0.603,
      "step": 4399
    },
    {
      "epoch": 8.853118712273641,
      "grad_norm": 0.3198969066143036,
      "learning_rate": 0.00018231210383338364,
      "loss": 0.586,
      "step": 4400
    },
    {
      "epoch": 8.85513078470825,
      "grad_norm": 0.3330870270729065,
      "learning_rate": 0.00018230807928363017,
      "loss": 0.6029,
      "step": 4401
    },
    {
      "epoch": 8.857142857142858,
      "grad_norm": 0.32470953464508057,
      "learning_rate": 0.00018230405473387666,
      "loss": 0.5851,
      "step": 4402
    },
    {
      "epoch": 8.859154929577464,
      "grad_norm": 0.3212059736251831,
      "learning_rate": 0.00018230003018412317,
      "loss": 0.5922,
      "step": 4403
    },
    {
      "epoch": 8.861167002012072,
      "grad_norm": 0.33759328722953796,
      "learning_rate": 0.00018229600563436966,
      "loss": 0.6071,
      "step": 4404
    },
    {
      "epoch": 8.86317907444668,
      "grad_norm": 0.3054650127887726,
      "learning_rate": 0.00018229198108461617,
      "loss": 0.551,
      "step": 4405
    },
    {
      "epoch": 8.865191146881287,
      "grad_norm": 0.32409459352493286,
      "learning_rate": 0.00018228795653486268,
      "loss": 0.6039,
      "step": 4406
    },
    {
      "epoch": 8.867203219315895,
      "grad_norm": 0.331898957490921,
      "learning_rate": 0.00018228393198510917,
      "loss": 0.5841,
      "step": 4407
    },
    {
      "epoch": 8.869215291750503,
      "grad_norm": 0.3264295160770416,
      "learning_rate": 0.00018227990743535568,
      "loss": 0.6033,
      "step": 4408
    },
    {
      "epoch": 8.87122736418511,
      "grad_norm": 0.3174096941947937,
      "learning_rate": 0.0001822758828856022,
      "loss": 0.5943,
      "step": 4409
    },
    {
      "epoch": 8.873239436619718,
      "grad_norm": 0.30731436610221863,
      "learning_rate": 0.00018227185833584868,
      "loss": 0.5973,
      "step": 4410
    },
    {
      "epoch": 8.875251509054326,
      "grad_norm": 0.32748839259147644,
      "learning_rate": 0.0001822678337860952,
      "loss": 0.5879,
      "step": 4411
    },
    {
      "epoch": 8.877263581488933,
      "grad_norm": 0.3315056562423706,
      "learning_rate": 0.0001822638092363417,
      "loss": 0.5924,
      "step": 4412
    },
    {
      "epoch": 8.879275653923541,
      "grad_norm": 0.3588087558746338,
      "learning_rate": 0.00018225978468658819,
      "loss": 0.5889,
      "step": 4413
    },
    {
      "epoch": 8.88128772635815,
      "grad_norm": 0.3243250548839569,
      "learning_rate": 0.0001822557601368347,
      "loss": 0.5715,
      "step": 4414
    },
    {
      "epoch": 8.883299798792757,
      "grad_norm": 0.3283257782459259,
      "learning_rate": 0.00018225173558708118,
      "loss": 0.5991,
      "step": 4415
    },
    {
      "epoch": 8.885311871227364,
      "grad_norm": 0.3309893310070038,
      "learning_rate": 0.00018224771103732772,
      "loss": 0.5835,
      "step": 4416
    },
    {
      "epoch": 8.887323943661972,
      "grad_norm": 0.3298376202583313,
      "learning_rate": 0.0001822436864875742,
      "loss": 0.6024,
      "step": 4417
    },
    {
      "epoch": 8.88933601609658,
      "grad_norm": 0.332091748714447,
      "learning_rate": 0.00018223966193782072,
      "loss": 0.6189,
      "step": 4418
    },
    {
      "epoch": 8.891348088531187,
      "grad_norm": 0.3155660629272461,
      "learning_rate": 0.0001822356373880672,
      "loss": 0.5429,
      "step": 4419
    },
    {
      "epoch": 8.893360160965795,
      "grad_norm": 0.34550878405570984,
      "learning_rate": 0.00018223161283831372,
      "loss": 0.6207,
      "step": 4420
    },
    {
      "epoch": 8.895372233400403,
      "grad_norm": 0.3285275995731354,
      "learning_rate": 0.00018222758828856023,
      "loss": 0.596,
      "step": 4421
    },
    {
      "epoch": 8.89738430583501,
      "grad_norm": 0.3230234682559967,
      "learning_rate": 0.00018222356373880674,
      "loss": 0.5805,
      "step": 4422
    },
    {
      "epoch": 8.899396378269618,
      "grad_norm": 0.3417440950870514,
      "learning_rate": 0.00018221953918905323,
      "loss": 0.5661,
      "step": 4423
    },
    {
      "epoch": 8.901408450704226,
      "grad_norm": 0.32072144746780396,
      "learning_rate": 0.00018221551463929974,
      "loss": 0.5899,
      "step": 4424
    },
    {
      "epoch": 8.903420523138832,
      "grad_norm": 0.34625139832496643,
      "learning_rate": 0.00018221149008954622,
      "loss": 0.5632,
      "step": 4425
    },
    {
      "epoch": 8.90543259557344,
      "grad_norm": 0.3173697888851166,
      "learning_rate": 0.00018220746553979276,
      "loss": 0.6037,
      "step": 4426
    },
    {
      "epoch": 8.907444668008049,
      "grad_norm": 0.3504907190799713,
      "learning_rate": 0.00018220344099003925,
      "loss": 0.5944,
      "step": 4427
    },
    {
      "epoch": 8.909456740442655,
      "grad_norm": 0.332701176404953,
      "learning_rate": 0.00018219941644028576,
      "loss": 0.5683,
      "step": 4428
    },
    {
      "epoch": 8.911468812877263,
      "grad_norm": 0.34265053272247314,
      "learning_rate": 0.00018219539189053225,
      "loss": 0.5903,
      "step": 4429
    },
    {
      "epoch": 8.913480885311872,
      "grad_norm": 0.32155585289001465,
      "learning_rate": 0.00018219136734077876,
      "loss": 0.5721,
      "step": 4430
    },
    {
      "epoch": 8.915492957746478,
      "grad_norm": 0.3607003092765808,
      "learning_rate": 0.00018218734279102527,
      "loss": 0.6364,
      "step": 4431
    },
    {
      "epoch": 8.917505030181086,
      "grad_norm": 0.31932058930397034,
      "learning_rate": 0.00018218331824127178,
      "loss": 0.6079,
      "step": 4432
    },
    {
      "epoch": 8.919517102615695,
      "grad_norm": 0.3533271253108978,
      "learning_rate": 0.00018217929369151827,
      "loss": 0.5925,
      "step": 4433
    },
    {
      "epoch": 8.921529175050303,
      "grad_norm": 0.34317055344581604,
      "learning_rate": 0.00018217526914176478,
      "loss": 0.5924,
      "step": 4434
    },
    {
      "epoch": 8.92354124748491,
      "grad_norm": 0.3291001319885254,
      "learning_rate": 0.00018217124459201126,
      "loss": 0.6169,
      "step": 4435
    },
    {
      "epoch": 8.925553319919517,
      "grad_norm": 0.3193294405937195,
      "learning_rate": 0.0001821672200422578,
      "loss": 0.5831,
      "step": 4436
    },
    {
      "epoch": 8.927565392354126,
      "grad_norm": 0.3278452754020691,
      "learning_rate": 0.0001821631954925043,
      "loss": 0.5748,
      "step": 4437
    },
    {
      "epoch": 8.929577464788732,
      "grad_norm": 0.32764551043510437,
      "learning_rate": 0.0001821591709427508,
      "loss": 0.6023,
      "step": 4438
    },
    {
      "epoch": 8.93158953722334,
      "grad_norm": 0.3146671652793884,
      "learning_rate": 0.00018215514639299729,
      "loss": 0.5476,
      "step": 4439
    },
    {
      "epoch": 8.933601609657948,
      "grad_norm": 0.30849218368530273,
      "learning_rate": 0.0001821511218432438,
      "loss": 0.5909,
      "step": 4440
    },
    {
      "epoch": 8.935613682092555,
      "grad_norm": 0.32242855429649353,
      "learning_rate": 0.0001821470972934903,
      "loss": 0.5769,
      "step": 4441
    },
    {
      "epoch": 8.937625754527163,
      "grad_norm": 0.3358529508113861,
      "learning_rate": 0.0001821430727437368,
      "loss": 0.605,
      "step": 4442
    },
    {
      "epoch": 8.939637826961771,
      "grad_norm": 0.31947022676467896,
      "learning_rate": 0.0001821390481939833,
      "loss": 0.5775,
      "step": 4443
    },
    {
      "epoch": 8.941649899396378,
      "grad_norm": 0.32063278555870056,
      "learning_rate": 0.00018213502364422982,
      "loss": 0.5652,
      "step": 4444
    },
    {
      "epoch": 8.943661971830986,
      "grad_norm": 0.32913750410079956,
      "learning_rate": 0.0001821309990944763,
      "loss": 0.5936,
      "step": 4445
    },
    {
      "epoch": 8.945674044265594,
      "grad_norm": 0.3085302412509918,
      "learning_rate": 0.00018212697454472282,
      "loss": 0.5837,
      "step": 4446
    },
    {
      "epoch": 8.9476861167002,
      "grad_norm": 0.31844261288642883,
      "learning_rate": 0.00018212294999496933,
      "loss": 0.5632,
      "step": 4447
    },
    {
      "epoch": 8.949698189134809,
      "grad_norm": 0.3380117416381836,
      "learning_rate": 0.00018211892544521582,
      "loss": 0.5681,
      "step": 4448
    },
    {
      "epoch": 8.951710261569417,
      "grad_norm": 0.32289543747901917,
      "learning_rate": 0.00018211490089546233,
      "loss": 0.5374,
      "step": 4449
    },
    {
      "epoch": 8.953722334004024,
      "grad_norm": 0.35289180278778076,
      "learning_rate": 0.0001821108763457088,
      "loss": 0.5943,
      "step": 4450
    },
    {
      "epoch": 8.955734406438632,
      "grad_norm": 0.3252187669277191,
      "learning_rate": 0.00018210685179595535,
      "loss": 0.5559,
      "step": 4451
    },
    {
      "epoch": 8.95774647887324,
      "grad_norm": 0.33013731241226196,
      "learning_rate": 0.00018210282724620184,
      "loss": 0.5961,
      "step": 4452
    },
    {
      "epoch": 8.959758551307846,
      "grad_norm": 0.3410528600215912,
      "learning_rate": 0.00018209880269644835,
      "loss": 0.5861,
      "step": 4453
    },
    {
      "epoch": 8.961770623742455,
      "grad_norm": 0.33461952209472656,
      "learning_rate": 0.00018209477814669483,
      "loss": 0.5736,
      "step": 4454
    },
    {
      "epoch": 8.963782696177063,
      "grad_norm": 0.33026742935180664,
      "learning_rate": 0.00018209075359694135,
      "loss": 0.532,
      "step": 4455
    },
    {
      "epoch": 8.96579476861167,
      "grad_norm": 0.33325254917144775,
      "learning_rate": 0.00018208672904718786,
      "loss": 0.5816,
      "step": 4456
    },
    {
      "epoch": 8.967806841046277,
      "grad_norm": 0.34702053666114807,
      "learning_rate": 0.00018208270449743437,
      "loss": 0.6028,
      "step": 4457
    },
    {
      "epoch": 8.969818913480886,
      "grad_norm": 0.3273792564868927,
      "learning_rate": 0.00018207867994768086,
      "loss": 0.5743,
      "step": 4458
    },
    {
      "epoch": 8.971830985915492,
      "grad_norm": 0.31968188285827637,
      "learning_rate": 0.00018207465539792737,
      "loss": 0.5944,
      "step": 4459
    },
    {
      "epoch": 8.9738430583501,
      "grad_norm": 0.3310566842556,
      "learning_rate": 0.00018207063084817385,
      "loss": 0.5727,
      "step": 4460
    },
    {
      "epoch": 8.975855130784709,
      "grad_norm": 0.32353562116622925,
      "learning_rate": 0.0001820666062984204,
      "loss": 0.5963,
      "step": 4461
    },
    {
      "epoch": 8.977867203219315,
      "grad_norm": 0.3303062617778778,
      "learning_rate": 0.00018206258174866688,
      "loss": 0.58,
      "step": 4462
    },
    {
      "epoch": 8.979879275653923,
      "grad_norm": 0.3184329569339752,
      "learning_rate": 0.0001820585571989134,
      "loss": 0.6005,
      "step": 4463
    },
    {
      "epoch": 8.981891348088531,
      "grad_norm": 0.33555927872657776,
      "learning_rate": 0.00018205453264915987,
      "loss": 0.5507,
      "step": 4464
    },
    {
      "epoch": 8.98390342052314,
      "grad_norm": 0.32341766357421875,
      "learning_rate": 0.0001820505080994064,
      "loss": 0.5996,
      "step": 4465
    },
    {
      "epoch": 8.985915492957746,
      "grad_norm": 0.3308480679988861,
      "learning_rate": 0.0001820464835496529,
      "loss": 0.6094,
      "step": 4466
    },
    {
      "epoch": 8.987927565392354,
      "grad_norm": 0.33688557147979736,
      "learning_rate": 0.0001820424589998994,
      "loss": 0.5873,
      "step": 4467
    },
    {
      "epoch": 8.989939637826962,
      "grad_norm": 0.34315040707588196,
      "learning_rate": 0.0001820384344501459,
      "loss": 0.5904,
      "step": 4468
    },
    {
      "epoch": 8.991951710261569,
      "grad_norm": 0.34869450330734253,
      "learning_rate": 0.0001820344099003924,
      "loss": 0.6073,
      "step": 4469
    },
    {
      "epoch": 8.993963782696177,
      "grad_norm": 0.327768474817276,
      "learning_rate": 0.0001820303853506389,
      "loss": 0.5695,
      "step": 4470
    },
    {
      "epoch": 8.995975855130785,
      "grad_norm": 0.3299146294593811,
      "learning_rate": 0.00018202636080088543,
      "loss": 0.5742,
      "step": 4471
    },
    {
      "epoch": 8.997987927565392,
      "grad_norm": 0.3229290246963501,
      "learning_rate": 0.00018202233625113192,
      "loss": 0.6141,
      "step": 4472
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.3373996317386627,
      "learning_rate": 0.00018201831170137843,
      "loss": 0.6108,
      "step": 4473
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.7353572249412537,
      "eval_runtime": 49.7812,
      "eval_samples_per_second": 19.927,
      "eval_steps_per_second": 2.491,
      "step": 4473
    },
    {
      "epoch": 9.002012072434608,
      "grad_norm": 0.3858930766582489,
      "learning_rate": 0.00018201428715162492,
      "loss": 0.5021,
      "step": 4474
    },
    {
      "epoch": 9.004024144869215,
      "grad_norm": 0.34921613335609436,
      "learning_rate": 0.00018201026260187143,
      "loss": 0.5218,
      "step": 4475
    },
    {
      "epoch": 9.006036217303823,
      "grad_norm": 0.32769688963890076,
      "learning_rate": 0.00018200623805211794,
      "loss": 0.582,
      "step": 4476
    },
    {
      "epoch": 9.008048289738431,
      "grad_norm": 0.37050190567970276,
      "learning_rate": 0.00018200221350236443,
      "loss": 0.5218,
      "step": 4477
    },
    {
      "epoch": 9.010060362173038,
      "grad_norm": 0.3733208179473877,
      "learning_rate": 0.00018199818895261094,
      "loss": 0.5476,
      "step": 4478
    },
    {
      "epoch": 9.012072434607646,
      "grad_norm": 0.34423792362213135,
      "learning_rate": 0.00018199416440285745,
      "loss": 0.5065,
      "step": 4479
    },
    {
      "epoch": 9.014084507042254,
      "grad_norm": 0.3487076163291931,
      "learning_rate": 0.00018199013985310393,
      "loss": 0.5377,
      "step": 4480
    },
    {
      "epoch": 9.01609657947686,
      "grad_norm": 0.3672633469104767,
      "learning_rate": 0.00018198611530335045,
      "loss": 0.5252,
      "step": 4481
    },
    {
      "epoch": 9.018108651911469,
      "grad_norm": 0.36635953187942505,
      "learning_rate": 0.00018198209075359696,
      "loss": 0.534,
      "step": 4482
    },
    {
      "epoch": 9.020120724346077,
      "grad_norm": 0.3390047252178192,
      "learning_rate": 0.00018197806620384344,
      "loss": 0.5248,
      "step": 4483
    },
    {
      "epoch": 9.022132796780683,
      "grad_norm": 0.331703245639801,
      "learning_rate": 0.00018197404165408996,
      "loss": 0.521,
      "step": 4484
    },
    {
      "epoch": 9.024144869215291,
      "grad_norm": 0.33080169558525085,
      "learning_rate": 0.00018197001710433644,
      "loss": 0.5107,
      "step": 4485
    },
    {
      "epoch": 9.0261569416499,
      "grad_norm": 0.323961466550827,
      "learning_rate": 0.00018196599255458298,
      "loss": 0.557,
      "step": 4486
    },
    {
      "epoch": 9.028169014084508,
      "grad_norm": 0.3642241060733795,
      "learning_rate": 0.00018196196800482947,
      "loss": 0.5832,
      "step": 4487
    },
    {
      "epoch": 9.030181086519114,
      "grad_norm": 0.3700006604194641,
      "learning_rate": 0.00018195794345507598,
      "loss": 0.5307,
      "step": 4488
    },
    {
      "epoch": 9.032193158953723,
      "grad_norm": 0.3464256227016449,
      "learning_rate": 0.00018195391890532246,
      "loss": 0.5013,
      "step": 4489
    },
    {
      "epoch": 9.03420523138833,
      "grad_norm": 0.3454773724079132,
      "learning_rate": 0.00018194989435556898,
      "loss": 0.5484,
      "step": 4490
    },
    {
      "epoch": 9.036217303822937,
      "grad_norm": 0.3531540632247925,
      "learning_rate": 0.00018194586980581546,
      "loss": 0.5491,
      "step": 4491
    },
    {
      "epoch": 9.038229376257545,
      "grad_norm": 0.33848294615745544,
      "learning_rate": 0.000181941845256062,
      "loss": 0.5647,
      "step": 4492
    },
    {
      "epoch": 9.040241448692154,
      "grad_norm": 0.33798378705978394,
      "learning_rate": 0.00018193782070630849,
      "loss": 0.5345,
      "step": 4493
    },
    {
      "epoch": 9.04225352112676,
      "grad_norm": 0.33223623037338257,
      "learning_rate": 0.000181933796156555,
      "loss": 0.5243,
      "step": 4494
    },
    {
      "epoch": 9.044265593561368,
      "grad_norm": 0.34000325202941895,
      "learning_rate": 0.00018192977160680148,
      "loss": 0.5193,
      "step": 4495
    },
    {
      "epoch": 9.046277665995976,
      "grad_norm": 0.3404308259487152,
      "learning_rate": 0.000181925747057048,
      "loss": 0.5608,
      "step": 4496
    },
    {
      "epoch": 9.048289738430583,
      "grad_norm": 0.3350019156932831,
      "learning_rate": 0.0001819217225072945,
      "loss": 0.5202,
      "step": 4497
    },
    {
      "epoch": 9.050301810865191,
      "grad_norm": 0.34704357385635376,
      "learning_rate": 0.00018191769795754102,
      "loss": 0.561,
      "step": 4498
    },
    {
      "epoch": 9.0523138832998,
      "grad_norm": 0.3700227737426758,
      "learning_rate": 0.0001819136734077875,
      "loss": 0.5563,
      "step": 4499
    },
    {
      "epoch": 9.054325955734406,
      "grad_norm": 0.34729698300361633,
      "learning_rate": 0.00018190964885803402,
      "loss": 0.5081,
      "step": 4500
    },
    {
      "epoch": 9.056338028169014,
      "grad_norm": 0.34217193722724915,
      "learning_rate": 0.0001819056243082805,
      "loss": 0.5325,
      "step": 4501
    },
    {
      "epoch": 9.058350100603622,
      "grad_norm": 0.3424144387245178,
      "learning_rate": 0.00018190159975852704,
      "loss": 0.5505,
      "step": 4502
    },
    {
      "epoch": 9.060362173038229,
      "grad_norm": 0.3641083836555481,
      "learning_rate": 0.00018189757520877353,
      "loss": 0.5287,
      "step": 4503
    },
    {
      "epoch": 9.062374245472837,
      "grad_norm": 0.35848715901374817,
      "learning_rate": 0.00018189355065902004,
      "loss": 0.5346,
      "step": 4504
    },
    {
      "epoch": 9.064386317907445,
      "grad_norm": 0.3478071093559265,
      "learning_rate": 0.00018188952610926652,
      "loss": 0.5703,
      "step": 4505
    },
    {
      "epoch": 9.066398390342052,
      "grad_norm": 0.334960013628006,
      "learning_rate": 0.00018188550155951304,
      "loss": 0.5635,
      "step": 4506
    },
    {
      "epoch": 9.06841046277666,
      "grad_norm": 0.3638325035572052,
      "learning_rate": 0.00018188147700975955,
      "loss": 0.5624,
      "step": 4507
    },
    {
      "epoch": 9.070422535211268,
      "grad_norm": 0.3442705273628235,
      "learning_rate": 0.00018187745246000606,
      "loss": 0.5359,
      "step": 4508
    },
    {
      "epoch": 9.072434607645874,
      "grad_norm": 0.34012073278427124,
      "learning_rate": 0.00018187342791025255,
      "loss": 0.5552,
      "step": 4509
    },
    {
      "epoch": 9.074446680080483,
      "grad_norm": 0.35439687967300415,
      "learning_rate": 0.00018186940336049906,
      "loss": 0.5447,
      "step": 4510
    },
    {
      "epoch": 9.07645875251509,
      "grad_norm": 0.3581143021583557,
      "learning_rate": 0.00018186537881074554,
      "loss": 0.5807,
      "step": 4511
    },
    {
      "epoch": 9.078470824949699,
      "grad_norm": 0.34794947504997253,
      "learning_rate": 0.00018186135426099205,
      "loss": 0.5017,
      "step": 4512
    },
    {
      "epoch": 9.080482897384305,
      "grad_norm": 0.36767375469207764,
      "learning_rate": 0.00018185732971123857,
      "loss": 0.5377,
      "step": 4513
    },
    {
      "epoch": 9.082494969818914,
      "grad_norm": 0.35272374749183655,
      "learning_rate": 0.00018185330516148508,
      "loss": 0.5508,
      "step": 4514
    },
    {
      "epoch": 9.084507042253522,
      "grad_norm": 0.3413488566875458,
      "learning_rate": 0.00018184928061173156,
      "loss": 0.5457,
      "step": 4515
    },
    {
      "epoch": 9.086519114688128,
      "grad_norm": 0.33723440766334534,
      "learning_rate": 0.00018184525606197808,
      "loss": 0.5039,
      "step": 4516
    },
    {
      "epoch": 9.088531187122737,
      "grad_norm": 0.35045042634010315,
      "learning_rate": 0.0001818412315122246,
      "loss": 0.5696,
      "step": 4517
    },
    {
      "epoch": 9.090543259557345,
      "grad_norm": 0.34014448523521423,
      "learning_rate": 0.00018183720696247107,
      "loss": 0.5676,
      "step": 4518
    },
    {
      "epoch": 9.092555331991951,
      "grad_norm": 0.33773931860923767,
      "learning_rate": 0.00018183318241271759,
      "loss": 0.5112,
      "step": 4519
    },
    {
      "epoch": 9.09456740442656,
      "grad_norm": 0.34280675649642944,
      "learning_rate": 0.00018182915786296407,
      "loss": 0.5369,
      "step": 4520
    },
    {
      "epoch": 9.096579476861168,
      "grad_norm": 0.38263869285583496,
      "learning_rate": 0.00018182513331321058,
      "loss": 0.5537,
      "step": 4521
    },
    {
      "epoch": 9.098591549295774,
      "grad_norm": 0.3449910283088684,
      "learning_rate": 0.0001818211087634571,
      "loss": 0.5724,
      "step": 4522
    },
    {
      "epoch": 9.100603621730382,
      "grad_norm": 0.35644903779029846,
      "learning_rate": 0.0001818170842137036,
      "loss": 0.5567,
      "step": 4523
    },
    {
      "epoch": 9.10261569416499,
      "grad_norm": 0.3609400689601898,
      "learning_rate": 0.0001818130596639501,
      "loss": 0.5425,
      "step": 4524
    },
    {
      "epoch": 9.104627766599597,
      "grad_norm": 0.33823350071907043,
      "learning_rate": 0.0001818090351141966,
      "loss": 0.5128,
      "step": 4525
    },
    {
      "epoch": 9.106639839034205,
      "grad_norm": 0.3435835838317871,
      "learning_rate": 0.0001818050105644431,
      "loss": 0.5473,
      "step": 4526
    },
    {
      "epoch": 9.108651911468813,
      "grad_norm": 0.33813124895095825,
      "learning_rate": 0.00018180098601468963,
      "loss": 0.5393,
      "step": 4527
    },
    {
      "epoch": 9.11066398390342,
      "grad_norm": 0.3740938603878021,
      "learning_rate": 0.00018179696146493611,
      "loss": 0.5505,
      "step": 4528
    },
    {
      "epoch": 9.112676056338028,
      "grad_norm": 0.355400413274765,
      "learning_rate": 0.00018179293691518263,
      "loss": 0.5475,
      "step": 4529
    },
    {
      "epoch": 9.114688128772636,
      "grad_norm": 0.33935225009918213,
      "learning_rate": 0.0001817889123654291,
      "loss": 0.5214,
      "step": 4530
    },
    {
      "epoch": 9.116700201207243,
      "grad_norm": 0.3316129148006439,
      "learning_rate": 0.00018178488781567562,
      "loss": 0.5688,
      "step": 4531
    },
    {
      "epoch": 9.11871227364185,
      "grad_norm": 0.3344170153141022,
      "learning_rate": 0.00018178086326592214,
      "loss": 0.5431,
      "step": 4532
    },
    {
      "epoch": 9.120724346076459,
      "grad_norm": 0.3289436399936676,
      "learning_rate": 0.00018177683871616865,
      "loss": 0.531,
      "step": 4533
    },
    {
      "epoch": 9.122736418511066,
      "grad_norm": 0.35789719223976135,
      "learning_rate": 0.00018177281416641513,
      "loss": 0.5379,
      "step": 4534
    },
    {
      "epoch": 9.124748490945674,
      "grad_norm": 0.33461305499076843,
      "learning_rate": 0.00018176878961666165,
      "loss": 0.5049,
      "step": 4535
    },
    {
      "epoch": 9.126760563380282,
      "grad_norm": 0.3379632532596588,
      "learning_rate": 0.00018176476506690813,
      "loss": 0.5398,
      "step": 4536
    },
    {
      "epoch": 9.12877263581489,
      "grad_norm": 0.3429467976093292,
      "learning_rate": 0.00018176074051715467,
      "loss": 0.5596,
      "step": 4537
    },
    {
      "epoch": 9.130784708249497,
      "grad_norm": 0.3559817969799042,
      "learning_rate": 0.00018175671596740116,
      "loss": 0.5553,
      "step": 4538
    },
    {
      "epoch": 9.132796780684105,
      "grad_norm": 0.3492472767829895,
      "learning_rate": 0.00018175269141764767,
      "loss": 0.5636,
      "step": 4539
    },
    {
      "epoch": 9.134808853118713,
      "grad_norm": 0.3501015901565552,
      "learning_rate": 0.00018174866686789415,
      "loss": 0.5806,
      "step": 4540
    },
    {
      "epoch": 9.13682092555332,
      "grad_norm": 0.36880919337272644,
      "learning_rate": 0.00018174464231814067,
      "loss": 0.5526,
      "step": 4541
    },
    {
      "epoch": 9.138832997987928,
      "grad_norm": 0.33912068605422974,
      "learning_rate": 0.00018174061776838718,
      "loss": 0.5765,
      "step": 4542
    },
    {
      "epoch": 9.140845070422536,
      "grad_norm": 0.3473772704601288,
      "learning_rate": 0.0001817365932186337,
      "loss": 0.5525,
      "step": 4543
    },
    {
      "epoch": 9.142857142857142,
      "grad_norm": 0.3869362473487854,
      "learning_rate": 0.00018173256866888017,
      "loss": 0.5765,
      "step": 4544
    },
    {
      "epoch": 9.14486921529175,
      "grad_norm": 0.3486441373825073,
      "learning_rate": 0.0001817285441191267,
      "loss": 0.543,
      "step": 4545
    },
    {
      "epoch": 9.146881287726359,
      "grad_norm": 0.3542096018791199,
      "learning_rate": 0.00018172451956937317,
      "loss": 0.5463,
      "step": 4546
    },
    {
      "epoch": 9.148893360160965,
      "grad_norm": 0.35328659415245056,
      "learning_rate": 0.00018172049501961968,
      "loss": 0.5463,
      "step": 4547
    },
    {
      "epoch": 9.150905432595573,
      "grad_norm": 0.35405537486076355,
      "learning_rate": 0.0001817164704698662,
      "loss": 0.6023,
      "step": 4548
    },
    {
      "epoch": 9.152917505030182,
      "grad_norm": 0.3279384672641754,
      "learning_rate": 0.00018171244592011268,
      "loss": 0.5133,
      "step": 4549
    },
    {
      "epoch": 9.154929577464788,
      "grad_norm": 0.33507540822029114,
      "learning_rate": 0.0001817084213703592,
      "loss": 0.5438,
      "step": 4550
    },
    {
      "epoch": 9.156941649899396,
      "grad_norm": 0.36532920598983765,
      "learning_rate": 0.0001817043968206057,
      "loss": 0.5571,
      "step": 4551
    },
    {
      "epoch": 9.158953722334005,
      "grad_norm": 0.3453003764152527,
      "learning_rate": 0.00018170037227085222,
      "loss": 0.5329,
      "step": 4552
    },
    {
      "epoch": 9.160965794768611,
      "grad_norm": 0.3513019382953644,
      "learning_rate": 0.0001816963477210987,
      "loss": 0.5115,
      "step": 4553
    },
    {
      "epoch": 9.16297786720322,
      "grad_norm": 0.368761271238327,
      "learning_rate": 0.00018169232317134522,
      "loss": 0.6059,
      "step": 4554
    },
    {
      "epoch": 9.164989939637827,
      "grad_norm": 0.3468153774738312,
      "learning_rate": 0.0001816882986215917,
      "loss": 0.5279,
      "step": 4555
    },
    {
      "epoch": 9.167002012072434,
      "grad_norm": 0.35367724299430847,
      "learning_rate": 0.0001816842740718382,
      "loss": 0.5769,
      "step": 4556
    },
    {
      "epoch": 9.169014084507042,
      "grad_norm": 0.35513025522232056,
      "learning_rate": 0.00018168024952208473,
      "loss": 0.5705,
      "step": 4557
    },
    {
      "epoch": 9.17102615694165,
      "grad_norm": 0.33776557445526123,
      "learning_rate": 0.00018167622497233124,
      "loss": 0.5141,
      "step": 4558
    },
    {
      "epoch": 9.173038229376257,
      "grad_norm": 0.3710504472255707,
      "learning_rate": 0.00018167220042257772,
      "loss": 0.5681,
      "step": 4559
    },
    {
      "epoch": 9.175050301810865,
      "grad_norm": 0.34588906168937683,
      "learning_rate": 0.00018166817587282423,
      "loss": 0.5312,
      "step": 4560
    },
    {
      "epoch": 9.177062374245473,
      "grad_norm": 0.34442248940467834,
      "learning_rate": 0.00018166415132307072,
      "loss": 0.5469,
      "step": 4561
    },
    {
      "epoch": 9.179074446680081,
      "grad_norm": 0.3584141433238983,
      "learning_rate": 0.00018166012677331726,
      "loss": 0.5562,
      "step": 4562
    },
    {
      "epoch": 9.181086519114688,
      "grad_norm": 0.3513322174549103,
      "learning_rate": 0.00018165610222356374,
      "loss": 0.5754,
      "step": 4563
    },
    {
      "epoch": 9.183098591549296,
      "grad_norm": 0.3584251403808594,
      "learning_rate": 0.00018165207767381026,
      "loss": 0.5153,
      "step": 4564
    },
    {
      "epoch": 9.185110663983904,
      "grad_norm": 0.36141839623451233,
      "learning_rate": 0.00018164805312405674,
      "loss": 0.5931,
      "step": 4565
    },
    {
      "epoch": 9.18712273641851,
      "grad_norm": 0.3731357753276825,
      "learning_rate": 0.00018164402857430325,
      "loss": 0.5707,
      "step": 4566
    },
    {
      "epoch": 9.189134808853119,
      "grad_norm": 0.34099268913269043,
      "learning_rate": 0.00018164000402454977,
      "loss": 0.5349,
      "step": 4567
    },
    {
      "epoch": 9.191146881287727,
      "grad_norm": 0.3470427393913269,
      "learning_rate": 0.00018163597947479628,
      "loss": 0.5628,
      "step": 4568
    },
    {
      "epoch": 9.193158953722333,
      "grad_norm": 0.3678503632545471,
      "learning_rate": 0.00018163195492504276,
      "loss": 0.5535,
      "step": 4569
    },
    {
      "epoch": 9.195171026156942,
      "grad_norm": 0.3688134253025055,
      "learning_rate": 0.00018162793037528928,
      "loss": 0.5221,
      "step": 4570
    },
    {
      "epoch": 9.19718309859155,
      "grad_norm": 0.3582690358161926,
      "learning_rate": 0.00018162390582553576,
      "loss": 0.5215,
      "step": 4571
    },
    {
      "epoch": 9.199195171026156,
      "grad_norm": 0.3502657115459442,
      "learning_rate": 0.0001816198812757823,
      "loss": 0.5275,
      "step": 4572
    },
    {
      "epoch": 9.201207243460765,
      "grad_norm": 0.35601794719696045,
      "learning_rate": 0.00018161585672602879,
      "loss": 0.5402,
      "step": 4573
    },
    {
      "epoch": 9.203219315895373,
      "grad_norm": 0.3485018014907837,
      "learning_rate": 0.0001816118321762753,
      "loss": 0.5628,
      "step": 4574
    },
    {
      "epoch": 9.20523138832998,
      "grad_norm": 0.3426527976989746,
      "learning_rate": 0.00018160780762652178,
      "loss": 0.5432,
      "step": 4575
    },
    {
      "epoch": 9.207243460764587,
      "grad_norm": 0.34917157888412476,
      "learning_rate": 0.0001816037830767683,
      "loss": 0.544,
      "step": 4576
    },
    {
      "epoch": 9.209255533199196,
      "grad_norm": 0.3448025584220886,
      "learning_rate": 0.0001815997585270148,
      "loss": 0.519,
      "step": 4577
    },
    {
      "epoch": 9.211267605633802,
      "grad_norm": 0.3480176031589508,
      "learning_rate": 0.00018159573397726132,
      "loss": 0.5893,
      "step": 4578
    },
    {
      "epoch": 9.21327967806841,
      "grad_norm": 0.34389224648475647,
      "learning_rate": 0.0001815917094275078,
      "loss": 0.5399,
      "step": 4579
    },
    {
      "epoch": 9.215291750503019,
      "grad_norm": 0.3537628948688507,
      "learning_rate": 0.00018158768487775432,
      "loss": 0.5379,
      "step": 4580
    },
    {
      "epoch": 9.217303822937625,
      "grad_norm": 0.42736345529556274,
      "learning_rate": 0.0001815836603280008,
      "loss": 0.5841,
      "step": 4581
    },
    {
      "epoch": 9.219315895372233,
      "grad_norm": 0.3386053144931793,
      "learning_rate": 0.00018157963577824731,
      "loss": 0.5234,
      "step": 4582
    },
    {
      "epoch": 9.221327967806841,
      "grad_norm": 0.3387525677680969,
      "learning_rate": 0.00018157561122849383,
      "loss": 0.5181,
      "step": 4583
    },
    {
      "epoch": 9.223340040241448,
      "grad_norm": 0.3668486177921295,
      "learning_rate": 0.0001815715866787403,
      "loss": 0.5638,
      "step": 4584
    },
    {
      "epoch": 9.225352112676056,
      "grad_norm": 0.37705087661743164,
      "learning_rate": 0.00018156756212898682,
      "loss": 0.5333,
      "step": 4585
    },
    {
      "epoch": 9.227364185110664,
      "grad_norm": 0.35320255160331726,
      "learning_rate": 0.00018156353757923334,
      "loss": 0.5026,
      "step": 4586
    },
    {
      "epoch": 9.229376257545272,
      "grad_norm": 0.37081894278526306,
      "learning_rate": 0.00018155951302947985,
      "loss": 0.5729,
      "step": 4587
    },
    {
      "epoch": 9.231388329979879,
      "grad_norm": 0.34325864911079407,
      "learning_rate": 0.00018155548847972633,
      "loss": 0.5355,
      "step": 4588
    },
    {
      "epoch": 9.233400402414487,
      "grad_norm": 0.3538573384284973,
      "learning_rate": 0.00018155146392997284,
      "loss": 0.5533,
      "step": 4589
    },
    {
      "epoch": 9.235412474849095,
      "grad_norm": 0.35203203558921814,
      "learning_rate": 0.00018154743938021933,
      "loss": 0.577,
      "step": 4590
    },
    {
      "epoch": 9.237424547283702,
      "grad_norm": 0.3552497923374176,
      "learning_rate": 0.00018154341483046584,
      "loss": 0.5644,
      "step": 4591
    },
    {
      "epoch": 9.23943661971831,
      "grad_norm": 0.35450297594070435,
      "learning_rate": 0.00018153939028071235,
      "loss": 0.5949,
      "step": 4592
    },
    {
      "epoch": 9.241448692152918,
      "grad_norm": 0.3845457434654236,
      "learning_rate": 0.00018153536573095887,
      "loss": 0.5163,
      "step": 4593
    },
    {
      "epoch": 9.243460764587525,
      "grad_norm": 0.3690043091773987,
      "learning_rate": 0.00018153134118120535,
      "loss": 0.5944,
      "step": 4594
    },
    {
      "epoch": 9.245472837022133,
      "grad_norm": 0.3511416018009186,
      "learning_rate": 0.00018152731663145186,
      "loss": 0.5659,
      "step": 4595
    },
    {
      "epoch": 9.247484909456741,
      "grad_norm": 0.3442552387714386,
      "learning_rate": 0.00018152329208169835,
      "loss": 0.5344,
      "step": 4596
    },
    {
      "epoch": 9.249496981891348,
      "grad_norm": 0.33608758449554443,
      "learning_rate": 0.0001815192675319449,
      "loss": 0.5251,
      "step": 4597
    },
    {
      "epoch": 9.251509054325956,
      "grad_norm": 0.3516383171081543,
      "learning_rate": 0.00018151524298219137,
      "loss": 0.5488,
      "step": 4598
    },
    {
      "epoch": 9.253521126760564,
      "grad_norm": 0.344001829624176,
      "learning_rate": 0.00018151121843243789,
      "loss": 0.5328,
      "step": 4599
    },
    {
      "epoch": 9.25553319919517,
      "grad_norm": 0.3594928979873657,
      "learning_rate": 0.00018150719388268437,
      "loss": 0.5423,
      "step": 4600
    },
    {
      "epoch": 9.257545271629779,
      "grad_norm": 0.3446718156337738,
      "learning_rate": 0.00018150316933293088,
      "loss": 0.5746,
      "step": 4601
    },
    {
      "epoch": 9.259557344064387,
      "grad_norm": 0.3518296480178833,
      "learning_rate": 0.0001814991447831774,
      "loss": 0.5597,
      "step": 4602
    },
    {
      "epoch": 9.261569416498993,
      "grad_norm": 0.3422892391681671,
      "learning_rate": 0.0001814951202334239,
      "loss": 0.5587,
      "step": 4603
    },
    {
      "epoch": 9.263581488933601,
      "grad_norm": 0.3457012176513672,
      "learning_rate": 0.0001814910956836704,
      "loss": 0.5617,
      "step": 4604
    },
    {
      "epoch": 9.26559356136821,
      "grad_norm": 0.3435511589050293,
      "learning_rate": 0.0001814870711339169,
      "loss": 0.532,
      "step": 4605
    },
    {
      "epoch": 9.267605633802816,
      "grad_norm": 0.3481968939304352,
      "learning_rate": 0.0001814830465841634,
      "loss": 0.5814,
      "step": 4606
    },
    {
      "epoch": 9.269617706237424,
      "grad_norm": 0.36751753091812134,
      "learning_rate": 0.00018147902203440993,
      "loss": 0.5731,
      "step": 4607
    },
    {
      "epoch": 9.271629778672033,
      "grad_norm": 0.3699830174446106,
      "learning_rate": 0.00018147499748465641,
      "loss": 0.5535,
      "step": 4608
    },
    {
      "epoch": 9.273641851106639,
      "grad_norm": 0.3351934850215912,
      "learning_rate": 0.00018147097293490293,
      "loss": 0.5334,
      "step": 4609
    },
    {
      "epoch": 9.275653923541247,
      "grad_norm": 0.32686567306518555,
      "learning_rate": 0.0001814669483851494,
      "loss": 0.5153,
      "step": 4610
    },
    {
      "epoch": 9.277665995975855,
      "grad_norm": 0.3432716131210327,
      "learning_rate": 0.00018146292383539592,
      "loss": 0.5192,
      "step": 4611
    },
    {
      "epoch": 9.279678068410464,
      "grad_norm": 0.38544681668281555,
      "learning_rate": 0.00018145889928564244,
      "loss": 0.5268,
      "step": 4612
    },
    {
      "epoch": 9.28169014084507,
      "grad_norm": 0.34957703948020935,
      "learning_rate": 0.00018145487473588895,
      "loss": 0.5253,
      "step": 4613
    },
    {
      "epoch": 9.283702213279678,
      "grad_norm": 0.36845430731773376,
      "learning_rate": 0.00018145085018613543,
      "loss": 0.6034,
      "step": 4614
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 0.3610791265964508,
      "learning_rate": 0.00018144682563638195,
      "loss": 0.5509,
      "step": 4615
    },
    {
      "epoch": 9.287726358148893,
      "grad_norm": 0.3821837902069092,
      "learning_rate": 0.00018144280108662843,
      "loss": 0.5759,
      "step": 4616
    },
    {
      "epoch": 9.289738430583501,
      "grad_norm": 0.3494117259979248,
      "learning_rate": 0.00018143877653687494,
      "loss": 0.5655,
      "step": 4617
    },
    {
      "epoch": 9.29175050301811,
      "grad_norm": 0.34932181239128113,
      "learning_rate": 0.00018143475198712146,
      "loss": 0.5699,
      "step": 4618
    },
    {
      "epoch": 9.293762575452716,
      "grad_norm": 0.34639984369277954,
      "learning_rate": 0.00018143072743736794,
      "loss": 0.5441,
      "step": 4619
    },
    {
      "epoch": 9.295774647887324,
      "grad_norm": 0.36713317036628723,
      "learning_rate": 0.00018142670288761445,
      "loss": 0.6036,
      "step": 4620
    },
    {
      "epoch": 9.297786720321932,
      "grad_norm": 0.3578859269618988,
      "learning_rate": 0.00018142267833786096,
      "loss": 0.5493,
      "step": 4621
    },
    {
      "epoch": 9.299798792756539,
      "grad_norm": 0.35972869396209717,
      "learning_rate": 0.00018141865378810748,
      "loss": 0.601,
      "step": 4622
    },
    {
      "epoch": 9.301810865191147,
      "grad_norm": 0.33342495560646057,
      "learning_rate": 0.00018141462923835396,
      "loss": 0.4969,
      "step": 4623
    },
    {
      "epoch": 9.303822937625755,
      "grad_norm": 0.3586186170578003,
      "learning_rate": 0.00018141060468860047,
      "loss": 0.5485,
      "step": 4624
    },
    {
      "epoch": 9.305835010060362,
      "grad_norm": 0.39203140139579773,
      "learning_rate": 0.00018140658013884696,
      "loss": 0.5606,
      "step": 4625
    },
    {
      "epoch": 9.30784708249497,
      "grad_norm": 0.35129714012145996,
      "learning_rate": 0.00018140255558909347,
      "loss": 0.5486,
      "step": 4626
    },
    {
      "epoch": 9.309859154929578,
      "grad_norm": 0.348409503698349,
      "learning_rate": 0.00018139853103933998,
      "loss": 0.5364,
      "step": 4627
    },
    {
      "epoch": 9.311871227364184,
      "grad_norm": 0.3508118987083435,
      "learning_rate": 0.0001813945064895865,
      "loss": 0.5639,
      "step": 4628
    },
    {
      "epoch": 9.313883299798793,
      "grad_norm": 0.3358716666698456,
      "learning_rate": 0.00018139048193983298,
      "loss": 0.5388,
      "step": 4629
    },
    {
      "epoch": 9.3158953722334,
      "grad_norm": 0.36700892448425293,
      "learning_rate": 0.0001813864573900795,
      "loss": 0.5769,
      "step": 4630
    },
    {
      "epoch": 9.317907444668007,
      "grad_norm": 0.34849315881729126,
      "learning_rate": 0.00018138243284032598,
      "loss": 0.5654,
      "step": 4631
    },
    {
      "epoch": 9.319919517102615,
      "grad_norm": 0.37332335114479065,
      "learning_rate": 0.00018137840829057252,
      "loss": 0.5455,
      "step": 4632
    },
    {
      "epoch": 9.321931589537224,
      "grad_norm": 0.35299205780029297,
      "learning_rate": 0.000181374383740819,
      "loss": 0.5823,
      "step": 4633
    },
    {
      "epoch": 9.323943661971832,
      "grad_norm": 0.36426103115081787,
      "learning_rate": 0.00018137035919106552,
      "loss": 0.5646,
      "step": 4634
    },
    {
      "epoch": 9.325955734406438,
      "grad_norm": 0.34422391653060913,
      "learning_rate": 0.000181366334641312,
      "loss": 0.5553,
      "step": 4635
    },
    {
      "epoch": 9.327967806841047,
      "grad_norm": 0.34616339206695557,
      "learning_rate": 0.0001813623100915585,
      "loss": 0.5442,
      "step": 4636
    },
    {
      "epoch": 9.329979879275655,
      "grad_norm": 0.35266631841659546,
      "learning_rate": 0.00018135828554180502,
      "loss": 0.5478,
      "step": 4637
    },
    {
      "epoch": 9.331991951710261,
      "grad_norm": 0.347365140914917,
      "learning_rate": 0.00018135426099205154,
      "loss": 0.5641,
      "step": 4638
    },
    {
      "epoch": 9.33400402414487,
      "grad_norm": 0.3510005474090576,
      "learning_rate": 0.00018135023644229802,
      "loss": 0.577,
      "step": 4639
    },
    {
      "epoch": 9.336016096579478,
      "grad_norm": 0.33646249771118164,
      "learning_rate": 0.00018134621189254453,
      "loss": 0.5839,
      "step": 4640
    },
    {
      "epoch": 9.338028169014084,
      "grad_norm": 0.35624679923057556,
      "learning_rate": 0.00018134218734279102,
      "loss": 0.5632,
      "step": 4641
    },
    {
      "epoch": 9.340040241448692,
      "grad_norm": 0.33296871185302734,
      "learning_rate": 0.00018133816279303756,
      "loss": 0.53,
      "step": 4642
    },
    {
      "epoch": 9.3420523138833,
      "grad_norm": 0.34760570526123047,
      "learning_rate": 0.00018133413824328404,
      "loss": 0.5653,
      "step": 4643
    },
    {
      "epoch": 9.344064386317907,
      "grad_norm": 0.3577646315097809,
      "learning_rate": 0.00018133011369353056,
      "loss": 0.5694,
      "step": 4644
    },
    {
      "epoch": 9.346076458752515,
      "grad_norm": 0.39347782731056213,
      "learning_rate": 0.00018132608914377704,
      "loss": 0.6174,
      "step": 4645
    },
    {
      "epoch": 9.348088531187123,
      "grad_norm": 0.3525921702384949,
      "learning_rate": 0.00018132206459402355,
      "loss": 0.5194,
      "step": 4646
    },
    {
      "epoch": 9.35010060362173,
      "grad_norm": 0.36297711730003357,
      "learning_rate": 0.00018131804004427007,
      "loss": 0.5192,
      "step": 4647
    },
    {
      "epoch": 9.352112676056338,
      "grad_norm": 0.35689863562583923,
      "learning_rate": 0.00018131401549451658,
      "loss": 0.5425,
      "step": 4648
    },
    {
      "epoch": 9.354124748490946,
      "grad_norm": 0.375782310962677,
      "learning_rate": 0.00018130999094476306,
      "loss": 0.5914,
      "step": 4649
    },
    {
      "epoch": 9.356136820925553,
      "grad_norm": 0.3473043739795685,
      "learning_rate": 0.00018130596639500958,
      "loss": 0.5507,
      "step": 4650
    },
    {
      "epoch": 9.35814889336016,
      "grad_norm": 0.33674052357673645,
      "learning_rate": 0.00018130194184525606,
      "loss": 0.5705,
      "step": 4651
    },
    {
      "epoch": 9.360160965794769,
      "grad_norm": 0.3444623053073883,
      "learning_rate": 0.00018129791729550257,
      "loss": 0.568,
      "step": 4652
    },
    {
      "epoch": 9.362173038229376,
      "grad_norm": 0.3493613004684448,
      "learning_rate": 0.00018129389274574908,
      "loss": 0.5741,
      "step": 4653
    },
    {
      "epoch": 9.364185110663984,
      "grad_norm": 0.37770402431488037,
      "learning_rate": 0.00018128986819599557,
      "loss": 0.5822,
      "step": 4654
    },
    {
      "epoch": 9.366197183098592,
      "grad_norm": 0.3550049066543579,
      "learning_rate": 0.00018128584364624208,
      "loss": 0.5334,
      "step": 4655
    },
    {
      "epoch": 9.368209255533198,
      "grad_norm": 0.367074191570282,
      "learning_rate": 0.0001812818190964886,
      "loss": 0.5907,
      "step": 4656
    },
    {
      "epoch": 9.370221327967807,
      "grad_norm": 0.37075430154800415,
      "learning_rate": 0.0001812777945467351,
      "loss": 0.5428,
      "step": 4657
    },
    {
      "epoch": 9.372233400402415,
      "grad_norm": 0.3784361779689789,
      "learning_rate": 0.0001812737699969816,
      "loss": 0.5129,
      "step": 4658
    },
    {
      "epoch": 9.374245472837021,
      "grad_norm": 0.3678511083126068,
      "learning_rate": 0.0001812697454472281,
      "loss": 0.5355,
      "step": 4659
    },
    {
      "epoch": 9.37625754527163,
      "grad_norm": 0.3674946129322052,
      "learning_rate": 0.0001812657208974746,
      "loss": 0.5543,
      "step": 4660
    },
    {
      "epoch": 9.378269617706238,
      "grad_norm": 0.3451845049858093,
      "learning_rate": 0.0001812616963477211,
      "loss": 0.5136,
      "step": 4661
    },
    {
      "epoch": 9.380281690140846,
      "grad_norm": 0.35530686378479004,
      "learning_rate": 0.0001812576717979676,
      "loss": 0.6005,
      "step": 4662
    },
    {
      "epoch": 9.382293762575452,
      "grad_norm": 0.3513447940349579,
      "learning_rate": 0.00018125364724821413,
      "loss": 0.5827,
      "step": 4663
    },
    {
      "epoch": 9.38430583501006,
      "grad_norm": 0.3481082022190094,
      "learning_rate": 0.0001812496226984606,
      "loss": 0.571,
      "step": 4664
    },
    {
      "epoch": 9.386317907444669,
      "grad_norm": 0.3412069082260132,
      "learning_rate": 0.00018124559814870712,
      "loss": 0.5424,
      "step": 4665
    },
    {
      "epoch": 9.388329979879275,
      "grad_norm": 0.3324672281742096,
      "learning_rate": 0.0001812415735989536,
      "loss": 0.546,
      "step": 4666
    },
    {
      "epoch": 9.390342052313883,
      "grad_norm": 0.33442601561546326,
      "learning_rate": 0.00018123754904920015,
      "loss": 0.5165,
      "step": 4667
    },
    {
      "epoch": 9.392354124748492,
      "grad_norm": 0.3624197542667389,
      "learning_rate": 0.00018123352449944663,
      "loss": 0.5726,
      "step": 4668
    },
    {
      "epoch": 9.394366197183098,
      "grad_norm": 0.37865543365478516,
      "learning_rate": 0.00018122949994969314,
      "loss": 0.5747,
      "step": 4669
    },
    {
      "epoch": 9.396378269617706,
      "grad_norm": 0.3693417012691498,
      "learning_rate": 0.00018122547539993963,
      "loss": 0.5623,
      "step": 4670
    },
    {
      "epoch": 9.398390342052314,
      "grad_norm": 0.34514015913009644,
      "learning_rate": 0.00018122145085018614,
      "loss": 0.5502,
      "step": 4671
    },
    {
      "epoch": 9.400402414486921,
      "grad_norm": 0.3632274866104126,
      "learning_rate": 0.00018121742630043265,
      "loss": 0.5909,
      "step": 4672
    },
    {
      "epoch": 9.40241448692153,
      "grad_norm": 0.364224374294281,
      "learning_rate": 0.00018121340175067917,
      "loss": 0.5474,
      "step": 4673
    },
    {
      "epoch": 9.404426559356137,
      "grad_norm": 0.3535699248313904,
      "learning_rate": 0.00018120937720092565,
      "loss": 0.5243,
      "step": 4674
    },
    {
      "epoch": 9.406438631790744,
      "grad_norm": 0.3704487979412079,
      "learning_rate": 0.00018120535265117216,
      "loss": 0.5504,
      "step": 4675
    },
    {
      "epoch": 9.408450704225352,
      "grad_norm": 0.3558073043823242,
      "learning_rate": 0.00018120132810141865,
      "loss": 0.5319,
      "step": 4676
    },
    {
      "epoch": 9.41046277665996,
      "grad_norm": 0.35321328043937683,
      "learning_rate": 0.0001811973035516652,
      "loss": 0.5684,
      "step": 4677
    },
    {
      "epoch": 9.412474849094567,
      "grad_norm": 0.35779526829719543,
      "learning_rate": 0.00018119327900191167,
      "loss": 0.5293,
      "step": 4678
    },
    {
      "epoch": 9.414486921529175,
      "grad_norm": 0.4068719744682312,
      "learning_rate": 0.00018118925445215819,
      "loss": 0.5822,
      "step": 4679
    },
    {
      "epoch": 9.416498993963783,
      "grad_norm": 0.36450842022895813,
      "learning_rate": 0.00018118522990240467,
      "loss": 0.5726,
      "step": 4680
    },
    {
      "epoch": 9.41851106639839,
      "grad_norm": 0.36242997646331787,
      "learning_rate": 0.00018118120535265118,
      "loss": 0.5768,
      "step": 4681
    },
    {
      "epoch": 9.420523138832998,
      "grad_norm": 0.3500164747238159,
      "learning_rate": 0.0001811771808028977,
      "loss": 0.5755,
      "step": 4682
    },
    {
      "epoch": 9.422535211267606,
      "grad_norm": 0.35873377323150635,
      "learning_rate": 0.0001811731562531442,
      "loss": 0.5625,
      "step": 4683
    },
    {
      "epoch": 9.424547283702214,
      "grad_norm": 0.37601903080940247,
      "learning_rate": 0.0001811691317033907,
      "loss": 0.5776,
      "step": 4684
    },
    {
      "epoch": 9.42655935613682,
      "grad_norm": 0.35454705357551575,
      "learning_rate": 0.0001811651071536372,
      "loss": 0.5738,
      "step": 4685
    },
    {
      "epoch": 9.428571428571429,
      "grad_norm": 0.3355911672115326,
      "learning_rate": 0.0001811610826038837,
      "loss": 0.5793,
      "step": 4686
    },
    {
      "epoch": 9.430583501006037,
      "grad_norm": 0.3637877106666565,
      "learning_rate": 0.0001811570580541302,
      "loss": 0.6101,
      "step": 4687
    },
    {
      "epoch": 9.432595573440643,
      "grad_norm": 0.373715877532959,
      "learning_rate": 0.00018115303350437671,
      "loss": 0.5444,
      "step": 4688
    },
    {
      "epoch": 9.434607645875252,
      "grad_norm": 0.3702619671821594,
      "learning_rate": 0.0001811490089546232,
      "loss": 0.5336,
      "step": 4689
    },
    {
      "epoch": 9.43661971830986,
      "grad_norm": 0.36542606353759766,
      "learning_rate": 0.0001811449844048697,
      "loss": 0.5917,
      "step": 4690
    },
    {
      "epoch": 9.438631790744466,
      "grad_norm": 0.3591802716255188,
      "learning_rate": 0.0001811409598551162,
      "loss": 0.5427,
      "step": 4691
    },
    {
      "epoch": 9.440643863179075,
      "grad_norm": 0.34343743324279785,
      "learning_rate": 0.00018113693530536274,
      "loss": 0.5747,
      "step": 4692
    },
    {
      "epoch": 9.442655935613683,
      "grad_norm": 0.34157174825668335,
      "learning_rate": 0.00018113291075560922,
      "loss": 0.5386,
      "step": 4693
    },
    {
      "epoch": 9.44466800804829,
      "grad_norm": 0.35880181193351746,
      "learning_rate": 0.00018112888620585573,
      "loss": 0.5871,
      "step": 4694
    },
    {
      "epoch": 9.446680080482897,
      "grad_norm": 0.3406245708465576,
      "learning_rate": 0.00018112486165610222,
      "loss": 0.5327,
      "step": 4695
    },
    {
      "epoch": 9.448692152917506,
      "grad_norm": 0.38228219747543335,
      "learning_rate": 0.00018112083710634873,
      "loss": 0.6157,
      "step": 4696
    },
    {
      "epoch": 9.450704225352112,
      "grad_norm": 0.3542819917201996,
      "learning_rate": 0.00018111681255659524,
      "loss": 0.5307,
      "step": 4697
    },
    {
      "epoch": 9.45271629778672,
      "grad_norm": 0.35836905241012573,
      "learning_rate": 0.00018111278800684176,
      "loss": 0.5497,
      "step": 4698
    },
    {
      "epoch": 9.454728370221329,
      "grad_norm": 0.34903863072395325,
      "learning_rate": 0.00018110876345708824,
      "loss": 0.5647,
      "step": 4699
    },
    {
      "epoch": 9.456740442655935,
      "grad_norm": 0.37279075384140015,
      "learning_rate": 0.00018110473890733475,
      "loss": 0.5538,
      "step": 4700
    },
    {
      "epoch": 9.458752515090543,
      "grad_norm": 0.36751291155815125,
      "learning_rate": 0.00018110071435758124,
      "loss": 0.5645,
      "step": 4701
    },
    {
      "epoch": 9.460764587525151,
      "grad_norm": 0.36398446559906006,
      "learning_rate": 0.00018109668980782778,
      "loss": 0.5856,
      "step": 4702
    },
    {
      "epoch": 9.462776659959758,
      "grad_norm": 0.3384600281715393,
      "learning_rate": 0.00018109266525807426,
      "loss": 0.5262,
      "step": 4703
    },
    {
      "epoch": 9.464788732394366,
      "grad_norm": 0.3510783016681671,
      "learning_rate": 0.00018108864070832077,
      "loss": 0.5436,
      "step": 4704
    },
    {
      "epoch": 9.466800804828974,
      "grad_norm": 0.38469165563583374,
      "learning_rate": 0.00018108461615856726,
      "loss": 0.5653,
      "step": 4705
    },
    {
      "epoch": 9.46881287726358,
      "grad_norm": 0.35024696588516235,
      "learning_rate": 0.00018108059160881377,
      "loss": 0.5577,
      "step": 4706
    },
    {
      "epoch": 9.470824949698189,
      "grad_norm": 0.3761312663555145,
      "learning_rate": 0.00018107656705906028,
      "loss": 0.6231,
      "step": 4707
    },
    {
      "epoch": 9.472837022132797,
      "grad_norm": 0.3391667306423187,
      "learning_rate": 0.0001810725425093068,
      "loss": 0.5022,
      "step": 4708
    },
    {
      "epoch": 9.474849094567404,
      "grad_norm": 0.3559359908103943,
      "learning_rate": 0.00018106851795955328,
      "loss": 0.5437,
      "step": 4709
    },
    {
      "epoch": 9.476861167002012,
      "grad_norm": 0.358104944229126,
      "learning_rate": 0.0001810644934097998,
      "loss": 0.5298,
      "step": 4710
    },
    {
      "epoch": 9.47887323943662,
      "grad_norm": 0.36314812302589417,
      "learning_rate": 0.00018106046886004628,
      "loss": 0.5419,
      "step": 4711
    },
    {
      "epoch": 9.480885311871228,
      "grad_norm": 0.37563076615333557,
      "learning_rate": 0.00018105644431029282,
      "loss": 0.545,
      "step": 4712
    },
    {
      "epoch": 9.482897384305835,
      "grad_norm": 0.34875181317329407,
      "learning_rate": 0.0001810524197605393,
      "loss": 0.5716,
      "step": 4713
    },
    {
      "epoch": 9.484909456740443,
      "grad_norm": 0.3499366044998169,
      "learning_rate": 0.00018104839521078582,
      "loss": 0.535,
      "step": 4714
    },
    {
      "epoch": 9.486921529175051,
      "grad_norm": 0.3447771966457367,
      "learning_rate": 0.0001810443706610323,
      "loss": 0.537,
      "step": 4715
    },
    {
      "epoch": 9.488933601609657,
      "grad_norm": 0.37700358033180237,
      "learning_rate": 0.0001810403461112788,
      "loss": 0.5974,
      "step": 4716
    },
    {
      "epoch": 9.490945674044266,
      "grad_norm": 0.3470170497894287,
      "learning_rate": 0.00018103632156152532,
      "loss": 0.5719,
      "step": 4717
    },
    {
      "epoch": 9.492957746478874,
      "grad_norm": 0.3709997534751892,
      "learning_rate": 0.0001810322970117718,
      "loss": 0.5536,
      "step": 4718
    },
    {
      "epoch": 9.49496981891348,
      "grad_norm": 0.3847135603427887,
      "learning_rate": 0.00018102827246201832,
      "loss": 0.5591,
      "step": 4719
    },
    {
      "epoch": 9.496981891348089,
      "grad_norm": 0.3749031722545624,
      "learning_rate": 0.00018102424791226483,
      "loss": 0.5608,
      "step": 4720
    },
    {
      "epoch": 9.498993963782697,
      "grad_norm": 0.372762531042099,
      "learning_rate": 0.00018102022336251132,
      "loss": 0.5651,
      "step": 4721
    },
    {
      "epoch": 9.501006036217303,
      "grad_norm": 0.36890262365341187,
      "learning_rate": 0.00018101619881275783,
      "loss": 0.5754,
      "step": 4722
    },
    {
      "epoch": 9.503018108651911,
      "grad_norm": 0.3466223478317261,
      "learning_rate": 0.00018101217426300434,
      "loss": 0.5424,
      "step": 4723
    },
    {
      "epoch": 9.50503018108652,
      "grad_norm": 0.37583306431770325,
      "learning_rate": 0.00018100814971325083,
      "loss": 0.5729,
      "step": 4724
    },
    {
      "epoch": 9.507042253521126,
      "grad_norm": 0.3479255437850952,
      "learning_rate": 0.00018100412516349734,
      "loss": 0.5792,
      "step": 4725
    },
    {
      "epoch": 9.509054325955734,
      "grad_norm": 0.33656179904937744,
      "learning_rate": 0.00018100010061374383,
      "loss": 0.5345,
      "step": 4726
    },
    {
      "epoch": 9.511066398390343,
      "grad_norm": 0.35045841336250305,
      "learning_rate": 0.00018099607606399037,
      "loss": 0.5507,
      "step": 4727
    },
    {
      "epoch": 9.513078470824949,
      "grad_norm": 0.3325101435184479,
      "learning_rate": 0.00018099205151423685,
      "loss": 0.5563,
      "step": 4728
    },
    {
      "epoch": 9.515090543259557,
      "grad_norm": 0.3784441649913788,
      "learning_rate": 0.00018098802696448336,
      "loss": 0.5686,
      "step": 4729
    },
    {
      "epoch": 9.517102615694165,
      "grad_norm": 0.3644827902317047,
      "learning_rate": 0.00018098400241472985,
      "loss": 0.5718,
      "step": 4730
    },
    {
      "epoch": 9.519114688128772,
      "grad_norm": 0.3724856674671173,
      "learning_rate": 0.00018097997786497636,
      "loss": 0.5788,
      "step": 4731
    },
    {
      "epoch": 9.52112676056338,
      "grad_norm": 0.3532944619655609,
      "learning_rate": 0.00018097595331522287,
      "loss": 0.5824,
      "step": 4732
    },
    {
      "epoch": 9.523138832997988,
      "grad_norm": 0.3440187871456146,
      "learning_rate": 0.00018097192876546938,
      "loss": 0.5675,
      "step": 4733
    },
    {
      "epoch": 9.525150905432596,
      "grad_norm": 0.3500426411628723,
      "learning_rate": 0.00018096790421571587,
      "loss": 0.5761,
      "step": 4734
    },
    {
      "epoch": 9.527162977867203,
      "grad_norm": 0.3594013750553131,
      "learning_rate": 0.00018096387966596238,
      "loss": 0.5644,
      "step": 4735
    },
    {
      "epoch": 9.529175050301811,
      "grad_norm": 0.38887184858322144,
      "learning_rate": 0.00018095985511620887,
      "loss": 0.5765,
      "step": 4736
    },
    {
      "epoch": 9.53118712273642,
      "grad_norm": 0.3654461205005646,
      "learning_rate": 0.00018095583056645538,
      "loss": 0.61,
      "step": 4737
    },
    {
      "epoch": 9.533199195171026,
      "grad_norm": 0.3690873086452484,
      "learning_rate": 0.0001809518060167019,
      "loss": 0.5484,
      "step": 4738
    },
    {
      "epoch": 9.535211267605634,
      "grad_norm": 0.38298991322517395,
      "learning_rate": 0.0001809477814669484,
      "loss": 0.576,
      "step": 4739
    },
    {
      "epoch": 9.537223340040242,
      "grad_norm": 0.3574650287628174,
      "learning_rate": 0.0001809437569171949,
      "loss": 0.5834,
      "step": 4740
    },
    {
      "epoch": 9.539235412474849,
      "grad_norm": 0.32533836364746094,
      "learning_rate": 0.0001809397323674414,
      "loss": 0.5244,
      "step": 4741
    },
    {
      "epoch": 9.541247484909457,
      "grad_norm": 0.3333783745765686,
      "learning_rate": 0.00018093570781768789,
      "loss": 0.5261,
      "step": 4742
    },
    {
      "epoch": 9.543259557344065,
      "grad_norm": 0.3606277406215668,
      "learning_rate": 0.00018093168326793443,
      "loss": 0.5404,
      "step": 4743
    },
    {
      "epoch": 9.545271629778671,
      "grad_norm": 0.3496987819671631,
      "learning_rate": 0.0001809276587181809,
      "loss": 0.5547,
      "step": 4744
    },
    {
      "epoch": 9.54728370221328,
      "grad_norm": 0.37410828471183777,
      "learning_rate": 0.00018092363416842742,
      "loss": 0.6168,
      "step": 4745
    },
    {
      "epoch": 9.549295774647888,
      "grad_norm": 0.3466324210166931,
      "learning_rate": 0.0001809196096186739,
      "loss": 0.5492,
      "step": 4746
    },
    {
      "epoch": 9.551307847082494,
      "grad_norm": 0.3435801863670349,
      "learning_rate": 0.00018091558506892042,
      "loss": 0.547,
      "step": 4747
    },
    {
      "epoch": 9.553319919517103,
      "grad_norm": 0.3501264452934265,
      "learning_rate": 0.00018091156051916693,
      "loss": 0.5791,
      "step": 4748
    },
    {
      "epoch": 9.55533199195171,
      "grad_norm": 0.358267605304718,
      "learning_rate": 0.00018090753596941344,
      "loss": 0.5712,
      "step": 4749
    },
    {
      "epoch": 9.557344064386317,
      "grad_norm": 0.339009165763855,
      "learning_rate": 0.00018090351141965993,
      "loss": 0.5388,
      "step": 4750
    },
    {
      "epoch": 9.559356136820925,
      "grad_norm": 0.3307923376560211,
      "learning_rate": 0.00018089948686990644,
      "loss": 0.5742,
      "step": 4751
    },
    {
      "epoch": 9.561368209255534,
      "grad_norm": 0.34680014848709106,
      "learning_rate": 0.00018089546232015293,
      "loss": 0.5466,
      "step": 4752
    },
    {
      "epoch": 9.56338028169014,
      "grad_norm": 0.35449737310409546,
      "learning_rate": 0.00018089143777039944,
      "loss": 0.5672,
      "step": 4753
    },
    {
      "epoch": 9.565392354124748,
      "grad_norm": 0.37688764929771423,
      "learning_rate": 0.00018088741322064595,
      "loss": 0.5578,
      "step": 4754
    },
    {
      "epoch": 9.567404426559357,
      "grad_norm": 0.3583296835422516,
      "learning_rate": 0.00018088338867089246,
      "loss": 0.5248,
      "step": 4755
    },
    {
      "epoch": 9.569416498993963,
      "grad_norm": 0.3758125603199005,
      "learning_rate": 0.00018087936412113895,
      "loss": 0.5693,
      "step": 4756
    },
    {
      "epoch": 9.571428571428571,
      "grad_norm": 0.3534766137599945,
      "learning_rate": 0.00018087533957138546,
      "loss": 0.5526,
      "step": 4757
    },
    {
      "epoch": 9.57344064386318,
      "grad_norm": 0.3681204617023468,
      "learning_rate": 0.00018087131502163197,
      "loss": 0.6041,
      "step": 4758
    },
    {
      "epoch": 9.575452716297786,
      "grad_norm": 0.3445116877555847,
      "learning_rate": 0.00018086729047187846,
      "loss": 0.5469,
      "step": 4759
    },
    {
      "epoch": 9.577464788732394,
      "grad_norm": 0.3620321750640869,
      "learning_rate": 0.00018086326592212497,
      "loss": 0.5306,
      "step": 4760
    },
    {
      "epoch": 9.579476861167002,
      "grad_norm": 0.3409840762615204,
      "learning_rate": 0.00018085924137237146,
      "loss": 0.599,
      "step": 4761
    },
    {
      "epoch": 9.58148893360161,
      "grad_norm": 0.33815956115722656,
      "learning_rate": 0.00018085521682261797,
      "loss": 0.5931,
      "step": 4762
    },
    {
      "epoch": 9.583501006036217,
      "grad_norm": 0.34919196367263794,
      "learning_rate": 0.00018085119227286448,
      "loss": 0.5337,
      "step": 4763
    },
    {
      "epoch": 9.585513078470825,
      "grad_norm": 0.34015098214149475,
      "learning_rate": 0.000180847167723111,
      "loss": 0.5303,
      "step": 4764
    },
    {
      "epoch": 9.587525150905433,
      "grad_norm": 0.36127516627311707,
      "learning_rate": 0.00018084314317335748,
      "loss": 0.5688,
      "step": 4765
    },
    {
      "epoch": 9.58953722334004,
      "grad_norm": 0.3566461205482483,
      "learning_rate": 0.000180839118623604,
      "loss": 0.568,
      "step": 4766
    },
    {
      "epoch": 9.591549295774648,
      "grad_norm": 0.3645856976509094,
      "learning_rate": 0.00018083509407385047,
      "loss": 0.5735,
      "step": 4767
    },
    {
      "epoch": 9.593561368209256,
      "grad_norm": 0.3585897386074066,
      "learning_rate": 0.00018083106952409701,
      "loss": 0.5983,
      "step": 4768
    },
    {
      "epoch": 9.595573440643863,
      "grad_norm": 0.3508857488632202,
      "learning_rate": 0.0001808270449743435,
      "loss": 0.5903,
      "step": 4769
    },
    {
      "epoch": 9.59758551307847,
      "grad_norm": 0.35977092385292053,
      "learning_rate": 0.00018082302042459,
      "loss": 0.5667,
      "step": 4770
    },
    {
      "epoch": 9.599597585513079,
      "grad_norm": 0.36928224563598633,
      "learning_rate": 0.0001808189958748365,
      "loss": 0.5918,
      "step": 4771
    },
    {
      "epoch": 9.601609657947686,
      "grad_norm": 0.35108691453933716,
      "learning_rate": 0.000180814971325083,
      "loss": 0.5667,
      "step": 4772
    },
    {
      "epoch": 9.603621730382294,
      "grad_norm": 0.3591991662979126,
      "learning_rate": 0.00018081094677532952,
      "loss": 0.5944,
      "step": 4773
    },
    {
      "epoch": 9.605633802816902,
      "grad_norm": 0.35334160923957825,
      "learning_rate": 0.00018080692222557603,
      "loss": 0.6198,
      "step": 4774
    },
    {
      "epoch": 9.607645875251508,
      "grad_norm": 0.332337886095047,
      "learning_rate": 0.00018080289767582252,
      "loss": 0.52,
      "step": 4775
    },
    {
      "epoch": 9.609657947686117,
      "grad_norm": 0.35127508640289307,
      "learning_rate": 0.00018079887312606903,
      "loss": 0.5283,
      "step": 4776
    },
    {
      "epoch": 9.611670020120725,
      "grad_norm": 0.3603105843067169,
      "learning_rate": 0.00018079484857631552,
      "loss": 0.5666,
      "step": 4777
    },
    {
      "epoch": 9.613682092555331,
      "grad_norm": 0.3572683036327362,
      "learning_rate": 0.00018079082402656205,
      "loss": 0.6041,
      "step": 4778
    },
    {
      "epoch": 9.61569416498994,
      "grad_norm": 0.34450238943099976,
      "learning_rate": 0.00018078679947680854,
      "loss": 0.5591,
      "step": 4779
    },
    {
      "epoch": 9.617706237424548,
      "grad_norm": 0.37091967463493347,
      "learning_rate": 0.00018078277492705505,
      "loss": 0.6194,
      "step": 4780
    },
    {
      "epoch": 9.619718309859154,
      "grad_norm": 0.3507932722568512,
      "learning_rate": 0.00018077875037730154,
      "loss": 0.5872,
      "step": 4781
    },
    {
      "epoch": 9.621730382293762,
      "grad_norm": 0.34474414587020874,
      "learning_rate": 0.00018077472582754805,
      "loss": 0.5937,
      "step": 4782
    },
    {
      "epoch": 9.62374245472837,
      "grad_norm": 0.3372708559036255,
      "learning_rate": 0.00018077070127779456,
      "loss": 0.5929,
      "step": 4783
    },
    {
      "epoch": 9.625754527162979,
      "grad_norm": 0.3565520942211151,
      "learning_rate": 0.00018076667672804107,
      "loss": 0.5496,
      "step": 4784
    },
    {
      "epoch": 9.627766599597585,
      "grad_norm": 0.35153549909591675,
      "learning_rate": 0.00018076265217828756,
      "loss": 0.5619,
      "step": 4785
    },
    {
      "epoch": 9.629778672032193,
      "grad_norm": 0.3560488522052765,
      "learning_rate": 0.00018075862762853407,
      "loss": 0.5424,
      "step": 4786
    },
    {
      "epoch": 9.631790744466802,
      "grad_norm": 0.35441505908966064,
      "learning_rate": 0.00018075460307878056,
      "loss": 0.5849,
      "step": 4787
    },
    {
      "epoch": 9.633802816901408,
      "grad_norm": 0.36146774888038635,
      "learning_rate": 0.00018075057852902707,
      "loss": 0.5618,
      "step": 4788
    },
    {
      "epoch": 9.635814889336016,
      "grad_norm": 0.33723706007003784,
      "learning_rate": 0.00018074655397927358,
      "loss": 0.5325,
      "step": 4789
    },
    {
      "epoch": 9.637826961770624,
      "grad_norm": 0.34610116481781006,
      "learning_rate": 0.0001807425294295201,
      "loss": 0.575,
      "step": 4790
    },
    {
      "epoch": 9.639839034205231,
      "grad_norm": 0.3438272476196289,
      "learning_rate": 0.00018073850487976658,
      "loss": 0.516,
      "step": 4791
    },
    {
      "epoch": 9.64185110663984,
      "grad_norm": 0.37304580211639404,
      "learning_rate": 0.0001807344803300131,
      "loss": 0.5989,
      "step": 4792
    },
    {
      "epoch": 9.643863179074447,
      "grad_norm": 0.3578254282474518,
      "learning_rate": 0.0001807304557802596,
      "loss": 0.5928,
      "step": 4793
    },
    {
      "epoch": 9.645875251509054,
      "grad_norm": 0.33150967955589294,
      "learning_rate": 0.0001807264312305061,
      "loss": 0.5713,
      "step": 4794
    },
    {
      "epoch": 9.647887323943662,
      "grad_norm": 0.36115917563438416,
      "learning_rate": 0.0001807224066807526,
      "loss": 0.5583,
      "step": 4795
    },
    {
      "epoch": 9.64989939637827,
      "grad_norm": 0.3562026023864746,
      "learning_rate": 0.00018071838213099908,
      "loss": 0.5593,
      "step": 4796
    },
    {
      "epoch": 9.651911468812877,
      "grad_norm": 0.3500782251358032,
      "learning_rate": 0.0001807143575812456,
      "loss": 0.5685,
      "step": 4797
    },
    {
      "epoch": 9.653923541247485,
      "grad_norm": 0.34731876850128174,
      "learning_rate": 0.0001807103330314921,
      "loss": 0.5648,
      "step": 4798
    },
    {
      "epoch": 9.655935613682093,
      "grad_norm": 0.37187469005584717,
      "learning_rate": 0.00018070630848173862,
      "loss": 0.5886,
      "step": 4799
    },
    {
      "epoch": 9.6579476861167,
      "grad_norm": 0.3416086733341217,
      "learning_rate": 0.0001807022839319851,
      "loss": 0.5622,
      "step": 4800
    },
    {
      "epoch": 9.659959758551308,
      "grad_norm": 0.3685278296470642,
      "learning_rate": 0.00018069825938223162,
      "loss": 0.5603,
      "step": 4801
    },
    {
      "epoch": 9.661971830985916,
      "grad_norm": 0.37467268109321594,
      "learning_rate": 0.0001806942348324781,
      "loss": 0.5894,
      "step": 4802
    },
    {
      "epoch": 9.663983903420522,
      "grad_norm": 0.3555832505226135,
      "learning_rate": 0.00018069021028272464,
      "loss": 0.5109,
      "step": 4803
    },
    {
      "epoch": 9.66599597585513,
      "grad_norm": 0.3587283790111542,
      "learning_rate": 0.00018068618573297113,
      "loss": 0.5953,
      "step": 4804
    },
    {
      "epoch": 9.668008048289739,
      "grad_norm": 0.3599751591682434,
      "learning_rate": 0.00018068216118321764,
      "loss": 0.5636,
      "step": 4805
    },
    {
      "epoch": 9.670020120724345,
      "grad_norm": 0.3597901463508606,
      "learning_rate": 0.00018067813663346413,
      "loss": 0.5475,
      "step": 4806
    },
    {
      "epoch": 9.672032193158953,
      "grad_norm": 0.3422403037548065,
      "learning_rate": 0.00018067411208371064,
      "loss": 0.5783,
      "step": 4807
    },
    {
      "epoch": 9.674044265593562,
      "grad_norm": 0.35265031456947327,
      "learning_rate": 0.00018067008753395715,
      "loss": 0.578,
      "step": 4808
    },
    {
      "epoch": 9.676056338028168,
      "grad_norm": 0.3476364314556122,
      "learning_rate": 0.00018066606298420366,
      "loss": 0.5682,
      "step": 4809
    },
    {
      "epoch": 9.678068410462776,
      "grad_norm": 0.3520721197128296,
      "learning_rate": 0.00018066203843445015,
      "loss": 0.5489,
      "step": 4810
    },
    {
      "epoch": 9.680080482897385,
      "grad_norm": 0.36237090826034546,
      "learning_rate": 0.00018065801388469666,
      "loss": 0.5871,
      "step": 4811
    },
    {
      "epoch": 9.682092555331993,
      "grad_norm": 0.3534314036369324,
      "learning_rate": 0.00018065398933494314,
      "loss": 0.5566,
      "step": 4812
    },
    {
      "epoch": 9.6841046277666,
      "grad_norm": 0.3498741090297699,
      "learning_rate": 0.00018064996478518968,
      "loss": 0.4881,
      "step": 4813
    },
    {
      "epoch": 9.686116700201207,
      "grad_norm": 0.34242063760757446,
      "learning_rate": 0.00018064594023543617,
      "loss": 0.5672,
      "step": 4814
    },
    {
      "epoch": 9.688128772635816,
      "grad_norm": 0.358982652425766,
      "learning_rate": 0.00018064191568568268,
      "loss": 0.5654,
      "step": 4815
    },
    {
      "epoch": 9.690140845070422,
      "grad_norm": 0.369579941034317,
      "learning_rate": 0.00018063789113592917,
      "loss": 0.5544,
      "step": 4816
    },
    {
      "epoch": 9.69215291750503,
      "grad_norm": 0.3671717643737793,
      "learning_rate": 0.00018063386658617568,
      "loss": 0.5633,
      "step": 4817
    },
    {
      "epoch": 9.694164989939638,
      "grad_norm": 0.350315123796463,
      "learning_rate": 0.0001806298420364222,
      "loss": 0.5623,
      "step": 4818
    },
    {
      "epoch": 9.696177062374245,
      "grad_norm": 0.36234214901924133,
      "learning_rate": 0.0001806258174866687,
      "loss": 0.5706,
      "step": 4819
    },
    {
      "epoch": 9.698189134808853,
      "grad_norm": 0.34343090653419495,
      "learning_rate": 0.0001806217929369152,
      "loss": 0.5791,
      "step": 4820
    },
    {
      "epoch": 9.700201207243461,
      "grad_norm": 0.3484065532684326,
      "learning_rate": 0.0001806177683871617,
      "loss": 0.5948,
      "step": 4821
    },
    {
      "epoch": 9.702213279678068,
      "grad_norm": 0.3444001376628876,
      "learning_rate": 0.00018061374383740819,
      "loss": 0.5519,
      "step": 4822
    },
    {
      "epoch": 9.704225352112676,
      "grad_norm": 0.35017135739326477,
      "learning_rate": 0.0001806097192876547,
      "loss": 0.5604,
      "step": 4823
    },
    {
      "epoch": 9.706237424547284,
      "grad_norm": 0.34673237800598145,
      "learning_rate": 0.0001806056947379012,
      "loss": 0.5226,
      "step": 4824
    },
    {
      "epoch": 9.70824949698189,
      "grad_norm": 0.34527674317359924,
      "learning_rate": 0.00018060167018814772,
      "loss": 0.5636,
      "step": 4825
    },
    {
      "epoch": 9.710261569416499,
      "grad_norm": 0.39383578300476074,
      "learning_rate": 0.0001805976456383942,
      "loss": 0.5847,
      "step": 4826
    },
    {
      "epoch": 9.712273641851107,
      "grad_norm": 0.341341495513916,
      "learning_rate": 0.00018059362108864072,
      "loss": 0.5351,
      "step": 4827
    },
    {
      "epoch": 9.714285714285714,
      "grad_norm": 0.37440934777259827,
      "learning_rate": 0.00018058959653888723,
      "loss": 0.5763,
      "step": 4828
    },
    {
      "epoch": 9.716297786720322,
      "grad_norm": 0.35596150159835815,
      "learning_rate": 0.00018058557198913372,
      "loss": 0.5926,
      "step": 4829
    },
    {
      "epoch": 9.71830985915493,
      "grad_norm": 0.3652293086051941,
      "learning_rate": 0.00018058154743938023,
      "loss": 0.6024,
      "step": 4830
    },
    {
      "epoch": 9.720321931589538,
      "grad_norm": 0.35572221875190735,
      "learning_rate": 0.00018057752288962671,
      "loss": 0.5802,
      "step": 4831
    },
    {
      "epoch": 9.722334004024145,
      "grad_norm": 0.3679911494255066,
      "learning_rate": 0.00018057349833987323,
      "loss": 0.5623,
      "step": 4832
    },
    {
      "epoch": 9.724346076458753,
      "grad_norm": 0.33843207359313965,
      "learning_rate": 0.00018056947379011974,
      "loss": 0.5916,
      "step": 4833
    },
    {
      "epoch": 9.726358148893361,
      "grad_norm": 0.3668239116668701,
      "learning_rate": 0.00018056544924036625,
      "loss": 0.5693,
      "step": 4834
    },
    {
      "epoch": 9.728370221327967,
      "grad_norm": 0.3422381281852722,
      "learning_rate": 0.00018056142469061274,
      "loss": 0.5619,
      "step": 4835
    },
    {
      "epoch": 9.730382293762576,
      "grad_norm": 0.398911714553833,
      "learning_rate": 0.00018055740014085925,
      "loss": 0.5885,
      "step": 4836
    },
    {
      "epoch": 9.732394366197184,
      "grad_norm": 0.35605984926223755,
      "learning_rate": 0.00018055337559110573,
      "loss": 0.5703,
      "step": 4837
    },
    {
      "epoch": 9.73440643863179,
      "grad_norm": 0.3388073146343231,
      "learning_rate": 0.00018054935104135227,
      "loss": 0.5567,
      "step": 4838
    },
    {
      "epoch": 9.736418511066399,
      "grad_norm": 0.3607851564884186,
      "learning_rate": 0.00018054532649159876,
      "loss": 0.5736,
      "step": 4839
    },
    {
      "epoch": 9.738430583501007,
      "grad_norm": 0.3240211009979248,
      "learning_rate": 0.00018054130194184527,
      "loss": 0.5512,
      "step": 4840
    },
    {
      "epoch": 9.740442655935613,
      "grad_norm": 0.33364900946617126,
      "learning_rate": 0.00018053727739209176,
      "loss": 0.5169,
      "step": 4841
    },
    {
      "epoch": 9.742454728370221,
      "grad_norm": 0.34997761249542236,
      "learning_rate": 0.00018053325284233827,
      "loss": 0.5897,
      "step": 4842
    },
    {
      "epoch": 9.74446680080483,
      "grad_norm": 0.339263916015625,
      "learning_rate": 0.00018052922829258478,
      "loss": 0.5635,
      "step": 4843
    },
    {
      "epoch": 9.746478873239436,
      "grad_norm": 0.3571915328502655,
      "learning_rate": 0.0001805252037428313,
      "loss": 0.5812,
      "step": 4844
    },
    {
      "epoch": 9.748490945674044,
      "grad_norm": 0.3656499981880188,
      "learning_rate": 0.00018052117919307778,
      "loss": 0.6022,
      "step": 4845
    },
    {
      "epoch": 9.750503018108652,
      "grad_norm": 0.3569958209991455,
      "learning_rate": 0.0001805171546433243,
      "loss": 0.5697,
      "step": 4846
    },
    {
      "epoch": 9.752515090543259,
      "grad_norm": 0.36888110637664795,
      "learning_rate": 0.00018051313009357077,
      "loss": 0.5431,
      "step": 4847
    },
    {
      "epoch": 9.754527162977867,
      "grad_norm": 0.36755335330963135,
      "learning_rate": 0.00018050910554381731,
      "loss": 0.5708,
      "step": 4848
    },
    {
      "epoch": 9.756539235412475,
      "grad_norm": 0.335574746131897,
      "learning_rate": 0.0001805050809940638,
      "loss": 0.5437,
      "step": 4849
    },
    {
      "epoch": 9.758551307847082,
      "grad_norm": 0.36838576197624207,
      "learning_rate": 0.0001805010564443103,
      "loss": 0.5742,
      "step": 4850
    },
    {
      "epoch": 9.76056338028169,
      "grad_norm": 0.3560454845428467,
      "learning_rate": 0.0001804970318945568,
      "loss": 0.5714,
      "step": 4851
    },
    {
      "epoch": 9.762575452716298,
      "grad_norm": 0.34757840633392334,
      "learning_rate": 0.0001804930073448033,
      "loss": 0.5122,
      "step": 4852
    },
    {
      "epoch": 9.764587525150905,
      "grad_norm": 0.35308995842933655,
      "learning_rate": 0.00018048898279504982,
      "loss": 0.586,
      "step": 4853
    },
    {
      "epoch": 9.766599597585513,
      "grad_norm": 0.34571826457977295,
      "learning_rate": 0.00018048495824529633,
      "loss": 0.6143,
      "step": 4854
    },
    {
      "epoch": 9.768611670020121,
      "grad_norm": 0.35241222381591797,
      "learning_rate": 0.00018048093369554282,
      "loss": 0.5484,
      "step": 4855
    },
    {
      "epoch": 9.770623742454728,
      "grad_norm": 0.35544055700302124,
      "learning_rate": 0.00018047690914578933,
      "loss": 0.5522,
      "step": 4856
    },
    {
      "epoch": 9.772635814889336,
      "grad_norm": 0.37483668327331543,
      "learning_rate": 0.00018047288459603582,
      "loss": 0.5801,
      "step": 4857
    },
    {
      "epoch": 9.774647887323944,
      "grad_norm": 0.3432201147079468,
      "learning_rate": 0.00018046886004628233,
      "loss": 0.5675,
      "step": 4858
    },
    {
      "epoch": 9.77665995975855,
      "grad_norm": 0.33478307723999023,
      "learning_rate": 0.00018046483549652884,
      "loss": 0.5791,
      "step": 4859
    },
    {
      "epoch": 9.778672032193159,
      "grad_norm": 0.36124253273010254,
      "learning_rate": 0.00018046081094677532,
      "loss": 0.5666,
      "step": 4860
    },
    {
      "epoch": 9.780684104627767,
      "grad_norm": 0.3884226381778717,
      "learning_rate": 0.00018045678639702184,
      "loss": 0.5683,
      "step": 4861
    },
    {
      "epoch": 9.782696177062375,
      "grad_norm": 0.3494463264942169,
      "learning_rate": 0.00018045276184726835,
      "loss": 0.5516,
      "step": 4862
    },
    {
      "epoch": 9.784708249496981,
      "grad_norm": 0.3541344702243805,
      "learning_rate": 0.00018044873729751486,
      "loss": 0.5782,
      "step": 4863
    },
    {
      "epoch": 9.78672032193159,
      "grad_norm": 0.3560067415237427,
      "learning_rate": 0.00018044471274776135,
      "loss": 0.5908,
      "step": 4864
    },
    {
      "epoch": 9.788732394366198,
      "grad_norm": 0.33094486594200134,
      "learning_rate": 0.00018044068819800786,
      "loss": 0.5489,
      "step": 4865
    },
    {
      "epoch": 9.790744466800804,
      "grad_norm": 0.3714370131492615,
      "learning_rate": 0.00018043666364825434,
      "loss": 0.5803,
      "step": 4866
    },
    {
      "epoch": 9.792756539235413,
      "grad_norm": 0.33804941177368164,
      "learning_rate": 0.00018043263909850086,
      "loss": 0.5485,
      "step": 4867
    },
    {
      "epoch": 9.79476861167002,
      "grad_norm": 0.35288241505622864,
      "learning_rate": 0.00018042861454874737,
      "loss": 0.5676,
      "step": 4868
    },
    {
      "epoch": 9.796780684104627,
      "grad_norm": 0.37755370140075684,
      "learning_rate": 0.00018042458999899388,
      "loss": 0.5645,
      "step": 4869
    },
    {
      "epoch": 9.798792756539235,
      "grad_norm": 0.3779866099357605,
      "learning_rate": 0.00018042056544924037,
      "loss": 0.5635,
      "step": 4870
    },
    {
      "epoch": 9.800804828973844,
      "grad_norm": 0.3581145703792572,
      "learning_rate": 0.00018041654089948688,
      "loss": 0.6009,
      "step": 4871
    },
    {
      "epoch": 9.80281690140845,
      "grad_norm": 0.356549471616745,
      "learning_rate": 0.00018041251634973336,
      "loss": 0.6279,
      "step": 4872
    },
    {
      "epoch": 9.804828973843058,
      "grad_norm": 0.36263003945350647,
      "learning_rate": 0.0001804084917999799,
      "loss": 0.5641,
      "step": 4873
    },
    {
      "epoch": 9.806841046277667,
      "grad_norm": 0.3462962210178375,
      "learning_rate": 0.0001804044672502264,
      "loss": 0.5499,
      "step": 4874
    },
    {
      "epoch": 9.808853118712273,
      "grad_norm": 0.35319480299949646,
      "learning_rate": 0.0001804004427004729,
      "loss": 0.5937,
      "step": 4875
    },
    {
      "epoch": 9.810865191146881,
      "grad_norm": 0.3451138734817505,
      "learning_rate": 0.00018039641815071938,
      "loss": 0.5889,
      "step": 4876
    },
    {
      "epoch": 9.81287726358149,
      "grad_norm": 0.3393942415714264,
      "learning_rate": 0.0001803923936009659,
      "loss": 0.5833,
      "step": 4877
    },
    {
      "epoch": 9.814889336016096,
      "grad_norm": 0.33269673585891724,
      "learning_rate": 0.0001803883690512124,
      "loss": 0.4913,
      "step": 4878
    },
    {
      "epoch": 9.816901408450704,
      "grad_norm": 0.3612777292728424,
      "learning_rate": 0.00018038434450145892,
      "loss": 0.5798,
      "step": 4879
    },
    {
      "epoch": 9.818913480885312,
      "grad_norm": 0.34751462936401367,
      "learning_rate": 0.0001803803199517054,
      "loss": 0.5744,
      "step": 4880
    },
    {
      "epoch": 9.82092555331992,
      "grad_norm": 0.35061657428741455,
      "learning_rate": 0.00018037629540195192,
      "loss": 0.5852,
      "step": 4881
    },
    {
      "epoch": 9.822937625754527,
      "grad_norm": 0.3349118232727051,
      "learning_rate": 0.0001803722708521984,
      "loss": 0.5593,
      "step": 4882
    },
    {
      "epoch": 9.824949698189135,
      "grad_norm": 0.3759094476699829,
      "learning_rate": 0.00018036824630244494,
      "loss": 0.623,
      "step": 4883
    },
    {
      "epoch": 9.826961770623743,
      "grad_norm": 0.3525601625442505,
      "learning_rate": 0.00018036422175269143,
      "loss": 0.5979,
      "step": 4884
    },
    {
      "epoch": 9.82897384305835,
      "grad_norm": 0.35911956429481506,
      "learning_rate": 0.00018036019720293794,
      "loss": 0.5947,
      "step": 4885
    },
    {
      "epoch": 9.830985915492958,
      "grad_norm": 0.34019792079925537,
      "learning_rate": 0.00018035617265318443,
      "loss": 0.5805,
      "step": 4886
    },
    {
      "epoch": 9.832997987927566,
      "grad_norm": 0.34043699502944946,
      "learning_rate": 0.00018035214810343094,
      "loss": 0.563,
      "step": 4887
    },
    {
      "epoch": 9.835010060362173,
      "grad_norm": 0.3730458915233612,
      "learning_rate": 0.00018034812355367745,
      "loss": 0.5536,
      "step": 4888
    },
    {
      "epoch": 9.83702213279678,
      "grad_norm": 0.3560144603252411,
      "learning_rate": 0.00018034409900392396,
      "loss": 0.5607,
      "step": 4889
    },
    {
      "epoch": 9.839034205231389,
      "grad_norm": 0.3893660008907318,
      "learning_rate": 0.00018034007445417045,
      "loss": 0.5638,
      "step": 4890
    },
    {
      "epoch": 9.841046277665995,
      "grad_norm": 0.37386637926101685,
      "learning_rate": 0.00018033604990441696,
      "loss": 0.5886,
      "step": 4891
    },
    {
      "epoch": 9.843058350100604,
      "grad_norm": 0.35911956429481506,
      "learning_rate": 0.00018033202535466344,
      "loss": 0.5624,
      "step": 4892
    },
    {
      "epoch": 9.845070422535212,
      "grad_norm": 0.3498803377151489,
      "learning_rate": 0.00018032800080490996,
      "loss": 0.5641,
      "step": 4893
    },
    {
      "epoch": 9.847082494969818,
      "grad_norm": 0.34862157702445984,
      "learning_rate": 0.00018032397625515647,
      "loss": 0.5366,
      "step": 4894
    },
    {
      "epoch": 9.849094567404427,
      "grad_norm": 0.3443422317504883,
      "learning_rate": 0.00018031995170540295,
      "loss": 0.5774,
      "step": 4895
    },
    {
      "epoch": 9.851106639839035,
      "grad_norm": 0.3526162803173065,
      "learning_rate": 0.00018031592715564947,
      "loss": 0.5265,
      "step": 4896
    },
    {
      "epoch": 9.853118712273641,
      "grad_norm": 0.3497621417045593,
      "learning_rate": 0.00018031190260589598,
      "loss": 0.5772,
      "step": 4897
    },
    {
      "epoch": 9.85513078470825,
      "grad_norm": 0.34568923711776733,
      "learning_rate": 0.0001803078780561425,
      "loss": 0.5861,
      "step": 4898
    },
    {
      "epoch": 9.857142857142858,
      "grad_norm": 0.33870819211006165,
      "learning_rate": 0.00018030385350638898,
      "loss": 0.553,
      "step": 4899
    },
    {
      "epoch": 9.859154929577464,
      "grad_norm": 0.3538069725036621,
      "learning_rate": 0.0001802998289566355,
      "loss": 0.5852,
      "step": 4900
    },
    {
      "epoch": 9.861167002012072,
      "grad_norm": 0.36150071024894714,
      "learning_rate": 0.00018029580440688197,
      "loss": 0.5911,
      "step": 4901
    },
    {
      "epoch": 9.86317907444668,
      "grad_norm": 0.35626453161239624,
      "learning_rate": 0.00018029177985712849,
      "loss": 0.5662,
      "step": 4902
    },
    {
      "epoch": 9.865191146881287,
      "grad_norm": 0.35035836696624756,
      "learning_rate": 0.000180287755307375,
      "loss": 0.5645,
      "step": 4903
    },
    {
      "epoch": 9.867203219315895,
      "grad_norm": 0.3562323749065399,
      "learning_rate": 0.0001802837307576215,
      "loss": 0.5735,
      "step": 4904
    },
    {
      "epoch": 9.869215291750503,
      "grad_norm": 0.33430880308151245,
      "learning_rate": 0.000180279706207868,
      "loss": 0.567,
      "step": 4905
    },
    {
      "epoch": 9.87122736418511,
      "grad_norm": 0.37350183725357056,
      "learning_rate": 0.0001802756816581145,
      "loss": 0.5876,
      "step": 4906
    },
    {
      "epoch": 9.873239436619718,
      "grad_norm": 0.3553186058998108,
      "learning_rate": 0.000180271657108361,
      "loss": 0.5676,
      "step": 4907
    },
    {
      "epoch": 9.875251509054326,
      "grad_norm": 0.3575316071510315,
      "learning_rate": 0.00018026763255860753,
      "loss": 0.6128,
      "step": 4908
    },
    {
      "epoch": 9.877263581488933,
      "grad_norm": 0.35593506693840027,
      "learning_rate": 0.00018026360800885402,
      "loss": 0.5551,
      "step": 4909
    },
    {
      "epoch": 9.879275653923541,
      "grad_norm": 0.3543851375579834,
      "learning_rate": 0.00018025958345910053,
      "loss": 0.614,
      "step": 4910
    },
    {
      "epoch": 9.88128772635815,
      "grad_norm": 0.34909188747406006,
      "learning_rate": 0.00018025555890934701,
      "loss": 0.583,
      "step": 4911
    },
    {
      "epoch": 9.883299798792757,
      "grad_norm": 0.3510800004005432,
      "learning_rate": 0.00018025153435959353,
      "loss": 0.5901,
      "step": 4912
    },
    {
      "epoch": 9.885311871227364,
      "grad_norm": 0.3624645173549652,
      "learning_rate": 0.00018024750980984004,
      "loss": 0.597,
      "step": 4913
    },
    {
      "epoch": 9.887323943661972,
      "grad_norm": 0.34105566143989563,
      "learning_rate": 0.00018024348526008655,
      "loss": 0.5892,
      "step": 4914
    },
    {
      "epoch": 9.88933601609658,
      "grad_norm": 0.33909016847610474,
      "learning_rate": 0.00018023946071033304,
      "loss": 0.559,
      "step": 4915
    },
    {
      "epoch": 9.891348088531187,
      "grad_norm": 0.34476083517074585,
      "learning_rate": 0.00018023543616057955,
      "loss": 0.574,
      "step": 4916
    },
    {
      "epoch": 9.893360160965795,
      "grad_norm": 0.3525506854057312,
      "learning_rate": 0.00018023141161082603,
      "loss": 0.5734,
      "step": 4917
    },
    {
      "epoch": 9.895372233400403,
      "grad_norm": 0.38418999314308167,
      "learning_rate": 0.00018022738706107257,
      "loss": 0.5377,
      "step": 4918
    },
    {
      "epoch": 9.89738430583501,
      "grad_norm": 0.35367366671562195,
      "learning_rate": 0.00018022336251131906,
      "loss": 0.5612,
      "step": 4919
    },
    {
      "epoch": 9.899396378269618,
      "grad_norm": 0.3557761013507843,
      "learning_rate": 0.00018021933796156557,
      "loss": 0.6144,
      "step": 4920
    },
    {
      "epoch": 9.901408450704226,
      "grad_norm": 0.3494119346141815,
      "learning_rate": 0.00018021531341181205,
      "loss": 0.5699,
      "step": 4921
    },
    {
      "epoch": 9.903420523138832,
      "grad_norm": 0.3519037663936615,
      "learning_rate": 0.00018021128886205857,
      "loss": 0.5911,
      "step": 4922
    },
    {
      "epoch": 9.90543259557344,
      "grad_norm": 0.3353153169155121,
      "learning_rate": 0.00018020726431230508,
      "loss": 0.5486,
      "step": 4923
    },
    {
      "epoch": 9.907444668008049,
      "grad_norm": 0.34754249453544617,
      "learning_rate": 0.0001802032397625516,
      "loss": 0.6092,
      "step": 4924
    },
    {
      "epoch": 9.909456740442655,
      "grad_norm": 0.3661913573741913,
      "learning_rate": 0.00018019921521279808,
      "loss": 0.5638,
      "step": 4925
    },
    {
      "epoch": 9.911468812877263,
      "grad_norm": 0.3459285497665405,
      "learning_rate": 0.0001801951906630446,
      "loss": 0.5576,
      "step": 4926
    },
    {
      "epoch": 9.913480885311872,
      "grad_norm": 0.35191449522972107,
      "learning_rate": 0.00018019116611329107,
      "loss": 0.5791,
      "step": 4927
    },
    {
      "epoch": 9.915492957746478,
      "grad_norm": 0.3410801887512207,
      "learning_rate": 0.00018018714156353759,
      "loss": 0.5496,
      "step": 4928
    },
    {
      "epoch": 9.917505030181086,
      "grad_norm": 0.3377167880535126,
      "learning_rate": 0.0001801831170137841,
      "loss": 0.5965,
      "step": 4929
    },
    {
      "epoch": 9.919517102615695,
      "grad_norm": 0.3485901653766632,
      "learning_rate": 0.00018017909246403058,
      "loss": 0.5483,
      "step": 4930
    },
    {
      "epoch": 9.921529175050303,
      "grad_norm": 0.3594309985637665,
      "learning_rate": 0.0001801750679142771,
      "loss": 0.5501,
      "step": 4931
    },
    {
      "epoch": 9.92354124748491,
      "grad_norm": 0.3405435383319855,
      "learning_rate": 0.0001801710433645236,
      "loss": 0.5797,
      "step": 4932
    },
    {
      "epoch": 9.925553319919517,
      "grad_norm": 0.35528266429901123,
      "learning_rate": 0.00018016701881477012,
      "loss": 0.5421,
      "step": 4933
    },
    {
      "epoch": 9.927565392354126,
      "grad_norm": 0.34789249300956726,
      "learning_rate": 0.0001801629942650166,
      "loss": 0.5609,
      "step": 4934
    },
    {
      "epoch": 9.929577464788732,
      "grad_norm": 0.373801052570343,
      "learning_rate": 0.00018015896971526312,
      "loss": 0.5704,
      "step": 4935
    },
    {
      "epoch": 9.93158953722334,
      "grad_norm": 0.36262503266334534,
      "learning_rate": 0.0001801549451655096,
      "loss": 0.5798,
      "step": 4936
    },
    {
      "epoch": 9.933601609657948,
      "grad_norm": 0.35435280203819275,
      "learning_rate": 0.00018015092061575611,
      "loss": 0.5653,
      "step": 4937
    },
    {
      "epoch": 9.935613682092555,
      "grad_norm": 0.37217527627944946,
      "learning_rate": 0.00018014689606600263,
      "loss": 0.5099,
      "step": 4938
    },
    {
      "epoch": 9.937625754527163,
      "grad_norm": 0.3372628092765808,
      "learning_rate": 0.00018014287151624914,
      "loss": 0.5446,
      "step": 4939
    },
    {
      "epoch": 9.939637826961771,
      "grad_norm": 0.3501838445663452,
      "learning_rate": 0.00018013884696649562,
      "loss": 0.5819,
      "step": 4940
    },
    {
      "epoch": 9.941649899396378,
      "grad_norm": 0.36252859234809875,
      "learning_rate": 0.00018013482241674214,
      "loss": 0.5991,
      "step": 4941
    },
    {
      "epoch": 9.943661971830986,
      "grad_norm": 0.34871068596839905,
      "learning_rate": 0.00018013079786698862,
      "loss": 0.5907,
      "step": 4942
    },
    {
      "epoch": 9.945674044265594,
      "grad_norm": 0.37288057804107666,
      "learning_rate": 0.00018012677331723516,
      "loss": 0.5611,
      "step": 4943
    },
    {
      "epoch": 9.9476861167002,
      "grad_norm": 0.35940414667129517,
      "learning_rate": 0.00018012274876748165,
      "loss": 0.6006,
      "step": 4944
    },
    {
      "epoch": 9.949698189134809,
      "grad_norm": 0.3487005829811096,
      "learning_rate": 0.00018011872421772816,
      "loss": 0.5655,
      "step": 4945
    },
    {
      "epoch": 9.951710261569417,
      "grad_norm": 0.34229975938796997,
      "learning_rate": 0.00018011469966797464,
      "loss": 0.5637,
      "step": 4946
    },
    {
      "epoch": 9.953722334004024,
      "grad_norm": 0.33767956495285034,
      "learning_rate": 0.00018011067511822116,
      "loss": 0.5578,
      "step": 4947
    },
    {
      "epoch": 9.955734406438632,
      "grad_norm": 0.3429078161716461,
      "learning_rate": 0.00018010665056846767,
      "loss": 0.6278,
      "step": 4948
    },
    {
      "epoch": 9.95774647887324,
      "grad_norm": 0.3262741267681122,
      "learning_rate": 0.00018010262601871418,
      "loss": 0.578,
      "step": 4949
    },
    {
      "epoch": 9.959758551307846,
      "grad_norm": 0.3717859387397766,
      "learning_rate": 0.00018009860146896067,
      "loss": 0.5728,
      "step": 4950
    },
    {
      "epoch": 9.961770623742455,
      "grad_norm": 0.33880409598350525,
      "learning_rate": 0.00018009457691920718,
      "loss": 0.5786,
      "step": 4951
    },
    {
      "epoch": 9.963782696177063,
      "grad_norm": 0.33836856484413147,
      "learning_rate": 0.00018009055236945366,
      "loss": 0.5393,
      "step": 4952
    },
    {
      "epoch": 9.96579476861167,
      "grad_norm": 0.3354634940624237,
      "learning_rate": 0.0001800865278197002,
      "loss": 0.6046,
      "step": 4953
    },
    {
      "epoch": 9.967806841046277,
      "grad_norm": 0.3445919454097748,
      "learning_rate": 0.0001800825032699467,
      "loss": 0.5311,
      "step": 4954
    },
    {
      "epoch": 9.969818913480886,
      "grad_norm": 0.33780112862586975,
      "learning_rate": 0.0001800784787201932,
      "loss": 0.5654,
      "step": 4955
    },
    {
      "epoch": 9.971830985915492,
      "grad_norm": 0.3495793640613556,
      "learning_rate": 0.00018007445417043968,
      "loss": 0.5517,
      "step": 4956
    },
    {
      "epoch": 9.9738430583501,
      "grad_norm": 0.3400918245315552,
      "learning_rate": 0.0001800704296206862,
      "loss": 0.5633,
      "step": 4957
    },
    {
      "epoch": 9.975855130784709,
      "grad_norm": 0.35070961713790894,
      "learning_rate": 0.0001800664050709327,
      "loss": 0.5828,
      "step": 4958
    },
    {
      "epoch": 9.977867203219315,
      "grad_norm": 0.3531152904033661,
      "learning_rate": 0.00018006238052117922,
      "loss": 0.5594,
      "step": 4959
    },
    {
      "epoch": 9.979879275653923,
      "grad_norm": 0.3439214527606964,
      "learning_rate": 0.0001800583559714257,
      "loss": 0.5504,
      "step": 4960
    },
    {
      "epoch": 9.981891348088531,
      "grad_norm": 0.3437210023403168,
      "learning_rate": 0.00018005433142167222,
      "loss": 0.5562,
      "step": 4961
    },
    {
      "epoch": 9.98390342052314,
      "grad_norm": 0.3521598279476166,
      "learning_rate": 0.0001800503068719187,
      "loss": 0.5901,
      "step": 4962
    },
    {
      "epoch": 9.985915492957746,
      "grad_norm": 0.3653697073459625,
      "learning_rate": 0.00018004628232216522,
      "loss": 0.5287,
      "step": 4963
    },
    {
      "epoch": 9.987927565392354,
      "grad_norm": 0.3440077602863312,
      "learning_rate": 0.00018004225777241173,
      "loss": 0.5874,
      "step": 4964
    },
    {
      "epoch": 9.989939637826962,
      "grad_norm": 0.33240747451782227,
      "learning_rate": 0.0001800382332226582,
      "loss": 0.548,
      "step": 4965
    },
    {
      "epoch": 9.991951710261569,
      "grad_norm": 0.338581383228302,
      "learning_rate": 0.00018003420867290473,
      "loss": 0.5658,
      "step": 4966
    },
    {
      "epoch": 9.993963782696177,
      "grad_norm": 0.3640759587287903,
      "learning_rate": 0.00018003018412315124,
      "loss": 0.5463,
      "step": 4967
    },
    {
      "epoch": 9.995975855130785,
      "grad_norm": 0.3527768552303314,
      "learning_rate": 0.00018002615957339775,
      "loss": 0.5705,
      "step": 4968
    },
    {
      "epoch": 9.997987927565392,
      "grad_norm": 0.337507426738739,
      "learning_rate": 0.00018002213502364423,
      "loss": 0.5095,
      "step": 4969
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.3458356559276581,
      "learning_rate": 0.00018001811047389075,
      "loss": 0.5537,
      "step": 4970
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.7476135492324829,
      "eval_runtime": 49.796,
      "eval_samples_per_second": 19.921,
      "eval_steps_per_second": 2.49,
      "step": 4970
    },
    {
      "epoch": 10.002012072434608,
      "grad_norm": 0.3690842390060425,
      "learning_rate": 0.00018001408592413723,
      "loss": 0.5205,
      "step": 4971
    },
    {
      "epoch": 10.004024144869215,
      "grad_norm": 0.33695530891418457,
      "learning_rate": 0.00018001006137438374,
      "loss": 0.5441,
      "step": 4972
    },
    {
      "epoch": 10.006036217303823,
      "grad_norm": 0.3354913294315338,
      "learning_rate": 0.00018000603682463026,
      "loss": 0.4958,
      "step": 4973
    },
    {
      "epoch": 10.008048289738431,
      "grad_norm": 0.3827817142009735,
      "learning_rate": 0.00018000201227487677,
      "loss": 0.5133,
      "step": 4974
    },
    {
      "epoch": 10.010060362173038,
      "grad_norm": 0.3892000913619995,
      "learning_rate": 0.00017999798772512325,
      "loss": 0.5098,
      "step": 4975
    },
    {
      "epoch": 10.012072434607646,
      "grad_norm": 0.3879324793815613,
      "learning_rate": 0.00017999396317536977,
      "loss": 0.5205,
      "step": 4976
    },
    {
      "epoch": 10.014084507042254,
      "grad_norm": 0.37596389651298523,
      "learning_rate": 0.00017998993862561625,
      "loss": 0.4641,
      "step": 4977
    },
    {
      "epoch": 10.01609657947686,
      "grad_norm": 0.36371099948883057,
      "learning_rate": 0.00017998591407586276,
      "loss": 0.4931,
      "step": 4978
    },
    {
      "epoch": 10.018108651911469,
      "grad_norm": 0.3435157835483551,
      "learning_rate": 0.00017998188952610928,
      "loss": 0.4965,
      "step": 4979
    },
    {
      "epoch": 10.020120724346077,
      "grad_norm": 0.38182127475738525,
      "learning_rate": 0.0001799778649763558,
      "loss": 0.5298,
      "step": 4980
    },
    {
      "epoch": 10.022132796780683,
      "grad_norm": 0.3741370737552643,
      "learning_rate": 0.00017997384042660227,
      "loss": 0.5346,
      "step": 4981
    },
    {
      "epoch": 10.024144869215291,
      "grad_norm": 0.3639966547489166,
      "learning_rate": 0.00017996981587684879,
      "loss": 0.5288,
      "step": 4982
    },
    {
      "epoch": 10.0261569416499,
      "grad_norm": 0.36514607071876526,
      "learning_rate": 0.00017996579132709527,
      "loss": 0.5194,
      "step": 4983
    },
    {
      "epoch": 10.028169014084508,
      "grad_norm": 0.3639172613620758,
      "learning_rate": 0.0001799617667773418,
      "loss": 0.5422,
      "step": 4984
    },
    {
      "epoch": 10.030181086519114,
      "grad_norm": 0.35493770241737366,
      "learning_rate": 0.0001799577422275883,
      "loss": 0.5151,
      "step": 4985
    },
    {
      "epoch": 10.032193158953723,
      "grad_norm": 0.3833332061767578,
      "learning_rate": 0.0001799537176778348,
      "loss": 0.5071,
      "step": 4986
    },
    {
      "epoch": 10.03420523138833,
      "grad_norm": 0.38369220495224,
      "learning_rate": 0.0001799496931280813,
      "loss": 0.5448,
      "step": 4987
    },
    {
      "epoch": 10.036217303822937,
      "grad_norm": 0.37236782908439636,
      "learning_rate": 0.0001799456685783278,
      "loss": 0.5178,
      "step": 4988
    },
    {
      "epoch": 10.038229376257545,
      "grad_norm": 0.3749331831932068,
      "learning_rate": 0.00017994164402857432,
      "loss": 0.4804,
      "step": 4989
    },
    {
      "epoch": 10.040241448692154,
      "grad_norm": 0.3458373248577118,
      "learning_rate": 0.00017993761947882083,
      "loss": 0.5153,
      "step": 4990
    },
    {
      "epoch": 10.04225352112676,
      "grad_norm": 0.37361884117126465,
      "learning_rate": 0.00017993359492906731,
      "loss": 0.5195,
      "step": 4991
    },
    {
      "epoch": 10.044265593561368,
      "grad_norm": 0.38594433665275574,
      "learning_rate": 0.00017992957037931383,
      "loss": 0.5437,
      "step": 4992
    },
    {
      "epoch": 10.046277665995976,
      "grad_norm": 0.3662940263748169,
      "learning_rate": 0.0001799255458295603,
      "loss": 0.5029,
      "step": 4993
    },
    {
      "epoch": 10.048289738430583,
      "grad_norm": 0.3528709411621094,
      "learning_rate": 0.00017992152127980685,
      "loss": 0.4727,
      "step": 4994
    },
    {
      "epoch": 10.050301810865191,
      "grad_norm": 0.3613729774951935,
      "learning_rate": 0.00017991749673005334,
      "loss": 0.4855,
      "step": 4995
    },
    {
      "epoch": 10.0523138832998,
      "grad_norm": 0.40285471081733704,
      "learning_rate": 0.00017991347218029985,
      "loss": 0.5094,
      "step": 4996
    },
    {
      "epoch": 10.054325955734406,
      "grad_norm": 0.3752765953540802,
      "learning_rate": 0.00017990944763054633,
      "loss": 0.537,
      "step": 4997
    },
    {
      "epoch": 10.056338028169014,
      "grad_norm": 0.3753962814807892,
      "learning_rate": 0.00017990542308079285,
      "loss": 0.5254,
      "step": 4998
    },
    {
      "epoch": 10.058350100603622,
      "grad_norm": 0.3666017949581146,
      "learning_rate": 0.00017990139853103936,
      "loss": 0.5218,
      "step": 4999
    },
    {
      "epoch": 10.060362173038229,
      "grad_norm": 0.36460891366004944,
      "learning_rate": 0.00017989737398128584,
      "loss": 0.4979,
      "step": 5000
    },
    {
      "epoch": 10.062374245472837,
      "grad_norm": 0.3689481019973755,
      "learning_rate": 0.00017989334943153235,
      "loss": 0.528,
      "step": 5001
    },
    {
      "epoch": 10.064386317907445,
      "grad_norm": 0.34538617730140686,
      "learning_rate": 0.00017988932488177887,
      "loss": 0.5119,
      "step": 5002
    },
    {
      "epoch": 10.066398390342052,
      "grad_norm": 0.37213319540023804,
      "learning_rate": 0.00017988530033202535,
      "loss": 0.4966,
      "step": 5003
    },
    {
      "epoch": 10.06841046277666,
      "grad_norm": 0.36858227849006653,
      "learning_rate": 0.00017988127578227186,
      "loss": 0.513,
      "step": 5004
    },
    {
      "epoch": 10.070422535211268,
      "grad_norm": 0.3839412033557892,
      "learning_rate": 0.00017987725123251838,
      "loss": 0.546,
      "step": 5005
    },
    {
      "epoch": 10.072434607645874,
      "grad_norm": 0.37527111172676086,
      "learning_rate": 0.00017987322668276486,
      "loss": 0.5048,
      "step": 5006
    },
    {
      "epoch": 10.074446680080483,
      "grad_norm": 0.377285897731781,
      "learning_rate": 0.00017986920213301137,
      "loss": 0.5174,
      "step": 5007
    },
    {
      "epoch": 10.07645875251509,
      "grad_norm": 0.3740529716014862,
      "learning_rate": 0.00017986517758325786,
      "loss": 0.5363,
      "step": 5008
    },
    {
      "epoch": 10.078470824949699,
      "grad_norm": 0.36697763204574585,
      "learning_rate": 0.0001798611530335044,
      "loss": 0.5278,
      "step": 5009
    },
    {
      "epoch": 10.080482897384305,
      "grad_norm": 0.37279677391052246,
      "learning_rate": 0.00017985712848375088,
      "loss": 0.483,
      "step": 5010
    },
    {
      "epoch": 10.082494969818914,
      "grad_norm": 0.3739689290523529,
      "learning_rate": 0.0001798531039339974,
      "loss": 0.4863,
      "step": 5011
    },
    {
      "epoch": 10.084507042253522,
      "grad_norm": 0.37163424491882324,
      "learning_rate": 0.00017984907938424388,
      "loss": 0.5451,
      "step": 5012
    },
    {
      "epoch": 10.086519114688128,
      "grad_norm": 0.3697953224182129,
      "learning_rate": 0.0001798450548344904,
      "loss": 0.487,
      "step": 5013
    },
    {
      "epoch": 10.088531187122737,
      "grad_norm": 0.35322409868240356,
      "learning_rate": 0.0001798410302847369,
      "loss": 0.4955,
      "step": 5014
    },
    {
      "epoch": 10.090543259557345,
      "grad_norm": 0.36029714345932007,
      "learning_rate": 0.00017983700573498342,
      "loss": 0.5202,
      "step": 5015
    },
    {
      "epoch": 10.092555331991951,
      "grad_norm": 0.35801073908805847,
      "learning_rate": 0.0001798329811852299,
      "loss": 0.549,
      "step": 5016
    },
    {
      "epoch": 10.09456740442656,
      "grad_norm": 0.36355477571487427,
      "learning_rate": 0.00017982895663547641,
      "loss": 0.5026,
      "step": 5017
    },
    {
      "epoch": 10.096579476861168,
      "grad_norm": 0.37089601159095764,
      "learning_rate": 0.0001798249320857229,
      "loss": 0.532,
      "step": 5018
    },
    {
      "epoch": 10.098591549295774,
      "grad_norm": 0.3701017200946808,
      "learning_rate": 0.00017982090753596944,
      "loss": 0.5251,
      "step": 5019
    },
    {
      "epoch": 10.100603621730382,
      "grad_norm": 0.3661392033100128,
      "learning_rate": 0.00017981688298621592,
      "loss": 0.5085,
      "step": 5020
    },
    {
      "epoch": 10.10261569416499,
      "grad_norm": 0.3959175646305084,
      "learning_rate": 0.00017981285843646244,
      "loss": 0.5664,
      "step": 5021
    },
    {
      "epoch": 10.104627766599597,
      "grad_norm": 0.39272060990333557,
      "learning_rate": 0.00017980883388670892,
      "loss": 0.5523,
      "step": 5022
    },
    {
      "epoch": 10.106639839034205,
      "grad_norm": 0.389057993888855,
      "learning_rate": 0.00017980480933695543,
      "loss": 0.5363,
      "step": 5023
    },
    {
      "epoch": 10.108651911468813,
      "grad_norm": 0.3914910554885864,
      "learning_rate": 0.00017980078478720195,
      "loss": 0.5566,
      "step": 5024
    },
    {
      "epoch": 10.11066398390342,
      "grad_norm": 0.38004010915756226,
      "learning_rate": 0.00017979676023744846,
      "loss": 0.5018,
      "step": 5025
    },
    {
      "epoch": 10.112676056338028,
      "grad_norm": 0.3806014657020569,
      "learning_rate": 0.00017979273568769494,
      "loss": 0.5034,
      "step": 5026
    },
    {
      "epoch": 10.114688128772636,
      "grad_norm": 0.3894408643245697,
      "learning_rate": 0.00017978871113794146,
      "loss": 0.565,
      "step": 5027
    },
    {
      "epoch": 10.116700201207243,
      "grad_norm": 0.3671639561653137,
      "learning_rate": 0.00017978468658818794,
      "loss": 0.529,
      "step": 5028
    },
    {
      "epoch": 10.11871227364185,
      "grad_norm": 0.3776705861091614,
      "learning_rate": 0.00017978066203843448,
      "loss": 0.5199,
      "step": 5029
    },
    {
      "epoch": 10.120724346076459,
      "grad_norm": 0.36518120765686035,
      "learning_rate": 0.00017977663748868097,
      "loss": 0.5296,
      "step": 5030
    },
    {
      "epoch": 10.122736418511066,
      "grad_norm": 0.4059963524341583,
      "learning_rate": 0.00017977261293892748,
      "loss": 0.5251,
      "step": 5031
    },
    {
      "epoch": 10.124748490945674,
      "grad_norm": 0.39089566469192505,
      "learning_rate": 0.00017976858838917396,
      "loss": 0.5332,
      "step": 5032
    },
    {
      "epoch": 10.126760563380282,
      "grad_norm": 0.37334364652633667,
      "learning_rate": 0.00017976456383942047,
      "loss": 0.5396,
      "step": 5033
    },
    {
      "epoch": 10.12877263581489,
      "grad_norm": 0.3579556941986084,
      "learning_rate": 0.000179760539289667,
      "loss": 0.5065,
      "step": 5034
    },
    {
      "epoch": 10.130784708249497,
      "grad_norm": 0.3694649040699005,
      "learning_rate": 0.00017975651473991347,
      "loss": 0.4914,
      "step": 5035
    },
    {
      "epoch": 10.132796780684105,
      "grad_norm": 0.3488827645778656,
      "learning_rate": 0.00017975249019015998,
      "loss": 0.4866,
      "step": 5036
    },
    {
      "epoch": 10.134808853118713,
      "grad_norm": 0.36894458532333374,
      "learning_rate": 0.00017974846564040647,
      "loss": 0.5428,
      "step": 5037
    },
    {
      "epoch": 10.13682092555332,
      "grad_norm": 0.38998502492904663,
      "learning_rate": 0.00017974444109065298,
      "loss": 0.5091,
      "step": 5038
    },
    {
      "epoch": 10.138832997987928,
      "grad_norm": 0.373628705739975,
      "learning_rate": 0.0001797404165408995,
      "loss": 0.505,
      "step": 5039
    },
    {
      "epoch": 10.140845070422536,
      "grad_norm": 0.3953457176685333,
      "learning_rate": 0.000179736391991146,
      "loss": 0.519,
      "step": 5040
    },
    {
      "epoch": 10.142857142857142,
      "grad_norm": 0.38899654150009155,
      "learning_rate": 0.0001797323674413925,
      "loss": 0.5153,
      "step": 5041
    },
    {
      "epoch": 10.14486921529175,
      "grad_norm": 0.364841103553772,
      "learning_rate": 0.000179728342891639,
      "loss": 0.5342,
      "step": 5042
    },
    {
      "epoch": 10.146881287726359,
      "grad_norm": 0.38133975863456726,
      "learning_rate": 0.0001797243183418855,
      "loss": 0.5396,
      "step": 5043
    },
    {
      "epoch": 10.148893360160965,
      "grad_norm": 0.35489410161972046,
      "learning_rate": 0.00017972029379213203,
      "loss": 0.5325,
      "step": 5044
    },
    {
      "epoch": 10.150905432595573,
      "grad_norm": 0.3710087835788727,
      "learning_rate": 0.0001797162692423785,
      "loss": 0.5257,
      "step": 5045
    },
    {
      "epoch": 10.152917505030182,
      "grad_norm": 0.38178184628486633,
      "learning_rate": 0.00017971224469262502,
      "loss": 0.535,
      "step": 5046
    },
    {
      "epoch": 10.154929577464788,
      "grad_norm": 0.3806014955043793,
      "learning_rate": 0.0001797082201428715,
      "loss": 0.4881,
      "step": 5047
    },
    {
      "epoch": 10.156941649899396,
      "grad_norm": 0.38508740067481995,
      "learning_rate": 0.00017970419559311802,
      "loss": 0.4871,
      "step": 5048
    },
    {
      "epoch": 10.158953722334005,
      "grad_norm": 0.38480040431022644,
      "learning_rate": 0.00017970017104336453,
      "loss": 0.5093,
      "step": 5049
    },
    {
      "epoch": 10.160965794768611,
      "grad_norm": 0.4133273959159851,
      "learning_rate": 0.00017969614649361105,
      "loss": 0.507,
      "step": 5050
    },
    {
      "epoch": 10.16297786720322,
      "grad_norm": 0.4001725912094116,
      "learning_rate": 0.00017969212194385753,
      "loss": 0.5384,
      "step": 5051
    },
    {
      "epoch": 10.164989939637827,
      "grad_norm": 0.36161962151527405,
      "learning_rate": 0.00017968809739410404,
      "loss": 0.5038,
      "step": 5052
    },
    {
      "epoch": 10.167002012072434,
      "grad_norm": 0.365213006734848,
      "learning_rate": 0.00017968407284435053,
      "loss": 0.5117,
      "step": 5053
    },
    {
      "epoch": 10.169014084507042,
      "grad_norm": 0.3972143530845642,
      "learning_rate": 0.00017968004829459707,
      "loss": 0.5462,
      "step": 5054
    },
    {
      "epoch": 10.17102615694165,
      "grad_norm": 0.3811624050140381,
      "learning_rate": 0.00017967602374484355,
      "loss": 0.5329,
      "step": 5055
    },
    {
      "epoch": 10.173038229376257,
      "grad_norm": 0.3633595407009125,
      "learning_rate": 0.00017967199919509007,
      "loss": 0.5211,
      "step": 5056
    },
    {
      "epoch": 10.175050301810865,
      "grad_norm": 0.3737384080886841,
      "learning_rate": 0.00017966797464533655,
      "loss": 0.5303,
      "step": 5057
    },
    {
      "epoch": 10.177062374245473,
      "grad_norm": 0.36714786291122437,
      "learning_rate": 0.00017966395009558306,
      "loss": 0.5316,
      "step": 5058
    },
    {
      "epoch": 10.179074446680081,
      "grad_norm": 0.3783376216888428,
      "learning_rate": 0.00017965992554582958,
      "loss": 0.5352,
      "step": 5059
    },
    {
      "epoch": 10.181086519114688,
      "grad_norm": 0.3631323575973511,
      "learning_rate": 0.0001796559009960761,
      "loss": 0.5429,
      "step": 5060
    },
    {
      "epoch": 10.183098591549296,
      "grad_norm": 0.38019636273384094,
      "learning_rate": 0.00017965187644632257,
      "loss": 0.5369,
      "step": 5061
    },
    {
      "epoch": 10.185110663983904,
      "grad_norm": 0.39083948731422424,
      "learning_rate": 0.00017964785189656908,
      "loss": 0.5415,
      "step": 5062
    },
    {
      "epoch": 10.18712273641851,
      "grad_norm": 0.4027758240699768,
      "learning_rate": 0.00017964382734681557,
      "loss": 0.5371,
      "step": 5063
    },
    {
      "epoch": 10.189134808853119,
      "grad_norm": 0.38228341937065125,
      "learning_rate": 0.00017963980279706208,
      "loss": 0.483,
      "step": 5064
    },
    {
      "epoch": 10.191146881287727,
      "grad_norm": 0.35861602425575256,
      "learning_rate": 0.0001796357782473086,
      "loss": 0.526,
      "step": 5065
    },
    {
      "epoch": 10.193158953722333,
      "grad_norm": 0.3718741536140442,
      "learning_rate": 0.0001796317536975551,
      "loss": 0.5063,
      "step": 5066
    },
    {
      "epoch": 10.195171026156942,
      "grad_norm": 0.3546677529811859,
      "learning_rate": 0.0001796277291478016,
      "loss": 0.5027,
      "step": 5067
    },
    {
      "epoch": 10.19718309859155,
      "grad_norm": 0.3998165726661682,
      "learning_rate": 0.0001796237045980481,
      "loss": 0.5473,
      "step": 5068
    },
    {
      "epoch": 10.199195171026156,
      "grad_norm": 0.3668525218963623,
      "learning_rate": 0.00017961968004829462,
      "loss": 0.4988,
      "step": 5069
    },
    {
      "epoch": 10.201207243460765,
      "grad_norm": 0.3820875585079193,
      "learning_rate": 0.0001796156554985411,
      "loss": 0.5232,
      "step": 5070
    },
    {
      "epoch": 10.203219315895373,
      "grad_norm": 0.37658339738845825,
      "learning_rate": 0.00017961163094878761,
      "loss": 0.5072,
      "step": 5071
    },
    {
      "epoch": 10.20523138832998,
      "grad_norm": 0.34323978424072266,
      "learning_rate": 0.0001796076063990341,
      "loss": 0.4833,
      "step": 5072
    },
    {
      "epoch": 10.207243460764587,
      "grad_norm": 0.37631696462631226,
      "learning_rate": 0.0001796035818492806,
      "loss": 0.5234,
      "step": 5073
    },
    {
      "epoch": 10.209255533199196,
      "grad_norm": 0.4029173254966736,
      "learning_rate": 0.00017959955729952712,
      "loss": 0.5323,
      "step": 5074
    },
    {
      "epoch": 10.211267605633802,
      "grad_norm": 0.3678097128868103,
      "learning_rate": 0.00017959553274977364,
      "loss": 0.5269,
      "step": 5075
    },
    {
      "epoch": 10.21327967806841,
      "grad_norm": 0.369313508272171,
      "learning_rate": 0.00017959150820002012,
      "loss": 0.4858,
      "step": 5076
    },
    {
      "epoch": 10.215291750503019,
      "grad_norm": 0.36545079946517944,
      "learning_rate": 0.00017958748365026663,
      "loss": 0.5215,
      "step": 5077
    },
    {
      "epoch": 10.217303822937625,
      "grad_norm": 0.38283446431159973,
      "learning_rate": 0.00017958345910051312,
      "loss": 0.5015,
      "step": 5078
    },
    {
      "epoch": 10.219315895372233,
      "grad_norm": 0.376627117395401,
      "learning_rate": 0.00017957943455075966,
      "loss": 0.5289,
      "step": 5079
    },
    {
      "epoch": 10.221327967806841,
      "grad_norm": 0.4160099923610687,
      "learning_rate": 0.00017957541000100614,
      "loss": 0.5394,
      "step": 5080
    },
    {
      "epoch": 10.223340040241448,
      "grad_norm": 0.37485191226005554,
      "learning_rate": 0.00017957138545125265,
      "loss": 0.5421,
      "step": 5081
    },
    {
      "epoch": 10.225352112676056,
      "grad_norm": 0.3673834204673767,
      "learning_rate": 0.00017956736090149914,
      "loss": 0.5218,
      "step": 5082
    },
    {
      "epoch": 10.227364185110664,
      "grad_norm": 0.3633537292480469,
      "learning_rate": 0.00017956333635174565,
      "loss": 0.5246,
      "step": 5083
    },
    {
      "epoch": 10.229376257545272,
      "grad_norm": 0.3691171407699585,
      "learning_rate": 0.00017955931180199216,
      "loss": 0.5444,
      "step": 5084
    },
    {
      "epoch": 10.231388329979879,
      "grad_norm": 0.37602436542510986,
      "learning_rate": 0.00017955528725223868,
      "loss": 0.5863,
      "step": 5085
    },
    {
      "epoch": 10.233400402414487,
      "grad_norm": 0.3906817138195038,
      "learning_rate": 0.00017955126270248516,
      "loss": 0.5411,
      "step": 5086
    },
    {
      "epoch": 10.235412474849095,
      "grad_norm": 0.38823065161705017,
      "learning_rate": 0.00017954723815273167,
      "loss": 0.5226,
      "step": 5087
    },
    {
      "epoch": 10.237424547283702,
      "grad_norm": 0.3920055329799652,
      "learning_rate": 0.00017954321360297816,
      "loss": 0.5561,
      "step": 5088
    },
    {
      "epoch": 10.23943661971831,
      "grad_norm": 0.3974265456199646,
      "learning_rate": 0.0001795391890532247,
      "loss": 0.5155,
      "step": 5089
    },
    {
      "epoch": 10.241448692152918,
      "grad_norm": 0.3883949816226959,
      "learning_rate": 0.00017953516450347118,
      "loss": 0.5434,
      "step": 5090
    },
    {
      "epoch": 10.243460764587525,
      "grad_norm": 0.37875887751579285,
      "learning_rate": 0.0001795311399537177,
      "loss": 0.5414,
      "step": 5091
    },
    {
      "epoch": 10.245472837022133,
      "grad_norm": 0.37228500843048096,
      "learning_rate": 0.00017952711540396418,
      "loss": 0.5344,
      "step": 5092
    },
    {
      "epoch": 10.247484909456741,
      "grad_norm": 0.37322452664375305,
      "learning_rate": 0.0001795230908542107,
      "loss": 0.531,
      "step": 5093
    },
    {
      "epoch": 10.249496981891348,
      "grad_norm": 0.3784087598323822,
      "learning_rate": 0.0001795190663044572,
      "loss": 0.4989,
      "step": 5094
    },
    {
      "epoch": 10.251509054325956,
      "grad_norm": 0.3805989623069763,
      "learning_rate": 0.00017951504175470372,
      "loss": 0.4868,
      "step": 5095
    },
    {
      "epoch": 10.253521126760564,
      "grad_norm": 0.36736783385276794,
      "learning_rate": 0.0001795110172049502,
      "loss": 0.5378,
      "step": 5096
    },
    {
      "epoch": 10.25553319919517,
      "grad_norm": 0.3721814453601837,
      "learning_rate": 0.00017950699265519671,
      "loss": 0.5009,
      "step": 5097
    },
    {
      "epoch": 10.257545271629779,
      "grad_norm": 0.384469598531723,
      "learning_rate": 0.0001795029681054432,
      "loss": 0.5248,
      "step": 5098
    },
    {
      "epoch": 10.259557344064387,
      "grad_norm": 0.3507644534111023,
      "learning_rate": 0.0001794989435556897,
      "loss": 0.5248,
      "step": 5099
    },
    {
      "epoch": 10.261569416498993,
      "grad_norm": 0.36514487862586975,
      "learning_rate": 0.00017949491900593622,
      "loss": 0.5357,
      "step": 5100
    },
    {
      "epoch": 10.263581488933601,
      "grad_norm": 0.36617711186408997,
      "learning_rate": 0.00017949089445618274,
      "loss": 0.5113,
      "step": 5101
    },
    {
      "epoch": 10.26559356136821,
      "grad_norm": 0.3797731101512909,
      "learning_rate": 0.00017948686990642922,
      "loss": 0.5173,
      "step": 5102
    },
    {
      "epoch": 10.267605633802816,
      "grad_norm": 0.3954164683818817,
      "learning_rate": 0.00017948284535667573,
      "loss": 0.525,
      "step": 5103
    },
    {
      "epoch": 10.269617706237424,
      "grad_norm": 0.38396701216697693,
      "learning_rate": 0.00017947882080692225,
      "loss": 0.5458,
      "step": 5104
    },
    {
      "epoch": 10.271629778672033,
      "grad_norm": 0.3649982810020447,
      "learning_rate": 0.00017947479625716873,
      "loss": 0.4901,
      "step": 5105
    },
    {
      "epoch": 10.273641851106639,
      "grad_norm": 0.3607618808746338,
      "learning_rate": 0.00017947077170741524,
      "loss": 0.5287,
      "step": 5106
    },
    {
      "epoch": 10.275653923541247,
      "grad_norm": 0.38590896129608154,
      "learning_rate": 0.00017946674715766173,
      "loss": 0.5055,
      "step": 5107
    },
    {
      "epoch": 10.277665995975855,
      "grad_norm": 0.37579986453056335,
      "learning_rate": 0.00017946272260790824,
      "loss": 0.5117,
      "step": 5108
    },
    {
      "epoch": 10.279678068410464,
      "grad_norm": 0.384782999753952,
      "learning_rate": 0.00017945869805815475,
      "loss": 0.5562,
      "step": 5109
    },
    {
      "epoch": 10.28169014084507,
      "grad_norm": 0.3759746253490448,
      "learning_rate": 0.00017945467350840126,
      "loss": 0.5342,
      "step": 5110
    },
    {
      "epoch": 10.283702213279678,
      "grad_norm": 0.3614439368247986,
      "learning_rate": 0.00017945064895864775,
      "loss": 0.5344,
      "step": 5111
    },
    {
      "epoch": 10.285714285714286,
      "grad_norm": 0.3666362762451172,
      "learning_rate": 0.00017944662440889426,
      "loss": 0.5384,
      "step": 5112
    },
    {
      "epoch": 10.287726358148893,
      "grad_norm": 0.3760668933391571,
      "learning_rate": 0.00017944259985914075,
      "loss": 0.5311,
      "step": 5113
    },
    {
      "epoch": 10.289738430583501,
      "grad_norm": 0.39763087034225464,
      "learning_rate": 0.0001794385753093873,
      "loss": 0.5462,
      "step": 5114
    },
    {
      "epoch": 10.29175050301811,
      "grad_norm": 0.38577497005462646,
      "learning_rate": 0.00017943455075963377,
      "loss": 0.5568,
      "step": 5115
    },
    {
      "epoch": 10.293762575452716,
      "grad_norm": 0.3857080042362213,
      "learning_rate": 0.00017943052620988028,
      "loss": 0.4936,
      "step": 5116
    },
    {
      "epoch": 10.295774647887324,
      "grad_norm": 0.3717896640300751,
      "learning_rate": 0.00017942650166012677,
      "loss": 0.5252,
      "step": 5117
    },
    {
      "epoch": 10.297786720321932,
      "grad_norm": 0.3851276636123657,
      "learning_rate": 0.00017942247711037328,
      "loss": 0.5775,
      "step": 5118
    },
    {
      "epoch": 10.299798792756539,
      "grad_norm": 0.38357412815093994,
      "learning_rate": 0.0001794184525606198,
      "loss": 0.5327,
      "step": 5119
    },
    {
      "epoch": 10.301810865191147,
      "grad_norm": 0.3750409781932831,
      "learning_rate": 0.0001794144280108663,
      "loss": 0.561,
      "step": 5120
    },
    {
      "epoch": 10.303822937625755,
      "grad_norm": 0.37869688868522644,
      "learning_rate": 0.0001794104034611128,
      "loss": 0.5434,
      "step": 5121
    },
    {
      "epoch": 10.305835010060362,
      "grad_norm": 0.43199047446250916,
      "learning_rate": 0.0001794063789113593,
      "loss": 0.5709,
      "step": 5122
    },
    {
      "epoch": 10.30784708249497,
      "grad_norm": 0.36985355615615845,
      "learning_rate": 0.0001794023543616058,
      "loss": 0.5533,
      "step": 5123
    },
    {
      "epoch": 10.309859154929578,
      "grad_norm": 0.3931595981121063,
      "learning_rate": 0.00017939832981185233,
      "loss": 0.5117,
      "step": 5124
    },
    {
      "epoch": 10.311871227364184,
      "grad_norm": 0.35882318019866943,
      "learning_rate": 0.0001793943052620988,
      "loss": 0.5115,
      "step": 5125
    },
    {
      "epoch": 10.313883299798793,
      "grad_norm": 0.39628133177757263,
      "learning_rate": 0.00017939028071234532,
      "loss": 0.562,
      "step": 5126
    },
    {
      "epoch": 10.3158953722334,
      "grad_norm": 0.3941851556301117,
      "learning_rate": 0.0001793862561625918,
      "loss": 0.5972,
      "step": 5127
    },
    {
      "epoch": 10.317907444668007,
      "grad_norm": 0.39430445432662964,
      "learning_rate": 0.00017938223161283832,
      "loss": 0.5433,
      "step": 5128
    },
    {
      "epoch": 10.319919517102615,
      "grad_norm": 0.39483097195625305,
      "learning_rate": 0.00017937820706308483,
      "loss": 0.537,
      "step": 5129
    },
    {
      "epoch": 10.321931589537224,
      "grad_norm": 0.4432125985622406,
      "learning_rate": 0.00017937418251333135,
      "loss": 0.5229,
      "step": 5130
    },
    {
      "epoch": 10.323943661971832,
      "grad_norm": 0.39843907952308655,
      "learning_rate": 0.00017937015796357783,
      "loss": 0.5243,
      "step": 5131
    },
    {
      "epoch": 10.325955734406438,
      "grad_norm": 0.381516695022583,
      "learning_rate": 0.00017936613341382434,
      "loss": 0.5191,
      "step": 5132
    },
    {
      "epoch": 10.327967806841047,
      "grad_norm": 0.38030171394348145,
      "learning_rate": 0.00017936210886407083,
      "loss": 0.5354,
      "step": 5133
    },
    {
      "epoch": 10.329979879275655,
      "grad_norm": 0.373609334230423,
      "learning_rate": 0.00017935808431431734,
      "loss": 0.5394,
      "step": 5134
    },
    {
      "epoch": 10.331991951710261,
      "grad_norm": 0.3881220817565918,
      "learning_rate": 0.00017935405976456385,
      "loss": 0.5569,
      "step": 5135
    },
    {
      "epoch": 10.33400402414487,
      "grad_norm": 0.38742393255233765,
      "learning_rate": 0.00017935003521481037,
      "loss": 0.5055,
      "step": 5136
    },
    {
      "epoch": 10.336016096579478,
      "grad_norm": 0.3855459690093994,
      "learning_rate": 0.00017934601066505685,
      "loss": 0.5082,
      "step": 5137
    },
    {
      "epoch": 10.338028169014084,
      "grad_norm": 0.4240454435348511,
      "learning_rate": 0.00017934198611530336,
      "loss": 0.5391,
      "step": 5138
    },
    {
      "epoch": 10.340040241448692,
      "grad_norm": 0.37807193398475647,
      "learning_rate": 0.00017933796156554988,
      "loss": 0.5548,
      "step": 5139
    },
    {
      "epoch": 10.3420523138833,
      "grad_norm": 0.38090944290161133,
      "learning_rate": 0.00017933393701579636,
      "loss": 0.502,
      "step": 5140
    },
    {
      "epoch": 10.344064386317907,
      "grad_norm": 0.37988364696502686,
      "learning_rate": 0.00017932991246604287,
      "loss": 0.5357,
      "step": 5141
    },
    {
      "epoch": 10.346076458752515,
      "grad_norm": 0.4057653844356537,
      "learning_rate": 0.00017932588791628936,
      "loss": 0.572,
      "step": 5142
    },
    {
      "epoch": 10.348088531187123,
      "grad_norm": 0.3959295153617859,
      "learning_rate": 0.00017932186336653587,
      "loss": 0.5181,
      "step": 5143
    },
    {
      "epoch": 10.35010060362173,
      "grad_norm": 0.37609541416168213,
      "learning_rate": 0.00017931783881678238,
      "loss": 0.5423,
      "step": 5144
    },
    {
      "epoch": 10.352112676056338,
      "grad_norm": 0.41172242164611816,
      "learning_rate": 0.0001793138142670289,
      "loss": 0.5235,
      "step": 5145
    },
    {
      "epoch": 10.354124748490946,
      "grad_norm": 0.37876078486442566,
      "learning_rate": 0.00017930978971727538,
      "loss": 0.5165,
      "step": 5146
    },
    {
      "epoch": 10.356136820925553,
      "grad_norm": 0.39177900552749634,
      "learning_rate": 0.0001793057651675219,
      "loss": 0.5323,
      "step": 5147
    },
    {
      "epoch": 10.35814889336016,
      "grad_norm": 0.4012252390384674,
      "learning_rate": 0.00017930174061776838,
      "loss": 0.5181,
      "step": 5148
    },
    {
      "epoch": 10.360160965794769,
      "grad_norm": 0.414200097322464,
      "learning_rate": 0.00017929771606801492,
      "loss": 0.5324,
      "step": 5149
    },
    {
      "epoch": 10.362173038229376,
      "grad_norm": 0.35902127623558044,
      "learning_rate": 0.0001792936915182614,
      "loss": 0.55,
      "step": 5150
    },
    {
      "epoch": 10.364185110663984,
      "grad_norm": 0.44749554991722107,
      "learning_rate": 0.0001792896669685079,
      "loss": 0.5402,
      "step": 5151
    },
    {
      "epoch": 10.366197183098592,
      "grad_norm": 0.3823908567428589,
      "learning_rate": 0.0001792856424187544,
      "loss": 0.5518,
      "step": 5152
    },
    {
      "epoch": 10.368209255533198,
      "grad_norm": 0.41691794991493225,
      "learning_rate": 0.0001792816178690009,
      "loss": 0.5339,
      "step": 5153
    },
    {
      "epoch": 10.370221327967807,
      "grad_norm": 0.37258008122444153,
      "learning_rate": 0.00017927759331924742,
      "loss": 0.5393,
      "step": 5154
    },
    {
      "epoch": 10.372233400402415,
      "grad_norm": 0.39129021763801575,
      "learning_rate": 0.00017927356876949394,
      "loss": 0.5342,
      "step": 5155
    },
    {
      "epoch": 10.374245472837021,
      "grad_norm": 0.36273789405822754,
      "learning_rate": 0.00017926954421974042,
      "loss": 0.5068,
      "step": 5156
    },
    {
      "epoch": 10.37625754527163,
      "grad_norm": 0.3924163579940796,
      "learning_rate": 0.00017926551966998693,
      "loss": 0.5207,
      "step": 5157
    },
    {
      "epoch": 10.378269617706238,
      "grad_norm": 0.3638525605201721,
      "learning_rate": 0.00017926149512023342,
      "loss": 0.547,
      "step": 5158
    },
    {
      "epoch": 10.380281690140846,
      "grad_norm": 0.3976166546344757,
      "learning_rate": 0.00017925747057047996,
      "loss": 0.5519,
      "step": 5159
    },
    {
      "epoch": 10.382293762575452,
      "grad_norm": 0.396986722946167,
      "learning_rate": 0.00017925344602072644,
      "loss": 0.5555,
      "step": 5160
    },
    {
      "epoch": 10.38430583501006,
      "grad_norm": 0.3652189373970032,
      "learning_rate": 0.00017924942147097295,
      "loss": 0.5426,
      "step": 5161
    },
    {
      "epoch": 10.386317907444669,
      "grad_norm": 0.4082029163837433,
      "learning_rate": 0.00017924539692121944,
      "loss": 0.5784,
      "step": 5162
    },
    {
      "epoch": 10.388329979879275,
      "grad_norm": 0.3673727810382843,
      "learning_rate": 0.00017924137237146595,
      "loss": 0.5465,
      "step": 5163
    },
    {
      "epoch": 10.390342052313883,
      "grad_norm": 0.38436076045036316,
      "learning_rate": 0.00017923734782171246,
      "loss": 0.5349,
      "step": 5164
    },
    {
      "epoch": 10.392354124748492,
      "grad_norm": 0.3761155605316162,
      "learning_rate": 0.00017923332327195898,
      "loss": 0.5381,
      "step": 5165
    },
    {
      "epoch": 10.394366197183098,
      "grad_norm": 0.37063276767730713,
      "learning_rate": 0.00017922929872220546,
      "loss": 0.5144,
      "step": 5166
    },
    {
      "epoch": 10.396378269617706,
      "grad_norm": 0.36359700560569763,
      "learning_rate": 0.00017922527417245197,
      "loss": 0.5418,
      "step": 5167
    },
    {
      "epoch": 10.398390342052314,
      "grad_norm": 0.3866249620914459,
      "learning_rate": 0.00017922124962269846,
      "loss": 0.5309,
      "step": 5168
    },
    {
      "epoch": 10.400402414486921,
      "grad_norm": 0.3967863917350769,
      "learning_rate": 0.00017921722507294497,
      "loss": 0.5168,
      "step": 5169
    },
    {
      "epoch": 10.40241448692153,
      "grad_norm": 0.39189016819000244,
      "learning_rate": 0.00017921320052319148,
      "loss": 0.5312,
      "step": 5170
    },
    {
      "epoch": 10.404426559356137,
      "grad_norm": 0.3771793246269226,
      "learning_rate": 0.000179209175973438,
      "loss": 0.5368,
      "step": 5171
    },
    {
      "epoch": 10.406438631790744,
      "grad_norm": 0.373006135225296,
      "learning_rate": 0.00017920515142368448,
      "loss": 0.5268,
      "step": 5172
    },
    {
      "epoch": 10.408450704225352,
      "grad_norm": 0.39459630846977234,
      "learning_rate": 0.000179201126873931,
      "loss": 0.5725,
      "step": 5173
    },
    {
      "epoch": 10.41046277665996,
      "grad_norm": 0.3752824068069458,
      "learning_rate": 0.0001791971023241775,
      "loss": 0.5327,
      "step": 5174
    },
    {
      "epoch": 10.412474849094567,
      "grad_norm": 0.36676791310310364,
      "learning_rate": 0.000179193077774424,
      "loss": 0.5156,
      "step": 5175
    },
    {
      "epoch": 10.414486921529175,
      "grad_norm": 0.3691016733646393,
      "learning_rate": 0.0001791890532246705,
      "loss": 0.5526,
      "step": 5176
    },
    {
      "epoch": 10.416498993963783,
      "grad_norm": 0.36394160985946655,
      "learning_rate": 0.000179185028674917,
      "loss": 0.5222,
      "step": 5177
    },
    {
      "epoch": 10.41851106639839,
      "grad_norm": 0.36705827713012695,
      "learning_rate": 0.0001791810041251635,
      "loss": 0.556,
      "step": 5178
    },
    {
      "epoch": 10.420523138832998,
      "grad_norm": 0.3754420876502991,
      "learning_rate": 0.00017917697957541,
      "loss": 0.5199,
      "step": 5179
    },
    {
      "epoch": 10.422535211267606,
      "grad_norm": 0.38105136156082153,
      "learning_rate": 0.00017917295502565652,
      "loss": 0.5267,
      "step": 5180
    },
    {
      "epoch": 10.424547283702214,
      "grad_norm": 0.3684186339378357,
      "learning_rate": 0.000179168930475903,
      "loss": 0.4983,
      "step": 5181
    },
    {
      "epoch": 10.42655935613682,
      "grad_norm": 0.3804228603839874,
      "learning_rate": 0.00017916490592614952,
      "loss": 0.5484,
      "step": 5182
    },
    {
      "epoch": 10.428571428571429,
      "grad_norm": 0.3877951502799988,
      "learning_rate": 0.000179160881376396,
      "loss": 0.5468,
      "step": 5183
    },
    {
      "epoch": 10.430583501006037,
      "grad_norm": 0.37391197681427,
      "learning_rate": 0.00017915685682664255,
      "loss": 0.582,
      "step": 5184
    },
    {
      "epoch": 10.432595573440643,
      "grad_norm": 0.4135783910751343,
      "learning_rate": 0.00017915283227688903,
      "loss": 0.5853,
      "step": 5185
    },
    {
      "epoch": 10.434607645875252,
      "grad_norm": 0.3631737530231476,
      "learning_rate": 0.00017914880772713554,
      "loss": 0.5154,
      "step": 5186
    },
    {
      "epoch": 10.43661971830986,
      "grad_norm": 0.380824476480484,
      "learning_rate": 0.00017914478317738203,
      "loss": 0.5312,
      "step": 5187
    },
    {
      "epoch": 10.438631790744466,
      "grad_norm": 0.3734966516494751,
      "learning_rate": 0.00017914075862762854,
      "loss": 0.5775,
      "step": 5188
    },
    {
      "epoch": 10.440643863179075,
      "grad_norm": 0.3662039339542389,
      "learning_rate": 0.00017913673407787505,
      "loss": 0.5127,
      "step": 5189
    },
    {
      "epoch": 10.442655935613683,
      "grad_norm": 0.3844768702983856,
      "learning_rate": 0.00017913270952812156,
      "loss": 0.5678,
      "step": 5190
    },
    {
      "epoch": 10.44466800804829,
      "grad_norm": 0.37716972827911377,
      "learning_rate": 0.00017912868497836805,
      "loss": 0.5222,
      "step": 5191
    },
    {
      "epoch": 10.446680080482897,
      "grad_norm": 0.386614590883255,
      "learning_rate": 0.00017912466042861456,
      "loss": 0.5489,
      "step": 5192
    },
    {
      "epoch": 10.448692152917506,
      "grad_norm": 0.4090859591960907,
      "learning_rate": 0.00017912063587886105,
      "loss": 0.5021,
      "step": 5193
    },
    {
      "epoch": 10.450704225352112,
      "grad_norm": 0.3766021728515625,
      "learning_rate": 0.00017911661132910759,
      "loss": 0.5549,
      "step": 5194
    },
    {
      "epoch": 10.45271629778672,
      "grad_norm": 0.37409666180610657,
      "learning_rate": 0.00017911258677935407,
      "loss": 0.5339,
      "step": 5195
    },
    {
      "epoch": 10.454728370221329,
      "grad_norm": 0.36714932322502136,
      "learning_rate": 0.00017910856222960058,
      "loss": 0.5644,
      "step": 5196
    },
    {
      "epoch": 10.456740442655935,
      "grad_norm": 0.4191083014011383,
      "learning_rate": 0.00017910453767984707,
      "loss": 0.5398,
      "step": 5197
    },
    {
      "epoch": 10.458752515090543,
      "grad_norm": 0.3786517083644867,
      "learning_rate": 0.00017910051313009358,
      "loss": 0.555,
      "step": 5198
    },
    {
      "epoch": 10.460764587525151,
      "grad_norm": 0.3723423480987549,
      "learning_rate": 0.0001790964885803401,
      "loss": 0.5916,
      "step": 5199
    },
    {
      "epoch": 10.462776659959758,
      "grad_norm": 0.3709535002708435,
      "learning_rate": 0.0001790924640305866,
      "loss": 0.5489,
      "step": 5200
    },
    {
      "epoch": 10.464788732394366,
      "grad_norm": 0.36407530307769775,
      "learning_rate": 0.0001790884394808331,
      "loss": 0.562,
      "step": 5201
    },
    {
      "epoch": 10.466800804828974,
      "grad_norm": 0.39499175548553467,
      "learning_rate": 0.0001790844149310796,
      "loss": 0.5687,
      "step": 5202
    },
    {
      "epoch": 10.46881287726358,
      "grad_norm": 0.39376863837242126,
      "learning_rate": 0.0001790803903813261,
      "loss": 0.5759,
      "step": 5203
    },
    {
      "epoch": 10.470824949698189,
      "grad_norm": 0.3771601915359497,
      "learning_rate": 0.0001790763658315726,
      "loss": 0.5368,
      "step": 5204
    },
    {
      "epoch": 10.472837022132797,
      "grad_norm": 0.3851224482059479,
      "learning_rate": 0.0001790723412818191,
      "loss": 0.5386,
      "step": 5205
    },
    {
      "epoch": 10.474849094567404,
      "grad_norm": 0.4091742932796478,
      "learning_rate": 0.0001790683167320656,
      "loss": 0.5811,
      "step": 5206
    },
    {
      "epoch": 10.476861167002012,
      "grad_norm": 0.37933772802352905,
      "learning_rate": 0.0001790642921823121,
      "loss": 0.5572,
      "step": 5207
    },
    {
      "epoch": 10.47887323943662,
      "grad_norm": 0.3696904480457306,
      "learning_rate": 0.00017906026763255862,
      "loss": 0.4921,
      "step": 5208
    },
    {
      "epoch": 10.480885311871228,
      "grad_norm": 0.39583784341812134,
      "learning_rate": 0.00017905624308280513,
      "loss": 0.529,
      "step": 5209
    },
    {
      "epoch": 10.482897384305835,
      "grad_norm": 0.3939161002635956,
      "learning_rate": 0.00017905221853305162,
      "loss": 0.5378,
      "step": 5210
    },
    {
      "epoch": 10.484909456740443,
      "grad_norm": 0.3757438361644745,
      "learning_rate": 0.00017904819398329813,
      "loss": 0.5599,
      "step": 5211
    },
    {
      "epoch": 10.486921529175051,
      "grad_norm": 0.3573918640613556,
      "learning_rate": 0.00017904416943354462,
      "loss": 0.5353,
      "step": 5212
    },
    {
      "epoch": 10.488933601609657,
      "grad_norm": 0.38239479064941406,
      "learning_rate": 0.00017904014488379113,
      "loss": 0.5263,
      "step": 5213
    },
    {
      "epoch": 10.490945674044266,
      "grad_norm": 0.3699627220630646,
      "learning_rate": 0.00017903612033403764,
      "loss": 0.5356,
      "step": 5214
    },
    {
      "epoch": 10.492957746478874,
      "grad_norm": 0.374271959066391,
      "learning_rate": 0.00017903209578428415,
      "loss": 0.5077,
      "step": 5215
    },
    {
      "epoch": 10.49496981891348,
      "grad_norm": 0.39939308166503906,
      "learning_rate": 0.00017902807123453064,
      "loss": 0.5472,
      "step": 5216
    },
    {
      "epoch": 10.496981891348089,
      "grad_norm": 0.37525802850723267,
      "learning_rate": 0.00017902404668477715,
      "loss": 0.5383,
      "step": 5217
    },
    {
      "epoch": 10.498993963782697,
      "grad_norm": 0.3767154812812805,
      "learning_rate": 0.00017902002213502364,
      "loss": 0.5193,
      "step": 5218
    },
    {
      "epoch": 10.501006036217303,
      "grad_norm": 0.39623770117759705,
      "learning_rate": 0.00017901599758527015,
      "loss": 0.5658,
      "step": 5219
    },
    {
      "epoch": 10.503018108651911,
      "grad_norm": 0.3719150125980377,
      "learning_rate": 0.00017901197303551666,
      "loss": 0.5448,
      "step": 5220
    },
    {
      "epoch": 10.50503018108652,
      "grad_norm": 0.364408940076828,
      "learning_rate": 0.00017900794848576317,
      "loss": 0.5451,
      "step": 5221
    },
    {
      "epoch": 10.507042253521126,
      "grad_norm": 0.39117926359176636,
      "learning_rate": 0.00017900392393600966,
      "loss": 0.5645,
      "step": 5222
    },
    {
      "epoch": 10.509054325955734,
      "grad_norm": 0.371702641248703,
      "learning_rate": 0.00017899989938625617,
      "loss": 0.5429,
      "step": 5223
    },
    {
      "epoch": 10.511066398390343,
      "grad_norm": 0.379570871591568,
      "learning_rate": 0.00017899587483650265,
      "loss": 0.5267,
      "step": 5224
    },
    {
      "epoch": 10.513078470824949,
      "grad_norm": 0.37642645835876465,
      "learning_rate": 0.0001789918502867492,
      "loss": 0.5171,
      "step": 5225
    },
    {
      "epoch": 10.515090543259557,
      "grad_norm": 0.3869745433330536,
      "learning_rate": 0.00017898782573699568,
      "loss": 0.5591,
      "step": 5226
    },
    {
      "epoch": 10.517102615694165,
      "grad_norm": 0.3603956699371338,
      "learning_rate": 0.0001789838011872422,
      "loss": 0.5302,
      "step": 5227
    },
    {
      "epoch": 10.519114688128772,
      "grad_norm": 0.3765111267566681,
      "learning_rate": 0.00017897977663748868,
      "loss": 0.5313,
      "step": 5228
    },
    {
      "epoch": 10.52112676056338,
      "grad_norm": 0.39977261424064636,
      "learning_rate": 0.0001789757520877352,
      "loss": 0.5692,
      "step": 5229
    },
    {
      "epoch": 10.523138832997988,
      "grad_norm": 0.36939308047294617,
      "learning_rate": 0.0001789717275379817,
      "loss": 0.5355,
      "step": 5230
    },
    {
      "epoch": 10.525150905432596,
      "grad_norm": 0.3955092430114746,
      "learning_rate": 0.0001789677029882282,
      "loss": 0.5951,
      "step": 5231
    },
    {
      "epoch": 10.527162977867203,
      "grad_norm": 0.37065234780311584,
      "learning_rate": 0.0001789636784384747,
      "loss": 0.5701,
      "step": 5232
    },
    {
      "epoch": 10.529175050301811,
      "grad_norm": 0.3810300827026367,
      "learning_rate": 0.0001789596538887212,
      "loss": 0.5199,
      "step": 5233
    },
    {
      "epoch": 10.53118712273642,
      "grad_norm": 0.3909320831298828,
      "learning_rate": 0.0001789556293389677,
      "loss": 0.533,
      "step": 5234
    },
    {
      "epoch": 10.533199195171026,
      "grad_norm": 0.3829079270362854,
      "learning_rate": 0.00017895160478921423,
      "loss": 0.5423,
      "step": 5235
    },
    {
      "epoch": 10.535211267605634,
      "grad_norm": 0.3647467792034149,
      "learning_rate": 0.00017894758023946072,
      "loss": 0.5426,
      "step": 5236
    },
    {
      "epoch": 10.537223340040242,
      "grad_norm": 0.384017676115036,
      "learning_rate": 0.00017894355568970723,
      "loss": 0.5653,
      "step": 5237
    },
    {
      "epoch": 10.539235412474849,
      "grad_norm": 0.3818098306655884,
      "learning_rate": 0.00017893953113995372,
      "loss": 0.5512,
      "step": 5238
    },
    {
      "epoch": 10.541247484909457,
      "grad_norm": 0.36231327056884766,
      "learning_rate": 0.00017893550659020023,
      "loss": 0.5181,
      "step": 5239
    },
    {
      "epoch": 10.543259557344065,
      "grad_norm": 0.3823174238204956,
      "learning_rate": 0.00017893148204044674,
      "loss": 0.5692,
      "step": 5240
    },
    {
      "epoch": 10.545271629778671,
      "grad_norm": 0.38088497519493103,
      "learning_rate": 0.00017892745749069323,
      "loss": 0.5393,
      "step": 5241
    },
    {
      "epoch": 10.54728370221328,
      "grad_norm": 0.375232458114624,
      "learning_rate": 0.00017892343294093974,
      "loss": 0.5177,
      "step": 5242
    },
    {
      "epoch": 10.549295774647888,
      "grad_norm": 0.3826887607574463,
      "learning_rate": 0.00017891940839118625,
      "loss": 0.5494,
      "step": 5243
    },
    {
      "epoch": 10.551307847082494,
      "grad_norm": 0.3752312660217285,
      "learning_rate": 0.00017891538384143274,
      "loss": 0.5254,
      "step": 5244
    },
    {
      "epoch": 10.553319919517103,
      "grad_norm": 0.4044961929321289,
      "learning_rate": 0.00017891135929167925,
      "loss": 0.5477,
      "step": 5245
    },
    {
      "epoch": 10.55533199195171,
      "grad_norm": 0.39147335290908813,
      "learning_rate": 0.00017890733474192576,
      "loss": 0.5306,
      "step": 5246
    },
    {
      "epoch": 10.557344064386317,
      "grad_norm": 0.3875245749950409,
      "learning_rate": 0.00017890331019217225,
      "loss": 0.5345,
      "step": 5247
    },
    {
      "epoch": 10.559356136820925,
      "grad_norm": 0.38697290420532227,
      "learning_rate": 0.00017889928564241876,
      "loss": 0.5523,
      "step": 5248
    },
    {
      "epoch": 10.561368209255534,
      "grad_norm": 0.36471185088157654,
      "learning_rate": 0.00017889526109266524,
      "loss": 0.5761,
      "step": 5249
    },
    {
      "epoch": 10.56338028169014,
      "grad_norm": 0.3551360070705414,
      "learning_rate": 0.00017889123654291178,
      "loss": 0.5724,
      "step": 5250
    },
    {
      "epoch": 10.565392354124748,
      "grad_norm": 0.3926118016242981,
      "learning_rate": 0.00017888721199315827,
      "loss": 0.5357,
      "step": 5251
    },
    {
      "epoch": 10.567404426559357,
      "grad_norm": 0.3674507439136505,
      "learning_rate": 0.00017888318744340478,
      "loss": 0.5369,
      "step": 5252
    },
    {
      "epoch": 10.569416498993963,
      "grad_norm": 0.3885609209537506,
      "learning_rate": 0.00017887916289365126,
      "loss": 0.5335,
      "step": 5253
    },
    {
      "epoch": 10.571428571428571,
      "grad_norm": 0.3890096843242645,
      "learning_rate": 0.00017887513834389778,
      "loss": 0.5714,
      "step": 5254
    },
    {
      "epoch": 10.57344064386318,
      "grad_norm": 0.3953584134578705,
      "learning_rate": 0.0001788711137941443,
      "loss": 0.4952,
      "step": 5255
    },
    {
      "epoch": 10.575452716297786,
      "grad_norm": 0.3890874981880188,
      "learning_rate": 0.0001788670892443908,
      "loss": 0.5919,
      "step": 5256
    },
    {
      "epoch": 10.577464788732394,
      "grad_norm": 0.38217684626579285,
      "learning_rate": 0.0001788630646946373,
      "loss": 0.5463,
      "step": 5257
    },
    {
      "epoch": 10.579476861167002,
      "grad_norm": 0.4388054609298706,
      "learning_rate": 0.0001788590401448838,
      "loss": 0.5769,
      "step": 5258
    },
    {
      "epoch": 10.58148893360161,
      "grad_norm": 0.3738771080970764,
      "learning_rate": 0.00017885501559513028,
      "loss": 0.5163,
      "step": 5259
    },
    {
      "epoch": 10.583501006036217,
      "grad_norm": 0.39805999398231506,
      "learning_rate": 0.00017885099104537682,
      "loss": 0.5267,
      "step": 5260
    },
    {
      "epoch": 10.585513078470825,
      "grad_norm": 0.3710290491580963,
      "learning_rate": 0.0001788469664956233,
      "loss": 0.5483,
      "step": 5261
    },
    {
      "epoch": 10.587525150905433,
      "grad_norm": 0.373264878988266,
      "learning_rate": 0.00017884294194586982,
      "loss": 0.5215,
      "step": 5262
    },
    {
      "epoch": 10.58953722334004,
      "grad_norm": 0.36278611421585083,
      "learning_rate": 0.0001788389173961163,
      "loss": 0.5497,
      "step": 5263
    },
    {
      "epoch": 10.591549295774648,
      "grad_norm": 0.3784922659397125,
      "learning_rate": 0.00017883489284636282,
      "loss": 0.5419,
      "step": 5264
    },
    {
      "epoch": 10.593561368209256,
      "grad_norm": 0.3688872456550598,
      "learning_rate": 0.00017883086829660933,
      "loss": 0.4929,
      "step": 5265
    },
    {
      "epoch": 10.595573440643863,
      "grad_norm": 0.4135358929634094,
      "learning_rate": 0.00017882684374685584,
      "loss": 0.5996,
      "step": 5266
    },
    {
      "epoch": 10.59758551307847,
      "grad_norm": 0.378157377243042,
      "learning_rate": 0.00017882281919710233,
      "loss": 0.5279,
      "step": 5267
    },
    {
      "epoch": 10.599597585513079,
      "grad_norm": 0.373279869556427,
      "learning_rate": 0.00017881879464734884,
      "loss": 0.5902,
      "step": 5268
    },
    {
      "epoch": 10.601609657947686,
      "grad_norm": 0.35784754157066345,
      "learning_rate": 0.00017881477009759532,
      "loss": 0.5289,
      "step": 5269
    },
    {
      "epoch": 10.603621730382294,
      "grad_norm": 0.37122029066085815,
      "learning_rate": 0.00017881074554784186,
      "loss": 0.5618,
      "step": 5270
    },
    {
      "epoch": 10.605633802816902,
      "grad_norm": 0.3513302505016327,
      "learning_rate": 0.00017880672099808835,
      "loss": 0.5509,
      "step": 5271
    },
    {
      "epoch": 10.607645875251508,
      "grad_norm": 0.37846893072128296,
      "learning_rate": 0.00017880269644833486,
      "loss": 0.5707,
      "step": 5272
    },
    {
      "epoch": 10.609657947686117,
      "grad_norm": 0.35621461272239685,
      "learning_rate": 0.00017879867189858135,
      "loss": 0.5236,
      "step": 5273
    },
    {
      "epoch": 10.611670020120725,
      "grad_norm": 0.38957086205482483,
      "learning_rate": 0.00017879464734882786,
      "loss": 0.5548,
      "step": 5274
    },
    {
      "epoch": 10.613682092555331,
      "grad_norm": 0.371653288602829,
      "learning_rate": 0.00017879062279907437,
      "loss": 0.5352,
      "step": 5275
    },
    {
      "epoch": 10.61569416498994,
      "grad_norm": 0.37162038683891296,
      "learning_rate": 0.00017878659824932086,
      "loss": 0.5448,
      "step": 5276
    },
    {
      "epoch": 10.617706237424548,
      "grad_norm": 0.38772863149642944,
      "learning_rate": 0.00017878257369956737,
      "loss": 0.5293,
      "step": 5277
    },
    {
      "epoch": 10.619718309859154,
      "grad_norm": 0.38796067237854004,
      "learning_rate": 0.00017877854914981388,
      "loss": 0.5444,
      "step": 5278
    },
    {
      "epoch": 10.621730382293762,
      "grad_norm": 0.3960300087928772,
      "learning_rate": 0.00017877452460006037,
      "loss": 0.5613,
      "step": 5279
    },
    {
      "epoch": 10.62374245472837,
      "grad_norm": 0.4032181203365326,
      "learning_rate": 0.00017877050005030688,
      "loss": 0.5696,
      "step": 5280
    },
    {
      "epoch": 10.625754527162979,
      "grad_norm": 0.3754449486732483,
      "learning_rate": 0.0001787664755005534,
      "loss": 0.5635,
      "step": 5281
    },
    {
      "epoch": 10.627766599597585,
      "grad_norm": 0.3620944619178772,
      "learning_rate": 0.00017876245095079988,
      "loss": 0.5157,
      "step": 5282
    },
    {
      "epoch": 10.629778672032193,
      "grad_norm": 0.377786785364151,
      "learning_rate": 0.0001787584264010464,
      "loss": 0.5112,
      "step": 5283
    },
    {
      "epoch": 10.631790744466802,
      "grad_norm": 0.36830198764801025,
      "learning_rate": 0.00017875440185129287,
      "loss": 0.5344,
      "step": 5284
    },
    {
      "epoch": 10.633802816901408,
      "grad_norm": 0.3864777684211731,
      "learning_rate": 0.0001787503773015394,
      "loss": 0.5346,
      "step": 5285
    },
    {
      "epoch": 10.635814889336016,
      "grad_norm": 0.3817148506641388,
      "learning_rate": 0.0001787463527517859,
      "loss": 0.5095,
      "step": 5286
    },
    {
      "epoch": 10.637826961770624,
      "grad_norm": 0.37746357917785645,
      "learning_rate": 0.0001787423282020324,
      "loss": 0.5365,
      "step": 5287
    },
    {
      "epoch": 10.639839034205231,
      "grad_norm": 0.38425517082214355,
      "learning_rate": 0.0001787383036522789,
      "loss": 0.5312,
      "step": 5288
    },
    {
      "epoch": 10.64185110663984,
      "grad_norm": 0.37046346068382263,
      "learning_rate": 0.0001787342791025254,
      "loss": 0.5335,
      "step": 5289
    },
    {
      "epoch": 10.643863179074447,
      "grad_norm": 0.38335663080215454,
      "learning_rate": 0.00017873025455277192,
      "loss": 0.5738,
      "step": 5290
    },
    {
      "epoch": 10.645875251509054,
      "grad_norm": 0.3793408274650574,
      "learning_rate": 0.00017872623000301843,
      "loss": 0.5446,
      "step": 5291
    },
    {
      "epoch": 10.647887323943662,
      "grad_norm": 0.3675108850002289,
      "learning_rate": 0.00017872220545326492,
      "loss": 0.5697,
      "step": 5292
    },
    {
      "epoch": 10.64989939637827,
      "grad_norm": 0.3691636323928833,
      "learning_rate": 0.00017871818090351143,
      "loss": 0.5503,
      "step": 5293
    },
    {
      "epoch": 10.651911468812877,
      "grad_norm": 0.37032896280288696,
      "learning_rate": 0.0001787141563537579,
      "loss": 0.5354,
      "step": 5294
    },
    {
      "epoch": 10.653923541247485,
      "grad_norm": 0.38785985112190247,
      "learning_rate": 0.00017871013180400445,
      "loss": 0.55,
      "step": 5295
    },
    {
      "epoch": 10.655935613682093,
      "grad_norm": 0.39137351512908936,
      "learning_rate": 0.00017870610725425094,
      "loss": 0.5213,
      "step": 5296
    },
    {
      "epoch": 10.6579476861167,
      "grad_norm": 0.36693158745765686,
      "learning_rate": 0.00017870208270449745,
      "loss": 0.5473,
      "step": 5297
    },
    {
      "epoch": 10.659959758551308,
      "grad_norm": 0.3908954858779907,
      "learning_rate": 0.00017869805815474394,
      "loss": 0.5827,
      "step": 5298
    },
    {
      "epoch": 10.661971830985916,
      "grad_norm": 0.39837661385536194,
      "learning_rate": 0.00017869403360499045,
      "loss": 0.5758,
      "step": 5299
    },
    {
      "epoch": 10.663983903420522,
      "grad_norm": 0.37476563453674316,
      "learning_rate": 0.00017869000905523696,
      "loss": 0.5396,
      "step": 5300
    },
    {
      "epoch": 10.66599597585513,
      "grad_norm": 0.3720041811466217,
      "learning_rate": 0.00017868598450548347,
      "loss": 0.5213,
      "step": 5301
    },
    {
      "epoch": 10.668008048289739,
      "grad_norm": 0.3711214065551758,
      "learning_rate": 0.00017868195995572996,
      "loss": 0.5568,
      "step": 5302
    },
    {
      "epoch": 10.670020120724345,
      "grad_norm": 0.37766242027282715,
      "learning_rate": 0.00017867793540597647,
      "loss": 0.5529,
      "step": 5303
    },
    {
      "epoch": 10.672032193158953,
      "grad_norm": 0.3706693649291992,
      "learning_rate": 0.00017867391085622295,
      "loss": 0.532,
      "step": 5304
    },
    {
      "epoch": 10.674044265593562,
      "grad_norm": 0.3823474943637848,
      "learning_rate": 0.0001786698863064695,
      "loss": 0.5624,
      "step": 5305
    },
    {
      "epoch": 10.676056338028168,
      "grad_norm": 0.3768455386161804,
      "learning_rate": 0.00017866586175671598,
      "loss": 0.551,
      "step": 5306
    },
    {
      "epoch": 10.678068410462776,
      "grad_norm": 0.3974255323410034,
      "learning_rate": 0.0001786618372069625,
      "loss": 0.558,
      "step": 5307
    },
    {
      "epoch": 10.680080482897385,
      "grad_norm": 0.3825826048851013,
      "learning_rate": 0.00017865781265720898,
      "loss": 0.5499,
      "step": 5308
    },
    {
      "epoch": 10.682092555331993,
      "grad_norm": 0.37208154797554016,
      "learning_rate": 0.0001786537881074555,
      "loss": 0.53,
      "step": 5309
    },
    {
      "epoch": 10.6841046277666,
      "grad_norm": 0.3791152536869049,
      "learning_rate": 0.000178649763557702,
      "loss": 0.5812,
      "step": 5310
    },
    {
      "epoch": 10.686116700201207,
      "grad_norm": 0.3589688539505005,
      "learning_rate": 0.00017864573900794849,
      "loss": 0.543,
      "step": 5311
    },
    {
      "epoch": 10.688128772635816,
      "grad_norm": 0.3597610890865326,
      "learning_rate": 0.000178641714458195,
      "loss": 0.5112,
      "step": 5312
    },
    {
      "epoch": 10.690140845070422,
      "grad_norm": 0.38640978932380676,
      "learning_rate": 0.0001786376899084415,
      "loss": 0.5213,
      "step": 5313
    },
    {
      "epoch": 10.69215291750503,
      "grad_norm": 0.3595409095287323,
      "learning_rate": 0.000178633665358688,
      "loss": 0.5314,
      "step": 5314
    },
    {
      "epoch": 10.694164989939638,
      "grad_norm": 0.365575909614563,
      "learning_rate": 0.0001786296408089345,
      "loss": 0.5416,
      "step": 5315
    },
    {
      "epoch": 10.696177062374245,
      "grad_norm": 0.3821105360984802,
      "learning_rate": 0.00017862561625918102,
      "loss": 0.5403,
      "step": 5316
    },
    {
      "epoch": 10.698189134808853,
      "grad_norm": 0.3892621397972107,
      "learning_rate": 0.0001786215917094275,
      "loss": 0.5058,
      "step": 5317
    },
    {
      "epoch": 10.700201207243461,
      "grad_norm": 0.3645532429218292,
      "learning_rate": 0.00017861756715967402,
      "loss": 0.5344,
      "step": 5318
    },
    {
      "epoch": 10.702213279678068,
      "grad_norm": 0.3699982464313507,
      "learning_rate": 0.0001786135426099205,
      "loss": 0.5465,
      "step": 5319
    },
    {
      "epoch": 10.704225352112676,
      "grad_norm": 0.3712610900402069,
      "learning_rate": 0.00017860951806016704,
      "loss": 0.5581,
      "step": 5320
    },
    {
      "epoch": 10.706237424547284,
      "grad_norm": 0.3857653737068176,
      "learning_rate": 0.00017860549351041353,
      "loss": 0.5481,
      "step": 5321
    },
    {
      "epoch": 10.70824949698189,
      "grad_norm": 0.3851621448993683,
      "learning_rate": 0.00017860146896066004,
      "loss": 0.5279,
      "step": 5322
    },
    {
      "epoch": 10.710261569416499,
      "grad_norm": 0.384893000125885,
      "learning_rate": 0.00017859744441090652,
      "loss": 0.5154,
      "step": 5323
    },
    {
      "epoch": 10.712273641851107,
      "grad_norm": 0.3796444535255432,
      "learning_rate": 0.00017859341986115304,
      "loss": 0.5763,
      "step": 5324
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 0.38781625032424927,
      "learning_rate": 0.00017858939531139955,
      "loss": 0.5266,
      "step": 5325
    },
    {
      "epoch": 10.716297786720322,
      "grad_norm": 0.3867841064929962,
      "learning_rate": 0.00017858537076164606,
      "loss": 0.5713,
      "step": 5326
    },
    {
      "epoch": 10.71830985915493,
      "grad_norm": 0.3920583724975586,
      "learning_rate": 0.00017858134621189255,
      "loss": 0.5478,
      "step": 5327
    },
    {
      "epoch": 10.720321931589538,
      "grad_norm": 0.37345561385154724,
      "learning_rate": 0.00017857732166213906,
      "loss": 0.5284,
      "step": 5328
    },
    {
      "epoch": 10.722334004024145,
      "grad_norm": 0.3810906708240509,
      "learning_rate": 0.00017857329711238554,
      "loss": 0.5588,
      "step": 5329
    },
    {
      "epoch": 10.724346076458753,
      "grad_norm": 0.37066295742988586,
      "learning_rate": 0.00017856927256263208,
      "loss": 0.5033,
      "step": 5330
    },
    {
      "epoch": 10.726358148893361,
      "grad_norm": 0.36515331268310547,
      "learning_rate": 0.00017856524801287857,
      "loss": 0.5171,
      "step": 5331
    },
    {
      "epoch": 10.728370221327967,
      "grad_norm": 0.3736633062362671,
      "learning_rate": 0.00017856122346312508,
      "loss": 0.5172,
      "step": 5332
    },
    {
      "epoch": 10.730382293762576,
      "grad_norm": 0.36450332403182983,
      "learning_rate": 0.00017855719891337156,
      "loss": 0.5297,
      "step": 5333
    },
    {
      "epoch": 10.732394366197184,
      "grad_norm": 0.3722134232521057,
      "learning_rate": 0.00017855317436361808,
      "loss": 0.5831,
      "step": 5334
    },
    {
      "epoch": 10.73440643863179,
      "grad_norm": 0.3785690367221832,
      "learning_rate": 0.0001785491498138646,
      "loss": 0.5526,
      "step": 5335
    },
    {
      "epoch": 10.736418511066399,
      "grad_norm": 0.37202197313308716,
      "learning_rate": 0.0001785451252641111,
      "loss": 0.5486,
      "step": 5336
    },
    {
      "epoch": 10.738430583501007,
      "grad_norm": 0.3724455237388611,
      "learning_rate": 0.00017854110071435759,
      "loss": 0.5659,
      "step": 5337
    },
    {
      "epoch": 10.740442655935613,
      "grad_norm": 0.3946499824523926,
      "learning_rate": 0.0001785370761646041,
      "loss": 0.5693,
      "step": 5338
    },
    {
      "epoch": 10.742454728370221,
      "grad_norm": 0.40008893609046936,
      "learning_rate": 0.00017853305161485058,
      "loss": 0.5556,
      "step": 5339
    },
    {
      "epoch": 10.74446680080483,
      "grad_norm": 0.3944317400455475,
      "learning_rate": 0.00017852902706509712,
      "loss": 0.5308,
      "step": 5340
    },
    {
      "epoch": 10.746478873239436,
      "grad_norm": 0.37990671396255493,
      "learning_rate": 0.0001785250025153436,
      "loss": 0.5561,
      "step": 5341
    },
    {
      "epoch": 10.748490945674044,
      "grad_norm": 0.38250502943992615,
      "learning_rate": 0.00017852097796559012,
      "loss": 0.5608,
      "step": 5342
    },
    {
      "epoch": 10.750503018108652,
      "grad_norm": 0.3838847279548645,
      "learning_rate": 0.0001785169534158366,
      "loss": 0.5687,
      "step": 5343
    },
    {
      "epoch": 10.752515090543259,
      "grad_norm": 0.3961229920387268,
      "learning_rate": 0.00017851292886608312,
      "loss": 0.5447,
      "step": 5344
    },
    {
      "epoch": 10.754527162977867,
      "grad_norm": 0.38294631242752075,
      "learning_rate": 0.00017850890431632963,
      "loss": 0.5385,
      "step": 5345
    },
    {
      "epoch": 10.756539235412475,
      "grad_norm": 0.37044844031333923,
      "learning_rate": 0.00017850487976657612,
      "loss": 0.5557,
      "step": 5346
    },
    {
      "epoch": 10.758551307847082,
      "grad_norm": 0.37509337067604065,
      "learning_rate": 0.00017850085521682263,
      "loss": 0.5781,
      "step": 5347
    },
    {
      "epoch": 10.76056338028169,
      "grad_norm": 0.37057387828826904,
      "learning_rate": 0.0001784968306670691,
      "loss": 0.5649,
      "step": 5348
    },
    {
      "epoch": 10.762575452716298,
      "grad_norm": 0.3914929926395416,
      "learning_rate": 0.00017849280611731562,
      "loss": 0.5524,
      "step": 5349
    },
    {
      "epoch": 10.764587525150905,
      "grad_norm": 0.3645429015159607,
      "learning_rate": 0.00017848878156756214,
      "loss": 0.5426,
      "step": 5350
    },
    {
      "epoch": 10.766599597585513,
      "grad_norm": 0.35292503237724304,
      "learning_rate": 0.00017848475701780865,
      "loss": 0.5421,
      "step": 5351
    },
    {
      "epoch": 10.768611670020121,
      "grad_norm": 0.3671368360519409,
      "learning_rate": 0.00017848073246805513,
      "loss": 0.5439,
      "step": 5352
    },
    {
      "epoch": 10.770623742454728,
      "grad_norm": 0.3650558590888977,
      "learning_rate": 0.00017847670791830165,
      "loss": 0.5378,
      "step": 5353
    },
    {
      "epoch": 10.772635814889336,
      "grad_norm": 0.4035244584083557,
      "learning_rate": 0.00017847268336854813,
      "loss": 0.5695,
      "step": 5354
    },
    {
      "epoch": 10.774647887323944,
      "grad_norm": 0.3791905641555786,
      "learning_rate": 0.00017846865881879467,
      "loss": 0.5292,
      "step": 5355
    },
    {
      "epoch": 10.77665995975855,
      "grad_norm": 0.38592779636383057,
      "learning_rate": 0.00017846463426904116,
      "loss": 0.5502,
      "step": 5356
    },
    {
      "epoch": 10.778672032193159,
      "grad_norm": 0.3832562267780304,
      "learning_rate": 0.00017846060971928767,
      "loss": 0.5205,
      "step": 5357
    },
    {
      "epoch": 10.780684104627767,
      "grad_norm": 0.4079044461250305,
      "learning_rate": 0.00017845658516953415,
      "loss": 0.5623,
      "step": 5358
    },
    {
      "epoch": 10.782696177062375,
      "grad_norm": 0.3679327666759491,
      "learning_rate": 0.00017845256061978067,
      "loss": 0.5464,
      "step": 5359
    },
    {
      "epoch": 10.784708249496981,
      "grad_norm": 0.3805278539657593,
      "learning_rate": 0.00017844853607002718,
      "loss": 0.5117,
      "step": 5360
    },
    {
      "epoch": 10.78672032193159,
      "grad_norm": 0.3789232671260834,
      "learning_rate": 0.0001784445115202737,
      "loss": 0.5491,
      "step": 5361
    },
    {
      "epoch": 10.788732394366198,
      "grad_norm": 0.3627757430076599,
      "learning_rate": 0.00017844048697052018,
      "loss": 0.5791,
      "step": 5362
    },
    {
      "epoch": 10.790744466800804,
      "grad_norm": 0.3738235831260681,
      "learning_rate": 0.0001784364624207667,
      "loss": 0.5823,
      "step": 5363
    },
    {
      "epoch": 10.792756539235413,
      "grad_norm": 0.38509708642959595,
      "learning_rate": 0.00017843243787101317,
      "loss": 0.5313,
      "step": 5364
    },
    {
      "epoch": 10.79476861167002,
      "grad_norm": 0.3772156238555908,
      "learning_rate": 0.0001784284133212597,
      "loss": 0.5602,
      "step": 5365
    },
    {
      "epoch": 10.796780684104627,
      "grad_norm": 0.4018229842185974,
      "learning_rate": 0.0001784243887715062,
      "loss": 0.5414,
      "step": 5366
    },
    {
      "epoch": 10.798792756539235,
      "grad_norm": 0.3624272644519806,
      "learning_rate": 0.0001784203642217527,
      "loss": 0.53,
      "step": 5367
    },
    {
      "epoch": 10.800804828973844,
      "grad_norm": 0.36487877368927,
      "learning_rate": 0.0001784163396719992,
      "loss": 0.5652,
      "step": 5368
    },
    {
      "epoch": 10.80281690140845,
      "grad_norm": 0.37158873677253723,
      "learning_rate": 0.0001784123151222457,
      "loss": 0.5577,
      "step": 5369
    },
    {
      "epoch": 10.804828973843058,
      "grad_norm": 0.358275443315506,
      "learning_rate": 0.00017840829057249222,
      "loss": 0.527,
      "step": 5370
    },
    {
      "epoch": 10.806841046277667,
      "grad_norm": 0.3731897175312042,
      "learning_rate": 0.00017840426602273873,
      "loss": 0.5911,
      "step": 5371
    },
    {
      "epoch": 10.808853118712273,
      "grad_norm": 0.3609794080257416,
      "learning_rate": 0.00017840024147298522,
      "loss": 0.5403,
      "step": 5372
    },
    {
      "epoch": 10.810865191146881,
      "grad_norm": 0.36284273862838745,
      "learning_rate": 0.00017839621692323173,
      "loss": 0.5961,
      "step": 5373
    },
    {
      "epoch": 10.81287726358149,
      "grad_norm": 0.3781346380710602,
      "learning_rate": 0.0001783921923734782,
      "loss": 0.5085,
      "step": 5374
    },
    {
      "epoch": 10.814889336016096,
      "grad_norm": 0.37711378931999207,
      "learning_rate": 0.00017838816782372473,
      "loss": 0.5777,
      "step": 5375
    },
    {
      "epoch": 10.816901408450704,
      "grad_norm": 0.36456429958343506,
      "learning_rate": 0.00017838414327397124,
      "loss": 0.5453,
      "step": 5376
    },
    {
      "epoch": 10.818913480885312,
      "grad_norm": 0.40027445554733276,
      "learning_rate": 0.00017838011872421775,
      "loss": 0.5626,
      "step": 5377
    },
    {
      "epoch": 10.82092555331992,
      "grad_norm": 0.36768633127212524,
      "learning_rate": 0.00017837609417446423,
      "loss": 0.5255,
      "step": 5378
    },
    {
      "epoch": 10.822937625754527,
      "grad_norm": 0.3579685389995575,
      "learning_rate": 0.00017837206962471075,
      "loss": 0.5294,
      "step": 5379
    },
    {
      "epoch": 10.824949698189135,
      "grad_norm": 0.3824232816696167,
      "learning_rate": 0.00017836804507495726,
      "loss": 0.5263,
      "step": 5380
    },
    {
      "epoch": 10.826961770623743,
      "grad_norm": 0.36974990367889404,
      "learning_rate": 0.00017836402052520374,
      "loss": 0.5465,
      "step": 5381
    },
    {
      "epoch": 10.82897384305835,
      "grad_norm": 0.4074777364730835,
      "learning_rate": 0.00017835999597545026,
      "loss": 0.5586,
      "step": 5382
    },
    {
      "epoch": 10.830985915492958,
      "grad_norm": 0.37779054045677185,
      "learning_rate": 0.00017835597142569674,
      "loss": 0.5374,
      "step": 5383
    },
    {
      "epoch": 10.832997987927566,
      "grad_norm": 0.37013283371925354,
      "learning_rate": 0.00017835194687594325,
      "loss": 0.5606,
      "step": 5384
    },
    {
      "epoch": 10.835010060362173,
      "grad_norm": 0.35898905992507935,
      "learning_rate": 0.00017834792232618977,
      "loss": 0.5266,
      "step": 5385
    },
    {
      "epoch": 10.83702213279678,
      "grad_norm": 0.3769676387310028,
      "learning_rate": 0.00017834389777643628,
      "loss": 0.5257,
      "step": 5386
    },
    {
      "epoch": 10.839034205231389,
      "grad_norm": 0.3722297251224518,
      "learning_rate": 0.00017833987322668276,
      "loss": 0.5348,
      "step": 5387
    },
    {
      "epoch": 10.841046277665995,
      "grad_norm": 0.36135968565940857,
      "learning_rate": 0.00017833584867692928,
      "loss": 0.5563,
      "step": 5388
    },
    {
      "epoch": 10.843058350100604,
      "grad_norm": 0.3815053403377533,
      "learning_rate": 0.00017833182412717576,
      "loss": 0.5481,
      "step": 5389
    },
    {
      "epoch": 10.845070422535212,
      "grad_norm": 0.37717095017433167,
      "learning_rate": 0.0001783277995774223,
      "loss": 0.5884,
      "step": 5390
    },
    {
      "epoch": 10.847082494969818,
      "grad_norm": 0.39054012298583984,
      "learning_rate": 0.00017832377502766879,
      "loss": 0.5444,
      "step": 5391
    },
    {
      "epoch": 10.849094567404427,
      "grad_norm": 0.38521602749824524,
      "learning_rate": 0.0001783197504779153,
      "loss": 0.5985,
      "step": 5392
    },
    {
      "epoch": 10.851106639839035,
      "grad_norm": 0.3741372227668762,
      "learning_rate": 0.00017831572592816178,
      "loss": 0.5385,
      "step": 5393
    },
    {
      "epoch": 10.853118712273641,
      "grad_norm": 0.3568776547908783,
      "learning_rate": 0.0001783117013784083,
      "loss": 0.5745,
      "step": 5394
    },
    {
      "epoch": 10.85513078470825,
      "grad_norm": 0.43212899565696716,
      "learning_rate": 0.0001783076768286548,
      "loss": 0.5495,
      "step": 5395
    },
    {
      "epoch": 10.857142857142858,
      "grad_norm": 0.364228755235672,
      "learning_rate": 0.00017830365227890132,
      "loss": 0.5616,
      "step": 5396
    },
    {
      "epoch": 10.859154929577464,
      "grad_norm": 0.3827361464500427,
      "learning_rate": 0.0001782996277291478,
      "loss": 0.5261,
      "step": 5397
    },
    {
      "epoch": 10.861167002012072,
      "grad_norm": 0.37875428795814514,
      "learning_rate": 0.00017829560317939432,
      "loss": 0.5588,
      "step": 5398
    },
    {
      "epoch": 10.86317907444668,
      "grad_norm": 0.350181519985199,
      "learning_rate": 0.0001782915786296408,
      "loss": 0.5381,
      "step": 5399
    },
    {
      "epoch": 10.865191146881287,
      "grad_norm": 0.36651960015296936,
      "learning_rate": 0.00017828755407988734,
      "loss": 0.5442,
      "step": 5400
    },
    {
      "epoch": 10.867203219315895,
      "grad_norm": 0.37644726037979126,
      "learning_rate": 0.00017828352953013383,
      "loss": 0.5482,
      "step": 5401
    },
    {
      "epoch": 10.869215291750503,
      "grad_norm": 0.3815857470035553,
      "learning_rate": 0.00017827950498038034,
      "loss": 0.5726,
      "step": 5402
    },
    {
      "epoch": 10.87122736418511,
      "grad_norm": 0.39969494938850403,
      "learning_rate": 0.00017827548043062682,
      "loss": 0.6056,
      "step": 5403
    },
    {
      "epoch": 10.873239436619718,
      "grad_norm": 0.3920467495918274,
      "learning_rate": 0.00017827145588087334,
      "loss": 0.5632,
      "step": 5404
    },
    {
      "epoch": 10.875251509054326,
      "grad_norm": 0.3777073323726654,
      "learning_rate": 0.00017826743133111985,
      "loss": 0.6061,
      "step": 5405
    },
    {
      "epoch": 10.877263581488933,
      "grad_norm": 0.3583329916000366,
      "learning_rate": 0.00017826340678136636,
      "loss": 0.5626,
      "step": 5406
    },
    {
      "epoch": 10.879275653923541,
      "grad_norm": 0.3617723286151886,
      "learning_rate": 0.00017825938223161285,
      "loss": 0.5495,
      "step": 5407
    },
    {
      "epoch": 10.88128772635815,
      "grad_norm": 0.3824663460254669,
      "learning_rate": 0.00017825535768185936,
      "loss": 0.5473,
      "step": 5408
    },
    {
      "epoch": 10.883299798792757,
      "grad_norm": 0.3963751196861267,
      "learning_rate": 0.00017825133313210584,
      "loss": 0.5481,
      "step": 5409
    },
    {
      "epoch": 10.885311871227364,
      "grad_norm": 0.36937499046325684,
      "learning_rate": 0.00017824730858235235,
      "loss": 0.5487,
      "step": 5410
    },
    {
      "epoch": 10.887323943661972,
      "grad_norm": 0.36849847435951233,
      "learning_rate": 0.00017824328403259887,
      "loss": 0.5459,
      "step": 5411
    },
    {
      "epoch": 10.88933601609658,
      "grad_norm": 0.39303654432296753,
      "learning_rate": 0.00017823925948284538,
      "loss": 0.588,
      "step": 5412
    },
    {
      "epoch": 10.891348088531187,
      "grad_norm": 0.37785205245018005,
      "learning_rate": 0.00017823523493309186,
      "loss": 0.553,
      "step": 5413
    },
    {
      "epoch": 10.893360160965795,
      "grad_norm": 0.39508306980133057,
      "learning_rate": 0.00017823121038333838,
      "loss": 0.5248,
      "step": 5414
    },
    {
      "epoch": 10.895372233400403,
      "grad_norm": 0.38702622056007385,
      "learning_rate": 0.0001782271858335849,
      "loss": 0.5355,
      "step": 5415
    },
    {
      "epoch": 10.89738430583501,
      "grad_norm": 0.3850308656692505,
      "learning_rate": 0.00017822316128383137,
      "loss": 0.5608,
      "step": 5416
    },
    {
      "epoch": 10.899396378269618,
      "grad_norm": 0.3760058581829071,
      "learning_rate": 0.00017821913673407789,
      "loss": 0.5792,
      "step": 5417
    },
    {
      "epoch": 10.901408450704226,
      "grad_norm": 0.3739783763885498,
      "learning_rate": 0.00017821511218432437,
      "loss": 0.5584,
      "step": 5418
    },
    {
      "epoch": 10.903420523138832,
      "grad_norm": 0.37395745515823364,
      "learning_rate": 0.00017821108763457088,
      "loss": 0.5296,
      "step": 5419
    },
    {
      "epoch": 10.90543259557344,
      "grad_norm": 0.35264852643013,
      "learning_rate": 0.0001782070630848174,
      "loss": 0.5303,
      "step": 5420
    },
    {
      "epoch": 10.907444668008049,
      "grad_norm": 0.378579705953598,
      "learning_rate": 0.0001782030385350639,
      "loss": 0.5376,
      "step": 5421
    },
    {
      "epoch": 10.909456740442655,
      "grad_norm": 0.3776262104511261,
      "learning_rate": 0.0001781990139853104,
      "loss": 0.5461,
      "step": 5422
    },
    {
      "epoch": 10.911468812877263,
      "grad_norm": 0.37825894355773926,
      "learning_rate": 0.0001781949894355569,
      "loss": 0.5587,
      "step": 5423
    },
    {
      "epoch": 10.913480885311872,
      "grad_norm": 0.38391467928886414,
      "learning_rate": 0.0001781909648858034,
      "loss": 0.5459,
      "step": 5424
    },
    {
      "epoch": 10.915492957746478,
      "grad_norm": 0.3981916904449463,
      "learning_rate": 0.00017818694033604993,
      "loss": 0.5968,
      "step": 5425
    },
    {
      "epoch": 10.917505030181086,
      "grad_norm": 0.376085489988327,
      "learning_rate": 0.00017818291578629641,
      "loss": 0.5749,
      "step": 5426
    },
    {
      "epoch": 10.919517102615695,
      "grad_norm": 0.3697701692581177,
      "learning_rate": 0.00017817889123654293,
      "loss": 0.5268,
      "step": 5427
    },
    {
      "epoch": 10.921529175050303,
      "grad_norm": 0.35561391711235046,
      "learning_rate": 0.0001781748666867894,
      "loss": 0.5609,
      "step": 5428
    },
    {
      "epoch": 10.92354124748491,
      "grad_norm": 0.3760216534137726,
      "learning_rate": 0.00017817084213703592,
      "loss": 0.5759,
      "step": 5429
    },
    {
      "epoch": 10.925553319919517,
      "grad_norm": 0.3906494379043579,
      "learning_rate": 0.00017816681758728244,
      "loss": 0.565,
      "step": 5430
    },
    {
      "epoch": 10.927565392354126,
      "grad_norm": 0.37748298048973083,
      "learning_rate": 0.00017816279303752895,
      "loss": 0.5886,
      "step": 5431
    },
    {
      "epoch": 10.929577464788732,
      "grad_norm": 0.38362884521484375,
      "learning_rate": 0.00017815876848777543,
      "loss": 0.5681,
      "step": 5432
    },
    {
      "epoch": 10.93158953722334,
      "grad_norm": 0.382114052772522,
      "learning_rate": 0.00017815474393802195,
      "loss": 0.5801,
      "step": 5433
    },
    {
      "epoch": 10.933601609657948,
      "grad_norm": 0.3830316364765167,
      "learning_rate": 0.00017815071938826843,
      "loss": 0.5794,
      "step": 5434
    },
    {
      "epoch": 10.935613682092555,
      "grad_norm": 0.3845027983188629,
      "learning_rate": 0.00017814669483851497,
      "loss": 0.5761,
      "step": 5435
    },
    {
      "epoch": 10.937625754527163,
      "grad_norm": 0.3638353645801544,
      "learning_rate": 0.00017814267028876146,
      "loss": 0.5199,
      "step": 5436
    },
    {
      "epoch": 10.939637826961771,
      "grad_norm": 0.3696017861366272,
      "learning_rate": 0.00017813864573900797,
      "loss": 0.5605,
      "step": 5437
    },
    {
      "epoch": 10.941649899396378,
      "grad_norm": 0.3766638934612274,
      "learning_rate": 0.00017813462118925445,
      "loss": 0.5835,
      "step": 5438
    },
    {
      "epoch": 10.943661971830986,
      "grad_norm": 0.3740488290786743,
      "learning_rate": 0.00017813059663950097,
      "loss": 0.6299,
      "step": 5439
    },
    {
      "epoch": 10.945674044265594,
      "grad_norm": 0.38523945212364197,
      "learning_rate": 0.00017812657208974748,
      "loss": 0.5426,
      "step": 5440
    },
    {
      "epoch": 10.9476861167002,
      "grad_norm": 0.36649224162101746,
      "learning_rate": 0.000178122547539994,
      "loss": 0.5368,
      "step": 5441
    },
    {
      "epoch": 10.949698189134809,
      "grad_norm": 0.3587144911289215,
      "learning_rate": 0.00017811852299024047,
      "loss": 0.5346,
      "step": 5442
    },
    {
      "epoch": 10.951710261569417,
      "grad_norm": 0.3871573805809021,
      "learning_rate": 0.000178114498440487,
      "loss": 0.5605,
      "step": 5443
    },
    {
      "epoch": 10.953722334004024,
      "grad_norm": 0.3848250210285187,
      "learning_rate": 0.00017811047389073347,
      "loss": 0.5843,
      "step": 5444
    },
    {
      "epoch": 10.955734406438632,
      "grad_norm": 0.38260045647621155,
      "learning_rate": 0.00017810644934097998,
      "loss": 0.5494,
      "step": 5445
    },
    {
      "epoch": 10.95774647887324,
      "grad_norm": 0.3819403052330017,
      "learning_rate": 0.0001781024247912265,
      "loss": 0.5765,
      "step": 5446
    },
    {
      "epoch": 10.959758551307846,
      "grad_norm": 0.38276904821395874,
      "learning_rate": 0.000178098400241473,
      "loss": 0.5578,
      "step": 5447
    },
    {
      "epoch": 10.961770623742455,
      "grad_norm": 0.37459856271743774,
      "learning_rate": 0.0001780943756917195,
      "loss": 0.5574,
      "step": 5448
    },
    {
      "epoch": 10.963782696177063,
      "grad_norm": 0.368297815322876,
      "learning_rate": 0.000178090351141966,
      "loss": 0.553,
      "step": 5449
    },
    {
      "epoch": 10.96579476861167,
      "grad_norm": 0.3576352298259735,
      "learning_rate": 0.00017808632659221252,
      "loss": 0.5343,
      "step": 5450
    },
    {
      "epoch": 10.967806841046277,
      "grad_norm": 0.3550668954849243,
      "learning_rate": 0.000178082302042459,
      "loss": 0.5404,
      "step": 5451
    },
    {
      "epoch": 10.969818913480886,
      "grad_norm": 0.36582332849502563,
      "learning_rate": 0.00017807827749270552,
      "loss": 0.5729,
      "step": 5452
    },
    {
      "epoch": 10.971830985915492,
      "grad_norm": 0.3637057840824127,
      "learning_rate": 0.000178074252942952,
      "loss": 0.5606,
      "step": 5453
    },
    {
      "epoch": 10.9738430583501,
      "grad_norm": 0.3939175605773926,
      "learning_rate": 0.0001780702283931985,
      "loss": 0.5483,
      "step": 5454
    },
    {
      "epoch": 10.975855130784709,
      "grad_norm": 0.37701278924942017,
      "learning_rate": 0.00017806620384344503,
      "loss": 0.5724,
      "step": 5455
    },
    {
      "epoch": 10.977867203219315,
      "grad_norm": 0.40079838037490845,
      "learning_rate": 0.00017806217929369154,
      "loss": 0.582,
      "step": 5456
    },
    {
      "epoch": 10.979879275653923,
      "grad_norm": 0.3769160807132721,
      "learning_rate": 0.00017805815474393802,
      "loss": 0.5554,
      "step": 5457
    },
    {
      "epoch": 10.981891348088531,
      "grad_norm": 0.36415234208106995,
      "learning_rate": 0.00017805413019418453,
      "loss": 0.5294,
      "step": 5458
    },
    {
      "epoch": 10.98390342052314,
      "grad_norm": 0.3592081367969513,
      "learning_rate": 0.00017805010564443102,
      "loss": 0.5238,
      "step": 5459
    },
    {
      "epoch": 10.985915492957746,
      "grad_norm": 0.3727228343486786,
      "learning_rate": 0.00017804608109467753,
      "loss": 0.5431,
      "step": 5460
    },
    {
      "epoch": 10.987927565392354,
      "grad_norm": 0.37962013483047485,
      "learning_rate": 0.00017804205654492404,
      "loss": 0.5762,
      "step": 5461
    },
    {
      "epoch": 10.989939637826962,
      "grad_norm": 0.3877531588077545,
      "learning_rate": 0.00017803803199517056,
      "loss": 0.5633,
      "step": 5462
    },
    {
      "epoch": 10.991951710261569,
      "grad_norm": 0.3545421063899994,
      "learning_rate": 0.00017803400744541704,
      "loss": 0.5382,
      "step": 5463
    },
    {
      "epoch": 10.993963782696177,
      "grad_norm": 0.3969998061656952,
      "learning_rate": 0.00017802998289566355,
      "loss": 0.5553,
      "step": 5464
    },
    {
      "epoch": 10.995975855130785,
      "grad_norm": 0.3643963634967804,
      "learning_rate": 0.00017802595834591004,
      "loss": 0.5418,
      "step": 5465
    },
    {
      "epoch": 10.997987927565392,
      "grad_norm": 0.3635111153125763,
      "learning_rate": 0.00017802193379615658,
      "loss": 0.5327,
      "step": 5466
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.3693694472312927,
      "learning_rate": 0.00017801790924640306,
      "loss": 0.5655,
      "step": 5467
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.7627135515213013,
      "eval_runtime": 49.8144,
      "eval_samples_per_second": 19.914,
      "eval_steps_per_second": 2.489,
      "step": 5467
    },
    {
      "epoch": 11.002012072434608,
      "grad_norm": 0.3882821202278137,
      "learning_rate": 0.00017801388469664958,
      "loss": 0.511,
      "step": 5468
    },
    {
      "epoch": 11.004024144869215,
      "grad_norm": 0.35205090045928955,
      "learning_rate": 0.00017800986014689606,
      "loss": 0.4802,
      "step": 5469
    },
    {
      "epoch": 11.006036217303823,
      "grad_norm": 0.37286800146102905,
      "learning_rate": 0.00017800583559714257,
      "loss": 0.4754,
      "step": 5470
    },
    {
      "epoch": 11.008048289738431,
      "grad_norm": 0.390724241733551,
      "learning_rate": 0.00017800181104738909,
      "loss": 0.4857,
      "step": 5471
    },
    {
      "epoch": 11.010060362173038,
      "grad_norm": 0.45184600353240967,
      "learning_rate": 0.0001779977864976356,
      "loss": 0.5166,
      "step": 5472
    },
    {
      "epoch": 11.012072434607646,
      "grad_norm": 0.4316669702529907,
      "learning_rate": 0.00017799376194788208,
      "loss": 0.4943,
      "step": 5473
    },
    {
      "epoch": 11.014084507042254,
      "grad_norm": 0.43231815099716187,
      "learning_rate": 0.0001779897373981286,
      "loss": 0.4485,
      "step": 5474
    },
    {
      "epoch": 11.01609657947686,
      "grad_norm": 0.39766690135002136,
      "learning_rate": 0.00017798571284837508,
      "loss": 0.4889,
      "step": 5475
    },
    {
      "epoch": 11.018108651911469,
      "grad_norm": 0.4020484387874603,
      "learning_rate": 0.00017798168829862162,
      "loss": 0.5141,
      "step": 5476
    },
    {
      "epoch": 11.020120724346077,
      "grad_norm": 0.3703417479991913,
      "learning_rate": 0.0001779776637488681,
      "loss": 0.4759,
      "step": 5477
    },
    {
      "epoch": 11.022132796780683,
      "grad_norm": 0.41017842292785645,
      "learning_rate": 0.00017797363919911462,
      "loss": 0.523,
      "step": 5478
    },
    {
      "epoch": 11.024144869215291,
      "grad_norm": 0.37548309564590454,
      "learning_rate": 0.0001779696146493611,
      "loss": 0.494,
      "step": 5479
    },
    {
      "epoch": 11.0261569416499,
      "grad_norm": 0.3861834704875946,
      "learning_rate": 0.00017796559009960761,
      "loss": 0.533,
      "step": 5480
    },
    {
      "epoch": 11.028169014084508,
      "grad_norm": 0.3996359705924988,
      "learning_rate": 0.00017796156554985413,
      "loss": 0.488,
      "step": 5481
    },
    {
      "epoch": 11.030181086519114,
      "grad_norm": 0.42407160997390747,
      "learning_rate": 0.00017795754100010064,
      "loss": 0.5188,
      "step": 5482
    },
    {
      "epoch": 11.032193158953723,
      "grad_norm": 0.39458656311035156,
      "learning_rate": 0.00017795351645034712,
      "loss": 0.4763,
      "step": 5483
    },
    {
      "epoch": 11.03420523138833,
      "grad_norm": 0.4147721230983734,
      "learning_rate": 0.00017794949190059364,
      "loss": 0.4849,
      "step": 5484
    },
    {
      "epoch": 11.036217303822937,
      "grad_norm": 0.38795778155326843,
      "learning_rate": 0.00017794546735084012,
      "loss": 0.4924,
      "step": 5485
    },
    {
      "epoch": 11.038229376257545,
      "grad_norm": 0.39546695351600647,
      "learning_rate": 0.00017794144280108663,
      "loss": 0.5093,
      "step": 5486
    },
    {
      "epoch": 11.040241448692154,
      "grad_norm": 0.3898472785949707,
      "learning_rate": 0.00017793741825133315,
      "loss": 0.5041,
      "step": 5487
    },
    {
      "epoch": 11.04225352112676,
      "grad_norm": 0.40764617919921875,
      "learning_rate": 0.00017793339370157963,
      "loss": 0.4998,
      "step": 5488
    },
    {
      "epoch": 11.044265593561368,
      "grad_norm": 0.41860637068748474,
      "learning_rate": 0.00017792936915182614,
      "loss": 0.4796,
      "step": 5489
    },
    {
      "epoch": 11.046277665995976,
      "grad_norm": 0.4025944769382477,
      "learning_rate": 0.00017792534460207265,
      "loss": 0.5119,
      "step": 5490
    },
    {
      "epoch": 11.048289738430583,
      "grad_norm": 0.40899330377578735,
      "learning_rate": 0.00017792132005231917,
      "loss": 0.4781,
      "step": 5491
    },
    {
      "epoch": 11.050301810865191,
      "grad_norm": 0.4055958390235901,
      "learning_rate": 0.00017791729550256565,
      "loss": 0.4609,
      "step": 5492
    },
    {
      "epoch": 11.0523138832998,
      "grad_norm": 0.4054317772388458,
      "learning_rate": 0.00017791327095281216,
      "loss": 0.4765,
      "step": 5493
    },
    {
      "epoch": 11.054325955734406,
      "grad_norm": 0.3898395299911499,
      "learning_rate": 0.00017790924640305865,
      "loss": 0.5017,
      "step": 5494
    },
    {
      "epoch": 11.056338028169014,
      "grad_norm": 0.39888837933540344,
      "learning_rate": 0.00017790522185330516,
      "loss": 0.4738,
      "step": 5495
    },
    {
      "epoch": 11.058350100603622,
      "grad_norm": 0.3773742616176605,
      "learning_rate": 0.00017790119730355167,
      "loss": 0.4971,
      "step": 5496
    },
    {
      "epoch": 11.060362173038229,
      "grad_norm": 0.40271851420402527,
      "learning_rate": 0.00017789717275379819,
      "loss": 0.4988,
      "step": 5497
    },
    {
      "epoch": 11.062374245472837,
      "grad_norm": 0.37767356634140015,
      "learning_rate": 0.00017789314820404467,
      "loss": 0.5079,
      "step": 5498
    },
    {
      "epoch": 11.064386317907445,
      "grad_norm": 0.4028191864490509,
      "learning_rate": 0.00017788912365429118,
      "loss": 0.5022,
      "step": 5499
    },
    {
      "epoch": 11.066398390342052,
      "grad_norm": 0.4172782897949219,
      "learning_rate": 0.00017788509910453767,
      "loss": 0.5503,
      "step": 5500
    },
    {
      "epoch": 11.06841046277666,
      "grad_norm": 0.38475868105888367,
      "learning_rate": 0.0001778810745547842,
      "loss": 0.5015,
      "step": 5501
    },
    {
      "epoch": 11.070422535211268,
      "grad_norm": 0.3779195547103882,
      "learning_rate": 0.0001778770500050307,
      "loss": 0.4593,
      "step": 5502
    },
    {
      "epoch": 11.072434607645874,
      "grad_norm": 0.4155726730823517,
      "learning_rate": 0.0001778730254552772,
      "loss": 0.5354,
      "step": 5503
    },
    {
      "epoch": 11.074446680080483,
      "grad_norm": 0.38572344183921814,
      "learning_rate": 0.0001778690009055237,
      "loss": 0.4961,
      "step": 5504
    },
    {
      "epoch": 11.07645875251509,
      "grad_norm": 0.396996408700943,
      "learning_rate": 0.0001778649763557702,
      "loss": 0.4969,
      "step": 5505
    },
    {
      "epoch": 11.078470824949699,
      "grad_norm": 0.3658992052078247,
      "learning_rate": 0.00017786095180601671,
      "loss": 0.4637,
      "step": 5506
    },
    {
      "epoch": 11.080482897384305,
      "grad_norm": 0.3649062216281891,
      "learning_rate": 0.00017785692725626323,
      "loss": 0.4836,
      "step": 5507
    },
    {
      "epoch": 11.082494969818914,
      "grad_norm": 0.3880539536476135,
      "learning_rate": 0.0001778529027065097,
      "loss": 0.4795,
      "step": 5508
    },
    {
      "epoch": 11.084507042253522,
      "grad_norm": 0.4067988097667694,
      "learning_rate": 0.00017784887815675622,
      "loss": 0.4827,
      "step": 5509
    },
    {
      "epoch": 11.086519114688128,
      "grad_norm": 0.3804799020290375,
      "learning_rate": 0.0001778448536070027,
      "loss": 0.4793,
      "step": 5510
    },
    {
      "epoch": 11.088531187122737,
      "grad_norm": 0.39136722683906555,
      "learning_rate": 0.00017784082905724925,
      "loss": 0.5088,
      "step": 5511
    },
    {
      "epoch": 11.090543259557345,
      "grad_norm": 0.3890623152256012,
      "learning_rate": 0.00017783680450749573,
      "loss": 0.5051,
      "step": 5512
    },
    {
      "epoch": 11.092555331991951,
      "grad_norm": 0.40455862879753113,
      "learning_rate": 0.00017783277995774225,
      "loss": 0.5423,
      "step": 5513
    },
    {
      "epoch": 11.09456740442656,
      "grad_norm": 0.40622198581695557,
      "learning_rate": 0.00017782875540798873,
      "loss": 0.5303,
      "step": 5514
    },
    {
      "epoch": 11.096579476861168,
      "grad_norm": 0.3777880072593689,
      "learning_rate": 0.00017782473085823524,
      "loss": 0.5097,
      "step": 5515
    },
    {
      "epoch": 11.098591549295774,
      "grad_norm": 0.4005480110645294,
      "learning_rate": 0.00017782070630848176,
      "loss": 0.5303,
      "step": 5516
    },
    {
      "epoch": 11.100603621730382,
      "grad_norm": 0.41411280632019043,
      "learning_rate": 0.00017781668175872827,
      "loss": 0.5091,
      "step": 5517
    },
    {
      "epoch": 11.10261569416499,
      "grad_norm": 0.37959444522857666,
      "learning_rate": 0.00017781265720897475,
      "loss": 0.4941,
      "step": 5518
    },
    {
      "epoch": 11.104627766599597,
      "grad_norm": 0.399902880191803,
      "learning_rate": 0.00017780863265922126,
      "loss": 0.4797,
      "step": 5519
    },
    {
      "epoch": 11.106639839034205,
      "grad_norm": 0.3971695005893707,
      "learning_rate": 0.00017780460810946775,
      "loss": 0.5201,
      "step": 5520
    },
    {
      "epoch": 11.108651911468813,
      "grad_norm": 0.4076932370662689,
      "learning_rate": 0.00017780058355971426,
      "loss": 0.516,
      "step": 5521
    },
    {
      "epoch": 11.11066398390342,
      "grad_norm": 0.40593206882476807,
      "learning_rate": 0.00017779655900996077,
      "loss": 0.484,
      "step": 5522
    },
    {
      "epoch": 11.112676056338028,
      "grad_norm": 0.3986119031906128,
      "learning_rate": 0.00017779253446020726,
      "loss": 0.5279,
      "step": 5523
    },
    {
      "epoch": 11.114688128772636,
      "grad_norm": 0.4094460904598236,
      "learning_rate": 0.00017778850991045377,
      "loss": 0.5012,
      "step": 5524
    },
    {
      "epoch": 11.116700201207243,
      "grad_norm": 0.38352668285369873,
      "learning_rate": 0.00017778448536070026,
      "loss": 0.4972,
      "step": 5525
    },
    {
      "epoch": 11.11871227364185,
      "grad_norm": 0.3719450831413269,
      "learning_rate": 0.0001777804608109468,
      "loss": 0.4862,
      "step": 5526
    },
    {
      "epoch": 11.120724346076459,
      "grad_norm": 0.37786853313446045,
      "learning_rate": 0.00017777643626119328,
      "loss": 0.4837,
      "step": 5527
    },
    {
      "epoch": 11.122736418511066,
      "grad_norm": 0.3767319917678833,
      "learning_rate": 0.0001777724117114398,
      "loss": 0.4905,
      "step": 5528
    },
    {
      "epoch": 11.124748490945674,
      "grad_norm": 0.39479178190231323,
      "learning_rate": 0.00017776838716168628,
      "loss": 0.4635,
      "step": 5529
    },
    {
      "epoch": 11.126760563380282,
      "grad_norm": 0.4584072530269623,
      "learning_rate": 0.0001777643626119328,
      "loss": 0.5162,
      "step": 5530
    },
    {
      "epoch": 11.12877263581489,
      "grad_norm": 0.39262354373931885,
      "learning_rate": 0.0001777603380621793,
      "loss": 0.4909,
      "step": 5531
    },
    {
      "epoch": 11.130784708249497,
      "grad_norm": 0.4127572178840637,
      "learning_rate": 0.00017775631351242582,
      "loss": 0.5137,
      "step": 5532
    },
    {
      "epoch": 11.132796780684105,
      "grad_norm": 0.40877434611320496,
      "learning_rate": 0.0001777522889626723,
      "loss": 0.5234,
      "step": 5533
    },
    {
      "epoch": 11.134808853118713,
      "grad_norm": 0.4102335572242737,
      "learning_rate": 0.0001777482644129188,
      "loss": 0.5133,
      "step": 5534
    },
    {
      "epoch": 11.13682092555332,
      "grad_norm": 0.4022766947746277,
      "learning_rate": 0.0001777442398631653,
      "loss": 0.5127,
      "step": 5535
    },
    {
      "epoch": 11.138832997987928,
      "grad_norm": 0.3947513997554779,
      "learning_rate": 0.00017774021531341184,
      "loss": 0.4799,
      "step": 5536
    },
    {
      "epoch": 11.140845070422536,
      "grad_norm": 0.3637709319591522,
      "learning_rate": 0.00017773619076365832,
      "loss": 0.4931,
      "step": 5537
    },
    {
      "epoch": 11.142857142857142,
      "grad_norm": 0.39134737849235535,
      "learning_rate": 0.00017773216621390483,
      "loss": 0.4669,
      "step": 5538
    },
    {
      "epoch": 11.14486921529175,
      "grad_norm": 0.42031633853912354,
      "learning_rate": 0.00017772814166415132,
      "loss": 0.5162,
      "step": 5539
    },
    {
      "epoch": 11.146881287726359,
      "grad_norm": 0.39742910861968994,
      "learning_rate": 0.00017772411711439783,
      "loss": 0.4983,
      "step": 5540
    },
    {
      "epoch": 11.148893360160965,
      "grad_norm": 0.4138835072517395,
      "learning_rate": 0.00017772009256464434,
      "loss": 0.5058,
      "step": 5541
    },
    {
      "epoch": 11.150905432595573,
      "grad_norm": 0.40294981002807617,
      "learning_rate": 0.00017771606801489086,
      "loss": 0.5211,
      "step": 5542
    },
    {
      "epoch": 11.152917505030182,
      "grad_norm": 0.39863321185112,
      "learning_rate": 0.00017771204346513734,
      "loss": 0.4904,
      "step": 5543
    },
    {
      "epoch": 11.154929577464788,
      "grad_norm": 0.4007723331451416,
      "learning_rate": 0.00017770801891538385,
      "loss": 0.5078,
      "step": 5544
    },
    {
      "epoch": 11.156941649899396,
      "grad_norm": 0.3795364797115326,
      "learning_rate": 0.00017770399436563034,
      "loss": 0.4768,
      "step": 5545
    },
    {
      "epoch": 11.158953722334005,
      "grad_norm": 0.3896039128303528,
      "learning_rate": 0.00017769996981587688,
      "loss": 0.4992,
      "step": 5546
    },
    {
      "epoch": 11.160965794768611,
      "grad_norm": 0.39549943804740906,
      "learning_rate": 0.00017769594526612336,
      "loss": 0.4787,
      "step": 5547
    },
    {
      "epoch": 11.16297786720322,
      "grad_norm": 0.41275572776794434,
      "learning_rate": 0.00017769192071636988,
      "loss": 0.5086,
      "step": 5548
    },
    {
      "epoch": 11.164989939637827,
      "grad_norm": 0.40384405851364136,
      "learning_rate": 0.00017768789616661636,
      "loss": 0.5397,
      "step": 5549
    },
    {
      "epoch": 11.167002012072434,
      "grad_norm": 0.41068997979164124,
      "learning_rate": 0.00017768387161686287,
      "loss": 0.4996,
      "step": 5550
    },
    {
      "epoch": 11.169014084507042,
      "grad_norm": 0.3908814787864685,
      "learning_rate": 0.00017767984706710938,
      "loss": 0.4918,
      "step": 5551
    },
    {
      "epoch": 11.17102615694165,
      "grad_norm": 0.40151578187942505,
      "learning_rate": 0.00017767582251735587,
      "loss": 0.4816,
      "step": 5552
    },
    {
      "epoch": 11.173038229376257,
      "grad_norm": 0.3925091028213501,
      "learning_rate": 0.00017767179796760238,
      "loss": 0.5076,
      "step": 5553
    },
    {
      "epoch": 11.175050301810865,
      "grad_norm": 0.40450993180274963,
      "learning_rate": 0.0001776677734178489,
      "loss": 0.4638,
      "step": 5554
    },
    {
      "epoch": 11.177062374245473,
      "grad_norm": 0.3891136646270752,
      "learning_rate": 0.00017766374886809538,
      "loss": 0.4792,
      "step": 5555
    },
    {
      "epoch": 11.179074446680081,
      "grad_norm": 0.4096796214580536,
      "learning_rate": 0.0001776597243183419,
      "loss": 0.5249,
      "step": 5556
    },
    {
      "epoch": 11.181086519114688,
      "grad_norm": 0.4056299328804016,
      "learning_rate": 0.0001776556997685884,
      "loss": 0.5304,
      "step": 5557
    },
    {
      "epoch": 11.183098591549296,
      "grad_norm": 0.39782780408859253,
      "learning_rate": 0.0001776516752188349,
      "loss": 0.4794,
      "step": 5558
    },
    {
      "epoch": 11.185110663983904,
      "grad_norm": 0.40843772888183594,
      "learning_rate": 0.0001776476506690814,
      "loss": 0.5188,
      "step": 5559
    },
    {
      "epoch": 11.18712273641851,
      "grad_norm": 0.40254077315330505,
      "learning_rate": 0.00017764362611932789,
      "loss": 0.5123,
      "step": 5560
    },
    {
      "epoch": 11.189134808853119,
      "grad_norm": 0.3886569142341614,
      "learning_rate": 0.00017763960156957443,
      "loss": 0.5064,
      "step": 5561
    },
    {
      "epoch": 11.191146881287727,
      "grad_norm": 0.41879841685295105,
      "learning_rate": 0.0001776355770198209,
      "loss": 0.5419,
      "step": 5562
    },
    {
      "epoch": 11.193158953722333,
      "grad_norm": 0.4139139652252197,
      "learning_rate": 0.00017763155247006742,
      "loss": 0.5317,
      "step": 5563
    },
    {
      "epoch": 11.195171026156942,
      "grad_norm": 0.3820750415325165,
      "learning_rate": 0.0001776275279203139,
      "loss": 0.4609,
      "step": 5564
    },
    {
      "epoch": 11.19718309859155,
      "grad_norm": 0.41272100806236267,
      "learning_rate": 0.00017762350337056042,
      "loss": 0.5059,
      "step": 5565
    },
    {
      "epoch": 11.199195171026156,
      "grad_norm": 0.4123654067516327,
      "learning_rate": 0.00017761947882080693,
      "loss": 0.4826,
      "step": 5566
    },
    {
      "epoch": 11.201207243460765,
      "grad_norm": 0.4155599772930145,
      "learning_rate": 0.00017761545427105344,
      "loss": 0.5148,
      "step": 5567
    },
    {
      "epoch": 11.203219315895373,
      "grad_norm": 0.39565977454185486,
      "learning_rate": 0.00017761142972129993,
      "loss": 0.5278,
      "step": 5568
    },
    {
      "epoch": 11.20523138832998,
      "grad_norm": 0.4038260877132416,
      "learning_rate": 0.00017760740517154644,
      "loss": 0.5094,
      "step": 5569
    },
    {
      "epoch": 11.207243460764587,
      "grad_norm": 0.4047389030456543,
      "learning_rate": 0.00017760338062179293,
      "loss": 0.481,
      "step": 5570
    },
    {
      "epoch": 11.209255533199196,
      "grad_norm": 0.4284015893936157,
      "learning_rate": 0.00017759935607203947,
      "loss": 0.5185,
      "step": 5571
    },
    {
      "epoch": 11.211267605633802,
      "grad_norm": 0.4084731638431549,
      "learning_rate": 0.00017759533152228595,
      "loss": 0.5412,
      "step": 5572
    },
    {
      "epoch": 11.21327967806841,
      "grad_norm": 0.4005679190158844,
      "learning_rate": 0.00017759130697253246,
      "loss": 0.5213,
      "step": 5573
    },
    {
      "epoch": 11.215291750503019,
      "grad_norm": 0.38805437088012695,
      "learning_rate": 0.00017758728242277895,
      "loss": 0.4921,
      "step": 5574
    },
    {
      "epoch": 11.217303822937625,
      "grad_norm": 0.404312402009964,
      "learning_rate": 0.00017758325787302546,
      "loss": 0.5101,
      "step": 5575
    },
    {
      "epoch": 11.219315895372233,
      "grad_norm": 0.4013444185256958,
      "learning_rate": 0.00017757923332327197,
      "loss": 0.5146,
      "step": 5576
    },
    {
      "epoch": 11.221327967806841,
      "grad_norm": 0.41042399406433105,
      "learning_rate": 0.00017757520877351849,
      "loss": 0.5268,
      "step": 5577
    },
    {
      "epoch": 11.223340040241448,
      "grad_norm": 0.40453851222991943,
      "learning_rate": 0.00017757118422376497,
      "loss": 0.5364,
      "step": 5578
    },
    {
      "epoch": 11.225352112676056,
      "grad_norm": 0.3881981670856476,
      "learning_rate": 0.00017756715967401148,
      "loss": 0.5247,
      "step": 5579
    },
    {
      "epoch": 11.227364185110664,
      "grad_norm": 0.39245039224624634,
      "learning_rate": 0.00017756313512425797,
      "loss": 0.5213,
      "step": 5580
    },
    {
      "epoch": 11.229376257545272,
      "grad_norm": 0.43567970395088196,
      "learning_rate": 0.0001775591105745045,
      "loss": 0.5342,
      "step": 5581
    },
    {
      "epoch": 11.231388329979879,
      "grad_norm": 0.39996418356895447,
      "learning_rate": 0.000177555086024751,
      "loss": 0.4827,
      "step": 5582
    },
    {
      "epoch": 11.233400402414487,
      "grad_norm": 0.3920159935951233,
      "learning_rate": 0.0001775510614749975,
      "loss": 0.539,
      "step": 5583
    },
    {
      "epoch": 11.235412474849095,
      "grad_norm": 0.39940863847732544,
      "learning_rate": 0.000177547036925244,
      "loss": 0.4601,
      "step": 5584
    },
    {
      "epoch": 11.237424547283702,
      "grad_norm": 0.4487103521823883,
      "learning_rate": 0.0001775430123754905,
      "loss": 0.5273,
      "step": 5585
    },
    {
      "epoch": 11.23943661971831,
      "grad_norm": 0.3935648500919342,
      "learning_rate": 0.00017753898782573701,
      "loss": 0.5443,
      "step": 5586
    },
    {
      "epoch": 11.241448692152918,
      "grad_norm": 0.4062318801879883,
      "learning_rate": 0.0001775349632759835,
      "loss": 0.4952,
      "step": 5587
    },
    {
      "epoch": 11.243460764587525,
      "grad_norm": 0.3870933949947357,
      "learning_rate": 0.00017753093872623,
      "loss": 0.5025,
      "step": 5588
    },
    {
      "epoch": 11.245472837022133,
      "grad_norm": 0.39927947521209717,
      "learning_rate": 0.00017752691417647652,
      "loss": 0.5241,
      "step": 5589
    },
    {
      "epoch": 11.247484909456741,
      "grad_norm": 0.39046594500541687,
      "learning_rate": 0.000177522889626723,
      "loss": 0.4975,
      "step": 5590
    },
    {
      "epoch": 11.249496981891348,
      "grad_norm": 0.40224695205688477,
      "learning_rate": 0.00017751886507696952,
      "loss": 0.5107,
      "step": 5591
    },
    {
      "epoch": 11.251509054325956,
      "grad_norm": 0.3819977045059204,
      "learning_rate": 0.00017751484052721603,
      "loss": 0.508,
      "step": 5592
    },
    {
      "epoch": 11.253521126760564,
      "grad_norm": 0.420408695936203,
      "learning_rate": 0.00017751081597746252,
      "loss": 0.5023,
      "step": 5593
    },
    {
      "epoch": 11.25553319919517,
      "grad_norm": 0.4205169081687927,
      "learning_rate": 0.00017750679142770903,
      "loss": 0.4852,
      "step": 5594
    },
    {
      "epoch": 11.257545271629779,
      "grad_norm": 0.41613104939460754,
      "learning_rate": 0.00017750276687795552,
      "loss": 0.4822,
      "step": 5595
    },
    {
      "epoch": 11.259557344064387,
      "grad_norm": 0.47805261611938477,
      "learning_rate": 0.00017749874232820206,
      "loss": 0.4458,
      "step": 5596
    },
    {
      "epoch": 11.261569416498993,
      "grad_norm": 0.4031245708465576,
      "learning_rate": 0.00017749471777844854,
      "loss": 0.5251,
      "step": 5597
    },
    {
      "epoch": 11.263581488933601,
      "grad_norm": 0.431408166885376,
      "learning_rate": 0.00017749069322869505,
      "loss": 0.5137,
      "step": 5598
    },
    {
      "epoch": 11.26559356136821,
      "grad_norm": 0.39804086089134216,
      "learning_rate": 0.00017748666867894154,
      "loss": 0.4881,
      "step": 5599
    },
    {
      "epoch": 11.267605633802816,
      "grad_norm": 0.4241875410079956,
      "learning_rate": 0.00017748264412918805,
      "loss": 0.5494,
      "step": 5600
    },
    {
      "epoch": 11.269617706237424,
      "grad_norm": 0.41597822308540344,
      "learning_rate": 0.00017747861957943456,
      "loss": 0.5359,
      "step": 5601
    },
    {
      "epoch": 11.271629778672033,
      "grad_norm": 0.4059118628501892,
      "learning_rate": 0.00017747459502968107,
      "loss": 0.5174,
      "step": 5602
    },
    {
      "epoch": 11.273641851106639,
      "grad_norm": 0.4195353388786316,
      "learning_rate": 0.00017747057047992756,
      "loss": 0.5242,
      "step": 5603
    },
    {
      "epoch": 11.275653923541247,
      "grad_norm": 0.41679254174232483,
      "learning_rate": 0.00017746654593017407,
      "loss": 0.4974,
      "step": 5604
    },
    {
      "epoch": 11.277665995975855,
      "grad_norm": 0.3941788077354431,
      "learning_rate": 0.00017746252138042056,
      "loss": 0.4877,
      "step": 5605
    },
    {
      "epoch": 11.279678068410464,
      "grad_norm": 0.38379231095314026,
      "learning_rate": 0.0001774584968306671,
      "loss": 0.4896,
      "step": 5606
    },
    {
      "epoch": 11.28169014084507,
      "grad_norm": 0.39973440766334534,
      "learning_rate": 0.00017745447228091358,
      "loss": 0.5348,
      "step": 5607
    },
    {
      "epoch": 11.283702213279678,
      "grad_norm": 0.4045165181159973,
      "learning_rate": 0.0001774504477311601,
      "loss": 0.5184,
      "step": 5608
    },
    {
      "epoch": 11.285714285714286,
      "grad_norm": 0.3897601366043091,
      "learning_rate": 0.00017744642318140658,
      "loss": 0.5038,
      "step": 5609
    },
    {
      "epoch": 11.287726358148893,
      "grad_norm": 0.45852378010749817,
      "learning_rate": 0.0001774423986316531,
      "loss": 0.4855,
      "step": 5610
    },
    {
      "epoch": 11.289738430583501,
      "grad_norm": 0.40503084659576416,
      "learning_rate": 0.0001774383740818996,
      "loss": 0.5093,
      "step": 5611
    },
    {
      "epoch": 11.29175050301811,
      "grad_norm": 0.4247318506240845,
      "learning_rate": 0.00017743434953214612,
      "loss": 0.5091,
      "step": 5612
    },
    {
      "epoch": 11.293762575452716,
      "grad_norm": 0.403168648481369,
      "learning_rate": 0.0001774303249823926,
      "loss": 0.5212,
      "step": 5613
    },
    {
      "epoch": 11.295774647887324,
      "grad_norm": 0.41935813426971436,
      "learning_rate": 0.0001774263004326391,
      "loss": 0.5589,
      "step": 5614
    },
    {
      "epoch": 11.297786720321932,
      "grad_norm": 0.41144874691963196,
      "learning_rate": 0.0001774222758828856,
      "loss": 0.5143,
      "step": 5615
    },
    {
      "epoch": 11.299798792756539,
      "grad_norm": 0.3820037841796875,
      "learning_rate": 0.00017741825133313214,
      "loss": 0.4917,
      "step": 5616
    },
    {
      "epoch": 11.301810865191147,
      "grad_norm": 0.40002790093421936,
      "learning_rate": 0.00017741422678337862,
      "loss": 0.5162,
      "step": 5617
    },
    {
      "epoch": 11.303822937625755,
      "grad_norm": 0.38703569769859314,
      "learning_rate": 0.00017741020223362513,
      "loss": 0.4844,
      "step": 5618
    },
    {
      "epoch": 11.305835010060362,
      "grad_norm": 0.3917253911495209,
      "learning_rate": 0.00017740617768387162,
      "loss": 0.4965,
      "step": 5619
    },
    {
      "epoch": 11.30784708249497,
      "grad_norm": 0.41169974207878113,
      "learning_rate": 0.00017740215313411813,
      "loss": 0.54,
      "step": 5620
    },
    {
      "epoch": 11.309859154929578,
      "grad_norm": 0.4075201153755188,
      "learning_rate": 0.00017739812858436464,
      "loss": 0.5401,
      "step": 5621
    },
    {
      "epoch": 11.311871227364184,
      "grad_norm": 0.4085231125354767,
      "learning_rate": 0.00017739410403461113,
      "loss": 0.5424,
      "step": 5622
    },
    {
      "epoch": 11.313883299798793,
      "grad_norm": 0.4070846438407898,
      "learning_rate": 0.00017739007948485764,
      "loss": 0.5458,
      "step": 5623
    },
    {
      "epoch": 11.3158953722334,
      "grad_norm": 0.38239341974258423,
      "learning_rate": 0.00017738605493510415,
      "loss": 0.5062,
      "step": 5624
    },
    {
      "epoch": 11.317907444668007,
      "grad_norm": 0.40331634879112244,
      "learning_rate": 0.00017738203038535064,
      "loss": 0.5417,
      "step": 5625
    },
    {
      "epoch": 11.319919517102615,
      "grad_norm": 0.4072747826576233,
      "learning_rate": 0.00017737800583559715,
      "loss": 0.5067,
      "step": 5626
    },
    {
      "epoch": 11.321931589537224,
      "grad_norm": 0.4142707884311676,
      "learning_rate": 0.00017737398128584366,
      "loss": 0.5634,
      "step": 5627
    },
    {
      "epoch": 11.323943661971832,
      "grad_norm": 0.37681934237480164,
      "learning_rate": 0.00017736995673609015,
      "loss": 0.4839,
      "step": 5628
    },
    {
      "epoch": 11.325955734406438,
      "grad_norm": 0.3999202251434326,
      "learning_rate": 0.00017736593218633666,
      "loss": 0.5083,
      "step": 5629
    },
    {
      "epoch": 11.327967806841047,
      "grad_norm": 0.4201420843601227,
      "learning_rate": 0.00017736190763658315,
      "loss": 0.5439,
      "step": 5630
    },
    {
      "epoch": 11.329979879275655,
      "grad_norm": 0.4009655714035034,
      "learning_rate": 0.00017735788308682968,
      "loss": 0.5164,
      "step": 5631
    },
    {
      "epoch": 11.331991951710261,
      "grad_norm": 0.40778812766075134,
      "learning_rate": 0.00017735385853707617,
      "loss": 0.5018,
      "step": 5632
    },
    {
      "epoch": 11.33400402414487,
      "grad_norm": 0.39296814799308777,
      "learning_rate": 0.00017734983398732268,
      "loss": 0.5095,
      "step": 5633
    },
    {
      "epoch": 11.336016096579478,
      "grad_norm": 0.41577088832855225,
      "learning_rate": 0.00017734580943756917,
      "loss": 0.5649,
      "step": 5634
    },
    {
      "epoch": 11.338028169014084,
      "grad_norm": 0.4056594967842102,
      "learning_rate": 0.00017734178488781568,
      "loss": 0.5204,
      "step": 5635
    },
    {
      "epoch": 11.340040241448692,
      "grad_norm": 0.40007248520851135,
      "learning_rate": 0.0001773377603380622,
      "loss": 0.5044,
      "step": 5636
    },
    {
      "epoch": 11.3420523138833,
      "grad_norm": 0.40100136399269104,
      "learning_rate": 0.0001773337357883087,
      "loss": 0.5367,
      "step": 5637
    },
    {
      "epoch": 11.344064386317907,
      "grad_norm": 0.39850226044654846,
      "learning_rate": 0.0001773297112385552,
      "loss": 0.5126,
      "step": 5638
    },
    {
      "epoch": 11.346076458752515,
      "grad_norm": 0.40295854210853577,
      "learning_rate": 0.0001773256866888017,
      "loss": 0.488,
      "step": 5639
    },
    {
      "epoch": 11.348088531187123,
      "grad_norm": 0.4103009104728699,
      "learning_rate": 0.00017732166213904819,
      "loss": 0.4977,
      "step": 5640
    },
    {
      "epoch": 11.35010060362173,
      "grad_norm": 0.42055100202560425,
      "learning_rate": 0.00017731763758929473,
      "loss": 0.4982,
      "step": 5641
    },
    {
      "epoch": 11.352112676056338,
      "grad_norm": 0.41315147280693054,
      "learning_rate": 0.0001773136130395412,
      "loss": 0.5254,
      "step": 5642
    },
    {
      "epoch": 11.354124748490946,
      "grad_norm": 0.39062467217445374,
      "learning_rate": 0.00017730958848978772,
      "loss": 0.5098,
      "step": 5643
    },
    {
      "epoch": 11.356136820925553,
      "grad_norm": 0.39988669753074646,
      "learning_rate": 0.0001773055639400342,
      "loss": 0.5313,
      "step": 5644
    },
    {
      "epoch": 11.35814889336016,
      "grad_norm": 0.3737028241157532,
      "learning_rate": 0.00017730153939028072,
      "loss": 0.4907,
      "step": 5645
    },
    {
      "epoch": 11.360160965794769,
      "grad_norm": 0.3896161913871765,
      "learning_rate": 0.00017729751484052723,
      "loss": 0.502,
      "step": 5646
    },
    {
      "epoch": 11.362173038229376,
      "grad_norm": 0.41374671459198,
      "learning_rate": 0.00017729349029077374,
      "loss": 0.5273,
      "step": 5647
    },
    {
      "epoch": 11.364185110663984,
      "grad_norm": 0.3807855248451233,
      "learning_rate": 0.00017728946574102023,
      "loss": 0.4882,
      "step": 5648
    },
    {
      "epoch": 11.366197183098592,
      "grad_norm": 0.42294594645500183,
      "learning_rate": 0.00017728544119126674,
      "loss": 0.4869,
      "step": 5649
    },
    {
      "epoch": 11.368209255533198,
      "grad_norm": 0.4037456214427948,
      "learning_rate": 0.00017728141664151323,
      "loss": 0.5095,
      "step": 5650
    },
    {
      "epoch": 11.370221327967807,
      "grad_norm": 0.39433640241622925,
      "learning_rate": 0.00017727739209175977,
      "loss": 0.497,
      "step": 5651
    },
    {
      "epoch": 11.372233400402415,
      "grad_norm": 0.44151216745376587,
      "learning_rate": 0.00017727336754200625,
      "loss": 0.5356,
      "step": 5652
    },
    {
      "epoch": 11.374245472837021,
      "grad_norm": 0.3978407382965088,
      "learning_rate": 0.00017726934299225276,
      "loss": 0.5189,
      "step": 5653
    },
    {
      "epoch": 11.37625754527163,
      "grad_norm": 0.4175505042076111,
      "learning_rate": 0.00017726531844249925,
      "loss": 0.5076,
      "step": 5654
    },
    {
      "epoch": 11.378269617706238,
      "grad_norm": 0.39811474084854126,
      "learning_rate": 0.00017726129389274576,
      "loss": 0.4792,
      "step": 5655
    },
    {
      "epoch": 11.380281690140846,
      "grad_norm": 0.3963182866573334,
      "learning_rate": 0.00017725726934299227,
      "loss": 0.5409,
      "step": 5656
    },
    {
      "epoch": 11.382293762575452,
      "grad_norm": 0.39392250776290894,
      "learning_rate": 0.00017725324479323876,
      "loss": 0.5047,
      "step": 5657
    },
    {
      "epoch": 11.38430583501006,
      "grad_norm": 0.40691667795181274,
      "learning_rate": 0.00017724922024348527,
      "loss": 0.4923,
      "step": 5658
    },
    {
      "epoch": 11.386317907444669,
      "grad_norm": 0.40705305337905884,
      "learning_rate": 0.00017724519569373178,
      "loss": 0.5618,
      "step": 5659
    },
    {
      "epoch": 11.388329979879275,
      "grad_norm": 0.40119507908821106,
      "learning_rate": 0.00017724117114397827,
      "loss": 0.4955,
      "step": 5660
    },
    {
      "epoch": 11.390342052313883,
      "grad_norm": 0.42266690731048584,
      "learning_rate": 0.00017723714659422478,
      "loss": 0.5295,
      "step": 5661
    },
    {
      "epoch": 11.392354124748492,
      "grad_norm": 0.4129962623119354,
      "learning_rate": 0.0001772331220444713,
      "loss": 0.5595,
      "step": 5662
    },
    {
      "epoch": 11.394366197183098,
      "grad_norm": 0.39797496795654297,
      "learning_rate": 0.00017722909749471778,
      "loss": 0.505,
      "step": 5663
    },
    {
      "epoch": 11.396378269617706,
      "grad_norm": 0.40474775433540344,
      "learning_rate": 0.0001772250729449643,
      "loss": 0.5611,
      "step": 5664
    },
    {
      "epoch": 11.398390342052314,
      "grad_norm": 0.38892862200737,
      "learning_rate": 0.00017722104839521077,
      "loss": 0.5068,
      "step": 5665
    },
    {
      "epoch": 11.400402414486921,
      "grad_norm": 0.3785519003868103,
      "learning_rate": 0.00017721702384545731,
      "loss": 0.4991,
      "step": 5666
    },
    {
      "epoch": 11.40241448692153,
      "grad_norm": 0.42895543575286865,
      "learning_rate": 0.0001772129992957038,
      "loss": 0.5236,
      "step": 5667
    },
    {
      "epoch": 11.404426559356137,
      "grad_norm": 0.4209968149662018,
      "learning_rate": 0.0001772089747459503,
      "loss": 0.4906,
      "step": 5668
    },
    {
      "epoch": 11.406438631790744,
      "grad_norm": 0.42568135261535645,
      "learning_rate": 0.0001772049501961968,
      "loss": 0.4962,
      "step": 5669
    },
    {
      "epoch": 11.408450704225352,
      "grad_norm": 0.43079817295074463,
      "learning_rate": 0.0001772009256464433,
      "loss": 0.5321,
      "step": 5670
    },
    {
      "epoch": 11.41046277665996,
      "grad_norm": 0.4075574278831482,
      "learning_rate": 0.00017719690109668982,
      "loss": 0.5322,
      "step": 5671
    },
    {
      "epoch": 11.412474849094567,
      "grad_norm": 0.43342125415802,
      "learning_rate": 0.00017719287654693633,
      "loss": 0.5423,
      "step": 5672
    },
    {
      "epoch": 11.414486921529175,
      "grad_norm": 0.4212069809436798,
      "learning_rate": 0.00017718885199718282,
      "loss": 0.5362,
      "step": 5673
    },
    {
      "epoch": 11.416498993963783,
      "grad_norm": 0.3913751542568207,
      "learning_rate": 0.00017718482744742933,
      "loss": 0.5321,
      "step": 5674
    },
    {
      "epoch": 11.41851106639839,
      "grad_norm": 0.40425100922584534,
      "learning_rate": 0.00017718080289767582,
      "loss": 0.5289,
      "step": 5675
    },
    {
      "epoch": 11.420523138832998,
      "grad_norm": 0.40399378538131714,
      "learning_rate": 0.00017717677834792235,
      "loss": 0.5127,
      "step": 5676
    },
    {
      "epoch": 11.422535211267606,
      "grad_norm": 0.4099106192588806,
      "learning_rate": 0.00017717275379816884,
      "loss": 0.5324,
      "step": 5677
    },
    {
      "epoch": 11.424547283702214,
      "grad_norm": 0.39617085456848145,
      "learning_rate": 0.00017716872924841535,
      "loss": 0.5359,
      "step": 5678
    },
    {
      "epoch": 11.42655935613682,
      "grad_norm": 0.3937859535217285,
      "learning_rate": 0.00017716470469866184,
      "loss": 0.4856,
      "step": 5679
    },
    {
      "epoch": 11.428571428571429,
      "grad_norm": 0.4104446470737457,
      "learning_rate": 0.00017716068014890835,
      "loss": 0.5326,
      "step": 5680
    },
    {
      "epoch": 11.430583501006037,
      "grad_norm": 0.4053756892681122,
      "learning_rate": 0.00017715665559915486,
      "loss": 0.5287,
      "step": 5681
    },
    {
      "epoch": 11.432595573440643,
      "grad_norm": 0.3998771607875824,
      "learning_rate": 0.00017715263104940137,
      "loss": 0.5277,
      "step": 5682
    },
    {
      "epoch": 11.434607645875252,
      "grad_norm": 0.41530197858810425,
      "learning_rate": 0.00017714860649964786,
      "loss": 0.5232,
      "step": 5683
    },
    {
      "epoch": 11.43661971830986,
      "grad_norm": 0.3935958743095398,
      "learning_rate": 0.00017714458194989437,
      "loss": 0.4801,
      "step": 5684
    },
    {
      "epoch": 11.438631790744466,
      "grad_norm": 0.3990984559059143,
      "learning_rate": 0.00017714055740014086,
      "loss": 0.5147,
      "step": 5685
    },
    {
      "epoch": 11.440643863179075,
      "grad_norm": 0.39965513348579407,
      "learning_rate": 0.0001771365328503874,
      "loss": 0.5352,
      "step": 5686
    },
    {
      "epoch": 11.442655935613683,
      "grad_norm": 0.3959588408470154,
      "learning_rate": 0.00017713250830063388,
      "loss": 0.4988,
      "step": 5687
    },
    {
      "epoch": 11.44466800804829,
      "grad_norm": 0.4147956073284149,
      "learning_rate": 0.0001771284837508804,
      "loss": 0.5737,
      "step": 5688
    },
    {
      "epoch": 11.446680080482897,
      "grad_norm": 0.38138076663017273,
      "learning_rate": 0.00017712445920112688,
      "loss": 0.5349,
      "step": 5689
    },
    {
      "epoch": 11.448692152917506,
      "grad_norm": 0.4080962538719177,
      "learning_rate": 0.0001771204346513734,
      "loss": 0.5236,
      "step": 5690
    },
    {
      "epoch": 11.450704225352112,
      "grad_norm": 0.40486282110214233,
      "learning_rate": 0.0001771164101016199,
      "loss": 0.4851,
      "step": 5691
    },
    {
      "epoch": 11.45271629778672,
      "grad_norm": 0.3842090964317322,
      "learning_rate": 0.0001771123855518664,
      "loss": 0.4838,
      "step": 5692
    },
    {
      "epoch": 11.454728370221329,
      "grad_norm": 0.4168688952922821,
      "learning_rate": 0.0001771083610021129,
      "loss": 0.5239,
      "step": 5693
    },
    {
      "epoch": 11.456740442655935,
      "grad_norm": 0.3860352635383606,
      "learning_rate": 0.00017710433645235938,
      "loss": 0.5051,
      "step": 5694
    },
    {
      "epoch": 11.458752515090543,
      "grad_norm": 0.41056400537490845,
      "learning_rate": 0.0001771003119026059,
      "loss": 0.5483,
      "step": 5695
    },
    {
      "epoch": 11.460764587525151,
      "grad_norm": 0.41192784905433655,
      "learning_rate": 0.0001770962873528524,
      "loss": 0.5098,
      "step": 5696
    },
    {
      "epoch": 11.462776659959758,
      "grad_norm": 0.41066887974739075,
      "learning_rate": 0.00017709226280309892,
      "loss": 0.5423,
      "step": 5697
    },
    {
      "epoch": 11.464788732394366,
      "grad_norm": 0.38649046421051025,
      "learning_rate": 0.0001770882382533454,
      "loss": 0.4621,
      "step": 5698
    },
    {
      "epoch": 11.466800804828974,
      "grad_norm": 0.4281725585460663,
      "learning_rate": 0.00017708421370359192,
      "loss": 0.4988,
      "step": 5699
    },
    {
      "epoch": 11.46881287726358,
      "grad_norm": 0.415339857339859,
      "learning_rate": 0.0001770801891538384,
      "loss": 0.549,
      "step": 5700
    },
    {
      "epoch": 11.470824949698189,
      "grad_norm": 0.4071355164051056,
      "learning_rate": 0.00017707616460408494,
      "loss": 0.5066,
      "step": 5701
    },
    {
      "epoch": 11.472837022132797,
      "grad_norm": 0.4003373086452484,
      "learning_rate": 0.00017707214005433143,
      "loss": 0.543,
      "step": 5702
    },
    {
      "epoch": 11.474849094567404,
      "grad_norm": 0.4036605656147003,
      "learning_rate": 0.00017706811550457794,
      "loss": 0.5324,
      "step": 5703
    },
    {
      "epoch": 11.476861167002012,
      "grad_norm": 0.3974805474281311,
      "learning_rate": 0.00017706409095482443,
      "loss": 0.5183,
      "step": 5704
    },
    {
      "epoch": 11.47887323943662,
      "grad_norm": 0.401635080575943,
      "learning_rate": 0.00017706006640507094,
      "loss": 0.5339,
      "step": 5705
    },
    {
      "epoch": 11.480885311871228,
      "grad_norm": 0.4138098955154419,
      "learning_rate": 0.00017705604185531742,
      "loss": 0.5248,
      "step": 5706
    },
    {
      "epoch": 11.482897384305835,
      "grad_norm": 0.4034479856491089,
      "learning_rate": 0.00017705201730556396,
      "loss": 0.5239,
      "step": 5707
    },
    {
      "epoch": 11.484909456740443,
      "grad_norm": 0.42019474506378174,
      "learning_rate": 0.00017704799275581045,
      "loss": 0.5664,
      "step": 5708
    },
    {
      "epoch": 11.486921529175051,
      "grad_norm": 0.3822663426399231,
      "learning_rate": 0.00017704396820605696,
      "loss": 0.5197,
      "step": 5709
    },
    {
      "epoch": 11.488933601609657,
      "grad_norm": 0.46953725814819336,
      "learning_rate": 0.00017703994365630344,
      "loss": 0.5427,
      "step": 5710
    },
    {
      "epoch": 11.490945674044266,
      "grad_norm": 0.42544105648994446,
      "learning_rate": 0.00017703591910654996,
      "loss": 0.5117,
      "step": 5711
    },
    {
      "epoch": 11.492957746478874,
      "grad_norm": 0.4160463809967041,
      "learning_rate": 0.00017703189455679647,
      "loss": 0.5483,
      "step": 5712
    },
    {
      "epoch": 11.49496981891348,
      "grad_norm": 0.4052826166152954,
      "learning_rate": 0.00017702787000704298,
      "loss": 0.5268,
      "step": 5713
    },
    {
      "epoch": 11.496981891348089,
      "grad_norm": 0.4044497609138489,
      "learning_rate": 0.00017702384545728947,
      "loss": 0.5347,
      "step": 5714
    },
    {
      "epoch": 11.498993963782697,
      "grad_norm": 0.4094986915588379,
      "learning_rate": 0.00017701982090753598,
      "loss": 0.5217,
      "step": 5715
    },
    {
      "epoch": 11.501006036217303,
      "grad_norm": 0.4332904517650604,
      "learning_rate": 0.00017701579635778246,
      "loss": 0.5421,
      "step": 5716
    },
    {
      "epoch": 11.503018108651911,
      "grad_norm": 0.4268752634525299,
      "learning_rate": 0.000177011771808029,
      "loss": 0.5159,
      "step": 5717
    },
    {
      "epoch": 11.50503018108652,
      "grad_norm": 0.41692063212394714,
      "learning_rate": 0.0001770077472582755,
      "loss": 0.5003,
      "step": 5718
    },
    {
      "epoch": 11.507042253521126,
      "grad_norm": 0.3997920751571655,
      "learning_rate": 0.000177003722708522,
      "loss": 0.529,
      "step": 5719
    },
    {
      "epoch": 11.509054325955734,
      "grad_norm": 0.38878923654556274,
      "learning_rate": 0.00017699969815876849,
      "loss": 0.5244,
      "step": 5720
    },
    {
      "epoch": 11.511066398390343,
      "grad_norm": 0.43208882212638855,
      "learning_rate": 0.000176995673609015,
      "loss": 0.5245,
      "step": 5721
    },
    {
      "epoch": 11.513078470824949,
      "grad_norm": 0.4193200170993805,
      "learning_rate": 0.0001769916490592615,
      "loss": 0.5458,
      "step": 5722
    },
    {
      "epoch": 11.515090543259557,
      "grad_norm": 0.411127507686615,
      "learning_rate": 0.00017698762450950802,
      "loss": 0.527,
      "step": 5723
    },
    {
      "epoch": 11.517102615694165,
      "grad_norm": 0.4389243423938751,
      "learning_rate": 0.0001769835999597545,
      "loss": 0.5002,
      "step": 5724
    },
    {
      "epoch": 11.519114688128772,
      "grad_norm": 0.3937523066997528,
      "learning_rate": 0.00017697957541000102,
      "loss": 0.4845,
      "step": 5725
    },
    {
      "epoch": 11.52112676056338,
      "grad_norm": 0.3752215504646301,
      "learning_rate": 0.0001769755508602475,
      "loss": 0.5052,
      "step": 5726
    },
    {
      "epoch": 11.523138832997988,
      "grad_norm": 0.402534157037735,
      "learning_rate": 0.00017697152631049402,
      "loss": 0.5161,
      "step": 5727
    },
    {
      "epoch": 11.525150905432596,
      "grad_norm": 0.40429407358169556,
      "learning_rate": 0.00017696750176074053,
      "loss": 0.5697,
      "step": 5728
    },
    {
      "epoch": 11.527162977867203,
      "grad_norm": 0.393199622631073,
      "learning_rate": 0.00017696347721098701,
      "loss": 0.5282,
      "step": 5729
    },
    {
      "epoch": 11.529175050301811,
      "grad_norm": 0.39197781682014465,
      "learning_rate": 0.00017695945266123353,
      "loss": 0.5303,
      "step": 5730
    },
    {
      "epoch": 11.53118712273642,
      "grad_norm": 0.42186209559440613,
      "learning_rate": 0.00017695542811148004,
      "loss": 0.5192,
      "step": 5731
    },
    {
      "epoch": 11.533199195171026,
      "grad_norm": 0.38462579250335693,
      "learning_rate": 0.00017695140356172655,
      "loss": 0.4619,
      "step": 5732
    },
    {
      "epoch": 11.535211267605634,
      "grad_norm": 0.4150905907154083,
      "learning_rate": 0.00017694737901197304,
      "loss": 0.5346,
      "step": 5733
    },
    {
      "epoch": 11.537223340040242,
      "grad_norm": 0.38509401679039,
      "learning_rate": 0.00017694335446221955,
      "loss": 0.5222,
      "step": 5734
    },
    {
      "epoch": 11.539235412474849,
      "grad_norm": 0.4027002453804016,
      "learning_rate": 0.00017693932991246603,
      "loss": 0.5571,
      "step": 5735
    },
    {
      "epoch": 11.541247484909457,
      "grad_norm": 0.37257716059684753,
      "learning_rate": 0.00017693530536271255,
      "loss": 0.4909,
      "step": 5736
    },
    {
      "epoch": 11.543259557344065,
      "grad_norm": 0.38710910081863403,
      "learning_rate": 0.00017693128081295906,
      "loss": 0.4859,
      "step": 5737
    },
    {
      "epoch": 11.545271629778671,
      "grad_norm": 0.4065290689468384,
      "learning_rate": 0.00017692725626320557,
      "loss": 0.5399,
      "step": 5738
    },
    {
      "epoch": 11.54728370221328,
      "grad_norm": 0.38737952709198,
      "learning_rate": 0.00017692323171345206,
      "loss": 0.4994,
      "step": 5739
    },
    {
      "epoch": 11.549295774647888,
      "grad_norm": 0.41184306144714355,
      "learning_rate": 0.00017691920716369857,
      "loss": 0.544,
      "step": 5740
    },
    {
      "epoch": 11.551307847082494,
      "grad_norm": 0.42080312967300415,
      "learning_rate": 0.00017691518261394505,
      "loss": 0.5305,
      "step": 5741
    },
    {
      "epoch": 11.553319919517103,
      "grad_norm": 0.41293010115623474,
      "learning_rate": 0.0001769111580641916,
      "loss": 0.5106,
      "step": 5742
    },
    {
      "epoch": 11.55533199195171,
      "grad_norm": 0.41153571009635925,
      "learning_rate": 0.00017690713351443808,
      "loss": 0.5045,
      "step": 5743
    },
    {
      "epoch": 11.557344064386317,
      "grad_norm": 0.42572158575057983,
      "learning_rate": 0.0001769031089646846,
      "loss": 0.5639,
      "step": 5744
    },
    {
      "epoch": 11.559356136820925,
      "grad_norm": 0.42747771739959717,
      "learning_rate": 0.00017689908441493107,
      "loss": 0.5154,
      "step": 5745
    },
    {
      "epoch": 11.561368209255534,
      "grad_norm": 0.401518851518631,
      "learning_rate": 0.0001768950598651776,
      "loss": 0.5127,
      "step": 5746
    },
    {
      "epoch": 11.56338028169014,
      "grad_norm": 0.3984266519546509,
      "learning_rate": 0.0001768910353154241,
      "loss": 0.5636,
      "step": 5747
    },
    {
      "epoch": 11.565392354124748,
      "grad_norm": 0.3871970772743225,
      "learning_rate": 0.0001768870107656706,
      "loss": 0.4985,
      "step": 5748
    },
    {
      "epoch": 11.567404426559357,
      "grad_norm": 0.40059590339660645,
      "learning_rate": 0.0001768829862159171,
      "loss": 0.5485,
      "step": 5749
    },
    {
      "epoch": 11.569416498993963,
      "grad_norm": 0.41686418652534485,
      "learning_rate": 0.0001768789616661636,
      "loss": 0.5124,
      "step": 5750
    },
    {
      "epoch": 11.571428571428571,
      "grad_norm": 0.40343302488327026,
      "learning_rate": 0.0001768749371164101,
      "loss": 0.5011,
      "step": 5751
    },
    {
      "epoch": 11.57344064386318,
      "grad_norm": 0.4243651032447815,
      "learning_rate": 0.00017687091256665663,
      "loss": 0.542,
      "step": 5752
    },
    {
      "epoch": 11.575452716297786,
      "grad_norm": 0.4120778739452362,
      "learning_rate": 0.00017686688801690312,
      "loss": 0.5868,
      "step": 5753
    },
    {
      "epoch": 11.577464788732394,
      "grad_norm": 0.38261210918426514,
      "learning_rate": 0.00017686286346714963,
      "loss": 0.5074,
      "step": 5754
    },
    {
      "epoch": 11.579476861167002,
      "grad_norm": 0.42606455087661743,
      "learning_rate": 0.00017685883891739612,
      "loss": 0.5294,
      "step": 5755
    },
    {
      "epoch": 11.58148893360161,
      "grad_norm": 0.4123525321483612,
      "learning_rate": 0.00017685481436764263,
      "loss": 0.532,
      "step": 5756
    },
    {
      "epoch": 11.583501006036217,
      "grad_norm": 0.3924182057380676,
      "learning_rate": 0.00017685078981788914,
      "loss": 0.497,
      "step": 5757
    },
    {
      "epoch": 11.585513078470825,
      "grad_norm": 0.3953663408756256,
      "learning_rate": 0.00017684676526813565,
      "loss": 0.5116,
      "step": 5758
    },
    {
      "epoch": 11.587525150905433,
      "grad_norm": 0.3789946436882019,
      "learning_rate": 0.00017684274071838214,
      "loss": 0.4885,
      "step": 5759
    },
    {
      "epoch": 11.58953722334004,
      "grad_norm": 0.41449010372161865,
      "learning_rate": 0.00017683871616862865,
      "loss": 0.5338,
      "step": 5760
    },
    {
      "epoch": 11.591549295774648,
      "grad_norm": 0.43088290095329285,
      "learning_rate": 0.00017683469161887513,
      "loss": 0.5469,
      "step": 5761
    },
    {
      "epoch": 11.593561368209256,
      "grad_norm": 0.4043434262275696,
      "learning_rate": 0.00017683066706912165,
      "loss": 0.5024,
      "step": 5762
    },
    {
      "epoch": 11.595573440643863,
      "grad_norm": 0.4177910387516022,
      "learning_rate": 0.00017682664251936816,
      "loss": 0.5257,
      "step": 5763
    },
    {
      "epoch": 11.59758551307847,
      "grad_norm": 0.3777881860733032,
      "learning_rate": 0.00017682261796961464,
      "loss": 0.4857,
      "step": 5764
    },
    {
      "epoch": 11.599597585513079,
      "grad_norm": 0.43509137630462646,
      "learning_rate": 0.00017681859341986116,
      "loss": 0.4999,
      "step": 5765
    },
    {
      "epoch": 11.601609657947686,
      "grad_norm": 0.4125858545303345,
      "learning_rate": 0.00017681456887010767,
      "loss": 0.5048,
      "step": 5766
    },
    {
      "epoch": 11.603621730382294,
      "grad_norm": 0.4560143053531647,
      "learning_rate": 0.00017681054432035418,
      "loss": 0.5418,
      "step": 5767
    },
    {
      "epoch": 11.605633802816902,
      "grad_norm": 0.4156031012535095,
      "learning_rate": 0.00017680651977060067,
      "loss": 0.4997,
      "step": 5768
    },
    {
      "epoch": 11.607645875251508,
      "grad_norm": 0.39479634165763855,
      "learning_rate": 0.00017680249522084718,
      "loss": 0.4858,
      "step": 5769
    },
    {
      "epoch": 11.609657947686117,
      "grad_norm": 0.4184732735157013,
      "learning_rate": 0.00017679847067109366,
      "loss": 0.5234,
      "step": 5770
    },
    {
      "epoch": 11.611670020120725,
      "grad_norm": 0.41898012161254883,
      "learning_rate": 0.00017679444612134018,
      "loss": 0.5475,
      "step": 5771
    },
    {
      "epoch": 11.613682092555331,
      "grad_norm": 0.4267861843109131,
      "learning_rate": 0.0001767904215715867,
      "loss": 0.5281,
      "step": 5772
    },
    {
      "epoch": 11.61569416498994,
      "grad_norm": 0.39248549938201904,
      "learning_rate": 0.0001767863970218332,
      "loss": 0.5297,
      "step": 5773
    },
    {
      "epoch": 11.617706237424548,
      "grad_norm": 0.3992781937122345,
      "learning_rate": 0.00017678237247207968,
      "loss": 0.5522,
      "step": 5774
    },
    {
      "epoch": 11.619718309859154,
      "grad_norm": 0.4042789041996002,
      "learning_rate": 0.0001767783479223262,
      "loss": 0.5374,
      "step": 5775
    },
    {
      "epoch": 11.621730382293762,
      "grad_norm": 0.40436920523643494,
      "learning_rate": 0.00017677432337257268,
      "loss": 0.5637,
      "step": 5776
    },
    {
      "epoch": 11.62374245472837,
      "grad_norm": 0.4015858769416809,
      "learning_rate": 0.00017677029882281922,
      "loss": 0.539,
      "step": 5777
    },
    {
      "epoch": 11.625754527162979,
      "grad_norm": 0.3993874490261078,
      "learning_rate": 0.0001767662742730657,
      "loss": 0.5286,
      "step": 5778
    },
    {
      "epoch": 11.627766599597585,
      "grad_norm": 0.38632529973983765,
      "learning_rate": 0.00017676224972331222,
      "loss": 0.5196,
      "step": 5779
    },
    {
      "epoch": 11.629778672032193,
      "grad_norm": 0.38627344369888306,
      "learning_rate": 0.0001767582251735587,
      "loss": 0.5471,
      "step": 5780
    },
    {
      "epoch": 11.631790744466802,
      "grad_norm": 0.4062129259109497,
      "learning_rate": 0.00017675420062380522,
      "loss": 0.5086,
      "step": 5781
    },
    {
      "epoch": 11.633802816901408,
      "grad_norm": 0.388805091381073,
      "learning_rate": 0.00017675017607405173,
      "loss": 0.5257,
      "step": 5782
    },
    {
      "epoch": 11.635814889336016,
      "grad_norm": 0.397742360830307,
      "learning_rate": 0.00017674615152429824,
      "loss": 0.5057,
      "step": 5783
    },
    {
      "epoch": 11.637826961770624,
      "grad_norm": 0.4034397304058075,
      "learning_rate": 0.00017674212697454473,
      "loss": 0.5024,
      "step": 5784
    },
    {
      "epoch": 11.639839034205231,
      "grad_norm": 0.38423460721969604,
      "learning_rate": 0.00017673810242479124,
      "loss": 0.5122,
      "step": 5785
    },
    {
      "epoch": 11.64185110663984,
      "grad_norm": 0.4256283640861511,
      "learning_rate": 0.00017673407787503772,
      "loss": 0.537,
      "step": 5786
    },
    {
      "epoch": 11.643863179074447,
      "grad_norm": 0.4233686625957489,
      "learning_rate": 0.00017673005332528426,
      "loss": 0.5369,
      "step": 5787
    },
    {
      "epoch": 11.645875251509054,
      "grad_norm": 0.3968579173088074,
      "learning_rate": 0.00017672602877553075,
      "loss": 0.5169,
      "step": 5788
    },
    {
      "epoch": 11.647887323943662,
      "grad_norm": 0.3862617313861847,
      "learning_rate": 0.00017672200422577726,
      "loss": 0.4867,
      "step": 5789
    },
    {
      "epoch": 11.64989939637827,
      "grad_norm": 0.41672125458717346,
      "learning_rate": 0.00017671797967602374,
      "loss": 0.5444,
      "step": 5790
    },
    {
      "epoch": 11.651911468812877,
      "grad_norm": 0.3979584276676178,
      "learning_rate": 0.00017671395512627026,
      "loss": 0.5432,
      "step": 5791
    },
    {
      "epoch": 11.653923541247485,
      "grad_norm": 0.40575486421585083,
      "learning_rate": 0.00017670993057651677,
      "loss": 0.5149,
      "step": 5792
    },
    {
      "epoch": 11.655935613682093,
      "grad_norm": 0.4047010540962219,
      "learning_rate": 0.00017670590602676328,
      "loss": 0.5114,
      "step": 5793
    },
    {
      "epoch": 11.6579476861167,
      "grad_norm": 0.394113153219223,
      "learning_rate": 0.00017670188147700977,
      "loss": 0.5403,
      "step": 5794
    },
    {
      "epoch": 11.659959758551308,
      "grad_norm": 0.40939784049987793,
      "learning_rate": 0.00017669785692725628,
      "loss": 0.5379,
      "step": 5795
    },
    {
      "epoch": 11.661971830985916,
      "grad_norm": 0.4136516749858856,
      "learning_rate": 0.00017669383237750276,
      "loss": 0.5524,
      "step": 5796
    },
    {
      "epoch": 11.663983903420522,
      "grad_norm": 0.4142157733440399,
      "learning_rate": 0.00017668980782774928,
      "loss": 0.5456,
      "step": 5797
    },
    {
      "epoch": 11.66599597585513,
      "grad_norm": 0.4072595536708832,
      "learning_rate": 0.0001766857832779958,
      "loss": 0.534,
      "step": 5798
    },
    {
      "epoch": 11.668008048289739,
      "grad_norm": 0.41012775897979736,
      "learning_rate": 0.00017668175872824227,
      "loss": 0.5194,
      "step": 5799
    },
    {
      "epoch": 11.670020120724345,
      "grad_norm": 0.37554121017456055,
      "learning_rate": 0.00017667773417848879,
      "loss": 0.4772,
      "step": 5800
    },
    {
      "epoch": 11.672032193158953,
      "grad_norm": 0.40537887811660767,
      "learning_rate": 0.0001766737096287353,
      "loss": 0.5412,
      "step": 5801
    },
    {
      "epoch": 11.674044265593562,
      "grad_norm": 0.38384419679641724,
      "learning_rate": 0.0001766696850789818,
      "loss": 0.5203,
      "step": 5802
    },
    {
      "epoch": 11.676056338028168,
      "grad_norm": 0.40999478101730347,
      "learning_rate": 0.0001766656605292283,
      "loss": 0.5433,
      "step": 5803
    },
    {
      "epoch": 11.678068410462776,
      "grad_norm": 0.3888147473335266,
      "learning_rate": 0.0001766616359794748,
      "loss": 0.5159,
      "step": 5804
    },
    {
      "epoch": 11.680080482897385,
      "grad_norm": 0.4297534227371216,
      "learning_rate": 0.0001766576114297213,
      "loss": 0.5386,
      "step": 5805
    },
    {
      "epoch": 11.682092555331993,
      "grad_norm": 0.4196755588054657,
      "learning_rate": 0.0001766535868799678,
      "loss": 0.5141,
      "step": 5806
    },
    {
      "epoch": 11.6841046277666,
      "grad_norm": 0.3864775002002716,
      "learning_rate": 0.00017664956233021432,
      "loss": 0.5466,
      "step": 5807
    },
    {
      "epoch": 11.686116700201207,
      "grad_norm": 0.4038577377796173,
      "learning_rate": 0.00017664553778046083,
      "loss": 0.5301,
      "step": 5808
    },
    {
      "epoch": 11.688128772635816,
      "grad_norm": 0.40904465317726135,
      "learning_rate": 0.00017664151323070731,
      "loss": 0.5197,
      "step": 5809
    },
    {
      "epoch": 11.690140845070422,
      "grad_norm": 0.4023391604423523,
      "learning_rate": 0.00017663748868095383,
      "loss": 0.5289,
      "step": 5810
    },
    {
      "epoch": 11.69215291750503,
      "grad_norm": 0.4171604514122009,
      "learning_rate": 0.0001766334641312003,
      "loss": 0.5657,
      "step": 5811
    },
    {
      "epoch": 11.694164989939638,
      "grad_norm": 0.38462287187576294,
      "learning_rate": 0.00017662943958144685,
      "loss": 0.4995,
      "step": 5812
    },
    {
      "epoch": 11.696177062374245,
      "grad_norm": 0.3847764730453491,
      "learning_rate": 0.00017662541503169334,
      "loss": 0.5344,
      "step": 5813
    },
    {
      "epoch": 11.698189134808853,
      "grad_norm": 0.4167737066745758,
      "learning_rate": 0.00017662139048193985,
      "loss": 0.5554,
      "step": 5814
    },
    {
      "epoch": 11.700201207243461,
      "grad_norm": 0.391079843044281,
      "learning_rate": 0.00017661736593218633,
      "loss": 0.5024,
      "step": 5815
    },
    {
      "epoch": 11.702213279678068,
      "grad_norm": 0.41691839694976807,
      "learning_rate": 0.00017661334138243285,
      "loss": 0.5387,
      "step": 5816
    },
    {
      "epoch": 11.704225352112676,
      "grad_norm": 0.39982956647872925,
      "learning_rate": 0.00017660931683267936,
      "loss": 0.506,
      "step": 5817
    },
    {
      "epoch": 11.706237424547284,
      "grad_norm": 0.427256315946579,
      "learning_rate": 0.00017660529228292587,
      "loss": 0.5538,
      "step": 5818
    },
    {
      "epoch": 11.70824949698189,
      "grad_norm": 0.39499950408935547,
      "learning_rate": 0.00017660126773317236,
      "loss": 0.5543,
      "step": 5819
    },
    {
      "epoch": 11.710261569416499,
      "grad_norm": 0.38774704933166504,
      "learning_rate": 0.00017659724318341887,
      "loss": 0.5303,
      "step": 5820
    },
    {
      "epoch": 11.712273641851107,
      "grad_norm": 0.4074358344078064,
      "learning_rate": 0.00017659321863366535,
      "loss": 0.5222,
      "step": 5821
    },
    {
      "epoch": 11.714285714285714,
      "grad_norm": 0.4073633849620819,
      "learning_rate": 0.0001765891940839119,
      "loss": 0.5268,
      "step": 5822
    },
    {
      "epoch": 11.716297786720322,
      "grad_norm": 0.4151453673839569,
      "learning_rate": 0.00017658516953415838,
      "loss": 0.5172,
      "step": 5823
    },
    {
      "epoch": 11.71830985915493,
      "grad_norm": 0.38900235295295715,
      "learning_rate": 0.0001765811449844049,
      "loss": 0.5352,
      "step": 5824
    },
    {
      "epoch": 11.720321931589538,
      "grad_norm": 0.3821735680103302,
      "learning_rate": 0.00017657712043465137,
      "loss": 0.5066,
      "step": 5825
    },
    {
      "epoch": 11.722334004024145,
      "grad_norm": 0.3738216757774353,
      "learning_rate": 0.00017657309588489789,
      "loss": 0.4853,
      "step": 5826
    },
    {
      "epoch": 11.724346076458753,
      "grad_norm": 0.4004978835582733,
      "learning_rate": 0.0001765690713351444,
      "loss": 0.5383,
      "step": 5827
    },
    {
      "epoch": 11.726358148893361,
      "grad_norm": 0.4033864736557007,
      "learning_rate": 0.0001765650467853909,
      "loss": 0.5045,
      "step": 5828
    },
    {
      "epoch": 11.728370221327967,
      "grad_norm": 0.421309232711792,
      "learning_rate": 0.0001765610222356374,
      "loss": 0.5683,
      "step": 5829
    },
    {
      "epoch": 11.730382293762576,
      "grad_norm": 0.44724392890930176,
      "learning_rate": 0.0001765569976858839,
      "loss": 0.5804,
      "step": 5830
    },
    {
      "epoch": 11.732394366197184,
      "grad_norm": 0.4104647934436798,
      "learning_rate": 0.0001765529731361304,
      "loss": 0.5449,
      "step": 5831
    },
    {
      "epoch": 11.73440643863179,
      "grad_norm": 0.4213736653327942,
      "learning_rate": 0.0001765489485863769,
      "loss": 0.5166,
      "step": 5832
    },
    {
      "epoch": 11.736418511066399,
      "grad_norm": 0.39011117815971375,
      "learning_rate": 0.00017654492403662342,
      "loss": 0.5183,
      "step": 5833
    },
    {
      "epoch": 11.738430583501007,
      "grad_norm": 0.3870165944099426,
      "learning_rate": 0.0001765408994868699,
      "loss": 0.5465,
      "step": 5834
    },
    {
      "epoch": 11.740442655935613,
      "grad_norm": 0.3962745666503906,
      "learning_rate": 0.00017653687493711641,
      "loss": 0.4952,
      "step": 5835
    },
    {
      "epoch": 11.742454728370221,
      "grad_norm": 0.4162037670612335,
      "learning_rate": 0.0001765328503873629,
      "loss": 0.5293,
      "step": 5836
    },
    {
      "epoch": 11.74446680080483,
      "grad_norm": 0.41246679425239563,
      "learning_rate": 0.00017652882583760944,
      "loss": 0.5116,
      "step": 5837
    },
    {
      "epoch": 11.746478873239436,
      "grad_norm": 0.38810351490974426,
      "learning_rate": 0.00017652480128785592,
      "loss": 0.5079,
      "step": 5838
    },
    {
      "epoch": 11.748490945674044,
      "grad_norm": 0.4064845144748688,
      "learning_rate": 0.00017652077673810244,
      "loss": 0.5302,
      "step": 5839
    },
    {
      "epoch": 11.750503018108652,
      "grad_norm": 0.3900521993637085,
      "learning_rate": 0.00017651675218834892,
      "loss": 0.5068,
      "step": 5840
    },
    {
      "epoch": 11.752515090543259,
      "grad_norm": 0.4071294665336609,
      "learning_rate": 0.00017651272763859543,
      "loss": 0.5384,
      "step": 5841
    },
    {
      "epoch": 11.754527162977867,
      "grad_norm": 0.42058807611465454,
      "learning_rate": 0.00017650870308884195,
      "loss": 0.5523,
      "step": 5842
    },
    {
      "epoch": 11.756539235412475,
      "grad_norm": 0.3903213143348694,
      "learning_rate": 0.00017650467853908846,
      "loss": 0.4961,
      "step": 5843
    },
    {
      "epoch": 11.758551307847082,
      "grad_norm": 0.4097726047039032,
      "learning_rate": 0.00017650065398933494,
      "loss": 0.5313,
      "step": 5844
    },
    {
      "epoch": 11.76056338028169,
      "grad_norm": 0.42560014128685,
      "learning_rate": 0.00017649662943958146,
      "loss": 0.5713,
      "step": 5845
    },
    {
      "epoch": 11.762575452716298,
      "grad_norm": 0.44770270586013794,
      "learning_rate": 0.00017649260488982794,
      "loss": 0.5123,
      "step": 5846
    },
    {
      "epoch": 11.764587525150905,
      "grad_norm": 0.3949486315250397,
      "learning_rate": 0.00017648858034007448,
      "loss": 0.5219,
      "step": 5847
    },
    {
      "epoch": 11.766599597585513,
      "grad_norm": 0.45210176706314087,
      "learning_rate": 0.00017648455579032097,
      "loss": 0.5909,
      "step": 5848
    },
    {
      "epoch": 11.768611670020121,
      "grad_norm": 0.41437309980392456,
      "learning_rate": 0.00017648053124056748,
      "loss": 0.511,
      "step": 5849
    },
    {
      "epoch": 11.770623742454728,
      "grad_norm": 0.41346922516822815,
      "learning_rate": 0.00017647650669081396,
      "loss": 0.5397,
      "step": 5850
    },
    {
      "epoch": 11.772635814889336,
      "grad_norm": 0.3952803611755371,
      "learning_rate": 0.00017647248214106047,
      "loss": 0.5441,
      "step": 5851
    },
    {
      "epoch": 11.774647887323944,
      "grad_norm": 0.4136429727077484,
      "learning_rate": 0.000176468457591307,
      "loss": 0.5079,
      "step": 5852
    },
    {
      "epoch": 11.77665995975855,
      "grad_norm": 0.43222710490226746,
      "learning_rate": 0.0001764644330415535,
      "loss": 0.543,
      "step": 5853
    },
    {
      "epoch": 11.778672032193159,
      "grad_norm": 0.3956198990345001,
      "learning_rate": 0.00017646040849179998,
      "loss": 0.5371,
      "step": 5854
    },
    {
      "epoch": 11.780684104627767,
      "grad_norm": 0.4169960916042328,
      "learning_rate": 0.0001764563839420465,
      "loss": 0.5414,
      "step": 5855
    },
    {
      "epoch": 11.782696177062375,
      "grad_norm": 0.3913785517215729,
      "learning_rate": 0.00017645235939229298,
      "loss": 0.5099,
      "step": 5856
    },
    {
      "epoch": 11.784708249496981,
      "grad_norm": 0.4083369970321655,
      "learning_rate": 0.00017644833484253952,
      "loss": 0.5323,
      "step": 5857
    },
    {
      "epoch": 11.78672032193159,
      "grad_norm": 0.4055801331996918,
      "learning_rate": 0.000176444310292786,
      "loss": 0.5417,
      "step": 5858
    },
    {
      "epoch": 11.788732394366198,
      "grad_norm": 0.3981718122959137,
      "learning_rate": 0.00017644028574303252,
      "loss": 0.4803,
      "step": 5859
    },
    {
      "epoch": 11.790744466800804,
      "grad_norm": 0.41616591811180115,
      "learning_rate": 0.000176436261193279,
      "loss": 0.5067,
      "step": 5860
    },
    {
      "epoch": 11.792756539235413,
      "grad_norm": 0.41238269209861755,
      "learning_rate": 0.00017643223664352552,
      "loss": 0.5806,
      "step": 5861
    },
    {
      "epoch": 11.79476861167002,
      "grad_norm": 0.3991619646549225,
      "learning_rate": 0.00017642821209377203,
      "loss": 0.5268,
      "step": 5862
    },
    {
      "epoch": 11.796780684104627,
      "grad_norm": 0.39845630526542664,
      "learning_rate": 0.0001764241875440185,
      "loss": 0.5413,
      "step": 5863
    },
    {
      "epoch": 11.798792756539235,
      "grad_norm": 0.4023001492023468,
      "learning_rate": 0.00017642016299426503,
      "loss": 0.5541,
      "step": 5864
    },
    {
      "epoch": 11.800804828973844,
      "grad_norm": 0.4036836326122284,
      "learning_rate": 0.00017641613844451154,
      "loss": 0.5427,
      "step": 5865
    },
    {
      "epoch": 11.80281690140845,
      "grad_norm": 0.4368763864040375,
      "learning_rate": 0.00017641211389475802,
      "loss": 0.5473,
      "step": 5866
    },
    {
      "epoch": 11.804828973843058,
      "grad_norm": 0.3920918107032776,
      "learning_rate": 0.00017640808934500453,
      "loss": 0.509,
      "step": 5867
    },
    {
      "epoch": 11.806841046277667,
      "grad_norm": 0.4147440195083618,
      "learning_rate": 0.00017640406479525105,
      "loss": 0.5485,
      "step": 5868
    },
    {
      "epoch": 11.808853118712273,
      "grad_norm": 0.38873016834259033,
      "learning_rate": 0.00017640004024549753,
      "loss": 0.5543,
      "step": 5869
    },
    {
      "epoch": 11.810865191146881,
      "grad_norm": 0.38089701533317566,
      "learning_rate": 0.00017639601569574404,
      "loss": 0.5032,
      "step": 5870
    },
    {
      "epoch": 11.81287726358149,
      "grad_norm": 0.4013775587081909,
      "learning_rate": 0.00017639199114599053,
      "loss": 0.5349,
      "step": 5871
    },
    {
      "epoch": 11.814889336016096,
      "grad_norm": 0.4046323895454407,
      "learning_rate": 0.00017638796659623707,
      "loss": 0.5307,
      "step": 5872
    },
    {
      "epoch": 11.816901408450704,
      "grad_norm": 0.41044870018959045,
      "learning_rate": 0.00017638394204648355,
      "loss": 0.5375,
      "step": 5873
    },
    {
      "epoch": 11.818913480885312,
      "grad_norm": 0.38347679376602173,
      "learning_rate": 0.00017637991749673007,
      "loss": 0.5285,
      "step": 5874
    },
    {
      "epoch": 11.82092555331992,
      "grad_norm": 0.40021559596061707,
      "learning_rate": 0.00017637589294697655,
      "loss": 0.5405,
      "step": 5875
    },
    {
      "epoch": 11.822937625754527,
      "grad_norm": 0.4344283640384674,
      "learning_rate": 0.00017637186839722306,
      "loss": 0.5317,
      "step": 5876
    },
    {
      "epoch": 11.824949698189135,
      "grad_norm": 0.39702367782592773,
      "learning_rate": 0.00017636784384746958,
      "loss": 0.5041,
      "step": 5877
    },
    {
      "epoch": 11.826961770623743,
      "grad_norm": 0.39772829413414,
      "learning_rate": 0.0001763638192977161,
      "loss": 0.5202,
      "step": 5878
    },
    {
      "epoch": 11.82897384305835,
      "grad_norm": 0.41822248697280884,
      "learning_rate": 0.00017635979474796257,
      "loss": 0.5098,
      "step": 5879
    },
    {
      "epoch": 11.830985915492958,
      "grad_norm": 0.39394259452819824,
      "learning_rate": 0.00017635577019820909,
      "loss": 0.5285,
      "step": 5880
    },
    {
      "epoch": 11.832997987927566,
      "grad_norm": 0.41571879386901855,
      "learning_rate": 0.00017635174564845557,
      "loss": 0.5856,
      "step": 5881
    },
    {
      "epoch": 11.835010060362173,
      "grad_norm": 0.40842822194099426,
      "learning_rate": 0.0001763477210987021,
      "loss": 0.5087,
      "step": 5882
    },
    {
      "epoch": 11.83702213279678,
      "grad_norm": 0.4201277196407318,
      "learning_rate": 0.0001763436965489486,
      "loss": 0.5593,
      "step": 5883
    },
    {
      "epoch": 11.839034205231389,
      "grad_norm": 0.3953785300254822,
      "learning_rate": 0.0001763396719991951,
      "loss": 0.5345,
      "step": 5884
    },
    {
      "epoch": 11.841046277665995,
      "grad_norm": 0.38986432552337646,
      "learning_rate": 0.0001763356474494416,
      "loss": 0.5271,
      "step": 5885
    },
    {
      "epoch": 11.843058350100604,
      "grad_norm": 0.3999522030353546,
      "learning_rate": 0.0001763316228996881,
      "loss": 0.5453,
      "step": 5886
    },
    {
      "epoch": 11.845070422535212,
      "grad_norm": 0.40587034821510315,
      "learning_rate": 0.00017632759834993462,
      "loss": 0.5117,
      "step": 5887
    },
    {
      "epoch": 11.847082494969818,
      "grad_norm": 0.4049050509929657,
      "learning_rate": 0.00017632357380018113,
      "loss": 0.5807,
      "step": 5888
    },
    {
      "epoch": 11.849094567404427,
      "grad_norm": 0.3836922347545624,
      "learning_rate": 0.00017631954925042761,
      "loss": 0.548,
      "step": 5889
    },
    {
      "epoch": 11.851106639839035,
      "grad_norm": 0.3980271518230438,
      "learning_rate": 0.00017631552470067413,
      "loss": 0.5343,
      "step": 5890
    },
    {
      "epoch": 11.853118712273641,
      "grad_norm": 0.38276639580726624,
      "learning_rate": 0.0001763115001509206,
      "loss": 0.5191,
      "step": 5891
    },
    {
      "epoch": 11.85513078470825,
      "grad_norm": 0.39468955993652344,
      "learning_rate": 0.00017630747560116715,
      "loss": 0.518,
      "step": 5892
    },
    {
      "epoch": 11.857142857142858,
      "grad_norm": 0.3866150379180908,
      "learning_rate": 0.00017630345105141364,
      "loss": 0.5409,
      "step": 5893
    },
    {
      "epoch": 11.859154929577464,
      "grad_norm": 0.38971376419067383,
      "learning_rate": 0.00017629942650166015,
      "loss": 0.5534,
      "step": 5894
    },
    {
      "epoch": 11.861167002012072,
      "grad_norm": 0.41619977355003357,
      "learning_rate": 0.00017629540195190663,
      "loss": 0.5528,
      "step": 5895
    },
    {
      "epoch": 11.86317907444668,
      "grad_norm": 0.4184890687465668,
      "learning_rate": 0.00017629137740215315,
      "loss": 0.5525,
      "step": 5896
    },
    {
      "epoch": 11.865191146881287,
      "grad_norm": 0.3984593152999878,
      "learning_rate": 0.00017628735285239966,
      "loss": 0.5573,
      "step": 5897
    },
    {
      "epoch": 11.867203219315895,
      "grad_norm": 0.40711238980293274,
      "learning_rate": 0.00017628332830264614,
      "loss": 0.5253,
      "step": 5898
    },
    {
      "epoch": 11.869215291750503,
      "grad_norm": 0.4057196080684662,
      "learning_rate": 0.00017627930375289265,
      "loss": 0.5133,
      "step": 5899
    },
    {
      "epoch": 11.87122736418511,
      "grad_norm": 0.4144662916660309,
      "learning_rate": 0.00017627527920313917,
      "loss": 0.5335,
      "step": 5900
    },
    {
      "epoch": 11.873239436619718,
      "grad_norm": 0.39640259742736816,
      "learning_rate": 0.00017627125465338565,
      "loss": 0.5356,
      "step": 5901
    },
    {
      "epoch": 11.875251509054326,
      "grad_norm": 0.39170199632644653,
      "learning_rate": 0.00017626723010363216,
      "loss": 0.5446,
      "step": 5902
    },
    {
      "epoch": 11.877263581488933,
      "grad_norm": 0.3792905807495117,
      "learning_rate": 0.00017626320555387868,
      "loss": 0.501,
      "step": 5903
    },
    {
      "epoch": 11.879275653923541,
      "grad_norm": 0.38228872418403625,
      "learning_rate": 0.00017625918100412516,
      "loss": 0.5148,
      "step": 5904
    },
    {
      "epoch": 11.88128772635815,
      "grad_norm": 0.3956635594367981,
      "learning_rate": 0.00017625515645437167,
      "loss": 0.5502,
      "step": 5905
    },
    {
      "epoch": 11.883299798792757,
      "grad_norm": 0.3955713212490082,
      "learning_rate": 0.00017625113190461816,
      "loss": 0.5304,
      "step": 5906
    },
    {
      "epoch": 11.885311871227364,
      "grad_norm": 0.38359522819519043,
      "learning_rate": 0.0001762471073548647,
      "loss": 0.5158,
      "step": 5907
    },
    {
      "epoch": 11.887323943661972,
      "grad_norm": 0.3899451494216919,
      "learning_rate": 0.00017624308280511118,
      "loss": 0.5184,
      "step": 5908
    },
    {
      "epoch": 11.88933601609658,
      "grad_norm": 0.39062708616256714,
      "learning_rate": 0.0001762390582553577,
      "loss": 0.5231,
      "step": 5909
    },
    {
      "epoch": 11.891348088531187,
      "grad_norm": 0.4342874586582184,
      "learning_rate": 0.00017623503370560418,
      "loss": 0.5777,
      "step": 5910
    },
    {
      "epoch": 11.893360160965795,
      "grad_norm": 0.40982335805892944,
      "learning_rate": 0.0001762310091558507,
      "loss": 0.5203,
      "step": 5911
    },
    {
      "epoch": 11.895372233400403,
      "grad_norm": 0.4063268303871155,
      "learning_rate": 0.0001762269846060972,
      "loss": 0.5338,
      "step": 5912
    },
    {
      "epoch": 11.89738430583501,
      "grad_norm": 0.39728376269340515,
      "learning_rate": 0.00017622296005634372,
      "loss": 0.5162,
      "step": 5913
    },
    {
      "epoch": 11.899396378269618,
      "grad_norm": 0.3972797691822052,
      "learning_rate": 0.0001762189355065902,
      "loss": 0.4988,
      "step": 5914
    },
    {
      "epoch": 11.901408450704226,
      "grad_norm": 0.3912964463233948,
      "learning_rate": 0.00017621491095683671,
      "loss": 0.5174,
      "step": 5915
    },
    {
      "epoch": 11.903420523138832,
      "grad_norm": 0.3939410448074341,
      "learning_rate": 0.0001762108864070832,
      "loss": 0.5321,
      "step": 5916
    },
    {
      "epoch": 11.90543259557344,
      "grad_norm": 0.40419328212738037,
      "learning_rate": 0.00017620686185732974,
      "loss": 0.5052,
      "step": 5917
    },
    {
      "epoch": 11.907444668008049,
      "grad_norm": 0.3742935061454773,
      "learning_rate": 0.00017620283730757622,
      "loss": 0.5066,
      "step": 5918
    },
    {
      "epoch": 11.909456740442655,
      "grad_norm": 0.4201473891735077,
      "learning_rate": 0.00017619881275782274,
      "loss": 0.5351,
      "step": 5919
    },
    {
      "epoch": 11.911468812877263,
      "grad_norm": 0.40697795152664185,
      "learning_rate": 0.00017619478820806922,
      "loss": 0.511,
      "step": 5920
    },
    {
      "epoch": 11.913480885311872,
      "grad_norm": 0.4066033363342285,
      "learning_rate": 0.00017619076365831573,
      "loss": 0.5374,
      "step": 5921
    },
    {
      "epoch": 11.915492957746478,
      "grad_norm": 0.3957032561302185,
      "learning_rate": 0.00017618673910856225,
      "loss": 0.5507,
      "step": 5922
    },
    {
      "epoch": 11.917505030181086,
      "grad_norm": 0.38824698328971863,
      "learning_rate": 0.00017618271455880876,
      "loss": 0.5016,
      "step": 5923
    },
    {
      "epoch": 11.919517102615695,
      "grad_norm": 0.41533011198043823,
      "learning_rate": 0.00017617869000905524,
      "loss": 0.5378,
      "step": 5924
    },
    {
      "epoch": 11.921529175050303,
      "grad_norm": 0.3937417268753052,
      "learning_rate": 0.00017617466545930176,
      "loss": 0.5073,
      "step": 5925
    },
    {
      "epoch": 11.92354124748491,
      "grad_norm": 0.4064350426197052,
      "learning_rate": 0.00017617064090954824,
      "loss": 0.5574,
      "step": 5926
    },
    {
      "epoch": 11.925553319919517,
      "grad_norm": 0.3868715465068817,
      "learning_rate": 0.00017616661635979478,
      "loss": 0.5517,
      "step": 5927
    },
    {
      "epoch": 11.927565392354126,
      "grad_norm": 0.4072619378566742,
      "learning_rate": 0.00017616259181004127,
      "loss": 0.5275,
      "step": 5928
    },
    {
      "epoch": 11.929577464788732,
      "grad_norm": 0.3905402719974518,
      "learning_rate": 0.00017615856726028778,
      "loss": 0.5522,
      "step": 5929
    },
    {
      "epoch": 11.93158953722334,
      "grad_norm": 0.4198376536369324,
      "learning_rate": 0.00017615454271053426,
      "loss": 0.4948,
      "step": 5930
    },
    {
      "epoch": 11.933601609657948,
      "grad_norm": 0.4115091860294342,
      "learning_rate": 0.00017615051816078077,
      "loss": 0.5384,
      "step": 5931
    },
    {
      "epoch": 11.935613682092555,
      "grad_norm": 0.3818936347961426,
      "learning_rate": 0.0001761464936110273,
      "loss": 0.5227,
      "step": 5932
    },
    {
      "epoch": 11.937625754527163,
      "grad_norm": 0.3942387104034424,
      "learning_rate": 0.00017614246906127377,
      "loss": 0.5472,
      "step": 5933
    },
    {
      "epoch": 11.939637826961771,
      "grad_norm": 0.38621121644973755,
      "learning_rate": 0.00017613844451152028,
      "loss": 0.5297,
      "step": 5934
    },
    {
      "epoch": 11.941649899396378,
      "grad_norm": 0.3749469816684723,
      "learning_rate": 0.0001761344199617668,
      "loss": 0.5247,
      "step": 5935
    },
    {
      "epoch": 11.943661971830986,
      "grad_norm": 0.41560685634613037,
      "learning_rate": 0.00017613039541201328,
      "loss": 0.5861,
      "step": 5936
    },
    {
      "epoch": 11.945674044265594,
      "grad_norm": 0.4048611521720886,
      "learning_rate": 0.0001761263708622598,
      "loss": 0.5088,
      "step": 5937
    },
    {
      "epoch": 11.9476861167002,
      "grad_norm": 0.3965374231338501,
      "learning_rate": 0.0001761223463125063,
      "loss": 0.5356,
      "step": 5938
    },
    {
      "epoch": 11.949698189134809,
      "grad_norm": 0.3990272879600525,
      "learning_rate": 0.0001761183217627528,
      "loss": 0.5455,
      "step": 5939
    },
    {
      "epoch": 11.951710261569417,
      "grad_norm": 0.3975737392902374,
      "learning_rate": 0.0001761142972129993,
      "loss": 0.5407,
      "step": 5940
    },
    {
      "epoch": 11.953722334004024,
      "grad_norm": 0.4055725932121277,
      "learning_rate": 0.0001761102726632458,
      "loss": 0.5517,
      "step": 5941
    },
    {
      "epoch": 11.955734406438632,
      "grad_norm": 0.39277100563049316,
      "learning_rate": 0.00017610624811349233,
      "loss": 0.5198,
      "step": 5942
    },
    {
      "epoch": 11.95774647887324,
      "grad_norm": 0.38678035140037537,
      "learning_rate": 0.0001761022235637388,
      "loss": 0.5243,
      "step": 5943
    },
    {
      "epoch": 11.959758551307846,
      "grad_norm": 0.3993385136127472,
      "learning_rate": 0.00017609819901398533,
      "loss": 0.544,
      "step": 5944
    },
    {
      "epoch": 11.961770623742455,
      "grad_norm": 0.41684162616729736,
      "learning_rate": 0.0001760941744642318,
      "loss": 0.5444,
      "step": 5945
    },
    {
      "epoch": 11.963782696177063,
      "grad_norm": 0.38625064492225647,
      "learning_rate": 0.00017609014991447832,
      "loss": 0.5283,
      "step": 5946
    },
    {
      "epoch": 11.96579476861167,
      "grad_norm": 0.39437082409858704,
      "learning_rate": 0.0001760861253647248,
      "loss": 0.52,
      "step": 5947
    },
    {
      "epoch": 11.967806841046277,
      "grad_norm": 0.4274764358997345,
      "learning_rate": 0.00017608210081497135,
      "loss": 0.5848,
      "step": 5948
    },
    {
      "epoch": 11.969818913480886,
      "grad_norm": 0.41054579615592957,
      "learning_rate": 0.00017607807626521783,
      "loss": 0.5378,
      "step": 5949
    },
    {
      "epoch": 11.971830985915492,
      "grad_norm": 0.3999253213405609,
      "learning_rate": 0.00017607405171546434,
      "loss": 0.574,
      "step": 5950
    },
    {
      "epoch": 11.9738430583501,
      "grad_norm": 0.39512625336647034,
      "learning_rate": 0.00017607002716571083,
      "loss": 0.4977,
      "step": 5951
    },
    {
      "epoch": 11.975855130784709,
      "grad_norm": 0.4066004157066345,
      "learning_rate": 0.00017606600261595734,
      "loss": 0.5247,
      "step": 5952
    },
    {
      "epoch": 11.977867203219315,
      "grad_norm": 0.3777036666870117,
      "learning_rate": 0.00017606197806620385,
      "loss": 0.5089,
      "step": 5953
    },
    {
      "epoch": 11.979879275653923,
      "grad_norm": 0.44392070174217224,
      "learning_rate": 0.00017605795351645037,
      "loss": 0.5501,
      "step": 5954
    },
    {
      "epoch": 11.981891348088531,
      "grad_norm": 0.39467307925224304,
      "learning_rate": 0.00017605392896669685,
      "loss": 0.5202,
      "step": 5955
    },
    {
      "epoch": 11.98390342052314,
      "grad_norm": 0.41725167632102966,
      "learning_rate": 0.00017604990441694336,
      "loss": 0.5185,
      "step": 5956
    },
    {
      "epoch": 11.985915492957746,
      "grad_norm": 0.3992764949798584,
      "learning_rate": 0.00017604587986718985,
      "loss": 0.5397,
      "step": 5957
    },
    {
      "epoch": 11.987927565392354,
      "grad_norm": 0.42695295810699463,
      "learning_rate": 0.0001760418553174364,
      "loss": 0.5697,
      "step": 5958
    },
    {
      "epoch": 11.989939637826962,
      "grad_norm": 0.40096181631088257,
      "learning_rate": 0.00017603783076768287,
      "loss": 0.4929,
      "step": 5959
    },
    {
      "epoch": 11.991951710261569,
      "grad_norm": 0.38361817598342896,
      "learning_rate": 0.00017603380621792938,
      "loss": 0.5258,
      "step": 5960
    },
    {
      "epoch": 11.993963782696177,
      "grad_norm": 0.37790441513061523,
      "learning_rate": 0.00017602978166817587,
      "loss": 0.476,
      "step": 5961
    },
    {
      "epoch": 11.995975855130785,
      "grad_norm": 0.3881710171699524,
      "learning_rate": 0.00017602575711842238,
      "loss": 0.5237,
      "step": 5962
    },
    {
      "epoch": 11.997987927565392,
      "grad_norm": 0.4943734407424927,
      "learning_rate": 0.0001760217325686689,
      "loss": 0.5602,
      "step": 5963
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.42677876353263855,
      "learning_rate": 0.0001760177080189154,
      "loss": 0.5487,
      "step": 5964
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.7805320620536804,
      "eval_runtime": 49.8213,
      "eval_samples_per_second": 19.911,
      "eval_steps_per_second": 2.489,
      "step": 5964
    },
    {
      "epoch": 12.002012072434608,
      "grad_norm": 0.3725545406341553,
      "learning_rate": 0.0001760136834691619,
      "loss": 0.4653,
      "step": 5965
    },
    {
      "epoch": 12.004024144869215,
      "grad_norm": 0.3856731057167053,
      "learning_rate": 0.0001760096589194084,
      "loss": 0.4663,
      "step": 5966
    },
    {
      "epoch": 12.006036217303823,
      "grad_norm": 0.3762012720108032,
      "learning_rate": 0.0001760056343696549,
      "loss": 0.4411,
      "step": 5967
    },
    {
      "epoch": 12.008048289738431,
      "grad_norm": 0.39564239978790283,
      "learning_rate": 0.0001760016098199014,
      "loss": 0.4783,
      "step": 5968
    },
    {
      "epoch": 12.010060362173038,
      "grad_norm": 0.44891732931137085,
      "learning_rate": 0.00017599758527014791,
      "loss": 0.5254,
      "step": 5969
    },
    {
      "epoch": 12.012072434607646,
      "grad_norm": 0.45054787397384644,
      "learning_rate": 0.00017599356072039443,
      "loss": 0.4479,
      "step": 5970
    },
    {
      "epoch": 12.014084507042254,
      "grad_norm": 0.44159018993377686,
      "learning_rate": 0.0001759895361706409,
      "loss": 0.4637,
      "step": 5971
    },
    {
      "epoch": 12.01609657947686,
      "grad_norm": 0.39621931314468384,
      "learning_rate": 0.00017598551162088742,
      "loss": 0.4514,
      "step": 5972
    },
    {
      "epoch": 12.018108651911469,
      "grad_norm": 0.4056660532951355,
      "learning_rate": 0.00017598148707113394,
      "loss": 0.5098,
      "step": 5973
    },
    {
      "epoch": 12.020120724346077,
      "grad_norm": 0.3964417278766632,
      "learning_rate": 0.00017597746252138042,
      "loss": 0.4205,
      "step": 5974
    },
    {
      "epoch": 12.022132796780683,
      "grad_norm": 0.40615102648735046,
      "learning_rate": 0.00017597343797162693,
      "loss": 0.4962,
      "step": 5975
    },
    {
      "epoch": 12.024144869215291,
      "grad_norm": 0.3871500492095947,
      "learning_rate": 0.00017596941342187342,
      "loss": 0.4443,
      "step": 5976
    },
    {
      "epoch": 12.0261569416499,
      "grad_norm": 0.42520955204963684,
      "learning_rate": 0.00017596538887211993,
      "loss": 0.4809,
      "step": 5977
    },
    {
      "epoch": 12.028169014084508,
      "grad_norm": 0.42455804347991943,
      "learning_rate": 0.00017596136432236644,
      "loss": 0.4805,
      "step": 5978
    },
    {
      "epoch": 12.030181086519114,
      "grad_norm": 0.41107115149497986,
      "learning_rate": 0.00017595733977261295,
      "loss": 0.4591,
      "step": 5979
    },
    {
      "epoch": 12.032193158953723,
      "grad_norm": 0.42365989089012146,
      "learning_rate": 0.00017595331522285944,
      "loss": 0.4914,
      "step": 5980
    },
    {
      "epoch": 12.03420523138833,
      "grad_norm": 0.40944111347198486,
      "learning_rate": 0.00017594929067310595,
      "loss": 0.4988,
      "step": 5981
    },
    {
      "epoch": 12.036217303822937,
      "grad_norm": 0.4230301082134247,
      "learning_rate": 0.00017594526612335244,
      "loss": 0.486,
      "step": 5982
    },
    {
      "epoch": 12.038229376257545,
      "grad_norm": 0.4172229766845703,
      "learning_rate": 0.00017594124157359898,
      "loss": 0.5124,
      "step": 5983
    },
    {
      "epoch": 12.040241448692154,
      "grad_norm": 0.40651747584342957,
      "learning_rate": 0.00017593721702384546,
      "loss": 0.4604,
      "step": 5984
    },
    {
      "epoch": 12.04225352112676,
      "grad_norm": 0.3924519121646881,
      "learning_rate": 0.00017593319247409197,
      "loss": 0.4672,
      "step": 5985
    },
    {
      "epoch": 12.044265593561368,
      "grad_norm": 0.4173620343208313,
      "learning_rate": 0.00017592916792433846,
      "loss": 0.4779,
      "step": 5986
    },
    {
      "epoch": 12.046277665995976,
      "grad_norm": 0.4092783033847809,
      "learning_rate": 0.00017592514337458497,
      "loss": 0.437,
      "step": 5987
    },
    {
      "epoch": 12.048289738430583,
      "grad_norm": 0.43516167998313904,
      "learning_rate": 0.00017592111882483148,
      "loss": 0.4731,
      "step": 5988
    },
    {
      "epoch": 12.050301810865191,
      "grad_norm": 0.4336547553539276,
      "learning_rate": 0.000175917094275078,
      "loss": 0.4726,
      "step": 5989
    },
    {
      "epoch": 12.0523138832998,
      "grad_norm": 0.46974092721939087,
      "learning_rate": 0.00017591306972532448,
      "loss": 0.4794,
      "step": 5990
    },
    {
      "epoch": 12.054325955734406,
      "grad_norm": 0.4396706223487854,
      "learning_rate": 0.000175909045175571,
      "loss": 0.4883,
      "step": 5991
    },
    {
      "epoch": 12.056338028169014,
      "grad_norm": 0.4404764473438263,
      "learning_rate": 0.00017590502062581748,
      "loss": 0.5056,
      "step": 5992
    },
    {
      "epoch": 12.058350100603622,
      "grad_norm": 0.4046514630317688,
      "learning_rate": 0.00017590099607606402,
      "loss": 0.4913,
      "step": 5993
    },
    {
      "epoch": 12.060362173038229,
      "grad_norm": 0.40265658497810364,
      "learning_rate": 0.0001758969715263105,
      "loss": 0.4843,
      "step": 5994
    },
    {
      "epoch": 12.062374245472837,
      "grad_norm": 0.42391589283943176,
      "learning_rate": 0.00017589294697655701,
      "loss": 0.488,
      "step": 5995
    },
    {
      "epoch": 12.064386317907445,
      "grad_norm": 0.41021397709846497,
      "learning_rate": 0.0001758889224268035,
      "loss": 0.4532,
      "step": 5996
    },
    {
      "epoch": 12.066398390342052,
      "grad_norm": 0.40392106771469116,
      "learning_rate": 0.00017588489787705,
      "loss": 0.479,
      "step": 5997
    },
    {
      "epoch": 12.06841046277666,
      "grad_norm": 0.4371766746044159,
      "learning_rate": 0.00017588087332729652,
      "loss": 0.4902,
      "step": 5998
    },
    {
      "epoch": 12.070422535211268,
      "grad_norm": 0.4252895414829254,
      "learning_rate": 0.00017587684877754304,
      "loss": 0.4736,
      "step": 5999
    },
    {
      "epoch": 12.072434607645874,
      "grad_norm": 0.4413687288761139,
      "learning_rate": 0.00017587282422778952,
      "loss": 0.4601,
      "step": 6000
    },
    {
      "epoch": 12.074446680080483,
      "grad_norm": 0.42578649520874023,
      "learning_rate": 0.00017586879967803603,
      "loss": 0.4865,
      "step": 6001
    },
    {
      "epoch": 12.07645875251509,
      "grad_norm": 0.40667665004730225,
      "learning_rate": 0.00017586477512828252,
      "loss": 0.4558,
      "step": 6002
    },
    {
      "epoch": 12.078470824949699,
      "grad_norm": 0.4169006645679474,
      "learning_rate": 0.00017586075057852903,
      "loss": 0.4821,
      "step": 6003
    },
    {
      "epoch": 12.080482897384305,
      "grad_norm": 0.418692946434021,
      "learning_rate": 0.00017585672602877554,
      "loss": 0.4411,
      "step": 6004
    },
    {
      "epoch": 12.082494969818914,
      "grad_norm": 0.4394775629043579,
      "learning_rate": 0.00017585270147902206,
      "loss": 0.4776,
      "step": 6005
    },
    {
      "epoch": 12.084507042253522,
      "grad_norm": 0.39400097727775574,
      "learning_rate": 0.00017584867692926854,
      "loss": 0.4101,
      "step": 6006
    },
    {
      "epoch": 12.086519114688128,
      "grad_norm": 0.43527838587760925,
      "learning_rate": 0.00017584465237951505,
      "loss": 0.4597,
      "step": 6007
    },
    {
      "epoch": 12.088531187122737,
      "grad_norm": 0.4106649160385132,
      "learning_rate": 0.00017584062782976156,
      "loss": 0.5045,
      "step": 6008
    },
    {
      "epoch": 12.090543259557345,
      "grad_norm": 0.40111294388771057,
      "learning_rate": 0.00017583660328000805,
      "loss": 0.4846,
      "step": 6009
    },
    {
      "epoch": 12.092555331991951,
      "grad_norm": 0.42464497685432434,
      "learning_rate": 0.00017583257873025456,
      "loss": 0.4285,
      "step": 6010
    },
    {
      "epoch": 12.09456740442656,
      "grad_norm": 0.43172407150268555,
      "learning_rate": 0.00017582855418050105,
      "loss": 0.4906,
      "step": 6011
    },
    {
      "epoch": 12.096579476861168,
      "grad_norm": 0.4257605969905853,
      "learning_rate": 0.00017582452963074756,
      "loss": 0.4599,
      "step": 6012
    },
    {
      "epoch": 12.098591549295774,
      "grad_norm": 0.4320679306983948,
      "learning_rate": 0.00017582050508099407,
      "loss": 0.4894,
      "step": 6013
    },
    {
      "epoch": 12.100603621730382,
      "grad_norm": 0.4140709340572357,
      "learning_rate": 0.00017581648053124058,
      "loss": 0.4806,
      "step": 6014
    },
    {
      "epoch": 12.10261569416499,
      "grad_norm": 0.4167454242706299,
      "learning_rate": 0.00017581245598148707,
      "loss": 0.4763,
      "step": 6015
    },
    {
      "epoch": 12.104627766599597,
      "grad_norm": 0.4439760744571686,
      "learning_rate": 0.00017580843143173358,
      "loss": 0.4709,
      "step": 6016
    },
    {
      "epoch": 12.106639839034205,
      "grad_norm": 0.42775487899780273,
      "learning_rate": 0.00017580440688198007,
      "loss": 0.467,
      "step": 6017
    },
    {
      "epoch": 12.108651911468813,
      "grad_norm": 0.45190656185150146,
      "learning_rate": 0.0001758003823322266,
      "loss": 0.515,
      "step": 6018
    },
    {
      "epoch": 12.11066398390342,
      "grad_norm": 0.4028935730457306,
      "learning_rate": 0.0001757963577824731,
      "loss": 0.4451,
      "step": 6019
    },
    {
      "epoch": 12.112676056338028,
      "grad_norm": 0.4210328161716461,
      "learning_rate": 0.0001757923332327196,
      "loss": 0.4594,
      "step": 6020
    },
    {
      "epoch": 12.114688128772636,
      "grad_norm": 0.4315360188484192,
      "learning_rate": 0.0001757883086829661,
      "loss": 0.4763,
      "step": 6021
    },
    {
      "epoch": 12.116700201207243,
      "grad_norm": 0.40738651156425476,
      "learning_rate": 0.0001757842841332126,
      "loss": 0.4649,
      "step": 6022
    },
    {
      "epoch": 12.11871227364185,
      "grad_norm": 0.42808103561401367,
      "learning_rate": 0.0001757802595834591,
      "loss": 0.499,
      "step": 6023
    },
    {
      "epoch": 12.120724346076459,
      "grad_norm": 0.41254448890686035,
      "learning_rate": 0.00017577623503370562,
      "loss": 0.4786,
      "step": 6024
    },
    {
      "epoch": 12.122736418511066,
      "grad_norm": 0.41277703642845154,
      "learning_rate": 0.0001757722104839521,
      "loss": 0.4979,
      "step": 6025
    },
    {
      "epoch": 12.124748490945674,
      "grad_norm": 0.425576388835907,
      "learning_rate": 0.00017576818593419862,
      "loss": 0.4738,
      "step": 6026
    },
    {
      "epoch": 12.126760563380282,
      "grad_norm": 0.43764424324035645,
      "learning_rate": 0.0001757641613844451,
      "loss": 0.4705,
      "step": 6027
    },
    {
      "epoch": 12.12877263581489,
      "grad_norm": 0.3996071517467499,
      "learning_rate": 0.00017576013683469165,
      "loss": 0.4462,
      "step": 6028
    },
    {
      "epoch": 12.130784708249497,
      "grad_norm": 0.42591017484664917,
      "learning_rate": 0.00017575611228493813,
      "loss": 0.5104,
      "step": 6029
    },
    {
      "epoch": 12.132796780684105,
      "grad_norm": 0.42121317982673645,
      "learning_rate": 0.00017575208773518464,
      "loss": 0.4898,
      "step": 6030
    },
    {
      "epoch": 12.134808853118713,
      "grad_norm": 0.419323205947876,
      "learning_rate": 0.00017574806318543113,
      "loss": 0.4927,
      "step": 6031
    },
    {
      "epoch": 12.13682092555332,
      "grad_norm": 0.43317317962646484,
      "learning_rate": 0.00017574403863567764,
      "loss": 0.4791,
      "step": 6032
    },
    {
      "epoch": 12.138832997987928,
      "grad_norm": 0.43907734751701355,
      "learning_rate": 0.00017574001408592415,
      "loss": 0.5066,
      "step": 6033
    },
    {
      "epoch": 12.140845070422536,
      "grad_norm": 0.4126969277858734,
      "learning_rate": 0.00017573598953617067,
      "loss": 0.4887,
      "step": 6034
    },
    {
      "epoch": 12.142857142857142,
      "grad_norm": 0.41330745816230774,
      "learning_rate": 0.00017573196498641715,
      "loss": 0.4491,
      "step": 6035
    },
    {
      "epoch": 12.14486921529175,
      "grad_norm": 0.421853244304657,
      "learning_rate": 0.00017572794043666366,
      "loss": 0.5008,
      "step": 6036
    },
    {
      "epoch": 12.146881287726359,
      "grad_norm": 0.39341726899147034,
      "learning_rate": 0.00017572391588691015,
      "loss": 0.4568,
      "step": 6037
    },
    {
      "epoch": 12.148893360160965,
      "grad_norm": 0.43598395586013794,
      "learning_rate": 0.00017571989133715666,
      "loss": 0.4884,
      "step": 6038
    },
    {
      "epoch": 12.150905432595573,
      "grad_norm": 0.43063291907310486,
      "learning_rate": 0.00017571586678740317,
      "loss": 0.4713,
      "step": 6039
    },
    {
      "epoch": 12.152917505030182,
      "grad_norm": 0.42234480381011963,
      "learning_rate": 0.00017571184223764966,
      "loss": 0.5042,
      "step": 6040
    },
    {
      "epoch": 12.154929577464788,
      "grad_norm": 0.4378322958946228,
      "learning_rate": 0.00017570781768789617,
      "loss": 0.5056,
      "step": 6041
    },
    {
      "epoch": 12.156941649899396,
      "grad_norm": 0.4015391170978546,
      "learning_rate": 0.00017570379313814268,
      "loss": 0.4432,
      "step": 6042
    },
    {
      "epoch": 12.158953722334005,
      "grad_norm": 0.4392998218536377,
      "learning_rate": 0.0001756997685883892,
      "loss": 0.492,
      "step": 6043
    },
    {
      "epoch": 12.160965794768611,
      "grad_norm": 0.42574527859687805,
      "learning_rate": 0.00017569574403863568,
      "loss": 0.4622,
      "step": 6044
    },
    {
      "epoch": 12.16297786720322,
      "grad_norm": 0.4044855833053589,
      "learning_rate": 0.0001756917194888822,
      "loss": 0.4734,
      "step": 6045
    },
    {
      "epoch": 12.164989939637827,
      "grad_norm": 0.45603427290916443,
      "learning_rate": 0.00017568769493912868,
      "loss": 0.4701,
      "step": 6046
    },
    {
      "epoch": 12.167002012072434,
      "grad_norm": 0.4416085183620453,
      "learning_rate": 0.0001756836703893752,
      "loss": 0.4743,
      "step": 6047
    },
    {
      "epoch": 12.169014084507042,
      "grad_norm": 0.42625534534454346,
      "learning_rate": 0.0001756796458396217,
      "loss": 0.4678,
      "step": 6048
    },
    {
      "epoch": 12.17102615694165,
      "grad_norm": 0.4498403072357178,
      "learning_rate": 0.0001756756212898682,
      "loss": 0.488,
      "step": 6049
    },
    {
      "epoch": 12.173038229376257,
      "grad_norm": 0.3967013955116272,
      "learning_rate": 0.0001756715967401147,
      "loss": 0.483,
      "step": 6050
    },
    {
      "epoch": 12.175050301810865,
      "grad_norm": 0.42720088362693787,
      "learning_rate": 0.0001756675721903612,
      "loss": 0.4864,
      "step": 6051
    },
    {
      "epoch": 12.177062374245473,
      "grad_norm": 0.42246994376182556,
      "learning_rate": 0.0001756635476406077,
      "loss": 0.5024,
      "step": 6052
    },
    {
      "epoch": 12.179074446680081,
      "grad_norm": 0.40917661786079407,
      "learning_rate": 0.00017565952309085424,
      "loss": 0.4811,
      "step": 6053
    },
    {
      "epoch": 12.181086519114688,
      "grad_norm": 0.4320950210094452,
      "learning_rate": 0.00017565549854110072,
      "loss": 0.4578,
      "step": 6054
    },
    {
      "epoch": 12.183098591549296,
      "grad_norm": 0.4057861268520355,
      "learning_rate": 0.00017565147399134723,
      "loss": 0.4757,
      "step": 6055
    },
    {
      "epoch": 12.185110663983904,
      "grad_norm": 0.41462481021881104,
      "learning_rate": 0.00017564744944159372,
      "loss": 0.4827,
      "step": 6056
    },
    {
      "epoch": 12.18712273641851,
      "grad_norm": 0.43753698468208313,
      "learning_rate": 0.00017564342489184023,
      "loss": 0.5019,
      "step": 6057
    },
    {
      "epoch": 12.189134808853119,
      "grad_norm": 0.44756564497947693,
      "learning_rate": 0.00017563940034208674,
      "loss": 0.478,
      "step": 6058
    },
    {
      "epoch": 12.191146881287727,
      "grad_norm": 0.4548506736755371,
      "learning_rate": 0.00017563537579233325,
      "loss": 0.4763,
      "step": 6059
    },
    {
      "epoch": 12.193158953722333,
      "grad_norm": 0.40522393584251404,
      "learning_rate": 0.00017563135124257974,
      "loss": 0.4771,
      "step": 6060
    },
    {
      "epoch": 12.195171026156942,
      "grad_norm": 0.4299294352531433,
      "learning_rate": 0.00017562732669282625,
      "loss": 0.4809,
      "step": 6061
    },
    {
      "epoch": 12.19718309859155,
      "grad_norm": 0.4155794382095337,
      "learning_rate": 0.00017562330214307274,
      "loss": 0.5089,
      "step": 6062
    },
    {
      "epoch": 12.199195171026156,
      "grad_norm": 0.4412841200828552,
      "learning_rate": 0.00017561927759331928,
      "loss": 0.5131,
      "step": 6063
    },
    {
      "epoch": 12.201207243460765,
      "grad_norm": 0.41155168414115906,
      "learning_rate": 0.00017561525304356576,
      "loss": 0.4857,
      "step": 6064
    },
    {
      "epoch": 12.203219315895373,
      "grad_norm": 0.45787322521209717,
      "learning_rate": 0.00017561122849381227,
      "loss": 0.49,
      "step": 6065
    },
    {
      "epoch": 12.20523138832998,
      "grad_norm": 0.43817421793937683,
      "learning_rate": 0.00017560720394405876,
      "loss": 0.4878,
      "step": 6066
    },
    {
      "epoch": 12.207243460764587,
      "grad_norm": 0.4229501783847809,
      "learning_rate": 0.00017560317939430527,
      "loss": 0.4905,
      "step": 6067
    },
    {
      "epoch": 12.209255533199196,
      "grad_norm": 0.424969345331192,
      "learning_rate": 0.00017559915484455178,
      "loss": 0.5015,
      "step": 6068
    },
    {
      "epoch": 12.211267605633802,
      "grad_norm": 0.44029590487480164,
      "learning_rate": 0.0001755951302947983,
      "loss": 0.5119,
      "step": 6069
    },
    {
      "epoch": 12.21327967806841,
      "grad_norm": 0.48113566637039185,
      "learning_rate": 0.00017559110574504478,
      "loss": 0.51,
      "step": 6070
    },
    {
      "epoch": 12.215291750503019,
      "grad_norm": 0.4554651081562042,
      "learning_rate": 0.0001755870811952913,
      "loss": 0.4829,
      "step": 6071
    },
    {
      "epoch": 12.217303822937625,
      "grad_norm": 0.4295019209384918,
      "learning_rate": 0.00017558305664553778,
      "loss": 0.5122,
      "step": 6072
    },
    {
      "epoch": 12.219315895372233,
      "grad_norm": 0.4379468858242035,
      "learning_rate": 0.0001755790320957843,
      "loss": 0.4998,
      "step": 6073
    },
    {
      "epoch": 12.221327967806841,
      "grad_norm": 0.41044995188713074,
      "learning_rate": 0.0001755750075460308,
      "loss": 0.4882,
      "step": 6074
    },
    {
      "epoch": 12.223340040241448,
      "grad_norm": 0.4299137592315674,
      "learning_rate": 0.0001755709829962773,
      "loss": 0.4944,
      "step": 6075
    },
    {
      "epoch": 12.225352112676056,
      "grad_norm": 0.423356294631958,
      "learning_rate": 0.0001755669584465238,
      "loss": 0.4901,
      "step": 6076
    },
    {
      "epoch": 12.227364185110664,
      "grad_norm": 0.44712328910827637,
      "learning_rate": 0.0001755629338967703,
      "loss": 0.4815,
      "step": 6077
    },
    {
      "epoch": 12.229376257545272,
      "grad_norm": 0.42253050208091736,
      "learning_rate": 0.00017555890934701682,
      "loss": 0.4805,
      "step": 6078
    },
    {
      "epoch": 12.231388329979879,
      "grad_norm": 0.4261910021305084,
      "learning_rate": 0.0001755548847972633,
      "loss": 0.5002,
      "step": 6079
    },
    {
      "epoch": 12.233400402414487,
      "grad_norm": 0.47667571902275085,
      "learning_rate": 0.00017555086024750982,
      "loss": 0.4673,
      "step": 6080
    },
    {
      "epoch": 12.235412474849095,
      "grad_norm": 0.4207093119621277,
      "learning_rate": 0.0001755468356977563,
      "loss": 0.4791,
      "step": 6081
    },
    {
      "epoch": 12.237424547283702,
      "grad_norm": 0.4540427625179291,
      "learning_rate": 0.00017554281114800282,
      "loss": 0.479,
      "step": 6082
    },
    {
      "epoch": 12.23943661971831,
      "grad_norm": 0.43097665905952454,
      "learning_rate": 0.00017553878659824933,
      "loss": 0.5025,
      "step": 6083
    },
    {
      "epoch": 12.241448692152918,
      "grad_norm": 0.4178260266780853,
      "learning_rate": 0.00017553476204849584,
      "loss": 0.4803,
      "step": 6084
    },
    {
      "epoch": 12.243460764587525,
      "grad_norm": 0.46609771251678467,
      "learning_rate": 0.00017553073749874233,
      "loss": 0.4989,
      "step": 6085
    },
    {
      "epoch": 12.245472837022133,
      "grad_norm": 0.4340495467185974,
      "learning_rate": 0.00017552671294898884,
      "loss": 0.4667,
      "step": 6086
    },
    {
      "epoch": 12.247484909456741,
      "grad_norm": 0.47851428389549255,
      "learning_rate": 0.00017552268839923533,
      "loss": 0.4968,
      "step": 6087
    },
    {
      "epoch": 12.249496981891348,
      "grad_norm": 0.42767229676246643,
      "learning_rate": 0.00017551866384948186,
      "loss": 0.4943,
      "step": 6088
    },
    {
      "epoch": 12.251509054325956,
      "grad_norm": 0.4081425964832306,
      "learning_rate": 0.00017551463929972835,
      "loss": 0.4884,
      "step": 6089
    },
    {
      "epoch": 12.253521126760564,
      "grad_norm": 0.4324520230293274,
      "learning_rate": 0.00017551061474997486,
      "loss": 0.495,
      "step": 6090
    },
    {
      "epoch": 12.25553319919517,
      "grad_norm": 0.4291696846485138,
      "learning_rate": 0.00017550659020022135,
      "loss": 0.5109,
      "step": 6091
    },
    {
      "epoch": 12.257545271629779,
      "grad_norm": 0.5019634962081909,
      "learning_rate": 0.00017550256565046786,
      "loss": 0.5452,
      "step": 6092
    },
    {
      "epoch": 12.259557344064387,
      "grad_norm": 0.42582476139068604,
      "learning_rate": 0.00017549854110071437,
      "loss": 0.5131,
      "step": 6093
    },
    {
      "epoch": 12.261569416498993,
      "grad_norm": 0.40807488560676575,
      "learning_rate": 0.00017549451655096088,
      "loss": 0.4907,
      "step": 6094
    },
    {
      "epoch": 12.263581488933601,
      "grad_norm": 0.41101449728012085,
      "learning_rate": 0.00017549049200120737,
      "loss": 0.4668,
      "step": 6095
    },
    {
      "epoch": 12.26559356136821,
      "grad_norm": 0.398681104183197,
      "learning_rate": 0.00017548646745145388,
      "loss": 0.4786,
      "step": 6096
    },
    {
      "epoch": 12.267605633802816,
      "grad_norm": 0.4360342025756836,
      "learning_rate": 0.00017548244290170037,
      "loss": 0.53,
      "step": 6097
    },
    {
      "epoch": 12.269617706237424,
      "grad_norm": 0.4212886393070221,
      "learning_rate": 0.0001754784183519469,
      "loss": 0.5111,
      "step": 6098
    },
    {
      "epoch": 12.271629778672033,
      "grad_norm": 0.40460652112960815,
      "learning_rate": 0.0001754743938021934,
      "loss": 0.4989,
      "step": 6099
    },
    {
      "epoch": 12.273641851106639,
      "grad_norm": 0.43433210253715515,
      "learning_rate": 0.0001754703692524399,
      "loss": 0.4943,
      "step": 6100
    },
    {
      "epoch": 12.275653923541247,
      "grad_norm": 0.43942245841026306,
      "learning_rate": 0.0001754663447026864,
      "loss": 0.4778,
      "step": 6101
    },
    {
      "epoch": 12.277665995975855,
      "grad_norm": 0.4179498851299286,
      "learning_rate": 0.0001754623201529329,
      "loss": 0.4497,
      "step": 6102
    },
    {
      "epoch": 12.279678068410464,
      "grad_norm": 0.3956325650215149,
      "learning_rate": 0.0001754582956031794,
      "loss": 0.4825,
      "step": 6103
    },
    {
      "epoch": 12.28169014084507,
      "grad_norm": 0.4618877172470093,
      "learning_rate": 0.00017545427105342592,
      "loss": 0.5017,
      "step": 6104
    },
    {
      "epoch": 12.283702213279678,
      "grad_norm": 0.40659767389297485,
      "learning_rate": 0.0001754502465036724,
      "loss": 0.4955,
      "step": 6105
    },
    {
      "epoch": 12.285714285714286,
      "grad_norm": 0.43372827768325806,
      "learning_rate": 0.00017544622195391892,
      "loss": 0.4811,
      "step": 6106
    },
    {
      "epoch": 12.287726358148893,
      "grad_norm": 0.4289754033088684,
      "learning_rate": 0.0001754421974041654,
      "loss": 0.5018,
      "step": 6107
    },
    {
      "epoch": 12.289738430583501,
      "grad_norm": 0.4345610439777374,
      "learning_rate": 0.00017543817285441192,
      "loss": 0.4907,
      "step": 6108
    },
    {
      "epoch": 12.29175050301811,
      "grad_norm": 0.4229840040206909,
      "learning_rate": 0.00017543414830465843,
      "loss": 0.5206,
      "step": 6109
    },
    {
      "epoch": 12.293762575452716,
      "grad_norm": 0.42308667302131653,
      "learning_rate": 0.00017543012375490492,
      "loss": 0.5208,
      "step": 6110
    },
    {
      "epoch": 12.295774647887324,
      "grad_norm": 0.41188904643058777,
      "learning_rate": 0.00017542609920515143,
      "loss": 0.4803,
      "step": 6111
    },
    {
      "epoch": 12.297786720321932,
      "grad_norm": 0.42994511127471924,
      "learning_rate": 0.00017542207465539794,
      "loss": 0.5034,
      "step": 6112
    },
    {
      "epoch": 12.299798792756539,
      "grad_norm": 0.4320897161960602,
      "learning_rate": 0.00017541805010564445,
      "loss": 0.5066,
      "step": 6113
    },
    {
      "epoch": 12.301810865191147,
      "grad_norm": 0.408418208360672,
      "learning_rate": 0.00017541402555589094,
      "loss": 0.4782,
      "step": 6114
    },
    {
      "epoch": 12.303822937625755,
      "grad_norm": 0.44361159205436707,
      "learning_rate": 0.00017541000100613745,
      "loss": 0.4918,
      "step": 6115
    },
    {
      "epoch": 12.305835010060362,
      "grad_norm": 0.4386967420578003,
      "learning_rate": 0.00017540597645638394,
      "loss": 0.4994,
      "step": 6116
    },
    {
      "epoch": 12.30784708249497,
      "grad_norm": 0.43293458223342896,
      "learning_rate": 0.00017540195190663045,
      "loss": 0.4696,
      "step": 6117
    },
    {
      "epoch": 12.309859154929578,
      "grad_norm": 0.40657252073287964,
      "learning_rate": 0.00017539792735687696,
      "loss": 0.4797,
      "step": 6118
    },
    {
      "epoch": 12.311871227364184,
      "grad_norm": 0.44363081455230713,
      "learning_rate": 0.00017539390280712347,
      "loss": 0.496,
      "step": 6119
    },
    {
      "epoch": 12.313883299798793,
      "grad_norm": 0.4100407063961029,
      "learning_rate": 0.00017538987825736996,
      "loss": 0.4471,
      "step": 6120
    },
    {
      "epoch": 12.3158953722334,
      "grad_norm": 0.42580944299697876,
      "learning_rate": 0.00017538585370761647,
      "loss": 0.4917,
      "step": 6121
    },
    {
      "epoch": 12.317907444668007,
      "grad_norm": 0.4242653548717499,
      "learning_rate": 0.00017538182915786295,
      "loss": 0.4841,
      "step": 6122
    },
    {
      "epoch": 12.319919517102615,
      "grad_norm": 0.4456191658973694,
      "learning_rate": 0.0001753778046081095,
      "loss": 0.4757,
      "step": 6123
    },
    {
      "epoch": 12.321931589537224,
      "grad_norm": 0.45187270641326904,
      "learning_rate": 0.00017537378005835598,
      "loss": 0.4939,
      "step": 6124
    },
    {
      "epoch": 12.323943661971832,
      "grad_norm": 0.43174105882644653,
      "learning_rate": 0.0001753697555086025,
      "loss": 0.5377,
      "step": 6125
    },
    {
      "epoch": 12.325955734406438,
      "grad_norm": 0.43204089999198914,
      "learning_rate": 0.00017536573095884898,
      "loss": 0.4976,
      "step": 6126
    },
    {
      "epoch": 12.327967806841047,
      "grad_norm": 0.42125648260116577,
      "learning_rate": 0.0001753617064090955,
      "loss": 0.459,
      "step": 6127
    },
    {
      "epoch": 12.329979879275655,
      "grad_norm": 0.44312790036201477,
      "learning_rate": 0.000175357681859342,
      "loss": 0.4563,
      "step": 6128
    },
    {
      "epoch": 12.331991951710261,
      "grad_norm": 0.4464148283004761,
      "learning_rate": 0.0001753536573095885,
      "loss": 0.4908,
      "step": 6129
    },
    {
      "epoch": 12.33400402414487,
      "grad_norm": 0.43177682161331177,
      "learning_rate": 0.000175349632759835,
      "loss": 0.5082,
      "step": 6130
    },
    {
      "epoch": 12.336016096579478,
      "grad_norm": 0.4436357915401459,
      "learning_rate": 0.0001753456082100815,
      "loss": 0.4851,
      "step": 6131
    },
    {
      "epoch": 12.338028169014084,
      "grad_norm": 0.43616822361946106,
      "learning_rate": 0.000175341583660328,
      "loss": 0.4881,
      "step": 6132
    },
    {
      "epoch": 12.340040241448692,
      "grad_norm": 0.43657800555229187,
      "learning_rate": 0.00017533755911057453,
      "loss": 0.4955,
      "step": 6133
    },
    {
      "epoch": 12.3420523138833,
      "grad_norm": 0.4250888526439667,
      "learning_rate": 0.00017533353456082102,
      "loss": 0.4889,
      "step": 6134
    },
    {
      "epoch": 12.344064386317907,
      "grad_norm": 0.43077635765075684,
      "learning_rate": 0.00017532951001106753,
      "loss": 0.5133,
      "step": 6135
    },
    {
      "epoch": 12.346076458752515,
      "grad_norm": 0.4221639633178711,
      "learning_rate": 0.00017532548546131402,
      "loss": 0.4921,
      "step": 6136
    },
    {
      "epoch": 12.348088531187123,
      "grad_norm": 0.4454423189163208,
      "learning_rate": 0.00017532146091156053,
      "loss": 0.4636,
      "step": 6137
    },
    {
      "epoch": 12.35010060362173,
      "grad_norm": 0.43136242032051086,
      "learning_rate": 0.00017531743636180704,
      "loss": 0.5087,
      "step": 6138
    },
    {
      "epoch": 12.352112676056338,
      "grad_norm": 0.4346921741962433,
      "learning_rate": 0.00017531341181205355,
      "loss": 0.526,
      "step": 6139
    },
    {
      "epoch": 12.354124748490946,
      "grad_norm": 0.4205692708492279,
      "learning_rate": 0.00017530938726230004,
      "loss": 0.4849,
      "step": 6140
    },
    {
      "epoch": 12.356136820925553,
      "grad_norm": 0.4438459873199463,
      "learning_rate": 0.00017530536271254655,
      "loss": 0.4927,
      "step": 6141
    },
    {
      "epoch": 12.35814889336016,
      "grad_norm": 0.41736900806427,
      "learning_rate": 0.00017530133816279304,
      "loss": 0.5014,
      "step": 6142
    },
    {
      "epoch": 12.360160965794769,
      "grad_norm": 0.43096649646759033,
      "learning_rate": 0.00017529731361303955,
      "loss": 0.5017,
      "step": 6143
    },
    {
      "epoch": 12.362173038229376,
      "grad_norm": 0.43210843205451965,
      "learning_rate": 0.00017529328906328606,
      "loss": 0.5057,
      "step": 6144
    },
    {
      "epoch": 12.364185110663984,
      "grad_norm": 0.4317132830619812,
      "learning_rate": 0.00017528926451353255,
      "loss": 0.5154,
      "step": 6145
    },
    {
      "epoch": 12.366197183098592,
      "grad_norm": 0.4213290512561798,
      "learning_rate": 0.00017528523996377906,
      "loss": 0.5071,
      "step": 6146
    },
    {
      "epoch": 12.368209255533198,
      "grad_norm": 0.4431968033313751,
      "learning_rate": 0.00017528121541402557,
      "loss": 0.5054,
      "step": 6147
    },
    {
      "epoch": 12.370221327967807,
      "grad_norm": 0.43435969948768616,
      "learning_rate": 0.00017527719086427208,
      "loss": 0.4638,
      "step": 6148
    },
    {
      "epoch": 12.372233400402415,
      "grad_norm": 0.42402610182762146,
      "learning_rate": 0.00017527316631451857,
      "loss": 0.4994,
      "step": 6149
    },
    {
      "epoch": 12.374245472837021,
      "grad_norm": 0.43589234352111816,
      "learning_rate": 0.00017526914176476508,
      "loss": 0.4906,
      "step": 6150
    },
    {
      "epoch": 12.37625754527163,
      "grad_norm": 0.44503408670425415,
      "learning_rate": 0.00017526511721501156,
      "loss": 0.5478,
      "step": 6151
    },
    {
      "epoch": 12.378269617706238,
      "grad_norm": 0.43540260195732117,
      "learning_rate": 0.00017526109266525808,
      "loss": 0.4982,
      "step": 6152
    },
    {
      "epoch": 12.380281690140846,
      "grad_norm": 0.4268137812614441,
      "learning_rate": 0.0001752570681155046,
      "loss": 0.5006,
      "step": 6153
    },
    {
      "epoch": 12.382293762575452,
      "grad_norm": 0.43277764320373535,
      "learning_rate": 0.0001752530435657511,
      "loss": 0.538,
      "step": 6154
    },
    {
      "epoch": 12.38430583501006,
      "grad_norm": 0.4243311583995819,
      "learning_rate": 0.0001752490190159976,
      "loss": 0.4879,
      "step": 6155
    },
    {
      "epoch": 12.386317907444669,
      "grad_norm": 0.4309258460998535,
      "learning_rate": 0.0001752449944662441,
      "loss": 0.4776,
      "step": 6156
    },
    {
      "epoch": 12.388329979879275,
      "grad_norm": 0.4114190340042114,
      "learning_rate": 0.00017524096991649058,
      "loss": 0.4938,
      "step": 6157
    },
    {
      "epoch": 12.390342052313883,
      "grad_norm": 0.4074550271034241,
      "learning_rate": 0.00017523694536673712,
      "loss": 0.4921,
      "step": 6158
    },
    {
      "epoch": 12.392354124748492,
      "grad_norm": 0.4302509129047394,
      "learning_rate": 0.0001752329208169836,
      "loss": 0.4879,
      "step": 6159
    },
    {
      "epoch": 12.394366197183098,
      "grad_norm": 0.4270973801612854,
      "learning_rate": 0.00017522889626723012,
      "loss": 0.4682,
      "step": 6160
    },
    {
      "epoch": 12.396378269617706,
      "grad_norm": 0.4182879626750946,
      "learning_rate": 0.0001752248717174766,
      "loss": 0.4668,
      "step": 6161
    },
    {
      "epoch": 12.398390342052314,
      "grad_norm": 0.4312901198863983,
      "learning_rate": 0.00017522084716772312,
      "loss": 0.4902,
      "step": 6162
    },
    {
      "epoch": 12.400402414486921,
      "grad_norm": 0.4142087399959564,
      "learning_rate": 0.00017521682261796963,
      "loss": 0.5139,
      "step": 6163
    },
    {
      "epoch": 12.40241448692153,
      "grad_norm": 0.4107074439525604,
      "learning_rate": 0.00017521279806821614,
      "loss": 0.4948,
      "step": 6164
    },
    {
      "epoch": 12.404426559356137,
      "grad_norm": 0.4327327311038971,
      "learning_rate": 0.00017520877351846263,
      "loss": 0.5287,
      "step": 6165
    },
    {
      "epoch": 12.406438631790744,
      "grad_norm": 0.4303314983844757,
      "learning_rate": 0.00017520474896870914,
      "loss": 0.4751,
      "step": 6166
    },
    {
      "epoch": 12.408450704225352,
      "grad_norm": 0.4451731741428375,
      "learning_rate": 0.00017520072441895562,
      "loss": 0.501,
      "step": 6167
    },
    {
      "epoch": 12.41046277665996,
      "grad_norm": 0.47532516717910767,
      "learning_rate": 0.00017519669986920216,
      "loss": 0.5722,
      "step": 6168
    },
    {
      "epoch": 12.412474849094567,
      "grad_norm": 0.41170814633369446,
      "learning_rate": 0.00017519267531944865,
      "loss": 0.4823,
      "step": 6169
    },
    {
      "epoch": 12.414486921529175,
      "grad_norm": 0.4277200698852539,
      "learning_rate": 0.00017518865076969516,
      "loss": 0.4989,
      "step": 6170
    },
    {
      "epoch": 12.416498993963783,
      "grad_norm": 0.4444698989391327,
      "learning_rate": 0.00017518462621994165,
      "loss": 0.5336,
      "step": 6171
    },
    {
      "epoch": 12.41851106639839,
      "grad_norm": 0.43296992778778076,
      "learning_rate": 0.00017518060167018816,
      "loss": 0.4996,
      "step": 6172
    },
    {
      "epoch": 12.420523138832998,
      "grad_norm": 0.45855143666267395,
      "learning_rate": 0.00017517657712043467,
      "loss": 0.5268,
      "step": 6173
    },
    {
      "epoch": 12.422535211267606,
      "grad_norm": 0.4470338225364685,
      "learning_rate": 0.00017517255257068118,
      "loss": 0.5089,
      "step": 6174
    },
    {
      "epoch": 12.424547283702214,
      "grad_norm": 0.41593942046165466,
      "learning_rate": 0.00017516852802092767,
      "loss": 0.4835,
      "step": 6175
    },
    {
      "epoch": 12.42655935613682,
      "grad_norm": 0.4318619966506958,
      "learning_rate": 0.00017516450347117418,
      "loss": 0.5067,
      "step": 6176
    },
    {
      "epoch": 12.428571428571429,
      "grad_norm": 0.415432870388031,
      "learning_rate": 0.00017516047892142067,
      "loss": 0.4714,
      "step": 6177
    },
    {
      "epoch": 12.430583501006037,
      "grad_norm": 0.4238690435886383,
      "learning_rate": 0.00017515645437166718,
      "loss": 0.5328,
      "step": 6178
    },
    {
      "epoch": 12.432595573440643,
      "grad_norm": 0.4087136387825012,
      "learning_rate": 0.0001751524298219137,
      "loss": 0.4575,
      "step": 6179
    },
    {
      "epoch": 12.434607645875252,
      "grad_norm": 0.4257376194000244,
      "learning_rate": 0.00017514840527216018,
      "loss": 0.4897,
      "step": 6180
    },
    {
      "epoch": 12.43661971830986,
      "grad_norm": 0.4175810217857361,
      "learning_rate": 0.0001751443807224067,
      "loss": 0.4774,
      "step": 6181
    },
    {
      "epoch": 12.438631790744466,
      "grad_norm": 0.4609586000442505,
      "learning_rate": 0.00017514035617265317,
      "loss": 0.4912,
      "step": 6182
    },
    {
      "epoch": 12.440643863179075,
      "grad_norm": 0.4315052628517151,
      "learning_rate": 0.0001751363316228997,
      "loss": 0.4944,
      "step": 6183
    },
    {
      "epoch": 12.442655935613683,
      "grad_norm": 0.442257285118103,
      "learning_rate": 0.0001751323070731462,
      "loss": 0.48,
      "step": 6184
    },
    {
      "epoch": 12.44466800804829,
      "grad_norm": 0.41223064064979553,
      "learning_rate": 0.0001751282825233927,
      "loss": 0.5253,
      "step": 6185
    },
    {
      "epoch": 12.446680080482897,
      "grad_norm": 0.4362165927886963,
      "learning_rate": 0.0001751242579736392,
      "loss": 0.5295,
      "step": 6186
    },
    {
      "epoch": 12.448692152917506,
      "grad_norm": 0.41877293586730957,
      "learning_rate": 0.0001751202334238857,
      "loss": 0.4955,
      "step": 6187
    },
    {
      "epoch": 12.450704225352112,
      "grad_norm": 0.423445463180542,
      "learning_rate": 0.0001751162088741322,
      "loss": 0.4894,
      "step": 6188
    },
    {
      "epoch": 12.45271629778672,
      "grad_norm": 0.4256284534931183,
      "learning_rate": 0.00017511218432437873,
      "loss": 0.5228,
      "step": 6189
    },
    {
      "epoch": 12.454728370221329,
      "grad_norm": 0.4308592975139618,
      "learning_rate": 0.00017510815977462522,
      "loss": 0.4743,
      "step": 6190
    },
    {
      "epoch": 12.456740442655935,
      "grad_norm": 0.4259334206581116,
      "learning_rate": 0.00017510413522487173,
      "loss": 0.5129,
      "step": 6191
    },
    {
      "epoch": 12.458752515090543,
      "grad_norm": 0.4502677023410797,
      "learning_rate": 0.0001751001106751182,
      "loss": 0.4942,
      "step": 6192
    },
    {
      "epoch": 12.460764587525151,
      "grad_norm": 0.4289110004901886,
      "learning_rate": 0.00017509608612536473,
      "loss": 0.5185,
      "step": 6193
    },
    {
      "epoch": 12.462776659959758,
      "grad_norm": 0.4014672338962555,
      "learning_rate": 0.00017509206157561124,
      "loss": 0.4873,
      "step": 6194
    },
    {
      "epoch": 12.464788732394366,
      "grad_norm": 0.4416649043560028,
      "learning_rate": 0.00017508803702585775,
      "loss": 0.5075,
      "step": 6195
    },
    {
      "epoch": 12.466800804828974,
      "grad_norm": 0.43590018153190613,
      "learning_rate": 0.00017508401247610424,
      "loss": 0.4951,
      "step": 6196
    },
    {
      "epoch": 12.46881287726358,
      "grad_norm": 0.44175153970718384,
      "learning_rate": 0.00017507998792635075,
      "loss": 0.5171,
      "step": 6197
    },
    {
      "epoch": 12.470824949698189,
      "grad_norm": 0.4317896068096161,
      "learning_rate": 0.00017507596337659723,
      "loss": 0.4876,
      "step": 6198
    },
    {
      "epoch": 12.472837022132797,
      "grad_norm": 0.4205101728439331,
      "learning_rate": 0.00017507193882684377,
      "loss": 0.5005,
      "step": 6199
    },
    {
      "epoch": 12.474849094567404,
      "grad_norm": 0.42164409160614014,
      "learning_rate": 0.00017506791427709026,
      "loss": 0.5038,
      "step": 6200
    },
    {
      "epoch": 12.476861167002012,
      "grad_norm": 0.44166263937950134,
      "learning_rate": 0.00017506388972733677,
      "loss": 0.5307,
      "step": 6201
    },
    {
      "epoch": 12.47887323943662,
      "grad_norm": 0.43455466628074646,
      "learning_rate": 0.00017505986517758325,
      "loss": 0.4995,
      "step": 6202
    },
    {
      "epoch": 12.480885311871228,
      "grad_norm": 0.4294218420982361,
      "learning_rate": 0.00017505584062782977,
      "loss": 0.5111,
      "step": 6203
    },
    {
      "epoch": 12.482897384305835,
      "grad_norm": 0.4396190941333771,
      "learning_rate": 0.00017505181607807628,
      "loss": 0.5025,
      "step": 6204
    },
    {
      "epoch": 12.484909456740443,
      "grad_norm": 0.4440014660358429,
      "learning_rate": 0.0001750477915283228,
      "loss": 0.5235,
      "step": 6205
    },
    {
      "epoch": 12.486921529175051,
      "grad_norm": 0.40538930892944336,
      "learning_rate": 0.00017504376697856928,
      "loss": 0.5091,
      "step": 6206
    },
    {
      "epoch": 12.488933601609657,
      "grad_norm": 0.430025190114975,
      "learning_rate": 0.0001750397424288158,
      "loss": 0.4925,
      "step": 6207
    },
    {
      "epoch": 12.490945674044266,
      "grad_norm": 0.4419710636138916,
      "learning_rate": 0.00017503571787906227,
      "loss": 0.5073,
      "step": 6208
    },
    {
      "epoch": 12.492957746478874,
      "grad_norm": 0.433160662651062,
      "learning_rate": 0.00017503169332930879,
      "loss": 0.4985,
      "step": 6209
    },
    {
      "epoch": 12.49496981891348,
      "grad_norm": 0.4336908161640167,
      "learning_rate": 0.0001750276687795553,
      "loss": 0.521,
      "step": 6210
    },
    {
      "epoch": 12.496981891348089,
      "grad_norm": 0.43340936303138733,
      "learning_rate": 0.0001750236442298018,
      "loss": 0.4863,
      "step": 6211
    },
    {
      "epoch": 12.498993963782697,
      "grad_norm": 0.4411217272281647,
      "learning_rate": 0.0001750196196800483,
      "loss": 0.5201,
      "step": 6212
    },
    {
      "epoch": 12.501006036217303,
      "grad_norm": 0.45104047656059265,
      "learning_rate": 0.0001750155951302948,
      "loss": 0.5162,
      "step": 6213
    },
    {
      "epoch": 12.503018108651911,
      "grad_norm": 0.4344188868999481,
      "learning_rate": 0.00017501157058054132,
      "loss": 0.5142,
      "step": 6214
    },
    {
      "epoch": 12.50503018108652,
      "grad_norm": 0.41813942790031433,
      "learning_rate": 0.0001750075460307878,
      "loss": 0.5186,
      "step": 6215
    },
    {
      "epoch": 12.507042253521126,
      "grad_norm": 0.41533082723617554,
      "learning_rate": 0.00017500352148103432,
      "loss": 0.4894,
      "step": 6216
    },
    {
      "epoch": 12.509054325955734,
      "grad_norm": 0.440809428691864,
      "learning_rate": 0.0001749994969312808,
      "loss": 0.509,
      "step": 6217
    },
    {
      "epoch": 12.511066398390343,
      "grad_norm": 0.44065266847610474,
      "learning_rate": 0.00017499547238152731,
      "loss": 0.5134,
      "step": 6218
    },
    {
      "epoch": 12.513078470824949,
      "grad_norm": 0.4214581549167633,
      "learning_rate": 0.00017499144783177383,
      "loss": 0.4729,
      "step": 6219
    },
    {
      "epoch": 12.515090543259557,
      "grad_norm": 0.4355221092700958,
      "learning_rate": 0.00017498742328202034,
      "loss": 0.5204,
      "step": 6220
    },
    {
      "epoch": 12.517102615694165,
      "grad_norm": 0.4334027171134949,
      "learning_rate": 0.00017498339873226682,
      "loss": 0.5021,
      "step": 6221
    },
    {
      "epoch": 12.519114688128772,
      "grad_norm": 0.4444841146469116,
      "learning_rate": 0.00017497937418251334,
      "loss": 0.5171,
      "step": 6222
    },
    {
      "epoch": 12.52112676056338,
      "grad_norm": 0.42213720083236694,
      "learning_rate": 0.00017497534963275982,
      "loss": 0.5227,
      "step": 6223
    },
    {
      "epoch": 12.523138832997988,
      "grad_norm": 0.4512346088886261,
      "learning_rate": 0.00017497132508300636,
      "loss": 0.4902,
      "step": 6224
    },
    {
      "epoch": 12.525150905432596,
      "grad_norm": 0.4289250075817108,
      "learning_rate": 0.00017496730053325285,
      "loss": 0.508,
      "step": 6225
    },
    {
      "epoch": 12.527162977867203,
      "grad_norm": 0.42437544465065,
      "learning_rate": 0.00017496327598349936,
      "loss": 0.5004,
      "step": 6226
    },
    {
      "epoch": 12.529175050301811,
      "grad_norm": 0.4142768085002899,
      "learning_rate": 0.00017495925143374584,
      "loss": 0.5038,
      "step": 6227
    },
    {
      "epoch": 12.53118712273642,
      "grad_norm": 0.4136442244052887,
      "learning_rate": 0.00017495522688399236,
      "loss": 0.4589,
      "step": 6228
    },
    {
      "epoch": 12.533199195171026,
      "grad_norm": 0.4361408054828644,
      "learning_rate": 0.00017495120233423887,
      "loss": 0.5128,
      "step": 6229
    },
    {
      "epoch": 12.535211267605634,
      "grad_norm": 0.4175238013267517,
      "learning_rate": 0.00017494717778448538,
      "loss": 0.4959,
      "step": 6230
    },
    {
      "epoch": 12.537223340040242,
      "grad_norm": 0.4475669860839844,
      "learning_rate": 0.00017494315323473186,
      "loss": 0.5219,
      "step": 6231
    },
    {
      "epoch": 12.539235412474849,
      "grad_norm": 0.41642120480537415,
      "learning_rate": 0.00017493912868497838,
      "loss": 0.5121,
      "step": 6232
    },
    {
      "epoch": 12.541247484909457,
      "grad_norm": 0.44734129309654236,
      "learning_rate": 0.00017493510413522486,
      "loss": 0.5757,
      "step": 6233
    },
    {
      "epoch": 12.543259557344065,
      "grad_norm": 0.4282185435295105,
      "learning_rate": 0.0001749310795854714,
      "loss": 0.52,
      "step": 6234
    },
    {
      "epoch": 12.545271629778671,
      "grad_norm": 0.420809805393219,
      "learning_rate": 0.0001749270550357179,
      "loss": 0.4975,
      "step": 6235
    },
    {
      "epoch": 12.54728370221328,
      "grad_norm": 0.3992922902107239,
      "learning_rate": 0.0001749230304859644,
      "loss": 0.4901,
      "step": 6236
    },
    {
      "epoch": 12.549295774647888,
      "grad_norm": 0.4261743724346161,
      "learning_rate": 0.00017491900593621088,
      "loss": 0.5192,
      "step": 6237
    },
    {
      "epoch": 12.551307847082494,
      "grad_norm": 0.4378593862056732,
      "learning_rate": 0.0001749149813864574,
      "loss": 0.5128,
      "step": 6238
    },
    {
      "epoch": 12.553319919517103,
      "grad_norm": 0.40903976559638977,
      "learning_rate": 0.0001749109568367039,
      "loss": 0.4894,
      "step": 6239
    },
    {
      "epoch": 12.55533199195171,
      "grad_norm": 0.42510882019996643,
      "learning_rate": 0.00017490693228695042,
      "loss": 0.4632,
      "step": 6240
    },
    {
      "epoch": 12.557344064386317,
      "grad_norm": 0.46046239137649536,
      "learning_rate": 0.0001749029077371969,
      "loss": 0.5023,
      "step": 6241
    },
    {
      "epoch": 12.559356136820925,
      "grad_norm": 0.47594311833381653,
      "learning_rate": 0.00017489888318744342,
      "loss": 0.4779,
      "step": 6242
    },
    {
      "epoch": 12.561368209255534,
      "grad_norm": 0.4418562948703766,
      "learning_rate": 0.0001748948586376899,
      "loss": 0.5025,
      "step": 6243
    },
    {
      "epoch": 12.56338028169014,
      "grad_norm": 0.4460093379020691,
      "learning_rate": 0.00017489083408793642,
      "loss": 0.5154,
      "step": 6244
    },
    {
      "epoch": 12.565392354124748,
      "grad_norm": 0.4340630769729614,
      "learning_rate": 0.00017488680953818293,
      "loss": 0.4786,
      "step": 6245
    },
    {
      "epoch": 12.567404426559357,
      "grad_norm": 0.42567646503448486,
      "learning_rate": 0.00017488278498842944,
      "loss": 0.4926,
      "step": 6246
    },
    {
      "epoch": 12.569416498993963,
      "grad_norm": 0.4526920020580292,
      "learning_rate": 0.00017487876043867592,
      "loss": 0.5095,
      "step": 6247
    },
    {
      "epoch": 12.571428571428571,
      "grad_norm": 0.42861121892929077,
      "learning_rate": 0.00017487473588892244,
      "loss": 0.4862,
      "step": 6248
    },
    {
      "epoch": 12.57344064386318,
      "grad_norm": 0.4126066565513611,
      "learning_rate": 0.00017487071133916895,
      "loss": 0.5054,
      "step": 6249
    },
    {
      "epoch": 12.575452716297786,
      "grad_norm": 0.42431995272636414,
      "learning_rate": 0.00017486668678941543,
      "loss": 0.5261,
      "step": 6250
    },
    {
      "epoch": 12.577464788732394,
      "grad_norm": 0.4207956790924072,
      "learning_rate": 0.00017486266223966195,
      "loss": 0.4903,
      "step": 6251
    },
    {
      "epoch": 12.579476861167002,
      "grad_norm": 0.3973749279975891,
      "learning_rate": 0.00017485863768990843,
      "loss": 0.5381,
      "step": 6252
    },
    {
      "epoch": 12.58148893360161,
      "grad_norm": 0.44096988439559937,
      "learning_rate": 0.00017485461314015494,
      "loss": 0.5748,
      "step": 6253
    },
    {
      "epoch": 12.583501006036217,
      "grad_norm": 0.4437068700790405,
      "learning_rate": 0.00017485058859040146,
      "loss": 0.519,
      "step": 6254
    },
    {
      "epoch": 12.585513078470825,
      "grad_norm": 0.4398098886013031,
      "learning_rate": 0.00017484656404064797,
      "loss": 0.5259,
      "step": 6255
    },
    {
      "epoch": 12.587525150905433,
      "grad_norm": 0.4060852825641632,
      "learning_rate": 0.00017484253949089445,
      "loss": 0.5246,
      "step": 6256
    },
    {
      "epoch": 12.58953722334004,
      "grad_norm": 0.44458895921707153,
      "learning_rate": 0.00017483851494114097,
      "loss": 0.5033,
      "step": 6257
    },
    {
      "epoch": 12.591549295774648,
      "grad_norm": 0.44689446687698364,
      "learning_rate": 0.00017483449039138745,
      "loss": 0.5564,
      "step": 6258
    },
    {
      "epoch": 12.593561368209256,
      "grad_norm": 0.46054962277412415,
      "learning_rate": 0.000174830465841634,
      "loss": 0.5281,
      "step": 6259
    },
    {
      "epoch": 12.595573440643863,
      "grad_norm": 0.4335819482803345,
      "learning_rate": 0.00017482644129188048,
      "loss": 0.4968,
      "step": 6260
    },
    {
      "epoch": 12.59758551307847,
      "grad_norm": 0.43240582942962646,
      "learning_rate": 0.000174822416742127,
      "loss": 0.5069,
      "step": 6261
    },
    {
      "epoch": 12.599597585513079,
      "grad_norm": 0.4073638617992401,
      "learning_rate": 0.00017481839219237347,
      "loss": 0.4918,
      "step": 6262
    },
    {
      "epoch": 12.601609657947686,
      "grad_norm": 0.41333499550819397,
      "learning_rate": 0.00017481436764261998,
      "loss": 0.471,
      "step": 6263
    },
    {
      "epoch": 12.603621730382294,
      "grad_norm": 0.46880853176116943,
      "learning_rate": 0.0001748103430928665,
      "loss": 0.5004,
      "step": 6264
    },
    {
      "epoch": 12.605633802816902,
      "grad_norm": 0.47993579506874084,
      "learning_rate": 0.000174806318543113,
      "loss": 0.5206,
      "step": 6265
    },
    {
      "epoch": 12.607645875251508,
      "grad_norm": 0.4252217710018158,
      "learning_rate": 0.0001748022939933595,
      "loss": 0.5091,
      "step": 6266
    },
    {
      "epoch": 12.609657947686117,
      "grad_norm": 0.4171600937843323,
      "learning_rate": 0.000174798269443606,
      "loss": 0.4747,
      "step": 6267
    },
    {
      "epoch": 12.611670020120725,
      "grad_norm": 0.41738593578338623,
      "learning_rate": 0.0001747942448938525,
      "loss": 0.4986,
      "step": 6268
    },
    {
      "epoch": 12.613682092555331,
      "grad_norm": 0.43312525749206543,
      "learning_rate": 0.00017479022034409903,
      "loss": 0.4981,
      "step": 6269
    },
    {
      "epoch": 12.61569416498994,
      "grad_norm": 0.4168929159641266,
      "learning_rate": 0.00017478619579434552,
      "loss": 0.4827,
      "step": 6270
    },
    {
      "epoch": 12.617706237424548,
      "grad_norm": 0.4353489279747009,
      "learning_rate": 0.00017478217124459203,
      "loss": 0.5143,
      "step": 6271
    },
    {
      "epoch": 12.619718309859154,
      "grad_norm": 0.4263094365596771,
      "learning_rate": 0.0001747781466948385,
      "loss": 0.4835,
      "step": 6272
    },
    {
      "epoch": 12.621730382293762,
      "grad_norm": 0.45408010482788086,
      "learning_rate": 0.00017477412214508503,
      "loss": 0.5237,
      "step": 6273
    },
    {
      "epoch": 12.62374245472837,
      "grad_norm": 0.42756587266921997,
      "learning_rate": 0.00017477009759533154,
      "loss": 0.4887,
      "step": 6274
    },
    {
      "epoch": 12.625754527162979,
      "grad_norm": 0.43490612506866455,
      "learning_rate": 0.00017476607304557805,
      "loss": 0.4558,
      "step": 6275
    },
    {
      "epoch": 12.627766599597585,
      "grad_norm": 0.44707638025283813,
      "learning_rate": 0.00017476204849582453,
      "loss": 0.5072,
      "step": 6276
    },
    {
      "epoch": 12.629778672032193,
      "grad_norm": 0.43426045775413513,
      "learning_rate": 0.00017475802394607105,
      "loss": 0.5299,
      "step": 6277
    },
    {
      "epoch": 12.631790744466802,
      "grad_norm": 0.42832091450691223,
      "learning_rate": 0.00017475399939631753,
      "loss": 0.5088,
      "step": 6278
    },
    {
      "epoch": 12.633802816901408,
      "grad_norm": 0.43421733379364014,
      "learning_rate": 0.00017474997484656404,
      "loss": 0.5126,
      "step": 6279
    },
    {
      "epoch": 12.635814889336016,
      "grad_norm": 0.43364813923835754,
      "learning_rate": 0.00017474595029681056,
      "loss": 0.4897,
      "step": 6280
    },
    {
      "epoch": 12.637826961770624,
      "grad_norm": 0.40105101466178894,
      "learning_rate": 0.00017474192574705707,
      "loss": 0.4631,
      "step": 6281
    },
    {
      "epoch": 12.639839034205231,
      "grad_norm": 0.44200101494789124,
      "learning_rate": 0.00017473790119730355,
      "loss": 0.4646,
      "step": 6282
    },
    {
      "epoch": 12.64185110663984,
      "grad_norm": 0.4103240966796875,
      "learning_rate": 0.00017473387664755007,
      "loss": 0.4823,
      "step": 6283
    },
    {
      "epoch": 12.643863179074447,
      "grad_norm": 0.42067742347717285,
      "learning_rate": 0.00017472985209779658,
      "loss": 0.4826,
      "step": 6284
    },
    {
      "epoch": 12.645875251509054,
      "grad_norm": 0.45608270168304443,
      "learning_rate": 0.00017472582754804306,
      "loss": 0.5544,
      "step": 6285
    },
    {
      "epoch": 12.647887323943662,
      "grad_norm": 0.4393139183521271,
      "learning_rate": 0.00017472180299828958,
      "loss": 0.4886,
      "step": 6286
    },
    {
      "epoch": 12.64989939637827,
      "grad_norm": 0.4330119788646698,
      "learning_rate": 0.00017471777844853606,
      "loss": 0.4774,
      "step": 6287
    },
    {
      "epoch": 12.651911468812877,
      "grad_norm": 0.4468306601047516,
      "learning_rate": 0.00017471375389878257,
      "loss": 0.4954,
      "step": 6288
    },
    {
      "epoch": 12.653923541247485,
      "grad_norm": 0.4266478717327118,
      "learning_rate": 0.00017470972934902909,
      "loss": 0.5038,
      "step": 6289
    },
    {
      "epoch": 12.655935613682093,
      "grad_norm": 0.4295567274093628,
      "learning_rate": 0.0001747057047992756,
      "loss": 0.4945,
      "step": 6290
    },
    {
      "epoch": 12.6579476861167,
      "grad_norm": 0.42877069115638733,
      "learning_rate": 0.00017470168024952208,
      "loss": 0.4789,
      "step": 6291
    },
    {
      "epoch": 12.659959758551308,
      "grad_norm": 0.4357261657714844,
      "learning_rate": 0.0001746976556997686,
      "loss": 0.5253,
      "step": 6292
    },
    {
      "epoch": 12.661971830985916,
      "grad_norm": 0.43583163619041443,
      "learning_rate": 0.00017469363115001508,
      "loss": 0.5229,
      "step": 6293
    },
    {
      "epoch": 12.663983903420522,
      "grad_norm": 0.455544650554657,
      "learning_rate": 0.00017468960660026162,
      "loss": 0.5074,
      "step": 6294
    },
    {
      "epoch": 12.66599597585513,
      "grad_norm": 0.43018269538879395,
      "learning_rate": 0.0001746855820505081,
      "loss": 0.4735,
      "step": 6295
    },
    {
      "epoch": 12.668008048289739,
      "grad_norm": 0.48427459597587585,
      "learning_rate": 0.00017468155750075462,
      "loss": 0.4987,
      "step": 6296
    },
    {
      "epoch": 12.670020120724345,
      "grad_norm": 0.4267004728317261,
      "learning_rate": 0.0001746775329510011,
      "loss": 0.484,
      "step": 6297
    },
    {
      "epoch": 12.672032193158953,
      "grad_norm": 0.42681196331977844,
      "learning_rate": 0.00017467350840124761,
      "loss": 0.5116,
      "step": 6298
    },
    {
      "epoch": 12.674044265593562,
      "grad_norm": 0.4205760955810547,
      "learning_rate": 0.00017466948385149413,
      "loss": 0.4913,
      "step": 6299
    },
    {
      "epoch": 12.676056338028168,
      "grad_norm": 0.4314754903316498,
      "learning_rate": 0.00017466545930174064,
      "loss": 0.499,
      "step": 6300
    },
    {
      "epoch": 12.678068410462776,
      "grad_norm": 0.4285828769207001,
      "learning_rate": 0.00017466143475198712,
      "loss": 0.5037,
      "step": 6301
    },
    {
      "epoch": 12.680080482897385,
      "grad_norm": 0.4390956461429596,
      "learning_rate": 0.00017465741020223364,
      "loss": 0.4983,
      "step": 6302
    },
    {
      "epoch": 12.682092555331993,
      "grad_norm": 0.4383014142513275,
      "learning_rate": 0.00017465338565248012,
      "loss": 0.5304,
      "step": 6303
    },
    {
      "epoch": 12.6841046277666,
      "grad_norm": 0.4416881799697876,
      "learning_rate": 0.00017464936110272666,
      "loss": 0.4981,
      "step": 6304
    },
    {
      "epoch": 12.686116700201207,
      "grad_norm": 0.4122285842895508,
      "learning_rate": 0.00017464533655297315,
      "loss": 0.4909,
      "step": 6305
    },
    {
      "epoch": 12.688128772635816,
      "grad_norm": 0.43287554383277893,
      "learning_rate": 0.00017464131200321966,
      "loss": 0.5407,
      "step": 6306
    },
    {
      "epoch": 12.690140845070422,
      "grad_norm": 0.4206686317920685,
      "learning_rate": 0.00017463728745346614,
      "loss": 0.5289,
      "step": 6307
    },
    {
      "epoch": 12.69215291750503,
      "grad_norm": 0.40129849314689636,
      "learning_rate": 0.00017463326290371265,
      "loss": 0.4953,
      "step": 6308
    },
    {
      "epoch": 12.694164989939638,
      "grad_norm": 0.4711688160896301,
      "learning_rate": 0.00017462923835395917,
      "loss": 0.5246,
      "step": 6309
    },
    {
      "epoch": 12.696177062374245,
      "grad_norm": 0.4466496706008911,
      "learning_rate": 0.00017462521380420568,
      "loss": 0.5117,
      "step": 6310
    },
    {
      "epoch": 12.698189134808853,
      "grad_norm": 0.43272194266319275,
      "learning_rate": 0.00017462118925445216,
      "loss": 0.5215,
      "step": 6311
    },
    {
      "epoch": 12.700201207243461,
      "grad_norm": 0.46087998151779175,
      "learning_rate": 0.00017461716470469868,
      "loss": 0.497,
      "step": 6312
    },
    {
      "epoch": 12.702213279678068,
      "grad_norm": 0.42412814497947693,
      "learning_rate": 0.00017461314015494516,
      "loss": 0.5026,
      "step": 6313
    },
    {
      "epoch": 12.704225352112676,
      "grad_norm": 0.4220765233039856,
      "learning_rate": 0.00017460911560519167,
      "loss": 0.5134,
      "step": 6314
    },
    {
      "epoch": 12.706237424547284,
      "grad_norm": 0.44619184732437134,
      "learning_rate": 0.00017460509105543819,
      "loss": 0.5202,
      "step": 6315
    },
    {
      "epoch": 12.70824949698189,
      "grad_norm": 0.4091005027294159,
      "learning_rate": 0.0001746010665056847,
      "loss": 0.521,
      "step": 6316
    },
    {
      "epoch": 12.710261569416499,
      "grad_norm": 0.45580771565437317,
      "learning_rate": 0.00017459704195593118,
      "loss": 0.5646,
      "step": 6317
    },
    {
      "epoch": 12.712273641851107,
      "grad_norm": 0.42031192779541016,
      "learning_rate": 0.0001745930174061777,
      "loss": 0.4935,
      "step": 6318
    },
    {
      "epoch": 12.714285714285714,
      "grad_norm": 0.40513309836387634,
      "learning_rate": 0.0001745889928564242,
      "loss": 0.5,
      "step": 6319
    },
    {
      "epoch": 12.716297786720322,
      "grad_norm": 0.4105813205242157,
      "learning_rate": 0.0001745849683066707,
      "loss": 0.4858,
      "step": 6320
    },
    {
      "epoch": 12.71830985915493,
      "grad_norm": 0.43718960881233215,
      "learning_rate": 0.0001745809437569172,
      "loss": 0.518,
      "step": 6321
    },
    {
      "epoch": 12.720321931589538,
      "grad_norm": 0.43086540699005127,
      "learning_rate": 0.0001745769192071637,
      "loss": 0.474,
      "step": 6322
    },
    {
      "epoch": 12.722334004024145,
      "grad_norm": 0.4381541609764099,
      "learning_rate": 0.0001745728946574102,
      "loss": 0.5391,
      "step": 6323
    },
    {
      "epoch": 12.724346076458753,
      "grad_norm": 0.4257146418094635,
      "learning_rate": 0.00017456887010765671,
      "loss": 0.4856,
      "step": 6324
    },
    {
      "epoch": 12.726358148893361,
      "grad_norm": 0.4353616237640381,
      "learning_rate": 0.00017456484555790323,
      "loss": 0.5157,
      "step": 6325
    },
    {
      "epoch": 12.728370221327967,
      "grad_norm": 0.4145623445510864,
      "learning_rate": 0.0001745608210081497,
      "loss": 0.4767,
      "step": 6326
    },
    {
      "epoch": 12.730382293762576,
      "grad_norm": 0.4472031891345978,
      "learning_rate": 0.00017455679645839622,
      "loss": 0.5214,
      "step": 6327
    },
    {
      "epoch": 12.732394366197184,
      "grad_norm": 0.44209909439086914,
      "learning_rate": 0.0001745527719086427,
      "loss": 0.5153,
      "step": 6328
    },
    {
      "epoch": 12.73440643863179,
      "grad_norm": 0.40437522530555725,
      "learning_rate": 0.00017454874735888925,
      "loss": 0.4918,
      "step": 6329
    },
    {
      "epoch": 12.736418511066399,
      "grad_norm": 0.4123278260231018,
      "learning_rate": 0.00017454472280913573,
      "loss": 0.5366,
      "step": 6330
    },
    {
      "epoch": 12.738430583501007,
      "grad_norm": 0.44192424416542053,
      "learning_rate": 0.00017454069825938225,
      "loss": 0.5406,
      "step": 6331
    },
    {
      "epoch": 12.740442655935613,
      "grad_norm": 0.43310636281967163,
      "learning_rate": 0.00017453667370962873,
      "loss": 0.5489,
      "step": 6332
    },
    {
      "epoch": 12.742454728370221,
      "grad_norm": 0.4501393139362335,
      "learning_rate": 0.00017453264915987524,
      "loss": 0.5571,
      "step": 6333
    },
    {
      "epoch": 12.74446680080483,
      "grad_norm": 0.4203068017959595,
      "learning_rate": 0.00017452862461012176,
      "loss": 0.4644,
      "step": 6334
    },
    {
      "epoch": 12.746478873239436,
      "grad_norm": 0.43595653772354126,
      "learning_rate": 0.00017452460006036827,
      "loss": 0.5432,
      "step": 6335
    },
    {
      "epoch": 12.748490945674044,
      "grad_norm": 0.4137417674064636,
      "learning_rate": 0.00017452057551061475,
      "loss": 0.4868,
      "step": 6336
    },
    {
      "epoch": 12.750503018108652,
      "grad_norm": 0.42274564504623413,
      "learning_rate": 0.00017451655096086127,
      "loss": 0.4939,
      "step": 6337
    },
    {
      "epoch": 12.752515090543259,
      "grad_norm": 0.4561016261577606,
      "learning_rate": 0.00017451252641110775,
      "loss": 0.4851,
      "step": 6338
    },
    {
      "epoch": 12.754527162977867,
      "grad_norm": 0.41281911730766296,
      "learning_rate": 0.0001745085018613543,
      "loss": 0.5129,
      "step": 6339
    },
    {
      "epoch": 12.756539235412475,
      "grad_norm": 0.44124117493629456,
      "learning_rate": 0.00017450447731160077,
      "loss": 0.4644,
      "step": 6340
    },
    {
      "epoch": 12.758551307847082,
      "grad_norm": 0.43592363595962524,
      "learning_rate": 0.0001745004527618473,
      "loss": 0.5036,
      "step": 6341
    },
    {
      "epoch": 12.76056338028169,
      "grad_norm": 0.43771788477897644,
      "learning_rate": 0.00017449642821209377,
      "loss": 0.5289,
      "step": 6342
    },
    {
      "epoch": 12.762575452716298,
      "grad_norm": 0.4406377673149109,
      "learning_rate": 0.00017449240366234028,
      "loss": 0.5312,
      "step": 6343
    },
    {
      "epoch": 12.764587525150905,
      "grad_norm": 0.49939680099487305,
      "learning_rate": 0.0001744883791125868,
      "loss": 0.5327,
      "step": 6344
    },
    {
      "epoch": 12.766599597585513,
      "grad_norm": 0.42876943945884705,
      "learning_rate": 0.0001744843545628333,
      "loss": 0.4876,
      "step": 6345
    },
    {
      "epoch": 12.768611670020121,
      "grad_norm": 0.41803792119026184,
      "learning_rate": 0.0001744803300130798,
      "loss": 0.5117,
      "step": 6346
    },
    {
      "epoch": 12.770623742454728,
      "grad_norm": 0.43590229749679565,
      "learning_rate": 0.0001744763054633263,
      "loss": 0.5494,
      "step": 6347
    },
    {
      "epoch": 12.772635814889336,
      "grad_norm": 0.40371426939964294,
      "learning_rate": 0.0001744722809135728,
      "loss": 0.5038,
      "step": 6348
    },
    {
      "epoch": 12.774647887323944,
      "grad_norm": 0.4202778935432434,
      "learning_rate": 0.0001744682563638193,
      "loss": 0.4912,
      "step": 6349
    },
    {
      "epoch": 12.77665995975855,
      "grad_norm": 0.40798160433769226,
      "learning_rate": 0.00017446423181406582,
      "loss": 0.5195,
      "step": 6350
    },
    {
      "epoch": 12.778672032193159,
      "grad_norm": 0.43965932726860046,
      "learning_rate": 0.0001744602072643123,
      "loss": 0.509,
      "step": 6351
    },
    {
      "epoch": 12.780684104627767,
      "grad_norm": 0.43065306544303894,
      "learning_rate": 0.0001744561827145588,
      "loss": 0.5143,
      "step": 6352
    },
    {
      "epoch": 12.782696177062375,
      "grad_norm": 0.49396198987960815,
      "learning_rate": 0.00017445215816480533,
      "loss": 0.5155,
      "step": 6353
    },
    {
      "epoch": 12.784708249496981,
      "grad_norm": 0.42967262864112854,
      "learning_rate": 0.00017444813361505184,
      "loss": 0.5431,
      "step": 6354
    },
    {
      "epoch": 12.78672032193159,
      "grad_norm": 0.4531976878643036,
      "learning_rate": 0.00017444410906529832,
      "loss": 0.5445,
      "step": 6355
    },
    {
      "epoch": 12.788732394366198,
      "grad_norm": 0.4091903269290924,
      "learning_rate": 0.00017444008451554483,
      "loss": 0.5,
      "step": 6356
    },
    {
      "epoch": 12.790744466800804,
      "grad_norm": 0.4440631866455078,
      "learning_rate": 0.00017443605996579132,
      "loss": 0.5144,
      "step": 6357
    },
    {
      "epoch": 12.792756539235413,
      "grad_norm": 0.4329470992088318,
      "learning_rate": 0.00017443203541603783,
      "loss": 0.4848,
      "step": 6358
    },
    {
      "epoch": 12.79476861167002,
      "grad_norm": 0.40729498863220215,
      "learning_rate": 0.00017442801086628434,
      "loss": 0.4729,
      "step": 6359
    },
    {
      "epoch": 12.796780684104627,
      "grad_norm": 0.4081934094429016,
      "learning_rate": 0.00017442398631653086,
      "loss": 0.506,
      "step": 6360
    },
    {
      "epoch": 12.798792756539235,
      "grad_norm": 0.4346294403076172,
      "learning_rate": 0.00017441996176677734,
      "loss": 0.5307,
      "step": 6361
    },
    {
      "epoch": 12.800804828973844,
      "grad_norm": 0.43783798813819885,
      "learning_rate": 0.00017441593721702385,
      "loss": 0.5231,
      "step": 6362
    },
    {
      "epoch": 12.80281690140845,
      "grad_norm": 0.4315645098686218,
      "learning_rate": 0.00017441191266727034,
      "loss": 0.5568,
      "step": 6363
    },
    {
      "epoch": 12.804828973843058,
      "grad_norm": 0.436091810464859,
      "learning_rate": 0.00017440788811751688,
      "loss": 0.5216,
      "step": 6364
    },
    {
      "epoch": 12.806841046277667,
      "grad_norm": 0.410968154668808,
      "learning_rate": 0.00017440386356776336,
      "loss": 0.5091,
      "step": 6365
    },
    {
      "epoch": 12.808853118712273,
      "grad_norm": 0.42388108372688293,
      "learning_rate": 0.00017439983901800988,
      "loss": 0.5098,
      "step": 6366
    },
    {
      "epoch": 12.810865191146881,
      "grad_norm": 0.43243226408958435,
      "learning_rate": 0.00017439581446825636,
      "loss": 0.5271,
      "step": 6367
    },
    {
      "epoch": 12.81287726358149,
      "grad_norm": 0.4593089818954468,
      "learning_rate": 0.00017439178991850287,
      "loss": 0.5678,
      "step": 6368
    },
    {
      "epoch": 12.814889336016096,
      "grad_norm": 0.4384896755218506,
      "learning_rate": 0.00017438776536874939,
      "loss": 0.4858,
      "step": 6369
    },
    {
      "epoch": 12.816901408450704,
      "grad_norm": 0.4344636797904968,
      "learning_rate": 0.0001743837408189959,
      "loss": 0.5019,
      "step": 6370
    },
    {
      "epoch": 12.818913480885312,
      "grad_norm": 0.43320730328559875,
      "learning_rate": 0.00017437971626924238,
      "loss": 0.5328,
      "step": 6371
    },
    {
      "epoch": 12.82092555331992,
      "grad_norm": 0.44163641333580017,
      "learning_rate": 0.0001743756917194889,
      "loss": 0.4641,
      "step": 6372
    },
    {
      "epoch": 12.822937625754527,
      "grad_norm": 0.4172952175140381,
      "learning_rate": 0.00017437166716973538,
      "loss": 0.5287,
      "step": 6373
    },
    {
      "epoch": 12.824949698189135,
      "grad_norm": 0.42677029967308044,
      "learning_rate": 0.00017436764261998192,
      "loss": 0.5002,
      "step": 6374
    },
    {
      "epoch": 12.826961770623743,
      "grad_norm": 0.4260718524456024,
      "learning_rate": 0.0001743636180702284,
      "loss": 0.5121,
      "step": 6375
    },
    {
      "epoch": 12.82897384305835,
      "grad_norm": 0.4286298453807831,
      "learning_rate": 0.00017435959352047492,
      "loss": 0.5202,
      "step": 6376
    },
    {
      "epoch": 12.830985915492958,
      "grad_norm": 0.43903204798698425,
      "learning_rate": 0.0001743555689707214,
      "loss": 0.5008,
      "step": 6377
    },
    {
      "epoch": 12.832997987927566,
      "grad_norm": 0.4451337158679962,
      "learning_rate": 0.00017435154442096791,
      "loss": 0.5022,
      "step": 6378
    },
    {
      "epoch": 12.835010060362173,
      "grad_norm": 0.44399920105934143,
      "learning_rate": 0.00017434751987121443,
      "loss": 0.5175,
      "step": 6379
    },
    {
      "epoch": 12.83702213279678,
      "grad_norm": 0.41945260763168335,
      "learning_rate": 0.00017434349532146094,
      "loss": 0.5087,
      "step": 6380
    },
    {
      "epoch": 12.839034205231389,
      "grad_norm": 0.4512610137462616,
      "learning_rate": 0.00017433947077170742,
      "loss": 0.5446,
      "step": 6381
    },
    {
      "epoch": 12.841046277665995,
      "grad_norm": 0.421784907579422,
      "learning_rate": 0.00017433544622195394,
      "loss": 0.5018,
      "step": 6382
    },
    {
      "epoch": 12.843058350100604,
      "grad_norm": 0.4081726670265198,
      "learning_rate": 0.00017433142167220042,
      "loss": 0.4993,
      "step": 6383
    },
    {
      "epoch": 12.845070422535212,
      "grad_norm": 0.4117928743362427,
      "learning_rate": 0.00017432739712244693,
      "loss": 0.4914,
      "step": 6384
    },
    {
      "epoch": 12.847082494969818,
      "grad_norm": 0.4190436005592346,
      "learning_rate": 0.00017432337257269345,
      "loss": 0.5249,
      "step": 6385
    },
    {
      "epoch": 12.849094567404427,
      "grad_norm": 0.44733726978302,
      "learning_rate": 0.00017431934802293993,
      "loss": 0.5131,
      "step": 6386
    },
    {
      "epoch": 12.851106639839035,
      "grad_norm": 0.4150651693344116,
      "learning_rate": 0.00017431532347318644,
      "loss": 0.5096,
      "step": 6387
    },
    {
      "epoch": 12.853118712273641,
      "grad_norm": 0.442162424325943,
      "learning_rate": 0.00017431129892343295,
      "loss": 0.5035,
      "step": 6388
    },
    {
      "epoch": 12.85513078470825,
      "grad_norm": 0.43354299664497375,
      "learning_rate": 0.00017430727437367947,
      "loss": 0.5036,
      "step": 6389
    },
    {
      "epoch": 12.857142857142858,
      "grad_norm": 0.40169814229011536,
      "learning_rate": 0.00017430324982392595,
      "loss": 0.5136,
      "step": 6390
    },
    {
      "epoch": 12.859154929577464,
      "grad_norm": 0.42572975158691406,
      "learning_rate": 0.00017429922527417246,
      "loss": 0.4963,
      "step": 6391
    },
    {
      "epoch": 12.861167002012072,
      "grad_norm": 0.4401017427444458,
      "learning_rate": 0.00017429520072441895,
      "loss": 0.5085,
      "step": 6392
    },
    {
      "epoch": 12.86317907444668,
      "grad_norm": 0.42346253991127014,
      "learning_rate": 0.00017429117617466546,
      "loss": 0.4792,
      "step": 6393
    },
    {
      "epoch": 12.865191146881287,
      "grad_norm": 0.4361843764781952,
      "learning_rate": 0.00017428715162491197,
      "loss": 0.5205,
      "step": 6394
    },
    {
      "epoch": 12.867203219315895,
      "grad_norm": 0.4188530147075653,
      "learning_rate": 0.00017428312707515849,
      "loss": 0.5007,
      "step": 6395
    },
    {
      "epoch": 12.869215291750503,
      "grad_norm": 0.4264262914657593,
      "learning_rate": 0.00017427910252540497,
      "loss": 0.5259,
      "step": 6396
    },
    {
      "epoch": 12.87122736418511,
      "grad_norm": 0.46019014716148376,
      "learning_rate": 0.00017427507797565148,
      "loss": 0.5111,
      "step": 6397
    },
    {
      "epoch": 12.873239436619718,
      "grad_norm": 0.43989697098731995,
      "learning_rate": 0.00017427105342589797,
      "loss": 0.4965,
      "step": 6398
    },
    {
      "epoch": 12.875251509054326,
      "grad_norm": 0.43532595038414,
      "learning_rate": 0.0001742670288761445,
      "loss": 0.525,
      "step": 6399
    },
    {
      "epoch": 12.877263581488933,
      "grad_norm": 0.41440433263778687,
      "learning_rate": 0.000174263004326391,
      "loss": 0.5258,
      "step": 6400
    },
    {
      "epoch": 12.879275653923541,
      "grad_norm": 0.40948086977005005,
      "learning_rate": 0.0001742589797766375,
      "loss": 0.5225,
      "step": 6401
    },
    {
      "epoch": 12.88128772635815,
      "grad_norm": 0.4249028265476227,
      "learning_rate": 0.000174254955226884,
      "loss": 0.4891,
      "step": 6402
    },
    {
      "epoch": 12.883299798792757,
      "grad_norm": 0.418766051530838,
      "learning_rate": 0.0001742509306771305,
      "loss": 0.5002,
      "step": 6403
    },
    {
      "epoch": 12.885311871227364,
      "grad_norm": 0.4150804877281189,
      "learning_rate": 0.00017424690612737701,
      "loss": 0.5251,
      "step": 6404
    },
    {
      "epoch": 12.887323943661972,
      "grad_norm": 0.4118511974811554,
      "learning_rate": 0.00017424288157762353,
      "loss": 0.5093,
      "step": 6405
    },
    {
      "epoch": 12.88933601609658,
      "grad_norm": 0.4123173654079437,
      "learning_rate": 0.00017423885702787,
      "loss": 0.5029,
      "step": 6406
    },
    {
      "epoch": 12.891348088531187,
      "grad_norm": 0.43951207399368286,
      "learning_rate": 0.00017423483247811652,
      "loss": 0.5137,
      "step": 6407
    },
    {
      "epoch": 12.893360160965795,
      "grad_norm": 0.45929044485092163,
      "learning_rate": 0.000174230807928363,
      "loss": 0.553,
      "step": 6408
    },
    {
      "epoch": 12.895372233400403,
      "grad_norm": 0.4330952763557434,
      "learning_rate": 0.00017422678337860955,
      "loss": 0.5212,
      "step": 6409
    },
    {
      "epoch": 12.89738430583501,
      "grad_norm": 0.43160387873649597,
      "learning_rate": 0.00017422275882885603,
      "loss": 0.4997,
      "step": 6410
    },
    {
      "epoch": 12.899396378269618,
      "grad_norm": 0.428836464881897,
      "learning_rate": 0.00017421873427910255,
      "loss": 0.498,
      "step": 6411
    },
    {
      "epoch": 12.901408450704226,
      "grad_norm": 0.40773504972457886,
      "learning_rate": 0.00017421470972934903,
      "loss": 0.4897,
      "step": 6412
    },
    {
      "epoch": 12.903420523138832,
      "grad_norm": 0.42498305439949036,
      "learning_rate": 0.00017421068517959554,
      "loss": 0.4696,
      "step": 6413
    },
    {
      "epoch": 12.90543259557344,
      "grad_norm": 0.4290212392807007,
      "learning_rate": 0.00017420666062984206,
      "loss": 0.513,
      "step": 6414
    },
    {
      "epoch": 12.907444668008049,
      "grad_norm": 0.43671613931655884,
      "learning_rate": 0.00017420263608008857,
      "loss": 0.5233,
      "step": 6415
    },
    {
      "epoch": 12.909456740442655,
      "grad_norm": 0.45778149366378784,
      "learning_rate": 0.00017419861153033505,
      "loss": 0.5664,
      "step": 6416
    },
    {
      "epoch": 12.911468812877263,
      "grad_norm": 0.44626182317733765,
      "learning_rate": 0.00017419458698058156,
      "loss": 0.4923,
      "step": 6417
    },
    {
      "epoch": 12.913480885311872,
      "grad_norm": 0.4221215546131134,
      "learning_rate": 0.00017419056243082805,
      "loss": 0.5275,
      "step": 6418
    },
    {
      "epoch": 12.915492957746478,
      "grad_norm": 0.43734437227249146,
      "learning_rate": 0.00017418653788107456,
      "loss": 0.5393,
      "step": 6419
    },
    {
      "epoch": 12.917505030181086,
      "grad_norm": 0.43681371212005615,
      "learning_rate": 0.00017418251333132107,
      "loss": 0.5376,
      "step": 6420
    },
    {
      "epoch": 12.919517102615695,
      "grad_norm": 0.4244450330734253,
      "learning_rate": 0.00017417848878156756,
      "loss": 0.494,
      "step": 6421
    },
    {
      "epoch": 12.921529175050303,
      "grad_norm": 0.42540788650512695,
      "learning_rate": 0.00017417446423181407,
      "loss": 0.5346,
      "step": 6422
    },
    {
      "epoch": 12.92354124748491,
      "grad_norm": 0.45471805334091187,
      "learning_rate": 0.00017417043968206058,
      "loss": 0.5032,
      "step": 6423
    },
    {
      "epoch": 12.925553319919517,
      "grad_norm": 0.43312087655067444,
      "learning_rate": 0.0001741664151323071,
      "loss": 0.5306,
      "step": 6424
    },
    {
      "epoch": 12.927565392354126,
      "grad_norm": 0.45109817385673523,
      "learning_rate": 0.00017416239058255358,
      "loss": 0.5264,
      "step": 6425
    },
    {
      "epoch": 12.929577464788732,
      "grad_norm": 0.4082059860229492,
      "learning_rate": 0.0001741583660328001,
      "loss": 0.4937,
      "step": 6426
    },
    {
      "epoch": 12.93158953722334,
      "grad_norm": 0.42523083090782166,
      "learning_rate": 0.00017415434148304658,
      "loss": 0.4651,
      "step": 6427
    },
    {
      "epoch": 12.933601609657948,
      "grad_norm": 0.4205300509929657,
      "learning_rate": 0.0001741503169332931,
      "loss": 0.4784,
      "step": 6428
    },
    {
      "epoch": 12.935613682092555,
      "grad_norm": 0.4663759469985962,
      "learning_rate": 0.00017414629238353958,
      "loss": 0.5411,
      "step": 6429
    },
    {
      "epoch": 12.937625754527163,
      "grad_norm": 0.4349537193775177,
      "learning_rate": 0.00017414226783378612,
      "loss": 0.5344,
      "step": 6430
    },
    {
      "epoch": 12.939637826961771,
      "grad_norm": 0.422489732503891,
      "learning_rate": 0.0001741382432840326,
      "loss": 0.5435,
      "step": 6431
    },
    {
      "epoch": 12.941649899396378,
      "grad_norm": 0.42159244418144226,
      "learning_rate": 0.0001741342187342791,
      "loss": 0.5233,
      "step": 6432
    },
    {
      "epoch": 12.943661971830986,
      "grad_norm": 0.4066964089870453,
      "learning_rate": 0.0001741301941845256,
      "loss": 0.5244,
      "step": 6433
    },
    {
      "epoch": 12.945674044265594,
      "grad_norm": 0.398676335811615,
      "learning_rate": 0.0001741261696347721,
      "loss": 0.5045,
      "step": 6434
    },
    {
      "epoch": 12.9476861167002,
      "grad_norm": 0.45095646381378174,
      "learning_rate": 0.00017412214508501862,
      "loss": 0.5592,
      "step": 6435
    },
    {
      "epoch": 12.949698189134809,
      "grad_norm": 0.4256616234779358,
      "learning_rate": 0.00017411812053526513,
      "loss": 0.4827,
      "step": 6436
    },
    {
      "epoch": 12.951710261569417,
      "grad_norm": 0.41867563128471375,
      "learning_rate": 0.00017411409598551162,
      "loss": 0.5088,
      "step": 6437
    },
    {
      "epoch": 12.953722334004024,
      "grad_norm": 0.41261401772499084,
      "learning_rate": 0.00017411007143575813,
      "loss": 0.5087,
      "step": 6438
    },
    {
      "epoch": 12.955734406438632,
      "grad_norm": 0.43333783745765686,
      "learning_rate": 0.00017410604688600462,
      "loss": 0.4696,
      "step": 6439
    },
    {
      "epoch": 12.95774647887324,
      "grad_norm": 0.4339156746864319,
      "learning_rate": 0.00017410202233625116,
      "loss": 0.5414,
      "step": 6440
    },
    {
      "epoch": 12.959758551307846,
      "grad_norm": 0.4381335377693176,
      "learning_rate": 0.00017409799778649764,
      "loss": 0.5088,
      "step": 6441
    },
    {
      "epoch": 12.961770623742455,
      "grad_norm": 0.42152848839759827,
      "learning_rate": 0.00017409397323674415,
      "loss": 0.52,
      "step": 6442
    },
    {
      "epoch": 12.963782696177063,
      "grad_norm": 0.4146144390106201,
      "learning_rate": 0.00017408994868699064,
      "loss": 0.5122,
      "step": 6443
    },
    {
      "epoch": 12.96579476861167,
      "grad_norm": 0.4185721278190613,
      "learning_rate": 0.00017408592413723715,
      "loss": 0.537,
      "step": 6444
    },
    {
      "epoch": 12.967806841046277,
      "grad_norm": 0.41798895597457886,
      "learning_rate": 0.00017408189958748366,
      "loss": 0.5177,
      "step": 6445
    },
    {
      "epoch": 12.969818913480886,
      "grad_norm": 0.45258432626724243,
      "learning_rate": 0.00017407787503773018,
      "loss": 0.5067,
      "step": 6446
    },
    {
      "epoch": 12.971830985915492,
      "grad_norm": 0.4206339418888092,
      "learning_rate": 0.00017407385048797666,
      "loss": 0.5114,
      "step": 6447
    },
    {
      "epoch": 12.9738430583501,
      "grad_norm": 0.43817755579948425,
      "learning_rate": 0.00017406982593822317,
      "loss": 0.5488,
      "step": 6448
    },
    {
      "epoch": 12.975855130784709,
      "grad_norm": 0.4129536747932434,
      "learning_rate": 0.00017406580138846966,
      "loss": 0.4795,
      "step": 6449
    },
    {
      "epoch": 12.977867203219315,
      "grad_norm": 0.4240711033344269,
      "learning_rate": 0.0001740617768387162,
      "loss": 0.5254,
      "step": 6450
    },
    {
      "epoch": 12.979879275653923,
      "grad_norm": 0.44219598174095154,
      "learning_rate": 0.00017405775228896268,
      "loss": 0.5407,
      "step": 6451
    },
    {
      "epoch": 12.981891348088531,
      "grad_norm": 0.41582146286964417,
      "learning_rate": 0.0001740537277392092,
      "loss": 0.5307,
      "step": 6452
    },
    {
      "epoch": 12.98390342052314,
      "grad_norm": 0.4193761348724365,
      "learning_rate": 0.00017404970318945568,
      "loss": 0.5218,
      "step": 6453
    },
    {
      "epoch": 12.985915492957746,
      "grad_norm": 0.3887180984020233,
      "learning_rate": 0.0001740456786397022,
      "loss": 0.4796,
      "step": 6454
    },
    {
      "epoch": 12.987927565392354,
      "grad_norm": 0.46232175827026367,
      "learning_rate": 0.0001740416540899487,
      "loss": 0.5366,
      "step": 6455
    },
    {
      "epoch": 12.989939637826962,
      "grad_norm": 0.40376317501068115,
      "learning_rate": 0.0001740376295401952,
      "loss": 0.5291,
      "step": 6456
    },
    {
      "epoch": 12.991951710261569,
      "grad_norm": 0.4252323508262634,
      "learning_rate": 0.0001740336049904417,
      "loss": 0.513,
      "step": 6457
    },
    {
      "epoch": 12.993963782696177,
      "grad_norm": 0.41229215264320374,
      "learning_rate": 0.0001740295804406882,
      "loss": 0.4833,
      "step": 6458
    },
    {
      "epoch": 12.995975855130785,
      "grad_norm": 0.4587571620941162,
      "learning_rate": 0.0001740255558909347,
      "loss": 0.5125,
      "step": 6459
    },
    {
      "epoch": 12.997987927565392,
      "grad_norm": 0.4303964376449585,
      "learning_rate": 0.0001740215313411812,
      "loss": 0.5248,
      "step": 6460
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.4447951018810272,
      "learning_rate": 0.00017401750679142772,
      "loss": 0.5392,
      "step": 6461
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.7996084094047546,
      "eval_runtime": 49.8527,
      "eval_samples_per_second": 19.899,
      "eval_steps_per_second": 2.487,
      "step": 6461
    },
    {
      "epoch": 13.002012072434608,
      "grad_norm": 0.40282192826271057,
      "learning_rate": 0.0001740134822416742,
      "loss": 0.4648,
      "step": 6462
    },
    {
      "epoch": 13.004024144869215,
      "grad_norm": 0.4243449866771698,
      "learning_rate": 0.00017400945769192072,
      "loss": 0.4517,
      "step": 6463
    },
    {
      "epoch": 13.006036217303823,
      "grad_norm": 0.3928908705711365,
      "learning_rate": 0.0001740054331421672,
      "loss": 0.4442,
      "step": 6464
    },
    {
      "epoch": 13.008048289738431,
      "grad_norm": 0.4117964804172516,
      "learning_rate": 0.00017400140859241374,
      "loss": 0.446,
      "step": 6465
    },
    {
      "epoch": 13.010060362173038,
      "grad_norm": 0.45260995626449585,
      "learning_rate": 0.00017399738404266023,
      "loss": 0.4361,
      "step": 6466
    },
    {
      "epoch": 13.012072434607646,
      "grad_norm": 0.4613657593727112,
      "learning_rate": 0.00017399335949290674,
      "loss": 0.4068,
      "step": 6467
    },
    {
      "epoch": 13.014084507042254,
      "grad_norm": 0.43249285221099854,
      "learning_rate": 0.00017398933494315323,
      "loss": 0.4409,
      "step": 6468
    },
    {
      "epoch": 13.01609657947686,
      "grad_norm": 0.4420509934425354,
      "learning_rate": 0.00017398531039339974,
      "loss": 0.4307,
      "step": 6469
    },
    {
      "epoch": 13.018108651911469,
      "grad_norm": 0.42864397168159485,
      "learning_rate": 0.00017398128584364625,
      "loss": 0.4345,
      "step": 6470
    },
    {
      "epoch": 13.020120724346077,
      "grad_norm": 0.4095989763736725,
      "learning_rate": 0.00017397726129389276,
      "loss": 0.4122,
      "step": 6471
    },
    {
      "epoch": 13.022132796780683,
      "grad_norm": 0.41372886300086975,
      "learning_rate": 0.00017397323674413925,
      "loss": 0.4195,
      "step": 6472
    },
    {
      "epoch": 13.024144869215291,
      "grad_norm": 0.43320921063423157,
      "learning_rate": 0.00017396921219438576,
      "loss": 0.4485,
      "step": 6473
    },
    {
      "epoch": 13.0261569416499,
      "grad_norm": 0.4308735430240631,
      "learning_rate": 0.00017396518764463225,
      "loss": 0.4383,
      "step": 6474
    },
    {
      "epoch": 13.028169014084508,
      "grad_norm": 0.44995763897895813,
      "learning_rate": 0.00017396116309487879,
      "loss": 0.4531,
      "step": 6475
    },
    {
      "epoch": 13.030181086519114,
      "grad_norm": 0.4446588158607483,
      "learning_rate": 0.00017395713854512527,
      "loss": 0.4336,
      "step": 6476
    },
    {
      "epoch": 13.032193158953723,
      "grad_norm": 0.46595215797424316,
      "learning_rate": 0.00017395311399537178,
      "loss": 0.4456,
      "step": 6477
    },
    {
      "epoch": 13.03420523138833,
      "grad_norm": 0.43420910835266113,
      "learning_rate": 0.00017394908944561827,
      "loss": 0.4313,
      "step": 6478
    },
    {
      "epoch": 13.036217303822937,
      "grad_norm": 0.43567970395088196,
      "learning_rate": 0.00017394506489586478,
      "loss": 0.4456,
      "step": 6479
    },
    {
      "epoch": 13.038229376257545,
      "grad_norm": 0.45863088965415955,
      "learning_rate": 0.0001739410403461113,
      "loss": 0.4304,
      "step": 6480
    },
    {
      "epoch": 13.040241448692154,
      "grad_norm": 0.4416854977607727,
      "learning_rate": 0.0001739370157963578,
      "loss": 0.4457,
      "step": 6481
    },
    {
      "epoch": 13.04225352112676,
      "grad_norm": 0.45792460441589355,
      "learning_rate": 0.0001739329912466043,
      "loss": 0.425,
      "step": 6482
    },
    {
      "epoch": 13.044265593561368,
      "grad_norm": 0.426833838224411,
      "learning_rate": 0.0001739289666968508,
      "loss": 0.46,
      "step": 6483
    },
    {
      "epoch": 13.046277665995976,
      "grad_norm": 0.44325128197669983,
      "learning_rate": 0.0001739249421470973,
      "loss": 0.4366,
      "step": 6484
    },
    {
      "epoch": 13.048289738430583,
      "grad_norm": 0.4863053262233734,
      "learning_rate": 0.00017392091759734383,
      "loss": 0.4869,
      "step": 6485
    },
    {
      "epoch": 13.050301810865191,
      "grad_norm": 0.43850669264793396,
      "learning_rate": 0.0001739168930475903,
      "loss": 0.4411,
      "step": 6486
    },
    {
      "epoch": 13.0523138832998,
      "grad_norm": 0.46311017870903015,
      "learning_rate": 0.00017391286849783682,
      "loss": 0.4736,
      "step": 6487
    },
    {
      "epoch": 13.054325955734406,
      "grad_norm": 0.46073323488235474,
      "learning_rate": 0.0001739088439480833,
      "loss": 0.4603,
      "step": 6488
    },
    {
      "epoch": 13.056338028169014,
      "grad_norm": 0.43458619713783264,
      "learning_rate": 0.00017390481939832982,
      "loss": 0.4625,
      "step": 6489
    },
    {
      "epoch": 13.058350100603622,
      "grad_norm": 0.46352219581604004,
      "learning_rate": 0.00017390079484857633,
      "loss": 0.503,
      "step": 6490
    },
    {
      "epoch": 13.060362173038229,
      "grad_norm": 0.43264061212539673,
      "learning_rate": 0.00017389677029882282,
      "loss": 0.4522,
      "step": 6491
    },
    {
      "epoch": 13.062374245472837,
      "grad_norm": 0.4227496087551117,
      "learning_rate": 0.00017389274574906933,
      "loss": 0.4645,
      "step": 6492
    },
    {
      "epoch": 13.064386317907445,
      "grad_norm": 0.41210898756980896,
      "learning_rate": 0.00017388872119931584,
      "loss": 0.4538,
      "step": 6493
    },
    {
      "epoch": 13.066398390342052,
      "grad_norm": 0.48516637086868286,
      "learning_rate": 0.00017388469664956233,
      "loss": 0.462,
      "step": 6494
    },
    {
      "epoch": 13.06841046277666,
      "grad_norm": 0.4467117488384247,
      "learning_rate": 0.00017388067209980884,
      "loss": 0.4357,
      "step": 6495
    },
    {
      "epoch": 13.070422535211268,
      "grad_norm": 0.4572743773460388,
      "learning_rate": 0.00017387664755005535,
      "loss": 0.4231,
      "step": 6496
    },
    {
      "epoch": 13.072434607645874,
      "grad_norm": 0.4526333510875702,
      "learning_rate": 0.00017387262300030184,
      "loss": 0.4692,
      "step": 6497
    },
    {
      "epoch": 13.074446680080483,
      "grad_norm": 0.4162702262401581,
      "learning_rate": 0.00017386859845054835,
      "loss": 0.4611,
      "step": 6498
    },
    {
      "epoch": 13.07645875251509,
      "grad_norm": 0.4946103096008301,
      "learning_rate": 0.00017386457390079483,
      "loss": 0.4699,
      "step": 6499
    },
    {
      "epoch": 13.078470824949699,
      "grad_norm": 0.46901556849479675,
      "learning_rate": 0.00017386054935104137,
      "loss": 0.4579,
      "step": 6500
    },
    {
      "epoch": 13.080482897384305,
      "grad_norm": 0.44143906235694885,
      "learning_rate": 0.00017385652480128786,
      "loss": 0.464,
      "step": 6501
    },
    {
      "epoch": 13.082494969818914,
      "grad_norm": 0.4341195225715637,
      "learning_rate": 0.00017385250025153437,
      "loss": 0.4518,
      "step": 6502
    },
    {
      "epoch": 13.084507042253522,
      "grad_norm": 0.4399471879005432,
      "learning_rate": 0.00017384847570178086,
      "loss": 0.4828,
      "step": 6503
    },
    {
      "epoch": 13.086519114688128,
      "grad_norm": 0.45695385336875916,
      "learning_rate": 0.00017384445115202737,
      "loss": 0.4742,
      "step": 6504
    },
    {
      "epoch": 13.088531187122737,
      "grad_norm": 0.4511585533618927,
      "learning_rate": 0.00017384042660227388,
      "loss": 0.4491,
      "step": 6505
    },
    {
      "epoch": 13.090543259557345,
      "grad_norm": 0.45867669582366943,
      "learning_rate": 0.0001738364020525204,
      "loss": 0.4683,
      "step": 6506
    },
    {
      "epoch": 13.092555331991951,
      "grad_norm": 0.47550666332244873,
      "learning_rate": 0.00017383237750276688,
      "loss": 0.4312,
      "step": 6507
    },
    {
      "epoch": 13.09456740442656,
      "grad_norm": 0.4501519799232483,
      "learning_rate": 0.0001738283529530134,
      "loss": 0.4647,
      "step": 6508
    },
    {
      "epoch": 13.096579476861168,
      "grad_norm": 0.4406573176383972,
      "learning_rate": 0.00017382432840325988,
      "loss": 0.4681,
      "step": 6509
    },
    {
      "epoch": 13.098591549295774,
      "grad_norm": 0.44935914874076843,
      "learning_rate": 0.00017382030385350642,
      "loss": 0.4814,
      "step": 6510
    },
    {
      "epoch": 13.100603621730382,
      "grad_norm": 0.4283791482448578,
      "learning_rate": 0.0001738162793037529,
      "loss": 0.466,
      "step": 6511
    },
    {
      "epoch": 13.10261569416499,
      "grad_norm": 0.45499104261398315,
      "learning_rate": 0.0001738122547539994,
      "loss": 0.4569,
      "step": 6512
    },
    {
      "epoch": 13.104627766599597,
      "grad_norm": 0.42608144879341125,
      "learning_rate": 0.0001738082302042459,
      "loss": 0.4978,
      "step": 6513
    },
    {
      "epoch": 13.106639839034205,
      "grad_norm": 0.42845049500465393,
      "learning_rate": 0.0001738042056544924,
      "loss": 0.4402,
      "step": 6514
    },
    {
      "epoch": 13.108651911468813,
      "grad_norm": 0.42733022570610046,
      "learning_rate": 0.00017380018110473892,
      "loss": 0.4371,
      "step": 6515
    },
    {
      "epoch": 13.11066398390342,
      "grad_norm": 0.4548271894454956,
      "learning_rate": 0.00017379615655498543,
      "loss": 0.487,
      "step": 6516
    },
    {
      "epoch": 13.112676056338028,
      "grad_norm": 0.4642947316169739,
      "learning_rate": 0.00017379213200523192,
      "loss": 0.4741,
      "step": 6517
    },
    {
      "epoch": 13.114688128772636,
      "grad_norm": 0.40514424443244934,
      "learning_rate": 0.00017378810745547843,
      "loss": 0.4663,
      "step": 6518
    },
    {
      "epoch": 13.116700201207243,
      "grad_norm": 0.4716126322746277,
      "learning_rate": 0.00017378408290572492,
      "loss": 0.4749,
      "step": 6519
    },
    {
      "epoch": 13.11871227364185,
      "grad_norm": 0.44654926657676697,
      "learning_rate": 0.00017378005835597143,
      "loss": 0.4732,
      "step": 6520
    },
    {
      "epoch": 13.120724346076459,
      "grad_norm": 0.42119747400283813,
      "learning_rate": 0.00017377603380621794,
      "loss": 0.4201,
      "step": 6521
    },
    {
      "epoch": 13.122736418511066,
      "grad_norm": 0.45679721236228943,
      "learning_rate": 0.00017377200925646445,
      "loss": 0.4662,
      "step": 6522
    },
    {
      "epoch": 13.124748490945674,
      "grad_norm": 0.45476001501083374,
      "learning_rate": 0.00017376798470671094,
      "loss": 0.4591,
      "step": 6523
    },
    {
      "epoch": 13.126760563380282,
      "grad_norm": 0.4421190619468689,
      "learning_rate": 0.00017376396015695745,
      "loss": 0.4656,
      "step": 6524
    },
    {
      "epoch": 13.12877263581489,
      "grad_norm": 0.46632295846939087,
      "learning_rate": 0.00017375993560720396,
      "loss": 0.4521,
      "step": 6525
    },
    {
      "epoch": 13.130784708249497,
      "grad_norm": 0.4736836850643158,
      "learning_rate": 0.00017375591105745045,
      "loss": 0.4581,
      "step": 6526
    },
    {
      "epoch": 13.132796780684105,
      "grad_norm": 0.46790871024131775,
      "learning_rate": 0.00017375188650769696,
      "loss": 0.4523,
      "step": 6527
    },
    {
      "epoch": 13.134808853118713,
      "grad_norm": 0.4700700640678406,
      "learning_rate": 0.00017374786195794345,
      "loss": 0.4937,
      "step": 6528
    },
    {
      "epoch": 13.13682092555332,
      "grad_norm": 0.430593341588974,
      "learning_rate": 0.00017374383740818996,
      "loss": 0.4908,
      "step": 6529
    },
    {
      "epoch": 13.138832997987928,
      "grad_norm": 0.44796881079673767,
      "learning_rate": 0.00017373981285843647,
      "loss": 0.465,
      "step": 6530
    },
    {
      "epoch": 13.140845070422536,
      "grad_norm": 0.4376143217086792,
      "learning_rate": 0.00017373578830868298,
      "loss": 0.4548,
      "step": 6531
    },
    {
      "epoch": 13.142857142857142,
      "grad_norm": 0.4320983290672302,
      "learning_rate": 0.00017373176375892947,
      "loss": 0.4381,
      "step": 6532
    },
    {
      "epoch": 13.14486921529175,
      "grad_norm": 0.4604281783103943,
      "learning_rate": 0.00017372773920917598,
      "loss": 0.4733,
      "step": 6533
    },
    {
      "epoch": 13.146881287726359,
      "grad_norm": 0.46679770946502686,
      "learning_rate": 0.00017372371465942246,
      "loss": 0.4778,
      "step": 6534
    },
    {
      "epoch": 13.148893360160965,
      "grad_norm": 0.4244880974292755,
      "learning_rate": 0.000173719690109669,
      "loss": 0.4562,
      "step": 6535
    },
    {
      "epoch": 13.150905432595573,
      "grad_norm": 0.43923497200012207,
      "learning_rate": 0.0001737156655599155,
      "loss": 0.4603,
      "step": 6536
    },
    {
      "epoch": 13.152917505030182,
      "grad_norm": 0.45379114151000977,
      "learning_rate": 0.000173711641010162,
      "loss": 0.4733,
      "step": 6537
    },
    {
      "epoch": 13.154929577464788,
      "grad_norm": 0.4333340525627136,
      "learning_rate": 0.00017370761646040849,
      "loss": 0.4615,
      "step": 6538
    },
    {
      "epoch": 13.156941649899396,
      "grad_norm": 0.42795994877815247,
      "learning_rate": 0.000173703591910655,
      "loss": 0.4303,
      "step": 6539
    },
    {
      "epoch": 13.158953722334005,
      "grad_norm": 0.47140976786613464,
      "learning_rate": 0.0001736995673609015,
      "loss": 0.4678,
      "step": 6540
    },
    {
      "epoch": 13.160965794768611,
      "grad_norm": 0.4604378640651703,
      "learning_rate": 0.00017369554281114802,
      "loss": 0.4592,
      "step": 6541
    },
    {
      "epoch": 13.16297786720322,
      "grad_norm": 0.43130049109458923,
      "learning_rate": 0.0001736915182613945,
      "loss": 0.4478,
      "step": 6542
    },
    {
      "epoch": 13.164989939637827,
      "grad_norm": 0.4575798213481903,
      "learning_rate": 0.00017368749371164102,
      "loss": 0.484,
      "step": 6543
    },
    {
      "epoch": 13.167002012072434,
      "grad_norm": 0.43935713171958923,
      "learning_rate": 0.0001736834691618875,
      "loss": 0.4436,
      "step": 6544
    },
    {
      "epoch": 13.169014084507042,
      "grad_norm": 0.44278454780578613,
      "learning_rate": 0.00017367944461213404,
      "loss": 0.4691,
      "step": 6545
    },
    {
      "epoch": 13.17102615694165,
      "grad_norm": 0.4550204277038574,
      "learning_rate": 0.00017367542006238053,
      "loss": 0.4727,
      "step": 6546
    },
    {
      "epoch": 13.173038229376257,
      "grad_norm": 0.4275866746902466,
      "learning_rate": 0.00017367139551262704,
      "loss": 0.4545,
      "step": 6547
    },
    {
      "epoch": 13.175050301810865,
      "grad_norm": 0.45935818552970886,
      "learning_rate": 0.00017366737096287353,
      "loss": 0.4811,
      "step": 6548
    },
    {
      "epoch": 13.177062374245473,
      "grad_norm": 0.4480517506599426,
      "learning_rate": 0.00017366334641312004,
      "loss": 0.4769,
      "step": 6549
    },
    {
      "epoch": 13.179074446680081,
      "grad_norm": 0.4506826400756836,
      "learning_rate": 0.00017365932186336655,
      "loss": 0.4399,
      "step": 6550
    },
    {
      "epoch": 13.181086519114688,
      "grad_norm": 0.45885908603668213,
      "learning_rate": 0.00017365529731361306,
      "loss": 0.4256,
      "step": 6551
    },
    {
      "epoch": 13.183098591549296,
      "grad_norm": 0.4492192566394806,
      "learning_rate": 0.00017365127276385955,
      "loss": 0.4811,
      "step": 6552
    },
    {
      "epoch": 13.185110663983904,
      "grad_norm": 0.44528618454933167,
      "learning_rate": 0.00017364724821410606,
      "loss": 0.4898,
      "step": 6553
    },
    {
      "epoch": 13.18712273641851,
      "grad_norm": 0.42864295840263367,
      "learning_rate": 0.00017364322366435255,
      "loss": 0.4762,
      "step": 6554
    },
    {
      "epoch": 13.189134808853119,
      "grad_norm": 0.4449898600578308,
      "learning_rate": 0.00017363919911459906,
      "loss": 0.4678,
      "step": 6555
    },
    {
      "epoch": 13.191146881287727,
      "grad_norm": 0.4413203299045563,
      "learning_rate": 0.00017363517456484557,
      "loss": 0.455,
      "step": 6556
    },
    {
      "epoch": 13.193158953722333,
      "grad_norm": 0.4453001916408539,
      "learning_rate": 0.00017363115001509208,
      "loss": 0.4543,
      "step": 6557
    },
    {
      "epoch": 13.195171026156942,
      "grad_norm": 0.45821309089660645,
      "learning_rate": 0.00017362712546533857,
      "loss": 0.469,
      "step": 6558
    },
    {
      "epoch": 13.19718309859155,
      "grad_norm": 0.46272921562194824,
      "learning_rate": 0.00017362310091558508,
      "loss": 0.4794,
      "step": 6559
    },
    {
      "epoch": 13.199195171026156,
      "grad_norm": 0.44725748896598816,
      "learning_rate": 0.0001736190763658316,
      "loss": 0.4682,
      "step": 6560
    },
    {
      "epoch": 13.201207243460765,
      "grad_norm": 0.4757053256034851,
      "learning_rate": 0.00017361505181607808,
      "loss": 0.4598,
      "step": 6561
    },
    {
      "epoch": 13.203219315895373,
      "grad_norm": 0.4513554573059082,
      "learning_rate": 0.0001736110272663246,
      "loss": 0.4778,
      "step": 6562
    },
    {
      "epoch": 13.20523138832998,
      "grad_norm": 0.4735548496246338,
      "learning_rate": 0.00017360700271657107,
      "loss": 0.4502,
      "step": 6563
    },
    {
      "epoch": 13.207243460764587,
      "grad_norm": 0.4524213969707489,
      "learning_rate": 0.0001736029781668176,
      "loss": 0.472,
      "step": 6564
    },
    {
      "epoch": 13.209255533199196,
      "grad_norm": 0.46583518385887146,
      "learning_rate": 0.0001735989536170641,
      "loss": 0.511,
      "step": 6565
    },
    {
      "epoch": 13.211267605633802,
      "grad_norm": 0.4500277042388916,
      "learning_rate": 0.0001735949290673106,
      "loss": 0.4398,
      "step": 6566
    },
    {
      "epoch": 13.21327967806841,
      "grad_norm": 0.4693271219730377,
      "learning_rate": 0.0001735909045175571,
      "loss": 0.4674,
      "step": 6567
    },
    {
      "epoch": 13.215291750503019,
      "grad_norm": 0.4418870210647583,
      "learning_rate": 0.0001735868799678036,
      "loss": 0.5015,
      "step": 6568
    },
    {
      "epoch": 13.217303822937625,
      "grad_norm": 0.4640120565891266,
      "learning_rate": 0.0001735828554180501,
      "loss": 0.4548,
      "step": 6569
    },
    {
      "epoch": 13.219315895372233,
      "grad_norm": 0.4700217843055725,
      "learning_rate": 0.00017357883086829663,
      "loss": 0.5216,
      "step": 6570
    },
    {
      "epoch": 13.221327967806841,
      "grad_norm": 0.4361763894557953,
      "learning_rate": 0.00017357480631854312,
      "loss": 0.4559,
      "step": 6571
    },
    {
      "epoch": 13.223340040241448,
      "grad_norm": 0.4402604103088379,
      "learning_rate": 0.00017357078176878963,
      "loss": 0.5029,
      "step": 6572
    },
    {
      "epoch": 13.225352112676056,
      "grad_norm": 0.45909392833709717,
      "learning_rate": 0.00017356675721903612,
      "loss": 0.5127,
      "step": 6573
    },
    {
      "epoch": 13.227364185110664,
      "grad_norm": 0.44273948669433594,
      "learning_rate": 0.00017356273266928263,
      "loss": 0.4581,
      "step": 6574
    },
    {
      "epoch": 13.229376257545272,
      "grad_norm": 0.45227155089378357,
      "learning_rate": 0.00017355870811952914,
      "loss": 0.4426,
      "step": 6575
    },
    {
      "epoch": 13.231388329979879,
      "grad_norm": 0.4411589801311493,
      "learning_rate": 0.00017355468356977565,
      "loss": 0.4698,
      "step": 6576
    },
    {
      "epoch": 13.233400402414487,
      "grad_norm": 0.4852369427680969,
      "learning_rate": 0.00017355065902002214,
      "loss": 0.465,
      "step": 6577
    },
    {
      "epoch": 13.235412474849095,
      "grad_norm": 0.46157851815223694,
      "learning_rate": 0.00017354663447026865,
      "loss": 0.4848,
      "step": 6578
    },
    {
      "epoch": 13.237424547283702,
      "grad_norm": 0.43172359466552734,
      "learning_rate": 0.00017354260992051513,
      "loss": 0.4549,
      "step": 6579
    },
    {
      "epoch": 13.23943661971831,
      "grad_norm": 0.4367213845252991,
      "learning_rate": 0.00017353858537076167,
      "loss": 0.4624,
      "step": 6580
    },
    {
      "epoch": 13.241448692152918,
      "grad_norm": 0.4449920356273651,
      "learning_rate": 0.00017353456082100816,
      "loss": 0.4336,
      "step": 6581
    },
    {
      "epoch": 13.243460764587525,
      "grad_norm": 0.4486157298088074,
      "learning_rate": 0.00017353053627125467,
      "loss": 0.4732,
      "step": 6582
    },
    {
      "epoch": 13.245472837022133,
      "grad_norm": 0.45280128717422485,
      "learning_rate": 0.00017352651172150116,
      "loss": 0.4847,
      "step": 6583
    },
    {
      "epoch": 13.247484909456741,
      "grad_norm": 0.4644579291343689,
      "learning_rate": 0.00017352248717174767,
      "loss": 0.4669,
      "step": 6584
    },
    {
      "epoch": 13.249496981891348,
      "grad_norm": 0.4516543745994568,
      "learning_rate": 0.00017351846262199418,
      "loss": 0.4762,
      "step": 6585
    },
    {
      "epoch": 13.251509054325956,
      "grad_norm": 0.44601553678512573,
      "learning_rate": 0.0001735144380722407,
      "loss": 0.4597,
      "step": 6586
    },
    {
      "epoch": 13.253521126760564,
      "grad_norm": 0.4635498821735382,
      "learning_rate": 0.00017351041352248718,
      "loss": 0.4758,
      "step": 6587
    },
    {
      "epoch": 13.25553319919517,
      "grad_norm": 0.4425904452800751,
      "learning_rate": 0.0001735063889727337,
      "loss": 0.4579,
      "step": 6588
    },
    {
      "epoch": 13.257545271629779,
      "grad_norm": 0.44303974509239197,
      "learning_rate": 0.00017350236442298018,
      "loss": 0.4834,
      "step": 6589
    },
    {
      "epoch": 13.259557344064387,
      "grad_norm": 0.4251357316970825,
      "learning_rate": 0.0001734983398732267,
      "loss": 0.4414,
      "step": 6590
    },
    {
      "epoch": 13.261569416498993,
      "grad_norm": 0.45264744758605957,
      "learning_rate": 0.0001734943153234732,
      "loss": 0.4735,
      "step": 6591
    },
    {
      "epoch": 13.263581488933601,
      "grad_norm": 0.460788369178772,
      "learning_rate": 0.0001734902907737197,
      "loss": 0.5013,
      "step": 6592
    },
    {
      "epoch": 13.26559356136821,
      "grad_norm": 0.43052321672439575,
      "learning_rate": 0.0001734862662239662,
      "loss": 0.4411,
      "step": 6593
    },
    {
      "epoch": 13.267605633802816,
      "grad_norm": 0.4958411753177643,
      "learning_rate": 0.0001734822416742127,
      "loss": 0.5172,
      "step": 6594
    },
    {
      "epoch": 13.269617706237424,
      "grad_norm": 0.4660015404224396,
      "learning_rate": 0.00017347821712445922,
      "loss": 0.4959,
      "step": 6595
    },
    {
      "epoch": 13.271629778672033,
      "grad_norm": 0.450957715511322,
      "learning_rate": 0.0001734741925747057,
      "loss": 0.4862,
      "step": 6596
    },
    {
      "epoch": 13.273641851106639,
      "grad_norm": 0.438638299703598,
      "learning_rate": 0.00017347016802495222,
      "loss": 0.4503,
      "step": 6597
    },
    {
      "epoch": 13.275653923541247,
      "grad_norm": 0.44041627645492554,
      "learning_rate": 0.0001734661434751987,
      "loss": 0.4705,
      "step": 6598
    },
    {
      "epoch": 13.277665995975855,
      "grad_norm": 0.45380526781082153,
      "learning_rate": 0.00017346211892544522,
      "loss": 0.4909,
      "step": 6599
    },
    {
      "epoch": 13.279678068410464,
      "grad_norm": 0.44390371441841125,
      "learning_rate": 0.00017345809437569173,
      "loss": 0.4893,
      "step": 6600
    },
    {
      "epoch": 13.28169014084507,
      "grad_norm": 0.45525023341178894,
      "learning_rate": 0.00017345406982593824,
      "loss": 0.471,
      "step": 6601
    },
    {
      "epoch": 13.283702213279678,
      "grad_norm": 0.4274044334888458,
      "learning_rate": 0.00017345004527618473,
      "loss": 0.4602,
      "step": 6602
    },
    {
      "epoch": 13.285714285714286,
      "grad_norm": 0.45818012952804565,
      "learning_rate": 0.00017344602072643124,
      "loss": 0.4822,
      "step": 6603
    },
    {
      "epoch": 13.287726358148893,
      "grad_norm": 0.47942060232162476,
      "learning_rate": 0.00017344199617667772,
      "loss": 0.4847,
      "step": 6604
    },
    {
      "epoch": 13.289738430583501,
      "grad_norm": 0.4445580542087555,
      "learning_rate": 0.00017343797162692426,
      "loss": 0.4719,
      "step": 6605
    },
    {
      "epoch": 13.29175050301811,
      "grad_norm": 0.4545058608055115,
      "learning_rate": 0.00017343394707717075,
      "loss": 0.4528,
      "step": 6606
    },
    {
      "epoch": 13.293762575452716,
      "grad_norm": 0.45995327830314636,
      "learning_rate": 0.00017342992252741726,
      "loss": 0.473,
      "step": 6607
    },
    {
      "epoch": 13.295774647887324,
      "grad_norm": 0.44649356603622437,
      "learning_rate": 0.00017342589797766374,
      "loss": 0.4669,
      "step": 6608
    },
    {
      "epoch": 13.297786720321932,
      "grad_norm": 0.4238341450691223,
      "learning_rate": 0.00017342187342791026,
      "loss": 0.4444,
      "step": 6609
    },
    {
      "epoch": 13.299798792756539,
      "grad_norm": 0.428166002035141,
      "learning_rate": 0.00017341784887815677,
      "loss": 0.4681,
      "step": 6610
    },
    {
      "epoch": 13.301810865191147,
      "grad_norm": 0.4377564787864685,
      "learning_rate": 0.00017341382432840328,
      "loss": 0.4304,
      "step": 6611
    },
    {
      "epoch": 13.303822937625755,
      "grad_norm": 0.44623592495918274,
      "learning_rate": 0.00017340979977864977,
      "loss": 0.4947,
      "step": 6612
    },
    {
      "epoch": 13.305835010060362,
      "grad_norm": 0.4576687812805176,
      "learning_rate": 0.00017340577522889628,
      "loss": 0.4778,
      "step": 6613
    },
    {
      "epoch": 13.30784708249497,
      "grad_norm": 0.4568891227245331,
      "learning_rate": 0.00017340175067914276,
      "loss": 0.4901,
      "step": 6614
    },
    {
      "epoch": 13.309859154929578,
      "grad_norm": 0.4322495460510254,
      "learning_rate": 0.0001733977261293893,
      "loss": 0.4868,
      "step": 6615
    },
    {
      "epoch": 13.311871227364184,
      "grad_norm": 0.466255247592926,
      "learning_rate": 0.0001733937015796358,
      "loss": 0.4566,
      "step": 6616
    },
    {
      "epoch": 13.313883299798793,
      "grad_norm": 0.47259101271629333,
      "learning_rate": 0.0001733896770298823,
      "loss": 0.4663,
      "step": 6617
    },
    {
      "epoch": 13.3158953722334,
      "grad_norm": 0.43625640869140625,
      "learning_rate": 0.00017338565248012879,
      "loss": 0.4752,
      "step": 6618
    },
    {
      "epoch": 13.317907444668007,
      "grad_norm": 0.4700191617012024,
      "learning_rate": 0.0001733816279303753,
      "loss": 0.4674,
      "step": 6619
    },
    {
      "epoch": 13.319919517102615,
      "grad_norm": 0.4774724543094635,
      "learning_rate": 0.0001733776033806218,
      "loss": 0.4942,
      "step": 6620
    },
    {
      "epoch": 13.321931589537224,
      "grad_norm": 0.4867822825908661,
      "learning_rate": 0.00017337357883086832,
      "loss": 0.4493,
      "step": 6621
    },
    {
      "epoch": 13.323943661971832,
      "grad_norm": 0.44471824169158936,
      "learning_rate": 0.0001733695542811148,
      "loss": 0.4246,
      "step": 6622
    },
    {
      "epoch": 13.325955734406438,
      "grad_norm": 0.4469146430492401,
      "learning_rate": 0.00017336552973136132,
      "loss": 0.5147,
      "step": 6623
    },
    {
      "epoch": 13.327967806841047,
      "grad_norm": 0.4419761002063751,
      "learning_rate": 0.0001733615051816078,
      "loss": 0.4449,
      "step": 6624
    },
    {
      "epoch": 13.329979879275655,
      "grad_norm": 0.45690372586250305,
      "learning_rate": 0.00017335748063185432,
      "loss": 0.4465,
      "step": 6625
    },
    {
      "epoch": 13.331991951710261,
      "grad_norm": 0.44578397274017334,
      "learning_rate": 0.00017335345608210083,
      "loss": 0.4413,
      "step": 6626
    },
    {
      "epoch": 13.33400402414487,
      "grad_norm": 0.45369017124176025,
      "learning_rate": 0.00017334943153234734,
      "loss": 0.4866,
      "step": 6627
    },
    {
      "epoch": 13.336016096579478,
      "grad_norm": 0.4478268027305603,
      "learning_rate": 0.00017334540698259383,
      "loss": 0.4661,
      "step": 6628
    },
    {
      "epoch": 13.338028169014084,
      "grad_norm": 0.48586031794548035,
      "learning_rate": 0.00017334138243284034,
      "loss": 0.4928,
      "step": 6629
    },
    {
      "epoch": 13.340040241448692,
      "grad_norm": 0.44729065895080566,
      "learning_rate": 0.00017333735788308685,
      "loss": 0.4498,
      "step": 6630
    },
    {
      "epoch": 13.3420523138833,
      "grad_norm": 0.46589192748069763,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4617,
      "step": 6631
    },
    {
      "epoch": 13.344064386317907,
      "grad_norm": 0.4792613685131073,
      "learning_rate": 0.00017332930878357985,
      "loss": 0.4855,
      "step": 6632
    },
    {
      "epoch": 13.346076458752515,
      "grad_norm": 0.4550052881240845,
      "learning_rate": 0.00017332528423382633,
      "loss": 0.492,
      "step": 6633
    },
    {
      "epoch": 13.348088531187123,
      "grad_norm": 0.44677308201789856,
      "learning_rate": 0.00017332125968407285,
      "loss": 0.4948,
      "step": 6634
    },
    {
      "epoch": 13.35010060362173,
      "grad_norm": 0.4453713893890381,
      "learning_rate": 0.00017331723513431936,
      "loss": 0.4882,
      "step": 6635
    },
    {
      "epoch": 13.352112676056338,
      "grad_norm": 0.4540204107761383,
      "learning_rate": 0.00017331321058456587,
      "loss": 0.4742,
      "step": 6636
    },
    {
      "epoch": 13.354124748490946,
      "grad_norm": 0.4560082256793976,
      "learning_rate": 0.00017330918603481236,
      "loss": 0.446,
      "step": 6637
    },
    {
      "epoch": 13.356136820925553,
      "grad_norm": 0.44698312878608704,
      "learning_rate": 0.00017330516148505887,
      "loss": 0.4727,
      "step": 6638
    },
    {
      "epoch": 13.35814889336016,
      "grad_norm": 0.41456300020217896,
      "learning_rate": 0.00017330113693530535,
      "loss": 0.4894,
      "step": 6639
    },
    {
      "epoch": 13.360160965794769,
      "grad_norm": 0.4370116591453552,
      "learning_rate": 0.0001732971123855519,
      "loss": 0.4831,
      "step": 6640
    },
    {
      "epoch": 13.362173038229376,
      "grad_norm": 0.4320690631866455,
      "learning_rate": 0.00017329308783579838,
      "loss": 0.4932,
      "step": 6641
    },
    {
      "epoch": 13.364185110663984,
      "grad_norm": 0.4506509006023407,
      "learning_rate": 0.0001732890632860449,
      "loss": 0.4778,
      "step": 6642
    },
    {
      "epoch": 13.366197183098592,
      "grad_norm": 0.4756832420825958,
      "learning_rate": 0.00017328503873629137,
      "loss": 0.4938,
      "step": 6643
    },
    {
      "epoch": 13.368209255533198,
      "grad_norm": 0.46191689372062683,
      "learning_rate": 0.0001732810141865379,
      "loss": 0.4609,
      "step": 6644
    },
    {
      "epoch": 13.370221327967807,
      "grad_norm": 0.44648316502571106,
      "learning_rate": 0.0001732769896367844,
      "loss": 0.4542,
      "step": 6645
    },
    {
      "epoch": 13.372233400402415,
      "grad_norm": 0.46142974495887756,
      "learning_rate": 0.0001732729650870309,
      "loss": 0.4591,
      "step": 6646
    },
    {
      "epoch": 13.374245472837021,
      "grad_norm": 0.4518600404262543,
      "learning_rate": 0.0001732689405372774,
      "loss": 0.4687,
      "step": 6647
    },
    {
      "epoch": 13.37625754527163,
      "grad_norm": 0.4654887020587921,
      "learning_rate": 0.0001732649159875239,
      "loss": 0.529,
      "step": 6648
    },
    {
      "epoch": 13.378269617706238,
      "grad_norm": 0.46958133578300476,
      "learning_rate": 0.0001732608914377704,
      "loss": 0.4687,
      "step": 6649
    },
    {
      "epoch": 13.380281690140846,
      "grad_norm": 0.4694656431674957,
      "learning_rate": 0.00017325686688801693,
      "loss": 0.4365,
      "step": 6650
    },
    {
      "epoch": 13.382293762575452,
      "grad_norm": 0.4598897695541382,
      "learning_rate": 0.00017325284233826342,
      "loss": 0.4522,
      "step": 6651
    },
    {
      "epoch": 13.38430583501006,
      "grad_norm": 0.45725491642951965,
      "learning_rate": 0.00017324881778850993,
      "loss": 0.5074,
      "step": 6652
    },
    {
      "epoch": 13.386317907444669,
      "grad_norm": 0.4330612123012543,
      "learning_rate": 0.00017324479323875642,
      "loss": 0.4613,
      "step": 6653
    },
    {
      "epoch": 13.388329979879275,
      "grad_norm": 0.468475341796875,
      "learning_rate": 0.00017324076868900293,
      "loss": 0.4808,
      "step": 6654
    },
    {
      "epoch": 13.390342052313883,
      "grad_norm": 0.44513601064682007,
      "learning_rate": 0.00017323674413924944,
      "loss": 0.4955,
      "step": 6655
    },
    {
      "epoch": 13.392354124748492,
      "grad_norm": 0.44409674406051636,
      "learning_rate": 0.00017323271958949595,
      "loss": 0.4516,
      "step": 6656
    },
    {
      "epoch": 13.394366197183098,
      "grad_norm": 0.4652820825576782,
      "learning_rate": 0.00017322869503974244,
      "loss": 0.5021,
      "step": 6657
    },
    {
      "epoch": 13.396378269617706,
      "grad_norm": 0.4624274969100952,
      "learning_rate": 0.00017322467048998895,
      "loss": 0.4606,
      "step": 6658
    },
    {
      "epoch": 13.398390342052314,
      "grad_norm": 0.4814695417881012,
      "learning_rate": 0.00017322064594023543,
      "loss": 0.4873,
      "step": 6659
    },
    {
      "epoch": 13.400402414486921,
      "grad_norm": 0.43339359760284424,
      "learning_rate": 0.00017321662139048195,
      "loss": 0.4572,
      "step": 6660
    },
    {
      "epoch": 13.40241448692153,
      "grad_norm": 0.4541744589805603,
      "learning_rate": 0.00017321259684072846,
      "loss": 0.483,
      "step": 6661
    },
    {
      "epoch": 13.404426559356137,
      "grad_norm": 0.46712663769721985,
      "learning_rate": 0.00017320857229097497,
      "loss": 0.4828,
      "step": 6662
    },
    {
      "epoch": 13.406438631790744,
      "grad_norm": 0.46091341972351074,
      "learning_rate": 0.00017320454774122146,
      "loss": 0.4946,
      "step": 6663
    },
    {
      "epoch": 13.408450704225352,
      "grad_norm": 0.4506703019142151,
      "learning_rate": 0.00017320052319146797,
      "loss": 0.4562,
      "step": 6664
    },
    {
      "epoch": 13.41046277665996,
      "grad_norm": 0.46988850831985474,
      "learning_rate": 0.00017319649864171448,
      "loss": 0.4828,
      "step": 6665
    },
    {
      "epoch": 13.412474849094567,
      "grad_norm": 0.47431081533432007,
      "learning_rate": 0.00017319247409196097,
      "loss": 0.4468,
      "step": 6666
    },
    {
      "epoch": 13.414486921529175,
      "grad_norm": 0.46789488196372986,
      "learning_rate": 0.00017318844954220748,
      "loss": 0.5165,
      "step": 6667
    },
    {
      "epoch": 13.416498993963783,
      "grad_norm": 0.461786687374115,
      "learning_rate": 0.00017318442499245396,
      "loss": 0.4766,
      "step": 6668
    },
    {
      "epoch": 13.41851106639839,
      "grad_norm": 0.4826470911502838,
      "learning_rate": 0.00017318040044270048,
      "loss": 0.457,
      "step": 6669
    },
    {
      "epoch": 13.420523138832998,
      "grad_norm": 0.45414039492607117,
      "learning_rate": 0.000173176375892947,
      "loss": 0.4957,
      "step": 6670
    },
    {
      "epoch": 13.422535211267606,
      "grad_norm": 0.42583590745925903,
      "learning_rate": 0.0001731723513431935,
      "loss": 0.4613,
      "step": 6671
    },
    {
      "epoch": 13.424547283702214,
      "grad_norm": 0.4590943157672882,
      "learning_rate": 0.00017316832679343998,
      "loss": 0.4871,
      "step": 6672
    },
    {
      "epoch": 13.42655935613682,
      "grad_norm": 0.4443960189819336,
      "learning_rate": 0.0001731643022436865,
      "loss": 0.4608,
      "step": 6673
    },
    {
      "epoch": 13.428571428571429,
      "grad_norm": 0.47927406430244446,
      "learning_rate": 0.00017316027769393298,
      "loss": 0.4715,
      "step": 6674
    },
    {
      "epoch": 13.430583501006037,
      "grad_norm": 0.4720500707626343,
      "learning_rate": 0.0001731562531441795,
      "loss": 0.4716,
      "step": 6675
    },
    {
      "epoch": 13.432595573440643,
      "grad_norm": 0.4568949043750763,
      "learning_rate": 0.000173152228594426,
      "loss": 0.5289,
      "step": 6676
    },
    {
      "epoch": 13.434607645875252,
      "grad_norm": 0.47152554988861084,
      "learning_rate": 0.00017314820404467252,
      "loss": 0.4867,
      "step": 6677
    },
    {
      "epoch": 13.43661971830986,
      "grad_norm": 0.44602170586586,
      "learning_rate": 0.000173144179494919,
      "loss": 0.4783,
      "step": 6678
    },
    {
      "epoch": 13.438631790744466,
      "grad_norm": 0.45261743664741516,
      "learning_rate": 0.00017314015494516552,
      "loss": 0.4969,
      "step": 6679
    },
    {
      "epoch": 13.440643863179075,
      "grad_norm": 0.48569491505622864,
      "learning_rate": 0.000173136130395412,
      "loss": 0.5053,
      "step": 6680
    },
    {
      "epoch": 13.442655935613683,
      "grad_norm": 0.46729183197021484,
      "learning_rate": 0.00017313210584565854,
      "loss": 0.4786,
      "step": 6681
    },
    {
      "epoch": 13.44466800804829,
      "grad_norm": 0.444379061460495,
      "learning_rate": 0.00017312808129590503,
      "loss": 0.5057,
      "step": 6682
    },
    {
      "epoch": 13.446680080482897,
      "grad_norm": 0.48000237345695496,
      "learning_rate": 0.00017312405674615154,
      "loss": 0.4679,
      "step": 6683
    },
    {
      "epoch": 13.448692152917506,
      "grad_norm": 0.47399356961250305,
      "learning_rate": 0.00017312003219639802,
      "loss": 0.4726,
      "step": 6684
    },
    {
      "epoch": 13.450704225352112,
      "grad_norm": 0.44588425755500793,
      "learning_rate": 0.00017311600764664454,
      "loss": 0.4508,
      "step": 6685
    },
    {
      "epoch": 13.45271629778672,
      "grad_norm": 0.45422419905662537,
      "learning_rate": 0.00017311198309689105,
      "loss": 0.4613,
      "step": 6686
    },
    {
      "epoch": 13.454728370221329,
      "grad_norm": 0.4521321654319763,
      "learning_rate": 0.00017310795854713756,
      "loss": 0.4635,
      "step": 6687
    },
    {
      "epoch": 13.456740442655935,
      "grad_norm": 0.474160373210907,
      "learning_rate": 0.00017310393399738404,
      "loss": 0.476,
      "step": 6688
    },
    {
      "epoch": 13.458752515090543,
      "grad_norm": 0.4608854055404663,
      "learning_rate": 0.00017309990944763056,
      "loss": 0.483,
      "step": 6689
    },
    {
      "epoch": 13.460764587525151,
      "grad_norm": 0.42274975776672363,
      "learning_rate": 0.00017309588489787704,
      "loss": 0.4636,
      "step": 6690
    },
    {
      "epoch": 13.462776659959758,
      "grad_norm": 0.4801750183105469,
      "learning_rate": 0.00017309186034812358,
      "loss": 0.4344,
      "step": 6691
    },
    {
      "epoch": 13.464788732394366,
      "grad_norm": 0.4488401412963867,
      "learning_rate": 0.00017308783579837007,
      "loss": 0.5116,
      "step": 6692
    },
    {
      "epoch": 13.466800804828974,
      "grad_norm": 0.47237369418144226,
      "learning_rate": 0.00017308381124861658,
      "loss": 0.5087,
      "step": 6693
    },
    {
      "epoch": 13.46881287726358,
      "grad_norm": 0.44611459970474243,
      "learning_rate": 0.00017307978669886306,
      "loss": 0.4842,
      "step": 6694
    },
    {
      "epoch": 13.470824949698189,
      "grad_norm": 0.4610566198825836,
      "learning_rate": 0.00017307576214910958,
      "loss": 0.477,
      "step": 6695
    },
    {
      "epoch": 13.472837022132797,
      "grad_norm": 0.45225849747657776,
      "learning_rate": 0.0001730717375993561,
      "loss": 0.4517,
      "step": 6696
    },
    {
      "epoch": 13.474849094567404,
      "grad_norm": 0.4552786350250244,
      "learning_rate": 0.00017306771304960257,
      "loss": 0.4388,
      "step": 6697
    },
    {
      "epoch": 13.476861167002012,
      "grad_norm": 0.4718431532382965,
      "learning_rate": 0.00017306368849984909,
      "loss": 0.4985,
      "step": 6698
    },
    {
      "epoch": 13.47887323943662,
      "grad_norm": 0.4954501688480377,
      "learning_rate": 0.0001730596639500956,
      "loss": 0.4957,
      "step": 6699
    },
    {
      "epoch": 13.480885311871228,
      "grad_norm": 0.45116955041885376,
      "learning_rate": 0.00017305563940034208,
      "loss": 0.4915,
      "step": 6700
    },
    {
      "epoch": 13.482897384305835,
      "grad_norm": 0.4677894413471222,
      "learning_rate": 0.0001730516148505886,
      "loss": 0.5262,
      "step": 6701
    },
    {
      "epoch": 13.484909456740443,
      "grad_norm": 0.43117615580558777,
      "learning_rate": 0.0001730475903008351,
      "loss": 0.5012,
      "step": 6702
    },
    {
      "epoch": 13.486921529175051,
      "grad_norm": 0.4475751519203186,
      "learning_rate": 0.0001730435657510816,
      "loss": 0.5096,
      "step": 6703
    },
    {
      "epoch": 13.488933601609657,
      "grad_norm": 0.44546666741371155,
      "learning_rate": 0.0001730395412013281,
      "loss": 0.4961,
      "step": 6704
    },
    {
      "epoch": 13.490945674044266,
      "grad_norm": 0.47628161311149597,
      "learning_rate": 0.0001730355166515746,
      "loss": 0.4631,
      "step": 6705
    },
    {
      "epoch": 13.492957746478874,
      "grad_norm": 0.45045721530914307,
      "learning_rate": 0.00017303149210182113,
      "loss": 0.4312,
      "step": 6706
    },
    {
      "epoch": 13.49496981891348,
      "grad_norm": 0.44875770807266235,
      "learning_rate": 0.00017302746755206761,
      "loss": 0.441,
      "step": 6707
    },
    {
      "epoch": 13.496981891348089,
      "grad_norm": 0.4592781364917755,
      "learning_rate": 0.00017302344300231413,
      "loss": 0.5382,
      "step": 6708
    },
    {
      "epoch": 13.498993963782697,
      "grad_norm": 0.4880411922931671,
      "learning_rate": 0.0001730194184525606,
      "loss": 0.5122,
      "step": 6709
    },
    {
      "epoch": 13.501006036217303,
      "grad_norm": 0.4573858380317688,
      "learning_rate": 0.00017301539390280712,
      "loss": 0.4722,
      "step": 6710
    },
    {
      "epoch": 13.503018108651911,
      "grad_norm": 0.4660152196884155,
      "learning_rate": 0.00017301136935305364,
      "loss": 0.4951,
      "step": 6711
    },
    {
      "epoch": 13.50503018108652,
      "grad_norm": 0.4505237638950348,
      "learning_rate": 0.00017300734480330015,
      "loss": 0.475,
      "step": 6712
    },
    {
      "epoch": 13.507042253521126,
      "grad_norm": 0.47432082891464233,
      "learning_rate": 0.00017300332025354663,
      "loss": 0.4777,
      "step": 6713
    },
    {
      "epoch": 13.509054325955734,
      "grad_norm": 0.44369804859161377,
      "learning_rate": 0.00017299929570379315,
      "loss": 0.4681,
      "step": 6714
    },
    {
      "epoch": 13.511066398390343,
      "grad_norm": 0.46003052592277527,
      "learning_rate": 0.00017299527115403963,
      "loss": 0.484,
      "step": 6715
    },
    {
      "epoch": 13.513078470824949,
      "grad_norm": 0.4574500620365143,
      "learning_rate": 0.00017299124660428617,
      "loss": 0.5202,
      "step": 6716
    },
    {
      "epoch": 13.515090543259557,
      "grad_norm": 0.45821911096572876,
      "learning_rate": 0.00017298722205453266,
      "loss": 0.4917,
      "step": 6717
    },
    {
      "epoch": 13.517102615694165,
      "grad_norm": 0.4871044158935547,
      "learning_rate": 0.00017298319750477917,
      "loss": 0.5057,
      "step": 6718
    },
    {
      "epoch": 13.519114688128772,
      "grad_norm": 0.4302845299243927,
      "learning_rate": 0.00017297917295502565,
      "loss": 0.4829,
      "step": 6719
    },
    {
      "epoch": 13.52112676056338,
      "grad_norm": 0.45037081837654114,
      "learning_rate": 0.00017297514840527216,
      "loss": 0.486,
      "step": 6720
    },
    {
      "epoch": 13.523138832997988,
      "grad_norm": 0.446146696805954,
      "learning_rate": 0.00017297112385551868,
      "loss": 0.4889,
      "step": 6721
    },
    {
      "epoch": 13.525150905432596,
      "grad_norm": 0.46141305565834045,
      "learning_rate": 0.0001729670993057652,
      "loss": 0.4829,
      "step": 6722
    },
    {
      "epoch": 13.527162977867203,
      "grad_norm": 0.4695071578025818,
      "learning_rate": 0.00017296307475601167,
      "loss": 0.4835,
      "step": 6723
    },
    {
      "epoch": 13.529175050301811,
      "grad_norm": 0.43980535864830017,
      "learning_rate": 0.00017295905020625819,
      "loss": 0.4724,
      "step": 6724
    },
    {
      "epoch": 13.53118712273642,
      "grad_norm": 0.44065722823143005,
      "learning_rate": 0.00017295502565650467,
      "loss": 0.4465,
      "step": 6725
    },
    {
      "epoch": 13.533199195171026,
      "grad_norm": 0.45059195160865784,
      "learning_rate": 0.0001729510011067512,
      "loss": 0.4615,
      "step": 6726
    },
    {
      "epoch": 13.535211267605634,
      "grad_norm": 0.4531846046447754,
      "learning_rate": 0.0001729469765569977,
      "loss": 0.4693,
      "step": 6727
    },
    {
      "epoch": 13.537223340040242,
      "grad_norm": 0.4593614935874939,
      "learning_rate": 0.0001729429520072442,
      "loss": 0.4825,
      "step": 6728
    },
    {
      "epoch": 13.539235412474849,
      "grad_norm": 0.46298474073410034,
      "learning_rate": 0.0001729389274574907,
      "loss": 0.4979,
      "step": 6729
    },
    {
      "epoch": 13.541247484909457,
      "grad_norm": 0.4497848451137543,
      "learning_rate": 0.0001729349029077372,
      "loss": 0.5106,
      "step": 6730
    },
    {
      "epoch": 13.543259557344065,
      "grad_norm": 0.43833181262016296,
      "learning_rate": 0.00017293087835798372,
      "loss": 0.4589,
      "step": 6731
    },
    {
      "epoch": 13.545271629778671,
      "grad_norm": 0.4537248909473419,
      "learning_rate": 0.0001729268538082302,
      "loss": 0.5095,
      "step": 6732
    },
    {
      "epoch": 13.54728370221328,
      "grad_norm": 0.46562859416007996,
      "learning_rate": 0.00017292282925847671,
      "loss": 0.522,
      "step": 6733
    },
    {
      "epoch": 13.549295774647888,
      "grad_norm": 0.47300130128860474,
      "learning_rate": 0.00017291880470872323,
      "loss": 0.5159,
      "step": 6734
    },
    {
      "epoch": 13.551307847082494,
      "grad_norm": 0.45703136920928955,
      "learning_rate": 0.0001729147801589697,
      "loss": 0.4655,
      "step": 6735
    },
    {
      "epoch": 13.553319919517103,
      "grad_norm": 0.45331284403800964,
      "learning_rate": 0.00017291075560921622,
      "loss": 0.4754,
      "step": 6736
    },
    {
      "epoch": 13.55533199195171,
      "grad_norm": 0.44870835542678833,
      "learning_rate": 0.00017290673105946274,
      "loss": 0.4785,
      "step": 6737
    },
    {
      "epoch": 13.557344064386317,
      "grad_norm": 0.48678523302078247,
      "learning_rate": 0.00017290270650970922,
      "loss": 0.5132,
      "step": 6738
    },
    {
      "epoch": 13.559356136820925,
      "grad_norm": 0.47286155819892883,
      "learning_rate": 0.00017289868195995573,
      "loss": 0.4873,
      "step": 6739
    },
    {
      "epoch": 13.561368209255534,
      "grad_norm": 0.46643269062042236,
      "learning_rate": 0.00017289465741020222,
      "loss": 0.4735,
      "step": 6740
    },
    {
      "epoch": 13.56338028169014,
      "grad_norm": 0.44346171617507935,
      "learning_rate": 0.00017289063286044876,
      "loss": 0.4563,
      "step": 6741
    },
    {
      "epoch": 13.565392354124748,
      "grad_norm": 0.4581216275691986,
      "learning_rate": 0.00017288660831069524,
      "loss": 0.4965,
      "step": 6742
    },
    {
      "epoch": 13.567404426559357,
      "grad_norm": 0.4396096169948578,
      "learning_rate": 0.00017288258376094176,
      "loss": 0.4821,
      "step": 6743
    },
    {
      "epoch": 13.569416498993963,
      "grad_norm": 0.45223554968833923,
      "learning_rate": 0.00017287855921118824,
      "loss": 0.4579,
      "step": 6744
    },
    {
      "epoch": 13.571428571428571,
      "grad_norm": 0.45289307832717896,
      "learning_rate": 0.00017287453466143475,
      "loss": 0.4943,
      "step": 6745
    },
    {
      "epoch": 13.57344064386318,
      "grad_norm": 0.5017739534378052,
      "learning_rate": 0.00017287051011168127,
      "loss": 0.4938,
      "step": 6746
    },
    {
      "epoch": 13.575452716297786,
      "grad_norm": 0.45494529604911804,
      "learning_rate": 0.00017286648556192778,
      "loss": 0.4919,
      "step": 6747
    },
    {
      "epoch": 13.577464788732394,
      "grad_norm": 0.48791584372520447,
      "learning_rate": 0.00017286246101217426,
      "loss": 0.4677,
      "step": 6748
    },
    {
      "epoch": 13.579476861167002,
      "grad_norm": 0.47060638666152954,
      "learning_rate": 0.00017285843646242077,
      "loss": 0.4937,
      "step": 6749
    },
    {
      "epoch": 13.58148893360161,
      "grad_norm": 0.4475337564945221,
      "learning_rate": 0.00017285441191266726,
      "loss": 0.5066,
      "step": 6750
    },
    {
      "epoch": 13.583501006036217,
      "grad_norm": 0.46414557099342346,
      "learning_rate": 0.0001728503873629138,
      "loss": 0.5007,
      "step": 6751
    },
    {
      "epoch": 13.585513078470825,
      "grad_norm": 0.481804221868515,
      "learning_rate": 0.00017284636281316028,
      "loss": 0.5258,
      "step": 6752
    },
    {
      "epoch": 13.587525150905433,
      "grad_norm": 0.46727779507637024,
      "learning_rate": 0.0001728423382634068,
      "loss": 0.4798,
      "step": 6753
    },
    {
      "epoch": 13.58953722334004,
      "grad_norm": 0.4443552792072296,
      "learning_rate": 0.00017283831371365328,
      "loss": 0.518,
      "step": 6754
    },
    {
      "epoch": 13.591549295774648,
      "grad_norm": 0.4404948651790619,
      "learning_rate": 0.0001728342891638998,
      "loss": 0.4972,
      "step": 6755
    },
    {
      "epoch": 13.593561368209256,
      "grad_norm": 0.4337463080883026,
      "learning_rate": 0.0001728302646141463,
      "loss": 0.4882,
      "step": 6756
    },
    {
      "epoch": 13.595573440643863,
      "grad_norm": 0.42818087339401245,
      "learning_rate": 0.00017282624006439282,
      "loss": 0.4726,
      "step": 6757
    },
    {
      "epoch": 13.59758551307847,
      "grad_norm": 0.4647473096847534,
      "learning_rate": 0.0001728222155146393,
      "loss": 0.4551,
      "step": 6758
    },
    {
      "epoch": 13.599597585513079,
      "grad_norm": 0.46885424852371216,
      "learning_rate": 0.00017281819096488582,
      "loss": 0.496,
      "step": 6759
    },
    {
      "epoch": 13.601609657947686,
      "grad_norm": 0.4913686513900757,
      "learning_rate": 0.0001728141664151323,
      "loss": 0.5033,
      "step": 6760
    },
    {
      "epoch": 13.603621730382294,
      "grad_norm": 0.44636407494544983,
      "learning_rate": 0.00017281014186537884,
      "loss": 0.4424,
      "step": 6761
    },
    {
      "epoch": 13.605633802816902,
      "grad_norm": 0.5055406093597412,
      "learning_rate": 0.00017280611731562533,
      "loss": 0.4849,
      "step": 6762
    },
    {
      "epoch": 13.607645875251508,
      "grad_norm": 0.45901012420654297,
      "learning_rate": 0.00017280209276587184,
      "loss": 0.4658,
      "step": 6763
    },
    {
      "epoch": 13.609657947686117,
      "grad_norm": 0.45297056436538696,
      "learning_rate": 0.00017279806821611832,
      "loss": 0.4698,
      "step": 6764
    },
    {
      "epoch": 13.611670020120725,
      "grad_norm": 0.45780161023139954,
      "learning_rate": 0.00017279404366636483,
      "loss": 0.4769,
      "step": 6765
    },
    {
      "epoch": 13.613682092555331,
      "grad_norm": 0.45254117250442505,
      "learning_rate": 0.00017279001911661135,
      "loss": 0.4809,
      "step": 6766
    },
    {
      "epoch": 13.61569416498994,
      "grad_norm": 0.44121527671813965,
      "learning_rate": 0.00017278599456685783,
      "loss": 0.4811,
      "step": 6767
    },
    {
      "epoch": 13.617706237424548,
      "grad_norm": 0.46418333053588867,
      "learning_rate": 0.00017278197001710434,
      "loss": 0.4763,
      "step": 6768
    },
    {
      "epoch": 13.619718309859154,
      "grad_norm": 0.4660837948322296,
      "learning_rate": 0.00017277794546735086,
      "loss": 0.4636,
      "step": 6769
    },
    {
      "epoch": 13.621730382293762,
      "grad_norm": 0.5038473010063171,
      "learning_rate": 0.00017277392091759734,
      "loss": 0.5047,
      "step": 6770
    },
    {
      "epoch": 13.62374245472837,
      "grad_norm": 0.47547072172164917,
      "learning_rate": 0.00017276989636784385,
      "loss": 0.5018,
      "step": 6771
    },
    {
      "epoch": 13.625754527162979,
      "grad_norm": 0.4493570625782013,
      "learning_rate": 0.00017276587181809037,
      "loss": 0.4994,
      "step": 6772
    },
    {
      "epoch": 13.627766599597585,
      "grad_norm": 0.4561305642127991,
      "learning_rate": 0.00017276184726833685,
      "loss": 0.5011,
      "step": 6773
    },
    {
      "epoch": 13.629778672032193,
      "grad_norm": 0.4340165853500366,
      "learning_rate": 0.00017275782271858336,
      "loss": 0.4966,
      "step": 6774
    },
    {
      "epoch": 13.631790744466802,
      "grad_norm": 0.4693948030471802,
      "learning_rate": 0.00017275379816882985,
      "loss": 0.4759,
      "step": 6775
    },
    {
      "epoch": 13.633802816901408,
      "grad_norm": 0.46202927827835083,
      "learning_rate": 0.0001727497736190764,
      "loss": 0.5161,
      "step": 6776
    },
    {
      "epoch": 13.635814889336016,
      "grad_norm": 0.47724995017051697,
      "learning_rate": 0.00017274574906932287,
      "loss": 0.5263,
      "step": 6777
    },
    {
      "epoch": 13.637826961770624,
      "grad_norm": 0.4879809617996216,
      "learning_rate": 0.00017274172451956939,
      "loss": 0.4949,
      "step": 6778
    },
    {
      "epoch": 13.639839034205231,
      "grad_norm": 0.4485979974269867,
      "learning_rate": 0.00017273769996981587,
      "loss": 0.4526,
      "step": 6779
    },
    {
      "epoch": 13.64185110663984,
      "grad_norm": 0.47666072845458984,
      "learning_rate": 0.00017273367542006238,
      "loss": 0.499,
      "step": 6780
    },
    {
      "epoch": 13.643863179074447,
      "grad_norm": 0.47580835223197937,
      "learning_rate": 0.0001727296508703089,
      "loss": 0.5131,
      "step": 6781
    },
    {
      "epoch": 13.645875251509054,
      "grad_norm": 0.4975127875804901,
      "learning_rate": 0.0001727256263205554,
      "loss": 0.4893,
      "step": 6782
    },
    {
      "epoch": 13.647887323943662,
      "grad_norm": 0.4658832550048828,
      "learning_rate": 0.0001727216017708019,
      "loss": 0.4887,
      "step": 6783
    },
    {
      "epoch": 13.64989939637827,
      "grad_norm": 0.4517718553543091,
      "learning_rate": 0.0001727175772210484,
      "loss": 0.4516,
      "step": 6784
    },
    {
      "epoch": 13.651911468812877,
      "grad_norm": 0.4433099925518036,
      "learning_rate": 0.0001727135526712949,
      "loss": 0.4999,
      "step": 6785
    },
    {
      "epoch": 13.653923541247485,
      "grad_norm": 0.44632288813591003,
      "learning_rate": 0.00017270952812154143,
      "loss": 0.4897,
      "step": 6786
    },
    {
      "epoch": 13.655935613682093,
      "grad_norm": 0.442064493894577,
      "learning_rate": 0.00017270550357178791,
      "loss": 0.482,
      "step": 6787
    },
    {
      "epoch": 13.6579476861167,
      "grad_norm": 0.5063746571540833,
      "learning_rate": 0.00017270147902203443,
      "loss": 0.4679,
      "step": 6788
    },
    {
      "epoch": 13.659959758551308,
      "grad_norm": 0.4541895389556885,
      "learning_rate": 0.0001726974544722809,
      "loss": 0.5131,
      "step": 6789
    },
    {
      "epoch": 13.661971830985916,
      "grad_norm": 0.4538188874721527,
      "learning_rate": 0.00017269342992252742,
      "loss": 0.4839,
      "step": 6790
    },
    {
      "epoch": 13.663983903420522,
      "grad_norm": 0.4524194300174713,
      "learning_rate": 0.00017268940537277394,
      "loss": 0.5056,
      "step": 6791
    },
    {
      "epoch": 13.66599597585513,
      "grad_norm": 0.46126067638397217,
      "learning_rate": 0.00017268538082302045,
      "loss": 0.4947,
      "step": 6792
    },
    {
      "epoch": 13.668008048289739,
      "grad_norm": 0.45178914070129395,
      "learning_rate": 0.00017268135627326693,
      "loss": 0.4728,
      "step": 6793
    },
    {
      "epoch": 13.670020120724345,
      "grad_norm": 0.45041078329086304,
      "learning_rate": 0.00017267733172351345,
      "loss": 0.4791,
      "step": 6794
    },
    {
      "epoch": 13.672032193158953,
      "grad_norm": 0.45350703597068787,
      "learning_rate": 0.00017267330717375993,
      "loss": 0.4765,
      "step": 6795
    },
    {
      "epoch": 13.674044265593562,
      "grad_norm": 0.4738403558731079,
      "learning_rate": 0.00017266928262400647,
      "loss": 0.4786,
      "step": 6796
    },
    {
      "epoch": 13.676056338028168,
      "grad_norm": 0.4800238609313965,
      "learning_rate": 0.00017266525807425295,
      "loss": 0.5289,
      "step": 6797
    },
    {
      "epoch": 13.678068410462776,
      "grad_norm": 0.4646954834461212,
      "learning_rate": 0.00017266123352449947,
      "loss": 0.4942,
      "step": 6798
    },
    {
      "epoch": 13.680080482897385,
      "grad_norm": 0.4487348794937134,
      "learning_rate": 0.00017265720897474595,
      "loss": 0.4948,
      "step": 6799
    },
    {
      "epoch": 13.682092555331993,
      "grad_norm": 0.4766154885292053,
      "learning_rate": 0.00017265318442499246,
      "loss": 0.4938,
      "step": 6800
    },
    {
      "epoch": 13.6841046277666,
      "grad_norm": 0.46269693970680237,
      "learning_rate": 0.00017264915987523898,
      "loss": 0.4985,
      "step": 6801
    },
    {
      "epoch": 13.686116700201207,
      "grad_norm": 0.4342919588088989,
      "learning_rate": 0.00017264513532548546,
      "loss": 0.46,
      "step": 6802
    },
    {
      "epoch": 13.688128772635816,
      "grad_norm": 0.4684652090072632,
      "learning_rate": 0.00017264111077573197,
      "loss": 0.4855,
      "step": 6803
    },
    {
      "epoch": 13.690140845070422,
      "grad_norm": 0.4362568259239197,
      "learning_rate": 0.00017263708622597849,
      "loss": 0.4884,
      "step": 6804
    },
    {
      "epoch": 13.69215291750503,
      "grad_norm": 0.45754578709602356,
      "learning_rate": 0.00017263306167622497,
      "loss": 0.5022,
      "step": 6805
    },
    {
      "epoch": 13.694164989939638,
      "grad_norm": 0.47352612018585205,
      "learning_rate": 0.00017262903712647148,
      "loss": 0.501,
      "step": 6806
    },
    {
      "epoch": 13.696177062374245,
      "grad_norm": 0.43636590242385864,
      "learning_rate": 0.000172625012576718,
      "loss": 0.4891,
      "step": 6807
    },
    {
      "epoch": 13.698189134808853,
      "grad_norm": 0.44793030619621277,
      "learning_rate": 0.00017262098802696448,
      "loss": 0.4941,
      "step": 6808
    },
    {
      "epoch": 13.700201207243461,
      "grad_norm": 0.44043219089508057,
      "learning_rate": 0.000172616963477211,
      "loss": 0.4654,
      "step": 6809
    },
    {
      "epoch": 13.702213279678068,
      "grad_norm": 0.48120030760765076,
      "learning_rate": 0.00017261293892745748,
      "loss": 0.451,
      "step": 6810
    },
    {
      "epoch": 13.704225352112676,
      "grad_norm": 0.48160064220428467,
      "learning_rate": 0.00017260891437770402,
      "loss": 0.5017,
      "step": 6811
    },
    {
      "epoch": 13.706237424547284,
      "grad_norm": 0.4742310345172882,
      "learning_rate": 0.0001726048898279505,
      "loss": 0.4915,
      "step": 6812
    },
    {
      "epoch": 13.70824949698189,
      "grad_norm": 0.4766596853733063,
      "learning_rate": 0.00017260086527819701,
      "loss": 0.5038,
      "step": 6813
    },
    {
      "epoch": 13.710261569416499,
      "grad_norm": 0.4393630921840668,
      "learning_rate": 0.0001725968407284435,
      "loss": 0.4898,
      "step": 6814
    },
    {
      "epoch": 13.712273641851107,
      "grad_norm": 0.4668694734573364,
      "learning_rate": 0.00017259281617869,
      "loss": 0.4773,
      "step": 6815
    },
    {
      "epoch": 13.714285714285714,
      "grad_norm": 0.44047775864601135,
      "learning_rate": 0.00017258879162893652,
      "loss": 0.4892,
      "step": 6816
    },
    {
      "epoch": 13.716297786720322,
      "grad_norm": 0.41807761788368225,
      "learning_rate": 0.00017258476707918304,
      "loss": 0.4545,
      "step": 6817
    },
    {
      "epoch": 13.71830985915493,
      "grad_norm": 0.4614827334880829,
      "learning_rate": 0.00017258074252942952,
      "loss": 0.4856,
      "step": 6818
    },
    {
      "epoch": 13.720321931589538,
      "grad_norm": 0.45180684328079224,
      "learning_rate": 0.00017257671797967603,
      "loss": 0.4872,
      "step": 6819
    },
    {
      "epoch": 13.722334004024145,
      "grad_norm": 0.4522598683834076,
      "learning_rate": 0.00017257269342992252,
      "loss": 0.4898,
      "step": 6820
    },
    {
      "epoch": 13.724346076458753,
      "grad_norm": 0.44234034419059753,
      "learning_rate": 0.00017256866888016906,
      "loss": 0.4911,
      "step": 6821
    },
    {
      "epoch": 13.726358148893361,
      "grad_norm": 0.4610097408294678,
      "learning_rate": 0.00017256464433041554,
      "loss": 0.4997,
      "step": 6822
    },
    {
      "epoch": 13.728370221327967,
      "grad_norm": 0.47553306818008423,
      "learning_rate": 0.00017256061978066206,
      "loss": 0.4594,
      "step": 6823
    },
    {
      "epoch": 13.730382293762576,
      "grad_norm": 0.48674869537353516,
      "learning_rate": 0.00017255659523090854,
      "loss": 0.4937,
      "step": 6824
    },
    {
      "epoch": 13.732394366197184,
      "grad_norm": 0.5211796760559082,
      "learning_rate": 0.00017255257068115505,
      "loss": 0.5234,
      "step": 6825
    },
    {
      "epoch": 13.73440643863179,
      "grad_norm": 0.45646098256111145,
      "learning_rate": 0.00017254854613140157,
      "loss": 0.505,
      "step": 6826
    },
    {
      "epoch": 13.736418511066399,
      "grad_norm": 0.4534609317779541,
      "learning_rate": 0.00017254452158164808,
      "loss": 0.4996,
      "step": 6827
    },
    {
      "epoch": 13.738430583501007,
      "grad_norm": 0.44416725635528564,
      "learning_rate": 0.00017254049703189456,
      "loss": 0.4968,
      "step": 6828
    },
    {
      "epoch": 13.740442655935613,
      "grad_norm": 0.43963998556137085,
      "learning_rate": 0.00017253647248214107,
      "loss": 0.4942,
      "step": 6829
    },
    {
      "epoch": 13.742454728370221,
      "grad_norm": 0.4711790680885315,
      "learning_rate": 0.00017253244793238756,
      "loss": 0.4772,
      "step": 6830
    },
    {
      "epoch": 13.74446680080483,
      "grad_norm": 0.44515374302864075,
      "learning_rate": 0.0001725284233826341,
      "loss": 0.4943,
      "step": 6831
    },
    {
      "epoch": 13.746478873239436,
      "grad_norm": 0.44424790143966675,
      "learning_rate": 0.00017252439883288058,
      "loss": 0.5265,
      "step": 6832
    },
    {
      "epoch": 13.748490945674044,
      "grad_norm": 0.47202420234680176,
      "learning_rate": 0.0001725203742831271,
      "loss": 0.5141,
      "step": 6833
    },
    {
      "epoch": 13.750503018108652,
      "grad_norm": 0.4506892263889313,
      "learning_rate": 0.00017251634973337358,
      "loss": 0.5135,
      "step": 6834
    },
    {
      "epoch": 13.752515090543259,
      "grad_norm": 0.43990856409072876,
      "learning_rate": 0.0001725123251836201,
      "loss": 0.4335,
      "step": 6835
    },
    {
      "epoch": 13.754527162977867,
      "grad_norm": 0.4422074854373932,
      "learning_rate": 0.0001725083006338666,
      "loss": 0.4764,
      "step": 6836
    },
    {
      "epoch": 13.756539235412475,
      "grad_norm": 0.45673054456710815,
      "learning_rate": 0.0001725042760841131,
      "loss": 0.5259,
      "step": 6837
    },
    {
      "epoch": 13.758551307847082,
      "grad_norm": 0.4413672089576721,
      "learning_rate": 0.0001725002515343596,
      "loss": 0.4773,
      "step": 6838
    },
    {
      "epoch": 13.76056338028169,
      "grad_norm": 0.45340973138809204,
      "learning_rate": 0.0001724962269846061,
      "loss": 0.4811,
      "step": 6839
    },
    {
      "epoch": 13.762575452716298,
      "grad_norm": 0.46172940731048584,
      "learning_rate": 0.0001724922024348526,
      "loss": 0.4803,
      "step": 6840
    },
    {
      "epoch": 13.764587525150905,
      "grad_norm": 0.4522828757762909,
      "learning_rate": 0.0001724881778850991,
      "loss": 0.4817,
      "step": 6841
    },
    {
      "epoch": 13.766599597585513,
      "grad_norm": 0.45377442240715027,
      "learning_rate": 0.00017248415333534563,
      "loss": 0.492,
      "step": 6842
    },
    {
      "epoch": 13.768611670020121,
      "grad_norm": 0.45011550188064575,
      "learning_rate": 0.0001724801287855921,
      "loss": 0.493,
      "step": 6843
    },
    {
      "epoch": 13.770623742454728,
      "grad_norm": 0.44840261340141296,
      "learning_rate": 0.00017247610423583862,
      "loss": 0.4489,
      "step": 6844
    },
    {
      "epoch": 13.772635814889336,
      "grad_norm": 0.48604586720466614,
      "learning_rate": 0.0001724720796860851,
      "loss": 0.4927,
      "step": 6845
    },
    {
      "epoch": 13.774647887323944,
      "grad_norm": 0.45615050196647644,
      "learning_rate": 0.00017246805513633165,
      "loss": 0.4934,
      "step": 6846
    },
    {
      "epoch": 13.77665995975855,
      "grad_norm": 0.4439641237258911,
      "learning_rate": 0.00017246403058657813,
      "loss": 0.5338,
      "step": 6847
    },
    {
      "epoch": 13.778672032193159,
      "grad_norm": 0.48326998949050903,
      "learning_rate": 0.00017246000603682464,
      "loss": 0.5292,
      "step": 6848
    },
    {
      "epoch": 13.780684104627767,
      "grad_norm": 0.4309985935688019,
      "learning_rate": 0.00017245598148707113,
      "loss": 0.4862,
      "step": 6849
    },
    {
      "epoch": 13.782696177062375,
      "grad_norm": 0.46242427825927734,
      "learning_rate": 0.00017245195693731764,
      "loss": 0.4712,
      "step": 6850
    },
    {
      "epoch": 13.784708249496981,
      "grad_norm": 0.44838765263557434,
      "learning_rate": 0.00017244793238756415,
      "loss": 0.4918,
      "step": 6851
    },
    {
      "epoch": 13.78672032193159,
      "grad_norm": 0.48447656631469727,
      "learning_rate": 0.00017244390783781067,
      "loss": 0.5233,
      "step": 6852
    },
    {
      "epoch": 13.788732394366198,
      "grad_norm": 0.45144328474998474,
      "learning_rate": 0.00017243988328805715,
      "loss": 0.4675,
      "step": 6853
    },
    {
      "epoch": 13.790744466800804,
      "grad_norm": 0.4602462351322174,
      "learning_rate": 0.00017243585873830366,
      "loss": 0.4867,
      "step": 6854
    },
    {
      "epoch": 13.792756539235413,
      "grad_norm": 0.4952567517757416,
      "learning_rate": 0.00017243183418855015,
      "loss": 0.4891,
      "step": 6855
    },
    {
      "epoch": 13.79476861167002,
      "grad_norm": 0.46602973341941833,
      "learning_rate": 0.0001724278096387967,
      "loss": 0.516,
      "step": 6856
    },
    {
      "epoch": 13.796780684104627,
      "grad_norm": 0.47521406412124634,
      "learning_rate": 0.00017242378508904317,
      "loss": 0.4912,
      "step": 6857
    },
    {
      "epoch": 13.798792756539235,
      "grad_norm": 0.4373890459537506,
      "learning_rate": 0.00017241976053928968,
      "loss": 0.5006,
      "step": 6858
    },
    {
      "epoch": 13.800804828973844,
      "grad_norm": 0.4423176050186157,
      "learning_rate": 0.00017241573598953617,
      "loss": 0.4779,
      "step": 6859
    },
    {
      "epoch": 13.80281690140845,
      "grad_norm": 0.4259648323059082,
      "learning_rate": 0.00017241171143978268,
      "loss": 0.4903,
      "step": 6860
    },
    {
      "epoch": 13.804828973843058,
      "grad_norm": 0.4543681740760803,
      "learning_rate": 0.0001724076868900292,
      "loss": 0.5019,
      "step": 6861
    },
    {
      "epoch": 13.806841046277667,
      "grad_norm": 0.46901699900627136,
      "learning_rate": 0.0001724036623402757,
      "loss": 0.4687,
      "step": 6862
    },
    {
      "epoch": 13.808853118712273,
      "grad_norm": 0.4734343886375427,
      "learning_rate": 0.0001723996377905222,
      "loss": 0.4752,
      "step": 6863
    },
    {
      "epoch": 13.810865191146881,
      "grad_norm": 0.45648089051246643,
      "learning_rate": 0.0001723956132407687,
      "loss": 0.5076,
      "step": 6864
    },
    {
      "epoch": 13.81287726358149,
      "grad_norm": 0.4780382812023163,
      "learning_rate": 0.0001723915886910152,
      "loss": 0.531,
      "step": 6865
    },
    {
      "epoch": 13.814889336016096,
      "grad_norm": 0.42764729261398315,
      "learning_rate": 0.0001723875641412617,
      "loss": 0.4894,
      "step": 6866
    },
    {
      "epoch": 13.816901408450704,
      "grad_norm": 0.45266595482826233,
      "learning_rate": 0.00017238353959150821,
      "loss": 0.4884,
      "step": 6867
    },
    {
      "epoch": 13.818913480885312,
      "grad_norm": 0.45434948801994324,
      "learning_rate": 0.00017237951504175473,
      "loss": 0.5275,
      "step": 6868
    },
    {
      "epoch": 13.82092555331992,
      "grad_norm": 0.46970781683921814,
      "learning_rate": 0.0001723754904920012,
      "loss": 0.4765,
      "step": 6869
    },
    {
      "epoch": 13.822937625754527,
      "grad_norm": 0.4572991132736206,
      "learning_rate": 0.00017237146594224772,
      "loss": 0.4456,
      "step": 6870
    },
    {
      "epoch": 13.824949698189135,
      "grad_norm": 0.4511297345161438,
      "learning_rate": 0.00017236744139249424,
      "loss": 0.473,
      "step": 6871
    },
    {
      "epoch": 13.826961770623743,
      "grad_norm": 0.46933382749557495,
      "learning_rate": 0.00017236341684274072,
      "loss": 0.5182,
      "step": 6872
    },
    {
      "epoch": 13.82897384305835,
      "grad_norm": 0.4448999762535095,
      "learning_rate": 0.00017235939229298723,
      "loss": 0.4769,
      "step": 6873
    },
    {
      "epoch": 13.830985915492958,
      "grad_norm": 0.45925435423851013,
      "learning_rate": 0.00017235536774323372,
      "loss": 0.5015,
      "step": 6874
    },
    {
      "epoch": 13.832997987927566,
      "grad_norm": 0.49521875381469727,
      "learning_rate": 0.00017235134319348023,
      "loss": 0.5138,
      "step": 6875
    },
    {
      "epoch": 13.835010060362173,
      "grad_norm": 0.45177263021469116,
      "learning_rate": 0.00017234731864372674,
      "loss": 0.4786,
      "step": 6876
    },
    {
      "epoch": 13.83702213279678,
      "grad_norm": 0.44672200083732605,
      "learning_rate": 0.00017234329409397325,
      "loss": 0.4645,
      "step": 6877
    },
    {
      "epoch": 13.839034205231389,
      "grad_norm": 0.43744584918022156,
      "learning_rate": 0.00017233926954421974,
      "loss": 0.5248,
      "step": 6878
    },
    {
      "epoch": 13.841046277665995,
      "grad_norm": 0.444835364818573,
      "learning_rate": 0.00017233524499446625,
      "loss": 0.4833,
      "step": 6879
    },
    {
      "epoch": 13.843058350100604,
      "grad_norm": 0.45810702443122864,
      "learning_rate": 0.00017233122044471274,
      "loss": 0.5101,
      "step": 6880
    },
    {
      "epoch": 13.845070422535212,
      "grad_norm": 0.4316956102848053,
      "learning_rate": 0.00017232719589495928,
      "loss": 0.4783,
      "step": 6881
    },
    {
      "epoch": 13.847082494969818,
      "grad_norm": 0.4780410826206207,
      "learning_rate": 0.00017232317134520576,
      "loss": 0.5132,
      "step": 6882
    },
    {
      "epoch": 13.849094567404427,
      "grad_norm": 0.4522661864757538,
      "learning_rate": 0.00017231914679545227,
      "loss": 0.5077,
      "step": 6883
    },
    {
      "epoch": 13.851106639839035,
      "grad_norm": 0.4754454791545868,
      "learning_rate": 0.00017231512224569876,
      "loss": 0.517,
      "step": 6884
    },
    {
      "epoch": 13.853118712273641,
      "grad_norm": 0.4517360031604767,
      "learning_rate": 0.00017231109769594527,
      "loss": 0.4757,
      "step": 6885
    },
    {
      "epoch": 13.85513078470825,
      "grad_norm": 0.4441979229450226,
      "learning_rate": 0.00017230707314619178,
      "loss": 0.4923,
      "step": 6886
    },
    {
      "epoch": 13.857142857142858,
      "grad_norm": 0.4272061288356781,
      "learning_rate": 0.0001723030485964383,
      "loss": 0.4831,
      "step": 6887
    },
    {
      "epoch": 13.859154929577464,
      "grad_norm": 0.4497365653514862,
      "learning_rate": 0.00017229902404668478,
      "loss": 0.5047,
      "step": 6888
    },
    {
      "epoch": 13.861167002012072,
      "grad_norm": 0.44475841522216797,
      "learning_rate": 0.0001722949994969313,
      "loss": 0.5043,
      "step": 6889
    },
    {
      "epoch": 13.86317907444668,
      "grad_norm": 0.48832741379737854,
      "learning_rate": 0.00017229097494717778,
      "loss": 0.5183,
      "step": 6890
    },
    {
      "epoch": 13.865191146881287,
      "grad_norm": 0.49886438250541687,
      "learning_rate": 0.00017228695039742432,
      "loss": 0.4882,
      "step": 6891
    },
    {
      "epoch": 13.867203219315895,
      "grad_norm": 0.43073445558547974,
      "learning_rate": 0.0001722829258476708,
      "loss": 0.4442,
      "step": 6892
    },
    {
      "epoch": 13.869215291750503,
      "grad_norm": 0.46007370948791504,
      "learning_rate": 0.00017227890129791731,
      "loss": 0.5024,
      "step": 6893
    },
    {
      "epoch": 13.87122736418511,
      "grad_norm": 0.4938929080963135,
      "learning_rate": 0.0001722748767481638,
      "loss": 0.5069,
      "step": 6894
    },
    {
      "epoch": 13.873239436619718,
      "grad_norm": 0.4704018533229828,
      "learning_rate": 0.0001722708521984103,
      "loss": 0.4771,
      "step": 6895
    },
    {
      "epoch": 13.875251509054326,
      "grad_norm": 0.4632374942302704,
      "learning_rate": 0.00017226682764865682,
      "loss": 0.5107,
      "step": 6896
    },
    {
      "epoch": 13.877263581488933,
      "grad_norm": 0.4589215815067291,
      "learning_rate": 0.00017226280309890334,
      "loss": 0.4984,
      "step": 6897
    },
    {
      "epoch": 13.879275653923541,
      "grad_norm": 0.4585888683795929,
      "learning_rate": 0.00017225877854914982,
      "loss": 0.4992,
      "step": 6898
    },
    {
      "epoch": 13.88128772635815,
      "grad_norm": 0.4708770215511322,
      "learning_rate": 0.00017225475399939633,
      "loss": 0.4893,
      "step": 6899
    },
    {
      "epoch": 13.883299798792757,
      "grad_norm": 0.44626420736312866,
      "learning_rate": 0.00017225072944964282,
      "loss": 0.5059,
      "step": 6900
    },
    {
      "epoch": 13.885311871227364,
      "grad_norm": 0.43874016404151917,
      "learning_rate": 0.00017224670489988933,
      "loss": 0.4819,
      "step": 6901
    },
    {
      "epoch": 13.887323943661972,
      "grad_norm": 0.4657047986984253,
      "learning_rate": 0.00017224268035013584,
      "loss": 0.4764,
      "step": 6902
    },
    {
      "epoch": 13.88933601609658,
      "grad_norm": 0.4547275900840759,
      "learning_rate": 0.00017223865580038236,
      "loss": 0.5051,
      "step": 6903
    },
    {
      "epoch": 13.891348088531187,
      "grad_norm": 0.4642075300216675,
      "learning_rate": 0.00017223463125062884,
      "loss": 0.4958,
      "step": 6904
    },
    {
      "epoch": 13.893360160965795,
      "grad_norm": 0.4543045461177826,
      "learning_rate": 0.00017223060670087535,
      "loss": 0.4739,
      "step": 6905
    },
    {
      "epoch": 13.895372233400403,
      "grad_norm": 0.4596186876296997,
      "learning_rate": 0.00017222658215112186,
      "loss": 0.4825,
      "step": 6906
    },
    {
      "epoch": 13.89738430583501,
      "grad_norm": 0.4656265377998352,
      "learning_rate": 0.00017222255760136835,
      "loss": 0.5126,
      "step": 6907
    },
    {
      "epoch": 13.899396378269618,
      "grad_norm": 0.4425930082798004,
      "learning_rate": 0.00017221853305161486,
      "loss": 0.4789,
      "step": 6908
    },
    {
      "epoch": 13.901408450704226,
      "grad_norm": 0.45853468775749207,
      "learning_rate": 0.00017221450850186135,
      "loss": 0.4761,
      "step": 6909
    },
    {
      "epoch": 13.903420523138832,
      "grad_norm": 0.5132831931114197,
      "learning_rate": 0.00017221048395210786,
      "loss": 0.4824,
      "step": 6910
    },
    {
      "epoch": 13.90543259557344,
      "grad_norm": 0.44560596346855164,
      "learning_rate": 0.00017220645940235437,
      "loss": 0.4839,
      "step": 6911
    },
    {
      "epoch": 13.907444668008049,
      "grad_norm": 0.48981356620788574,
      "learning_rate": 0.00017220243485260088,
      "loss": 0.5178,
      "step": 6912
    },
    {
      "epoch": 13.909456740442655,
      "grad_norm": 0.4711073338985443,
      "learning_rate": 0.00017219841030284737,
      "loss": 0.4852,
      "step": 6913
    },
    {
      "epoch": 13.911468812877263,
      "grad_norm": 0.4507506787776947,
      "learning_rate": 0.00017219438575309388,
      "loss": 0.4741,
      "step": 6914
    },
    {
      "epoch": 13.913480885311872,
      "grad_norm": 0.47177645564079285,
      "learning_rate": 0.00017219036120334037,
      "loss": 0.5166,
      "step": 6915
    },
    {
      "epoch": 13.915492957746478,
      "grad_norm": 0.45295703411102295,
      "learning_rate": 0.00017218633665358688,
      "loss": 0.4941,
      "step": 6916
    },
    {
      "epoch": 13.917505030181086,
      "grad_norm": 0.4378451406955719,
      "learning_rate": 0.0001721823121038334,
      "loss": 0.5252,
      "step": 6917
    },
    {
      "epoch": 13.919517102615695,
      "grad_norm": 0.42745712399482727,
      "learning_rate": 0.0001721782875540799,
      "loss": 0.4848,
      "step": 6918
    },
    {
      "epoch": 13.921529175050303,
      "grad_norm": 0.4495573341846466,
      "learning_rate": 0.0001721742630043264,
      "loss": 0.5074,
      "step": 6919
    },
    {
      "epoch": 13.92354124748491,
      "grad_norm": 0.4452798366546631,
      "learning_rate": 0.0001721702384545729,
      "loss": 0.5234,
      "step": 6920
    },
    {
      "epoch": 13.925553319919517,
      "grad_norm": 0.446237176656723,
      "learning_rate": 0.00017216621390481939,
      "loss": 0.4786,
      "step": 6921
    },
    {
      "epoch": 13.927565392354126,
      "grad_norm": 0.42716214060783386,
      "learning_rate": 0.00017216218935506592,
      "loss": 0.5006,
      "step": 6922
    },
    {
      "epoch": 13.929577464788732,
      "grad_norm": 0.4694240391254425,
      "learning_rate": 0.0001721581648053124,
      "loss": 0.5345,
      "step": 6923
    },
    {
      "epoch": 13.93158953722334,
      "grad_norm": 0.4813777804374695,
      "learning_rate": 0.00017215414025555892,
      "loss": 0.5266,
      "step": 6924
    },
    {
      "epoch": 13.933601609657948,
      "grad_norm": 0.4600837230682373,
      "learning_rate": 0.0001721501157058054,
      "loss": 0.4988,
      "step": 6925
    },
    {
      "epoch": 13.935613682092555,
      "grad_norm": 0.4665992259979248,
      "learning_rate": 0.00017214609115605192,
      "loss": 0.5206,
      "step": 6926
    },
    {
      "epoch": 13.937625754527163,
      "grad_norm": 0.46757209300994873,
      "learning_rate": 0.00017214206660629843,
      "loss": 0.5249,
      "step": 6927
    },
    {
      "epoch": 13.939637826961771,
      "grad_norm": 0.44357892870903015,
      "learning_rate": 0.00017213804205654494,
      "loss": 0.4921,
      "step": 6928
    },
    {
      "epoch": 13.941649899396378,
      "grad_norm": 0.4489964246749878,
      "learning_rate": 0.00017213401750679143,
      "loss": 0.5109,
      "step": 6929
    },
    {
      "epoch": 13.943661971830986,
      "grad_norm": 0.4506444036960602,
      "learning_rate": 0.00017212999295703794,
      "loss": 0.5449,
      "step": 6930
    },
    {
      "epoch": 13.945674044265594,
      "grad_norm": 0.43480315804481506,
      "learning_rate": 0.00017212596840728443,
      "loss": 0.4996,
      "step": 6931
    },
    {
      "epoch": 13.9476861167002,
      "grad_norm": 0.48701414465904236,
      "learning_rate": 0.00017212194385753097,
      "loss": 0.4487,
      "step": 6932
    },
    {
      "epoch": 13.949698189134809,
      "grad_norm": 0.47683003544807434,
      "learning_rate": 0.00017211791930777745,
      "loss": 0.4958,
      "step": 6933
    },
    {
      "epoch": 13.951710261569417,
      "grad_norm": 0.4786238670349121,
      "learning_rate": 0.00017211389475802396,
      "loss": 0.5183,
      "step": 6934
    },
    {
      "epoch": 13.953722334004024,
      "grad_norm": 0.4775027930736542,
      "learning_rate": 0.00017210987020827045,
      "loss": 0.4918,
      "step": 6935
    },
    {
      "epoch": 13.955734406438632,
      "grad_norm": 0.4442363679409027,
      "learning_rate": 0.00017210584565851696,
      "loss": 0.494,
      "step": 6936
    },
    {
      "epoch": 13.95774647887324,
      "grad_norm": 0.45103445649147034,
      "learning_rate": 0.00017210182110876347,
      "loss": 0.5131,
      "step": 6937
    },
    {
      "epoch": 13.959758551307846,
      "grad_norm": 0.4536769986152649,
      "learning_rate": 0.00017209779655900998,
      "loss": 0.4937,
      "step": 6938
    },
    {
      "epoch": 13.961770623742455,
      "grad_norm": 0.44202321767807007,
      "learning_rate": 0.00017209377200925647,
      "loss": 0.4694,
      "step": 6939
    },
    {
      "epoch": 13.963782696177063,
      "grad_norm": 0.42928236722946167,
      "learning_rate": 0.00017208974745950298,
      "loss": 0.5001,
      "step": 6940
    },
    {
      "epoch": 13.96579476861167,
      "grad_norm": 0.43755316734313965,
      "learning_rate": 0.00017208572290974947,
      "loss": 0.5251,
      "step": 6941
    },
    {
      "epoch": 13.967806841046277,
      "grad_norm": 0.4624420404434204,
      "learning_rate": 0.00017208169835999598,
      "loss": 0.4924,
      "step": 6942
    },
    {
      "epoch": 13.969818913480886,
      "grad_norm": 0.43190714716911316,
      "learning_rate": 0.0001720776738102425,
      "loss": 0.5085,
      "step": 6943
    },
    {
      "epoch": 13.971830985915492,
      "grad_norm": 0.4407663345336914,
      "learning_rate": 0.00017207364926048898,
      "loss": 0.503,
      "step": 6944
    },
    {
      "epoch": 13.9738430583501,
      "grad_norm": 0.46814239025115967,
      "learning_rate": 0.0001720696247107355,
      "loss": 0.5363,
      "step": 6945
    },
    {
      "epoch": 13.975855130784709,
      "grad_norm": 0.44519028067588806,
      "learning_rate": 0.000172065600160982,
      "loss": 0.4909,
      "step": 6946
    },
    {
      "epoch": 13.977867203219315,
      "grad_norm": 0.49310266971588135,
      "learning_rate": 0.0001720615756112285,
      "loss": 0.4949,
      "step": 6947
    },
    {
      "epoch": 13.979879275653923,
      "grad_norm": 0.4610134959220886,
      "learning_rate": 0.000172057551061475,
      "loss": 0.5474,
      "step": 6948
    },
    {
      "epoch": 13.981891348088531,
      "grad_norm": 0.48222804069519043,
      "learning_rate": 0.0001720535265117215,
      "loss": 0.4722,
      "step": 6949
    },
    {
      "epoch": 13.98390342052314,
      "grad_norm": 0.45623859763145447,
      "learning_rate": 0.000172049501961968,
      "loss": 0.4859,
      "step": 6950
    },
    {
      "epoch": 13.985915492957746,
      "grad_norm": 0.46450185775756836,
      "learning_rate": 0.0001720454774122145,
      "loss": 0.5138,
      "step": 6951
    },
    {
      "epoch": 13.987927565392354,
      "grad_norm": 0.44896334409713745,
      "learning_rate": 0.00017204145286246102,
      "loss": 0.4805,
      "step": 6952
    },
    {
      "epoch": 13.989939637826962,
      "grad_norm": 0.43393442034721375,
      "learning_rate": 0.00017203742831270753,
      "loss": 0.4859,
      "step": 6953
    },
    {
      "epoch": 13.991951710261569,
      "grad_norm": 0.46995460987091064,
      "learning_rate": 0.00017203340376295402,
      "loss": 0.5305,
      "step": 6954
    },
    {
      "epoch": 13.993963782696177,
      "grad_norm": 0.46845579147338867,
      "learning_rate": 0.00017202937921320053,
      "loss": 0.5194,
      "step": 6955
    },
    {
      "epoch": 13.995975855130785,
      "grad_norm": 0.45122694969177246,
      "learning_rate": 0.00017202535466344701,
      "loss": 0.5192,
      "step": 6956
    },
    {
      "epoch": 13.997987927565392,
      "grad_norm": 0.47004908323287964,
      "learning_rate": 0.00017202133011369355,
      "loss": 0.534,
      "step": 6957
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.4568425118923187,
      "learning_rate": 0.00017201730556394004,
      "loss": 0.5004,
      "step": 6958
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.814614474773407,
      "eval_runtime": 49.8332,
      "eval_samples_per_second": 19.906,
      "eval_steps_per_second": 2.488,
      "step": 6958
    },
    {
      "epoch": 14.002012072434608,
      "grad_norm": 0.41058894991874695,
      "learning_rate": 0.00017201328101418655,
      "loss": 0.4234,
      "step": 6959
    },
    {
      "epoch": 14.004024144869215,
      "grad_norm": 0.4140269458293915,
      "learning_rate": 0.00017200925646443304,
      "loss": 0.4362,
      "step": 6960
    },
    {
      "epoch": 14.006036217303823,
      "grad_norm": 0.4242187440395355,
      "learning_rate": 0.00017200523191467955,
      "loss": 0.4549,
      "step": 6961
    },
    {
      "epoch": 14.008048289738431,
      "grad_norm": 0.44683384895324707,
      "learning_rate": 0.00017200120736492606,
      "loss": 0.4389,
      "step": 6962
    },
    {
      "epoch": 14.010060362173038,
      "grad_norm": 0.4830301105976105,
      "learning_rate": 0.00017199718281517257,
      "loss": 0.4512,
      "step": 6963
    },
    {
      "epoch": 14.012072434607646,
      "grad_norm": 0.4941613972187042,
      "learning_rate": 0.00017199315826541906,
      "loss": 0.4327,
      "step": 6964
    },
    {
      "epoch": 14.014084507042254,
      "grad_norm": 0.46270766854286194,
      "learning_rate": 0.00017198913371566557,
      "loss": 0.4292,
      "step": 6965
    },
    {
      "epoch": 14.01609657947686,
      "grad_norm": 0.49395495653152466,
      "learning_rate": 0.00017198510916591206,
      "loss": 0.4542,
      "step": 6966
    },
    {
      "epoch": 14.018108651911469,
      "grad_norm": 0.4483734667301178,
      "learning_rate": 0.0001719810846161586,
      "loss": 0.4503,
      "step": 6967
    },
    {
      "epoch": 14.020120724346077,
      "grad_norm": 0.4954056441783905,
      "learning_rate": 0.00017197706006640508,
      "loss": 0.4791,
      "step": 6968
    },
    {
      "epoch": 14.022132796780683,
      "grad_norm": 0.433200865983963,
      "learning_rate": 0.0001719730355166516,
      "loss": 0.4389,
      "step": 6969
    },
    {
      "epoch": 14.024144869215291,
      "grad_norm": 0.4502614438533783,
      "learning_rate": 0.00017196901096689808,
      "loss": 0.445,
      "step": 6970
    },
    {
      "epoch": 14.0261569416499,
      "grad_norm": 0.45056387782096863,
      "learning_rate": 0.0001719649864171446,
      "loss": 0.453,
      "step": 6971
    },
    {
      "epoch": 14.028169014084508,
      "grad_norm": 0.479064404964447,
      "learning_rate": 0.0001719609618673911,
      "loss": 0.4542,
      "step": 6972
    },
    {
      "epoch": 14.030181086519114,
      "grad_norm": 0.49673375487327576,
      "learning_rate": 0.00017195693731763761,
      "loss": 0.3837,
      "step": 6973
    },
    {
      "epoch": 14.032193158953723,
      "grad_norm": 0.47246241569519043,
      "learning_rate": 0.0001719529127678841,
      "loss": 0.4656,
      "step": 6974
    },
    {
      "epoch": 14.03420523138833,
      "grad_norm": 0.4633725881576538,
      "learning_rate": 0.0001719488882181306,
      "loss": 0.4374,
      "step": 6975
    },
    {
      "epoch": 14.036217303822937,
      "grad_norm": 0.45324909687042236,
      "learning_rate": 0.0001719448636683771,
      "loss": 0.4624,
      "step": 6976
    },
    {
      "epoch": 14.038229376257545,
      "grad_norm": 0.47593456506729126,
      "learning_rate": 0.0001719408391186236,
      "loss": 0.4515,
      "step": 6977
    },
    {
      "epoch": 14.040241448692154,
      "grad_norm": 0.4488679766654968,
      "learning_rate": 0.00017193681456887012,
      "loss": 0.4208,
      "step": 6978
    },
    {
      "epoch": 14.04225352112676,
      "grad_norm": 0.4428252577781677,
      "learning_rate": 0.0001719327900191166,
      "loss": 0.4259,
      "step": 6979
    },
    {
      "epoch": 14.044265593561368,
      "grad_norm": 0.4418512284755707,
      "learning_rate": 0.00017192876546936312,
      "loss": 0.4346,
      "step": 6980
    },
    {
      "epoch": 14.046277665995976,
      "grad_norm": 0.47160816192626953,
      "learning_rate": 0.00017192474091960963,
      "loss": 0.45,
      "step": 6981
    },
    {
      "epoch": 14.048289738430583,
      "grad_norm": 0.4559469223022461,
      "learning_rate": 0.00017192071636985614,
      "loss": 0.3964,
      "step": 6982
    },
    {
      "epoch": 14.050301810865191,
      "grad_norm": 0.5124471187591553,
      "learning_rate": 0.00017191669182010263,
      "loss": 0.4146,
      "step": 6983
    },
    {
      "epoch": 14.0523138832998,
      "grad_norm": 0.4596616327762604,
      "learning_rate": 0.00017191266727034914,
      "loss": 0.4657,
      "step": 6984
    },
    {
      "epoch": 14.054325955734406,
      "grad_norm": 0.4541858434677124,
      "learning_rate": 0.00017190864272059563,
      "loss": 0.4464,
      "step": 6985
    },
    {
      "epoch": 14.056338028169014,
      "grad_norm": 0.4825682044029236,
      "learning_rate": 0.00017190461817084214,
      "loss": 0.4305,
      "step": 6986
    },
    {
      "epoch": 14.058350100603622,
      "grad_norm": 0.45492056012153625,
      "learning_rate": 0.00017190059362108865,
      "loss": 0.4403,
      "step": 6987
    },
    {
      "epoch": 14.060362173038229,
      "grad_norm": 0.4621187150478363,
      "learning_rate": 0.00017189656907133516,
      "loss": 0.4508,
      "step": 6988
    },
    {
      "epoch": 14.062374245472837,
      "grad_norm": 0.46491679549217224,
      "learning_rate": 0.00017189254452158165,
      "loss": 0.4318,
      "step": 6989
    },
    {
      "epoch": 14.064386317907445,
      "grad_norm": 0.489701509475708,
      "learning_rate": 0.00017188851997182816,
      "loss": 0.4259,
      "step": 6990
    },
    {
      "epoch": 14.066398390342052,
      "grad_norm": 0.4591967463493347,
      "learning_rate": 0.00017188449542207464,
      "loss": 0.4249,
      "step": 6991
    },
    {
      "epoch": 14.06841046277666,
      "grad_norm": 0.44949275255203247,
      "learning_rate": 0.00017188047087232118,
      "loss": 0.4308,
      "step": 6992
    },
    {
      "epoch": 14.070422535211268,
      "grad_norm": 0.47658464312553406,
      "learning_rate": 0.00017187644632256767,
      "loss": 0.4175,
      "step": 6993
    },
    {
      "epoch": 14.072434607645874,
      "grad_norm": 0.4568119943141937,
      "learning_rate": 0.00017187242177281418,
      "loss": 0.4229,
      "step": 6994
    },
    {
      "epoch": 14.074446680080483,
      "grad_norm": 0.46599292755126953,
      "learning_rate": 0.00017186839722306067,
      "loss": 0.4181,
      "step": 6995
    },
    {
      "epoch": 14.07645875251509,
      "grad_norm": 0.4557187557220459,
      "learning_rate": 0.00017186437267330718,
      "loss": 0.3931,
      "step": 6996
    },
    {
      "epoch": 14.078470824949699,
      "grad_norm": 0.48234251141548157,
      "learning_rate": 0.0001718603481235537,
      "loss": 0.4563,
      "step": 6997
    },
    {
      "epoch": 14.080482897384305,
      "grad_norm": 0.5099061727523804,
      "learning_rate": 0.0001718563235738002,
      "loss": 0.4601,
      "step": 6998
    },
    {
      "epoch": 14.082494969818914,
      "grad_norm": 0.4740048050880432,
      "learning_rate": 0.0001718522990240467,
      "loss": 0.4342,
      "step": 6999
    },
    {
      "epoch": 14.084507042253522,
      "grad_norm": 0.48460161685943604,
      "learning_rate": 0.0001718482744742932,
      "loss": 0.4592,
      "step": 7000
    },
    {
      "epoch": 14.086519114688128,
      "grad_norm": 0.4321780800819397,
      "learning_rate": 0.00017184424992453969,
      "loss": 0.403,
      "step": 7001
    },
    {
      "epoch": 14.088531187122737,
      "grad_norm": 0.44811898469924927,
      "learning_rate": 0.00017184022537478622,
      "loss": 0.3896,
      "step": 7002
    },
    {
      "epoch": 14.090543259557345,
      "grad_norm": 0.48739904165267944,
      "learning_rate": 0.0001718362008250327,
      "loss": 0.4474,
      "step": 7003
    },
    {
      "epoch": 14.092555331991951,
      "grad_norm": 0.45593783259391785,
      "learning_rate": 0.00017183217627527922,
      "loss": 0.4401,
      "step": 7004
    },
    {
      "epoch": 14.09456740442656,
      "grad_norm": 0.4738219082355499,
      "learning_rate": 0.0001718281517255257,
      "loss": 0.4684,
      "step": 7005
    },
    {
      "epoch": 14.096579476861168,
      "grad_norm": 0.49917590618133545,
      "learning_rate": 0.00017182412717577222,
      "loss": 0.4267,
      "step": 7006
    },
    {
      "epoch": 14.098591549295774,
      "grad_norm": 0.44557759165763855,
      "learning_rate": 0.00017182010262601873,
      "loss": 0.4379,
      "step": 7007
    },
    {
      "epoch": 14.100603621730382,
      "grad_norm": 0.4554020166397095,
      "learning_rate": 0.00017181607807626522,
      "loss": 0.4306,
      "step": 7008
    },
    {
      "epoch": 14.10261569416499,
      "grad_norm": 0.47190919518470764,
      "learning_rate": 0.00017181205352651173,
      "loss": 0.42,
      "step": 7009
    },
    {
      "epoch": 14.104627766599597,
      "grad_norm": 0.4855066239833832,
      "learning_rate": 0.00017180802897675824,
      "loss": 0.4359,
      "step": 7010
    },
    {
      "epoch": 14.106639839034205,
      "grad_norm": 0.46903347969055176,
      "learning_rate": 0.00017180400442700473,
      "loss": 0.4192,
      "step": 7011
    },
    {
      "epoch": 14.108651911468813,
      "grad_norm": 0.4412677586078644,
      "learning_rate": 0.00017179997987725124,
      "loss": 0.4439,
      "step": 7012
    },
    {
      "epoch": 14.11066398390342,
      "grad_norm": 0.47450917959213257,
      "learning_rate": 0.00017179595532749775,
      "loss": 0.4072,
      "step": 7013
    },
    {
      "epoch": 14.112676056338028,
      "grad_norm": 0.4535275101661682,
      "learning_rate": 0.00017179193077774424,
      "loss": 0.431,
      "step": 7014
    },
    {
      "epoch": 14.114688128772636,
      "grad_norm": 0.4786745011806488,
      "learning_rate": 0.00017178790622799075,
      "loss": 0.4151,
      "step": 7015
    },
    {
      "epoch": 14.116700201207243,
      "grad_norm": 0.4758010804653168,
      "learning_rate": 0.00017178388167823723,
      "loss": 0.4441,
      "step": 7016
    },
    {
      "epoch": 14.11871227364185,
      "grad_norm": 0.491130530834198,
      "learning_rate": 0.00017177985712848377,
      "loss": 0.4247,
      "step": 7017
    },
    {
      "epoch": 14.120724346076459,
      "grad_norm": 0.46456676721572876,
      "learning_rate": 0.00017177583257873026,
      "loss": 0.4202,
      "step": 7018
    },
    {
      "epoch": 14.122736418511066,
      "grad_norm": 0.4772298336029053,
      "learning_rate": 0.00017177180802897677,
      "loss": 0.4237,
      "step": 7019
    },
    {
      "epoch": 14.124748490945674,
      "grad_norm": 0.5293956995010376,
      "learning_rate": 0.00017176778347922325,
      "loss": 0.4543,
      "step": 7020
    },
    {
      "epoch": 14.126760563380282,
      "grad_norm": 0.4892985224723816,
      "learning_rate": 0.00017176375892946977,
      "loss": 0.4727,
      "step": 7021
    },
    {
      "epoch": 14.12877263581489,
      "grad_norm": 0.4574747681617737,
      "learning_rate": 0.00017175973437971628,
      "loss": 0.4527,
      "step": 7022
    },
    {
      "epoch": 14.130784708249497,
      "grad_norm": 0.4443148970603943,
      "learning_rate": 0.0001717557098299628,
      "loss": 0.4062,
      "step": 7023
    },
    {
      "epoch": 14.132796780684105,
      "grad_norm": 0.5284959673881531,
      "learning_rate": 0.00017175168528020928,
      "loss": 0.4678,
      "step": 7024
    },
    {
      "epoch": 14.134808853118713,
      "grad_norm": 0.4551987946033478,
      "learning_rate": 0.0001717476607304558,
      "loss": 0.4547,
      "step": 7025
    },
    {
      "epoch": 14.13682092555332,
      "grad_norm": 0.45121151208877563,
      "learning_rate": 0.00017174363618070227,
      "loss": 0.433,
      "step": 7026
    },
    {
      "epoch": 14.138832997987928,
      "grad_norm": 0.5101158618927002,
      "learning_rate": 0.0001717396116309488,
      "loss": 0.4493,
      "step": 7027
    },
    {
      "epoch": 14.140845070422536,
      "grad_norm": 0.4716704189777374,
      "learning_rate": 0.0001717355870811953,
      "loss": 0.4335,
      "step": 7028
    },
    {
      "epoch": 14.142857142857142,
      "grad_norm": 0.445200651884079,
      "learning_rate": 0.0001717315625314418,
      "loss": 0.4365,
      "step": 7029
    },
    {
      "epoch": 14.14486921529175,
      "grad_norm": 0.4787641763687134,
      "learning_rate": 0.0001717275379816883,
      "loss": 0.4791,
      "step": 7030
    },
    {
      "epoch": 14.146881287726359,
      "grad_norm": 0.4735274612903595,
      "learning_rate": 0.0001717235134319348,
      "loss": 0.4297,
      "step": 7031
    },
    {
      "epoch": 14.148893360160965,
      "grad_norm": 0.4702800214290619,
      "learning_rate": 0.00017171948888218132,
      "loss": 0.43,
      "step": 7032
    },
    {
      "epoch": 14.150905432595573,
      "grad_norm": 0.4723757803440094,
      "learning_rate": 0.00017171546433242783,
      "loss": 0.4819,
      "step": 7033
    },
    {
      "epoch": 14.152917505030182,
      "grad_norm": 0.45632466673851013,
      "learning_rate": 0.00017171143978267432,
      "loss": 0.4395,
      "step": 7034
    },
    {
      "epoch": 14.154929577464788,
      "grad_norm": 0.48770514130592346,
      "learning_rate": 0.00017170741523292083,
      "loss": 0.4663,
      "step": 7035
    },
    {
      "epoch": 14.156941649899396,
      "grad_norm": 0.4931354820728302,
      "learning_rate": 0.00017170339068316731,
      "loss": 0.4252,
      "step": 7036
    },
    {
      "epoch": 14.158953722334005,
      "grad_norm": 0.5029182434082031,
      "learning_rate": 0.00017169936613341385,
      "loss": 0.4231,
      "step": 7037
    },
    {
      "epoch": 14.160965794768611,
      "grad_norm": 0.4923095405101776,
      "learning_rate": 0.00017169534158366034,
      "loss": 0.4226,
      "step": 7038
    },
    {
      "epoch": 14.16297786720322,
      "grad_norm": 0.48942089080810547,
      "learning_rate": 0.00017169131703390685,
      "loss": 0.4339,
      "step": 7039
    },
    {
      "epoch": 14.164989939637827,
      "grad_norm": 0.4594174325466156,
      "learning_rate": 0.00017168729248415334,
      "loss": 0.4478,
      "step": 7040
    },
    {
      "epoch": 14.167002012072434,
      "grad_norm": 0.4805026352405548,
      "learning_rate": 0.00017168326793439985,
      "loss": 0.4567,
      "step": 7041
    },
    {
      "epoch": 14.169014084507042,
      "grad_norm": 0.47446689009666443,
      "learning_rate": 0.00017167924338464636,
      "loss": 0.4517,
      "step": 7042
    },
    {
      "epoch": 14.17102615694165,
      "grad_norm": 0.47916722297668457,
      "learning_rate": 0.00017167521883489285,
      "loss": 0.4403,
      "step": 7043
    },
    {
      "epoch": 14.173038229376257,
      "grad_norm": 0.4764266312122345,
      "learning_rate": 0.00017167119428513936,
      "loss": 0.438,
      "step": 7044
    },
    {
      "epoch": 14.175050301810865,
      "grad_norm": 0.49871402978897095,
      "learning_rate": 0.00017166716973538587,
      "loss": 0.4414,
      "step": 7045
    },
    {
      "epoch": 14.177062374245473,
      "grad_norm": 0.5029274821281433,
      "learning_rate": 0.00017166314518563236,
      "loss": 0.4513,
      "step": 7046
    },
    {
      "epoch": 14.179074446680081,
      "grad_norm": 0.4825816750526428,
      "learning_rate": 0.00017165912063587887,
      "loss": 0.441,
      "step": 7047
    },
    {
      "epoch": 14.181086519114688,
      "grad_norm": 0.5117731690406799,
      "learning_rate": 0.00017165509608612538,
      "loss": 0.4572,
      "step": 7048
    },
    {
      "epoch": 14.183098591549296,
      "grad_norm": 0.4869009852409363,
      "learning_rate": 0.00017165107153637187,
      "loss": 0.474,
      "step": 7049
    },
    {
      "epoch": 14.185110663983904,
      "grad_norm": 0.47868213057518005,
      "learning_rate": 0.00017164704698661838,
      "loss": 0.469,
      "step": 7050
    },
    {
      "epoch": 14.18712273641851,
      "grad_norm": 0.44464582204818726,
      "learning_rate": 0.00017164302243686486,
      "loss": 0.3954,
      "step": 7051
    },
    {
      "epoch": 14.189134808853119,
      "grad_norm": 0.45081108808517456,
      "learning_rate": 0.0001716389978871114,
      "loss": 0.404,
      "step": 7052
    },
    {
      "epoch": 14.191146881287727,
      "grad_norm": 0.46724778413772583,
      "learning_rate": 0.0001716349733373579,
      "loss": 0.4504,
      "step": 7053
    },
    {
      "epoch": 14.193158953722333,
      "grad_norm": 0.444134920835495,
      "learning_rate": 0.0001716309487876044,
      "loss": 0.4479,
      "step": 7054
    },
    {
      "epoch": 14.195171026156942,
      "grad_norm": 0.4865463674068451,
      "learning_rate": 0.00017162692423785088,
      "loss": 0.4485,
      "step": 7055
    },
    {
      "epoch": 14.19718309859155,
      "grad_norm": 0.5156015753746033,
      "learning_rate": 0.0001716228996880974,
      "loss": 0.4769,
      "step": 7056
    },
    {
      "epoch": 14.199195171026156,
      "grad_norm": 0.5230691432952881,
      "learning_rate": 0.0001716188751383439,
      "loss": 0.436,
      "step": 7057
    },
    {
      "epoch": 14.201207243460765,
      "grad_norm": 0.4853712320327759,
      "learning_rate": 0.00017161485058859042,
      "loss": 0.4398,
      "step": 7058
    },
    {
      "epoch": 14.203219315895373,
      "grad_norm": 0.46218013763427734,
      "learning_rate": 0.0001716108260388369,
      "loss": 0.4296,
      "step": 7059
    },
    {
      "epoch": 14.20523138832998,
      "grad_norm": 0.46590790152549744,
      "learning_rate": 0.00017160680148908342,
      "loss": 0.473,
      "step": 7060
    },
    {
      "epoch": 14.207243460764587,
      "grad_norm": 0.47379812598228455,
      "learning_rate": 0.0001716027769393299,
      "loss": 0.436,
      "step": 7061
    },
    {
      "epoch": 14.209255533199196,
      "grad_norm": 0.45649254322052,
      "learning_rate": 0.00017159875238957644,
      "loss": 0.4313,
      "step": 7062
    },
    {
      "epoch": 14.211267605633802,
      "grad_norm": 0.48908865451812744,
      "learning_rate": 0.00017159472783982293,
      "loss": 0.442,
      "step": 7063
    },
    {
      "epoch": 14.21327967806841,
      "grad_norm": 0.499803751707077,
      "learning_rate": 0.00017159070329006944,
      "loss": 0.4599,
      "step": 7064
    },
    {
      "epoch": 14.215291750503019,
      "grad_norm": 0.5030113458633423,
      "learning_rate": 0.00017158667874031592,
      "loss": 0.4558,
      "step": 7065
    },
    {
      "epoch": 14.217303822937625,
      "grad_norm": 0.5040146112442017,
      "learning_rate": 0.00017158265419056244,
      "loss": 0.4664,
      "step": 7066
    },
    {
      "epoch": 14.219315895372233,
      "grad_norm": 0.47168052196502686,
      "learning_rate": 0.00017157862964080895,
      "loss": 0.4568,
      "step": 7067
    },
    {
      "epoch": 14.221327967806841,
      "grad_norm": 0.4673472046852112,
      "learning_rate": 0.00017157460509105546,
      "loss": 0.4613,
      "step": 7068
    },
    {
      "epoch": 14.223340040241448,
      "grad_norm": 0.49871429800987244,
      "learning_rate": 0.00017157058054130195,
      "loss": 0.4495,
      "step": 7069
    },
    {
      "epoch": 14.225352112676056,
      "grad_norm": 0.49243706464767456,
      "learning_rate": 0.00017156655599154846,
      "loss": 0.4376,
      "step": 7070
    },
    {
      "epoch": 14.227364185110664,
      "grad_norm": 0.4713975191116333,
      "learning_rate": 0.00017156253144179494,
      "loss": 0.4928,
      "step": 7071
    },
    {
      "epoch": 14.229376257545272,
      "grad_norm": 0.5076934099197388,
      "learning_rate": 0.00017155850689204148,
      "loss": 0.4657,
      "step": 7072
    },
    {
      "epoch": 14.231388329979879,
      "grad_norm": 0.4886957108974457,
      "learning_rate": 0.00017155448234228797,
      "loss": 0.4729,
      "step": 7073
    },
    {
      "epoch": 14.233400402414487,
      "grad_norm": 0.48806312680244446,
      "learning_rate": 0.00017155045779253448,
      "loss": 0.4568,
      "step": 7074
    },
    {
      "epoch": 14.235412474849095,
      "grad_norm": 0.49895960092544556,
      "learning_rate": 0.00017154643324278097,
      "loss": 0.4477,
      "step": 7075
    },
    {
      "epoch": 14.237424547283702,
      "grad_norm": 0.4788588583469391,
      "learning_rate": 0.00017154240869302748,
      "loss": 0.4821,
      "step": 7076
    },
    {
      "epoch": 14.23943661971831,
      "grad_norm": 0.4782940447330475,
      "learning_rate": 0.000171538384143274,
      "loss": 0.4402,
      "step": 7077
    },
    {
      "epoch": 14.241448692152918,
      "grad_norm": 0.48674044013023376,
      "learning_rate": 0.00017153435959352048,
      "loss": 0.4527,
      "step": 7078
    },
    {
      "epoch": 14.243460764587525,
      "grad_norm": 0.4769466519355774,
      "learning_rate": 0.000171530335043767,
      "loss": 0.4512,
      "step": 7079
    },
    {
      "epoch": 14.245472837022133,
      "grad_norm": 0.4740608334541321,
      "learning_rate": 0.0001715263104940135,
      "loss": 0.4303,
      "step": 7080
    },
    {
      "epoch": 14.247484909456741,
      "grad_norm": 0.4659378230571747,
      "learning_rate": 0.00017152228594425998,
      "loss": 0.4026,
      "step": 7081
    },
    {
      "epoch": 14.249496981891348,
      "grad_norm": 0.5015613436698914,
      "learning_rate": 0.0001715182613945065,
      "loss": 0.4574,
      "step": 7082
    },
    {
      "epoch": 14.251509054325956,
      "grad_norm": 0.5070195198059082,
      "learning_rate": 0.000171514236844753,
      "loss": 0.4844,
      "step": 7083
    },
    {
      "epoch": 14.253521126760564,
      "grad_norm": 0.46171438694000244,
      "learning_rate": 0.0001715102122949995,
      "loss": 0.4703,
      "step": 7084
    },
    {
      "epoch": 14.25553319919517,
      "grad_norm": 0.5041384100914001,
      "learning_rate": 0.000171506187745246,
      "loss": 0.4592,
      "step": 7085
    },
    {
      "epoch": 14.257545271629779,
      "grad_norm": 0.4867444634437561,
      "learning_rate": 0.0001715021631954925,
      "loss": 0.4546,
      "step": 7086
    },
    {
      "epoch": 14.259557344064387,
      "grad_norm": 0.48328927159309387,
      "learning_rate": 0.00017149813864573903,
      "loss": 0.4298,
      "step": 7087
    },
    {
      "epoch": 14.261569416498993,
      "grad_norm": 0.4732760190963745,
      "learning_rate": 0.00017149411409598552,
      "loss": 0.4439,
      "step": 7088
    },
    {
      "epoch": 14.263581488933601,
      "grad_norm": 0.4958234131336212,
      "learning_rate": 0.00017149008954623203,
      "loss": 0.4499,
      "step": 7089
    },
    {
      "epoch": 14.26559356136821,
      "grad_norm": 0.46086180210113525,
      "learning_rate": 0.0001714860649964785,
      "loss": 0.4259,
      "step": 7090
    },
    {
      "epoch": 14.267605633802816,
      "grad_norm": 0.48089414834976196,
      "learning_rate": 0.00017148204044672503,
      "loss": 0.4367,
      "step": 7091
    },
    {
      "epoch": 14.269617706237424,
      "grad_norm": 0.496818482875824,
      "learning_rate": 0.00017147801589697154,
      "loss": 0.4392,
      "step": 7092
    },
    {
      "epoch": 14.271629778672033,
      "grad_norm": 0.48565706610679626,
      "learning_rate": 0.00017147399134721805,
      "loss": 0.4571,
      "step": 7093
    },
    {
      "epoch": 14.273641851106639,
      "grad_norm": 0.480925053358078,
      "learning_rate": 0.00017146996679746454,
      "loss": 0.4682,
      "step": 7094
    },
    {
      "epoch": 14.275653923541247,
      "grad_norm": 0.4638083577156067,
      "learning_rate": 0.00017146594224771105,
      "loss": 0.4372,
      "step": 7095
    },
    {
      "epoch": 14.277665995975855,
      "grad_norm": 0.4732370972633362,
      "learning_rate": 0.00017146191769795753,
      "loss": 0.4598,
      "step": 7096
    },
    {
      "epoch": 14.279678068410464,
      "grad_norm": 0.4984577000141144,
      "learning_rate": 0.00017145789314820407,
      "loss": 0.4416,
      "step": 7097
    },
    {
      "epoch": 14.28169014084507,
      "grad_norm": 0.49489593505859375,
      "learning_rate": 0.00017145386859845056,
      "loss": 0.4723,
      "step": 7098
    },
    {
      "epoch": 14.283702213279678,
      "grad_norm": 0.48104894161224365,
      "learning_rate": 0.00017144984404869707,
      "loss": 0.4816,
      "step": 7099
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 0.49589693546295166,
      "learning_rate": 0.00017144581949894355,
      "loss": 0.428,
      "step": 7100
    },
    {
      "epoch": 14.287726358148893,
      "grad_norm": 0.5214565396308899,
      "learning_rate": 0.00017144179494919007,
      "loss": 0.4352,
      "step": 7101
    },
    {
      "epoch": 14.289738430583501,
      "grad_norm": 0.5134240984916687,
      "learning_rate": 0.00017143777039943658,
      "loss": 0.4608,
      "step": 7102
    },
    {
      "epoch": 14.29175050301811,
      "grad_norm": 0.47657912969589233,
      "learning_rate": 0.0001714337458496831,
      "loss": 0.44,
      "step": 7103
    },
    {
      "epoch": 14.293762575452716,
      "grad_norm": 0.47614338994026184,
      "learning_rate": 0.00017142972129992958,
      "loss": 0.4529,
      "step": 7104
    },
    {
      "epoch": 14.295774647887324,
      "grad_norm": 0.5003198385238647,
      "learning_rate": 0.0001714256967501761,
      "loss": 0.4648,
      "step": 7105
    },
    {
      "epoch": 14.297786720321932,
      "grad_norm": 0.5069484114646912,
      "learning_rate": 0.00017142167220042257,
      "loss": 0.459,
      "step": 7106
    },
    {
      "epoch": 14.299798792756539,
      "grad_norm": 0.46971046924591064,
      "learning_rate": 0.0001714176476506691,
      "loss": 0.4633,
      "step": 7107
    },
    {
      "epoch": 14.301810865191147,
      "grad_norm": 0.49896448850631714,
      "learning_rate": 0.0001714136231009156,
      "loss": 0.4453,
      "step": 7108
    },
    {
      "epoch": 14.303822937625755,
      "grad_norm": 0.4872159957885742,
      "learning_rate": 0.0001714095985511621,
      "loss": 0.4511,
      "step": 7109
    },
    {
      "epoch": 14.305835010060362,
      "grad_norm": 0.49316659569740295,
      "learning_rate": 0.0001714055740014086,
      "loss": 0.4364,
      "step": 7110
    },
    {
      "epoch": 14.30784708249497,
      "grad_norm": 0.5046411156654358,
      "learning_rate": 0.0001714015494516551,
      "loss": 0.4438,
      "step": 7111
    },
    {
      "epoch": 14.309859154929578,
      "grad_norm": 0.4906505048274994,
      "learning_rate": 0.00017139752490190162,
      "loss": 0.4488,
      "step": 7112
    },
    {
      "epoch": 14.311871227364184,
      "grad_norm": 0.5200911164283752,
      "learning_rate": 0.0001713935003521481,
      "loss": 0.4674,
      "step": 7113
    },
    {
      "epoch": 14.313883299798793,
      "grad_norm": 0.496277391910553,
      "learning_rate": 0.00017138947580239462,
      "loss": 0.5002,
      "step": 7114
    },
    {
      "epoch": 14.3158953722334,
      "grad_norm": 0.49605023860931396,
      "learning_rate": 0.00017138545125264113,
      "loss": 0.4554,
      "step": 7115
    },
    {
      "epoch": 14.317907444668007,
      "grad_norm": 0.4758409857749939,
      "learning_rate": 0.00017138142670288761,
      "loss": 0.4491,
      "step": 7116
    },
    {
      "epoch": 14.319919517102615,
      "grad_norm": 0.4624132215976715,
      "learning_rate": 0.00017137740215313413,
      "loss": 0.4485,
      "step": 7117
    },
    {
      "epoch": 14.321931589537224,
      "grad_norm": 0.4731186330318451,
      "learning_rate": 0.00017137337760338064,
      "loss": 0.4738,
      "step": 7118
    },
    {
      "epoch": 14.323943661971832,
      "grad_norm": 0.47274893522262573,
      "learning_rate": 0.00017136935305362712,
      "loss": 0.4497,
      "step": 7119
    },
    {
      "epoch": 14.325955734406438,
      "grad_norm": 0.4810912311077118,
      "learning_rate": 0.00017136532850387364,
      "loss": 0.4776,
      "step": 7120
    },
    {
      "epoch": 14.327967806841047,
      "grad_norm": 0.5221952795982361,
      "learning_rate": 0.00017136130395412012,
      "loss": 0.4497,
      "step": 7121
    },
    {
      "epoch": 14.329979879275655,
      "grad_norm": 0.5068750381469727,
      "learning_rate": 0.00017135727940436666,
      "loss": 0.4838,
      "step": 7122
    },
    {
      "epoch": 14.331991951710261,
      "grad_norm": 0.48610401153564453,
      "learning_rate": 0.00017135325485461315,
      "loss": 0.4475,
      "step": 7123
    },
    {
      "epoch": 14.33400402414487,
      "grad_norm": 0.5143146514892578,
      "learning_rate": 0.00017134923030485966,
      "loss": 0.4693,
      "step": 7124
    },
    {
      "epoch": 14.336016096579478,
      "grad_norm": 0.4874024987220764,
      "learning_rate": 0.00017134520575510614,
      "loss": 0.4588,
      "step": 7125
    },
    {
      "epoch": 14.338028169014084,
      "grad_norm": 0.46684834361076355,
      "learning_rate": 0.00017134118120535266,
      "loss": 0.443,
      "step": 7126
    },
    {
      "epoch": 14.340040241448692,
      "grad_norm": 0.4374563992023468,
      "learning_rate": 0.00017133715665559917,
      "loss": 0.4411,
      "step": 7127
    },
    {
      "epoch": 14.3420523138833,
      "grad_norm": 0.493742436170578,
      "learning_rate": 0.00017133313210584568,
      "loss": 0.4869,
      "step": 7128
    },
    {
      "epoch": 14.344064386317907,
      "grad_norm": 0.464876651763916,
      "learning_rate": 0.00017132910755609216,
      "loss": 0.4508,
      "step": 7129
    },
    {
      "epoch": 14.346076458752515,
      "grad_norm": 0.4721657335758209,
      "learning_rate": 0.00017132508300633868,
      "loss": 0.4305,
      "step": 7130
    },
    {
      "epoch": 14.348088531187123,
      "grad_norm": 0.49544474482536316,
      "learning_rate": 0.00017132105845658516,
      "loss": 0.4694,
      "step": 7131
    },
    {
      "epoch": 14.35010060362173,
      "grad_norm": 0.4662492573261261,
      "learning_rate": 0.0001713170339068317,
      "loss": 0.4371,
      "step": 7132
    },
    {
      "epoch": 14.352112676056338,
      "grad_norm": 0.5434018969535828,
      "learning_rate": 0.0001713130093570782,
      "loss": 0.4478,
      "step": 7133
    },
    {
      "epoch": 14.354124748490946,
      "grad_norm": 0.48347172141075134,
      "learning_rate": 0.0001713089848073247,
      "loss": 0.4621,
      "step": 7134
    },
    {
      "epoch": 14.356136820925553,
      "grad_norm": 0.5044914484024048,
      "learning_rate": 0.00017130496025757118,
      "loss": 0.4628,
      "step": 7135
    },
    {
      "epoch": 14.35814889336016,
      "grad_norm": 0.4824117422103882,
      "learning_rate": 0.0001713009357078177,
      "loss": 0.4539,
      "step": 7136
    },
    {
      "epoch": 14.360160965794769,
      "grad_norm": 0.500163733959198,
      "learning_rate": 0.0001712969111580642,
      "loss": 0.4434,
      "step": 7137
    },
    {
      "epoch": 14.362173038229376,
      "grad_norm": 0.46888467669487,
      "learning_rate": 0.00017129288660831072,
      "loss": 0.4456,
      "step": 7138
    },
    {
      "epoch": 14.364185110663984,
      "grad_norm": 0.4617874026298523,
      "learning_rate": 0.0001712888620585572,
      "loss": 0.43,
      "step": 7139
    },
    {
      "epoch": 14.366197183098592,
      "grad_norm": 0.48204830288887024,
      "learning_rate": 0.00017128483750880372,
      "loss": 0.4574,
      "step": 7140
    },
    {
      "epoch": 14.368209255533198,
      "grad_norm": 0.503021776676178,
      "learning_rate": 0.0001712808129590502,
      "loss": 0.4877,
      "step": 7141
    },
    {
      "epoch": 14.370221327967807,
      "grad_norm": 0.5138957500457764,
      "learning_rate": 0.00017127678840929674,
      "loss": 0.4342,
      "step": 7142
    },
    {
      "epoch": 14.372233400402415,
      "grad_norm": 0.49317866563796997,
      "learning_rate": 0.00017127276385954323,
      "loss": 0.4988,
      "step": 7143
    },
    {
      "epoch": 14.374245472837021,
      "grad_norm": 0.48466721177101135,
      "learning_rate": 0.00017126873930978974,
      "loss": 0.4522,
      "step": 7144
    },
    {
      "epoch": 14.37625754527163,
      "grad_norm": 0.4763643443584442,
      "learning_rate": 0.00017126471476003622,
      "loss": 0.484,
      "step": 7145
    },
    {
      "epoch": 14.378269617706238,
      "grad_norm": 0.46381163597106934,
      "learning_rate": 0.00017126069021028274,
      "loss": 0.4267,
      "step": 7146
    },
    {
      "epoch": 14.380281690140846,
      "grad_norm": 0.5242968201637268,
      "learning_rate": 0.00017125666566052925,
      "loss": 0.4955,
      "step": 7147
    },
    {
      "epoch": 14.382293762575452,
      "grad_norm": 0.4775843322277069,
      "learning_rate": 0.00017125264111077573,
      "loss": 0.4657,
      "step": 7148
    },
    {
      "epoch": 14.38430583501006,
      "grad_norm": 0.4660336673259735,
      "learning_rate": 0.00017124861656102225,
      "loss": 0.4488,
      "step": 7149
    },
    {
      "epoch": 14.386317907444669,
      "grad_norm": 0.47835689783096313,
      "learning_rate": 0.00017124459201126876,
      "loss": 0.4186,
      "step": 7150
    },
    {
      "epoch": 14.388329979879275,
      "grad_norm": 0.4760496914386749,
      "learning_rate": 0.00017124056746151524,
      "loss": 0.4392,
      "step": 7151
    },
    {
      "epoch": 14.390342052313883,
      "grad_norm": 0.484750896692276,
      "learning_rate": 0.00017123654291176176,
      "loss": 0.4583,
      "step": 7152
    },
    {
      "epoch": 14.392354124748492,
      "grad_norm": 0.49008435010910034,
      "learning_rate": 0.00017123251836200827,
      "loss": 0.4673,
      "step": 7153
    },
    {
      "epoch": 14.394366197183098,
      "grad_norm": 0.47985899448394775,
      "learning_rate": 0.00017122849381225475,
      "loss": 0.4451,
      "step": 7154
    },
    {
      "epoch": 14.396378269617706,
      "grad_norm": 0.48496222496032715,
      "learning_rate": 0.00017122446926250127,
      "loss": 0.4419,
      "step": 7155
    },
    {
      "epoch": 14.398390342052314,
      "grad_norm": 0.5095177888870239,
      "learning_rate": 0.00017122044471274775,
      "loss": 0.4905,
      "step": 7156
    },
    {
      "epoch": 14.400402414486921,
      "grad_norm": 0.5155887603759766,
      "learning_rate": 0.00017121642016299426,
      "loss": 0.4537,
      "step": 7157
    },
    {
      "epoch": 14.40241448692153,
      "grad_norm": 0.4617890417575836,
      "learning_rate": 0.00017121239561324078,
      "loss": 0.4327,
      "step": 7158
    },
    {
      "epoch": 14.404426559356137,
      "grad_norm": 0.4648149907588959,
      "learning_rate": 0.0001712083710634873,
      "loss": 0.458,
      "step": 7159
    },
    {
      "epoch": 14.406438631790744,
      "grad_norm": 0.4735446572303772,
      "learning_rate": 0.00017120434651373377,
      "loss": 0.4587,
      "step": 7160
    },
    {
      "epoch": 14.408450704225352,
      "grad_norm": 0.4594191014766693,
      "learning_rate": 0.00017120032196398028,
      "loss": 0.4266,
      "step": 7161
    },
    {
      "epoch": 14.41046277665996,
      "grad_norm": 0.483845055103302,
      "learning_rate": 0.00017119629741422677,
      "loss": 0.494,
      "step": 7162
    },
    {
      "epoch": 14.412474849094567,
      "grad_norm": 0.4626694619655609,
      "learning_rate": 0.0001711922728644733,
      "loss": 0.4656,
      "step": 7163
    },
    {
      "epoch": 14.414486921529175,
      "grad_norm": 0.4843624234199524,
      "learning_rate": 0.0001711882483147198,
      "loss": 0.4801,
      "step": 7164
    },
    {
      "epoch": 14.416498993963783,
      "grad_norm": 0.44458845257759094,
      "learning_rate": 0.0001711842237649663,
      "loss": 0.3905,
      "step": 7165
    },
    {
      "epoch": 14.41851106639839,
      "grad_norm": 0.46325260400772095,
      "learning_rate": 0.0001711801992152128,
      "loss": 0.4771,
      "step": 7166
    },
    {
      "epoch": 14.420523138832998,
      "grad_norm": 0.4649773836135864,
      "learning_rate": 0.0001711761746654593,
      "loss": 0.4534,
      "step": 7167
    },
    {
      "epoch": 14.422535211267606,
      "grad_norm": 0.47740328311920166,
      "learning_rate": 0.00017117215011570582,
      "loss": 0.4491,
      "step": 7168
    },
    {
      "epoch": 14.424547283702214,
      "grad_norm": 0.4765014052391052,
      "learning_rate": 0.00017116812556595233,
      "loss": 0.4629,
      "step": 7169
    },
    {
      "epoch": 14.42655935613682,
      "grad_norm": 0.47654658555984497,
      "learning_rate": 0.0001711641010161988,
      "loss": 0.4599,
      "step": 7170
    },
    {
      "epoch": 14.428571428571429,
      "grad_norm": 0.46815431118011475,
      "learning_rate": 0.00017116007646644533,
      "loss": 0.4465,
      "step": 7171
    },
    {
      "epoch": 14.430583501006037,
      "grad_norm": 0.4788914620876312,
      "learning_rate": 0.0001711560519166918,
      "loss": 0.4741,
      "step": 7172
    },
    {
      "epoch": 14.432595573440643,
      "grad_norm": 0.5074394345283508,
      "learning_rate": 0.00017115202736693835,
      "loss": 0.4804,
      "step": 7173
    },
    {
      "epoch": 14.434607645875252,
      "grad_norm": 0.460817813873291,
      "learning_rate": 0.00017114800281718484,
      "loss": 0.4458,
      "step": 7174
    },
    {
      "epoch": 14.43661971830986,
      "grad_norm": 0.5000152587890625,
      "learning_rate": 0.00017114397826743135,
      "loss": 0.4814,
      "step": 7175
    },
    {
      "epoch": 14.438631790744466,
      "grad_norm": 0.4649430215358734,
      "learning_rate": 0.00017113995371767783,
      "loss": 0.4285,
      "step": 7176
    },
    {
      "epoch": 14.440643863179075,
      "grad_norm": 0.47289034724235535,
      "learning_rate": 0.00017113592916792434,
      "loss": 0.4772,
      "step": 7177
    },
    {
      "epoch": 14.442655935613683,
      "grad_norm": 0.4878728687763214,
      "learning_rate": 0.00017113190461817086,
      "loss": 0.4763,
      "step": 7178
    },
    {
      "epoch": 14.44466800804829,
      "grad_norm": 0.47223198413848877,
      "learning_rate": 0.00017112788006841737,
      "loss": 0.4652,
      "step": 7179
    },
    {
      "epoch": 14.446680080482897,
      "grad_norm": 0.48389649391174316,
      "learning_rate": 0.00017112385551866385,
      "loss": 0.4565,
      "step": 7180
    },
    {
      "epoch": 14.448692152917506,
      "grad_norm": 0.502816379070282,
      "learning_rate": 0.00017111983096891037,
      "loss": 0.4653,
      "step": 7181
    },
    {
      "epoch": 14.450704225352112,
      "grad_norm": 0.453128844499588,
      "learning_rate": 0.00017111580641915685,
      "loss": 0.4374,
      "step": 7182
    },
    {
      "epoch": 14.45271629778672,
      "grad_norm": 0.47602608799934387,
      "learning_rate": 0.00017111178186940336,
      "loss": 0.4449,
      "step": 7183
    },
    {
      "epoch": 14.454728370221329,
      "grad_norm": 0.4769562780857086,
      "learning_rate": 0.00017110775731964988,
      "loss": 0.4589,
      "step": 7184
    },
    {
      "epoch": 14.456740442655935,
      "grad_norm": 0.5001466870307922,
      "learning_rate": 0.00017110373276989636,
      "loss": 0.4559,
      "step": 7185
    },
    {
      "epoch": 14.458752515090543,
      "grad_norm": 0.47849491238594055,
      "learning_rate": 0.00017109970822014287,
      "loss": 0.4429,
      "step": 7186
    },
    {
      "epoch": 14.460764587525151,
      "grad_norm": 0.5091291069984436,
      "learning_rate": 0.00017109568367038939,
      "loss": 0.4712,
      "step": 7187
    },
    {
      "epoch": 14.462776659959758,
      "grad_norm": 0.453914612531662,
      "learning_rate": 0.0001710916591206359,
      "loss": 0.4447,
      "step": 7188
    },
    {
      "epoch": 14.464788732394366,
      "grad_norm": 0.46447309851646423,
      "learning_rate": 0.00017108763457088238,
      "loss": 0.46,
      "step": 7189
    },
    {
      "epoch": 14.466800804828974,
      "grad_norm": 0.4638996720314026,
      "learning_rate": 0.0001710836100211289,
      "loss": 0.4779,
      "step": 7190
    },
    {
      "epoch": 14.46881287726358,
      "grad_norm": 0.4812696874141693,
      "learning_rate": 0.00017107958547137538,
      "loss": 0.4496,
      "step": 7191
    },
    {
      "epoch": 14.470824949698189,
      "grad_norm": 0.5100499391555786,
      "learning_rate": 0.0001710755609216219,
      "loss": 0.4581,
      "step": 7192
    },
    {
      "epoch": 14.472837022132797,
      "grad_norm": 0.5067979097366333,
      "learning_rate": 0.0001710715363718684,
      "loss": 0.4624,
      "step": 7193
    },
    {
      "epoch": 14.474849094567404,
      "grad_norm": 0.5440659523010254,
      "learning_rate": 0.00017106751182211492,
      "loss": 0.488,
      "step": 7194
    },
    {
      "epoch": 14.476861167002012,
      "grad_norm": 0.49921515583992004,
      "learning_rate": 0.0001710634872723614,
      "loss": 0.4846,
      "step": 7195
    },
    {
      "epoch": 14.47887323943662,
      "grad_norm": 0.5188997983932495,
      "learning_rate": 0.00017105946272260791,
      "loss": 0.4801,
      "step": 7196
    },
    {
      "epoch": 14.480885311871228,
      "grad_norm": 0.49701830744743347,
      "learning_rate": 0.0001710554381728544,
      "loss": 0.488,
      "step": 7197
    },
    {
      "epoch": 14.482897384305835,
      "grad_norm": 0.4806509017944336,
      "learning_rate": 0.00017105141362310094,
      "loss": 0.4379,
      "step": 7198
    },
    {
      "epoch": 14.484909456740443,
      "grad_norm": 0.4773486852645874,
      "learning_rate": 0.00017104738907334742,
      "loss": 0.4729,
      "step": 7199
    },
    {
      "epoch": 14.486921529175051,
      "grad_norm": 0.46389731764793396,
      "learning_rate": 0.00017104336452359394,
      "loss": 0.4571,
      "step": 7200
    },
    {
      "epoch": 14.488933601609657,
      "grad_norm": 0.4816637337207794,
      "learning_rate": 0.00017103933997384042,
      "loss": 0.4831,
      "step": 7201
    },
    {
      "epoch": 14.490945674044266,
      "grad_norm": 0.500338613986969,
      "learning_rate": 0.00017103531542408693,
      "loss": 0.4854,
      "step": 7202
    },
    {
      "epoch": 14.492957746478874,
      "grad_norm": 0.46272149682044983,
      "learning_rate": 0.00017103129087433345,
      "loss": 0.4668,
      "step": 7203
    },
    {
      "epoch": 14.49496981891348,
      "grad_norm": 0.5010197758674622,
      "learning_rate": 0.00017102726632457996,
      "loss": 0.4779,
      "step": 7204
    },
    {
      "epoch": 14.496981891348089,
      "grad_norm": 0.49669963121414185,
      "learning_rate": 0.00017102324177482644,
      "loss": 0.4727,
      "step": 7205
    },
    {
      "epoch": 14.498993963782697,
      "grad_norm": 0.4725489020347595,
      "learning_rate": 0.00017101921722507295,
      "loss": 0.5022,
      "step": 7206
    },
    {
      "epoch": 14.501006036217303,
      "grad_norm": 0.4750453233718872,
      "learning_rate": 0.00017101519267531944,
      "loss": 0.4428,
      "step": 7207
    },
    {
      "epoch": 14.503018108651911,
      "grad_norm": 0.4746827483177185,
      "learning_rate": 0.00017101116812556598,
      "loss": 0.4793,
      "step": 7208
    },
    {
      "epoch": 14.50503018108652,
      "grad_norm": 0.4848077893257141,
      "learning_rate": 0.00017100714357581246,
      "loss": 0.4681,
      "step": 7209
    },
    {
      "epoch": 14.507042253521126,
      "grad_norm": 0.4776189625263214,
      "learning_rate": 0.00017100311902605898,
      "loss": 0.4687,
      "step": 7210
    },
    {
      "epoch": 14.509054325955734,
      "grad_norm": 0.47003722190856934,
      "learning_rate": 0.00017099909447630546,
      "loss": 0.4569,
      "step": 7211
    },
    {
      "epoch": 14.511066398390343,
      "grad_norm": 0.5088472962379456,
      "learning_rate": 0.00017099506992655197,
      "loss": 0.4394,
      "step": 7212
    },
    {
      "epoch": 14.513078470824949,
      "grad_norm": 0.4561311900615692,
      "learning_rate": 0.00017099104537679849,
      "loss": 0.4693,
      "step": 7213
    },
    {
      "epoch": 14.515090543259557,
      "grad_norm": 0.47542306780815125,
      "learning_rate": 0.000170987020827045,
      "loss": 0.4602,
      "step": 7214
    },
    {
      "epoch": 14.517102615694165,
      "grad_norm": 0.48230278491973877,
      "learning_rate": 0.00017098299627729148,
      "loss": 0.4585,
      "step": 7215
    },
    {
      "epoch": 14.519114688128772,
      "grad_norm": 0.4800373911857605,
      "learning_rate": 0.000170978971727538,
      "loss": 0.4577,
      "step": 7216
    },
    {
      "epoch": 14.52112676056338,
      "grad_norm": 0.5044830441474915,
      "learning_rate": 0.00017097494717778448,
      "loss": 0.4974,
      "step": 7217
    },
    {
      "epoch": 14.523138832997988,
      "grad_norm": 0.5066969394683838,
      "learning_rate": 0.000170970922628031,
      "loss": 0.5221,
      "step": 7218
    },
    {
      "epoch": 14.525150905432596,
      "grad_norm": 0.4922323524951935,
      "learning_rate": 0.0001709668980782775,
      "loss": 0.4639,
      "step": 7219
    },
    {
      "epoch": 14.527162977867203,
      "grad_norm": 0.5158060789108276,
      "learning_rate": 0.000170962873528524,
      "loss": 0.4922,
      "step": 7220
    },
    {
      "epoch": 14.529175050301811,
      "grad_norm": 0.4816388487815857,
      "learning_rate": 0.0001709588489787705,
      "loss": 0.4925,
      "step": 7221
    },
    {
      "epoch": 14.53118712273642,
      "grad_norm": 0.49098390340805054,
      "learning_rate": 0.00017095482442901701,
      "loss": 0.4773,
      "step": 7222
    },
    {
      "epoch": 14.533199195171026,
      "grad_norm": 0.4912322163581848,
      "learning_rate": 0.00017095079987926353,
      "loss": 0.4879,
      "step": 7223
    },
    {
      "epoch": 14.535211267605634,
      "grad_norm": 0.47908803820610046,
      "learning_rate": 0.00017094677532951,
      "loss": 0.4109,
      "step": 7224
    },
    {
      "epoch": 14.537223340040242,
      "grad_norm": 0.47254636883735657,
      "learning_rate": 0.00017094275077975652,
      "loss": 0.4626,
      "step": 7225
    },
    {
      "epoch": 14.539235412474849,
      "grad_norm": 0.47173136472702026,
      "learning_rate": 0.000170938726230003,
      "loss": 0.4607,
      "step": 7226
    },
    {
      "epoch": 14.541247484909457,
      "grad_norm": 0.47018247842788696,
      "learning_rate": 0.00017093470168024952,
      "loss": 0.4381,
      "step": 7227
    },
    {
      "epoch": 14.543259557344065,
      "grad_norm": 0.47395557165145874,
      "learning_rate": 0.00017093067713049603,
      "loss": 0.4621,
      "step": 7228
    },
    {
      "epoch": 14.545271629778671,
      "grad_norm": 0.5062156915664673,
      "learning_rate": 0.00017092665258074255,
      "loss": 0.513,
      "step": 7229
    },
    {
      "epoch": 14.54728370221328,
      "grad_norm": 0.5709519386291504,
      "learning_rate": 0.00017092262803098903,
      "loss": 0.4552,
      "step": 7230
    },
    {
      "epoch": 14.549295774647888,
      "grad_norm": 0.49148324131965637,
      "learning_rate": 0.00017091860348123554,
      "loss": 0.4641,
      "step": 7231
    },
    {
      "epoch": 14.551307847082494,
      "grad_norm": 0.49373099207878113,
      "learning_rate": 0.00017091457893148203,
      "loss": 0.4799,
      "step": 7232
    },
    {
      "epoch": 14.553319919517103,
      "grad_norm": 0.5215546488761902,
      "learning_rate": 0.00017091055438172857,
      "loss": 0.4419,
      "step": 7233
    },
    {
      "epoch": 14.55533199195171,
      "grad_norm": 0.4960629940032959,
      "learning_rate": 0.00017090652983197505,
      "loss": 0.4835,
      "step": 7234
    },
    {
      "epoch": 14.557344064386317,
      "grad_norm": 0.4793791174888611,
      "learning_rate": 0.00017090250528222157,
      "loss": 0.4577,
      "step": 7235
    },
    {
      "epoch": 14.559356136820925,
      "grad_norm": 0.4586917459964752,
      "learning_rate": 0.00017089848073246805,
      "loss": 0.4304,
      "step": 7236
    },
    {
      "epoch": 14.561368209255534,
      "grad_norm": 0.4760487377643585,
      "learning_rate": 0.00017089445618271456,
      "loss": 0.4486,
      "step": 7237
    },
    {
      "epoch": 14.56338028169014,
      "grad_norm": 0.4593583941459656,
      "learning_rate": 0.00017089043163296107,
      "loss": 0.4851,
      "step": 7238
    },
    {
      "epoch": 14.565392354124748,
      "grad_norm": 0.4737241864204407,
      "learning_rate": 0.0001708864070832076,
      "loss": 0.4627,
      "step": 7239
    },
    {
      "epoch": 14.567404426559357,
      "grad_norm": 0.46324774622917175,
      "learning_rate": 0.00017088238253345407,
      "loss": 0.471,
      "step": 7240
    },
    {
      "epoch": 14.569416498993963,
      "grad_norm": 0.48980239033699036,
      "learning_rate": 0.00017087835798370058,
      "loss": 0.4669,
      "step": 7241
    },
    {
      "epoch": 14.571428571428571,
      "grad_norm": 0.4740000367164612,
      "learning_rate": 0.00017087433343394707,
      "loss": 0.4337,
      "step": 7242
    },
    {
      "epoch": 14.57344064386318,
      "grad_norm": 0.5446264147758484,
      "learning_rate": 0.0001708703088841936,
      "loss": 0.4688,
      "step": 7243
    },
    {
      "epoch": 14.575452716297786,
      "grad_norm": 0.47593167424201965,
      "learning_rate": 0.0001708662843344401,
      "loss": 0.4774,
      "step": 7244
    },
    {
      "epoch": 14.577464788732394,
      "grad_norm": 0.5078542232513428,
      "learning_rate": 0.0001708622597846866,
      "loss": 0.4626,
      "step": 7245
    },
    {
      "epoch": 14.579476861167002,
      "grad_norm": 0.48588261008262634,
      "learning_rate": 0.0001708582352349331,
      "loss": 0.4697,
      "step": 7246
    },
    {
      "epoch": 14.58148893360161,
      "grad_norm": 0.5008232593536377,
      "learning_rate": 0.0001708542106851796,
      "loss": 0.5048,
      "step": 7247
    },
    {
      "epoch": 14.583501006036217,
      "grad_norm": 0.4603691101074219,
      "learning_rate": 0.00017085018613542612,
      "loss": 0.4756,
      "step": 7248
    },
    {
      "epoch": 14.585513078470825,
      "grad_norm": 0.5083519220352173,
      "learning_rate": 0.00017084616158567263,
      "loss": 0.5088,
      "step": 7249
    },
    {
      "epoch": 14.587525150905433,
      "grad_norm": 0.48337095975875854,
      "learning_rate": 0.0001708421370359191,
      "loss": 0.4688,
      "step": 7250
    },
    {
      "epoch": 14.58953722334004,
      "grad_norm": 0.5012140870094299,
      "learning_rate": 0.00017083811248616563,
      "loss": 0.5152,
      "step": 7251
    },
    {
      "epoch": 14.591549295774648,
      "grad_norm": 0.48384857177734375,
      "learning_rate": 0.0001708340879364121,
      "loss": 0.4752,
      "step": 7252
    },
    {
      "epoch": 14.593561368209256,
      "grad_norm": 0.4944993853569031,
      "learning_rate": 0.00017083006338665862,
      "loss": 0.4693,
      "step": 7253
    },
    {
      "epoch": 14.595573440643863,
      "grad_norm": 0.4801470637321472,
      "learning_rate": 0.00017082603883690513,
      "loss": 0.4669,
      "step": 7254
    },
    {
      "epoch": 14.59758551307847,
      "grad_norm": 0.4854316711425781,
      "learning_rate": 0.00017082201428715162,
      "loss": 0.4741,
      "step": 7255
    },
    {
      "epoch": 14.599597585513079,
      "grad_norm": 0.496780663728714,
      "learning_rate": 0.00017081798973739813,
      "loss": 0.4325,
      "step": 7256
    },
    {
      "epoch": 14.601609657947686,
      "grad_norm": 0.46434155106544495,
      "learning_rate": 0.00017081396518764464,
      "loss": 0.4584,
      "step": 7257
    },
    {
      "epoch": 14.603621730382294,
      "grad_norm": 0.47427356243133545,
      "learning_rate": 0.00017080994063789116,
      "loss": 0.4938,
      "step": 7258
    },
    {
      "epoch": 14.605633802816902,
      "grad_norm": 0.482557088136673,
      "learning_rate": 0.00017080591608813764,
      "loss": 0.4614,
      "step": 7259
    },
    {
      "epoch": 14.607645875251508,
      "grad_norm": 0.4605002999305725,
      "learning_rate": 0.00017080189153838415,
      "loss": 0.4488,
      "step": 7260
    },
    {
      "epoch": 14.609657947686117,
      "grad_norm": 0.459814190864563,
      "learning_rate": 0.00017079786698863064,
      "loss": 0.4741,
      "step": 7261
    },
    {
      "epoch": 14.611670020120725,
      "grad_norm": 0.5074692964553833,
      "learning_rate": 0.00017079384243887715,
      "loss": 0.4776,
      "step": 7262
    },
    {
      "epoch": 14.613682092555331,
      "grad_norm": 0.4595823585987091,
      "learning_rate": 0.00017078981788912366,
      "loss": 0.4697,
      "step": 7263
    },
    {
      "epoch": 14.61569416498994,
      "grad_norm": 0.4813165068626404,
      "learning_rate": 0.00017078579333937018,
      "loss": 0.4903,
      "step": 7264
    },
    {
      "epoch": 14.617706237424548,
      "grad_norm": 0.4832594096660614,
      "learning_rate": 0.00017078176878961666,
      "loss": 0.4847,
      "step": 7265
    },
    {
      "epoch": 14.619718309859154,
      "grad_norm": 0.5505824089050293,
      "learning_rate": 0.00017077774423986317,
      "loss": 0.4879,
      "step": 7266
    },
    {
      "epoch": 14.621730382293762,
      "grad_norm": 0.5244317650794983,
      "learning_rate": 0.00017077371969010966,
      "loss": 0.4562,
      "step": 7267
    },
    {
      "epoch": 14.62374245472837,
      "grad_norm": 0.48020103573799133,
      "learning_rate": 0.0001707696951403562,
      "loss": 0.475,
      "step": 7268
    },
    {
      "epoch": 14.625754527162979,
      "grad_norm": 0.49043509364128113,
      "learning_rate": 0.00017076567059060268,
      "loss": 0.5153,
      "step": 7269
    },
    {
      "epoch": 14.627766599597585,
      "grad_norm": 0.49314433336257935,
      "learning_rate": 0.0001707616460408492,
      "loss": 0.4643,
      "step": 7270
    },
    {
      "epoch": 14.629778672032193,
      "grad_norm": 0.46926623582839966,
      "learning_rate": 0.00017075762149109568,
      "loss": 0.4496,
      "step": 7271
    },
    {
      "epoch": 14.631790744466802,
      "grad_norm": 0.4954853653907776,
      "learning_rate": 0.0001707535969413422,
      "loss": 0.4957,
      "step": 7272
    },
    {
      "epoch": 14.633802816901408,
      "grad_norm": 0.47688066959381104,
      "learning_rate": 0.0001707495723915887,
      "loss": 0.4542,
      "step": 7273
    },
    {
      "epoch": 14.635814889336016,
      "grad_norm": 0.5146266222000122,
      "learning_rate": 0.00017074554784183522,
      "loss": 0.4588,
      "step": 7274
    },
    {
      "epoch": 14.637826961770624,
      "grad_norm": 0.4934869110584259,
      "learning_rate": 0.0001707415232920817,
      "loss": 0.4762,
      "step": 7275
    },
    {
      "epoch": 14.639839034205231,
      "grad_norm": 0.45199280977249146,
      "learning_rate": 0.00017073749874232821,
      "loss": 0.4915,
      "step": 7276
    },
    {
      "epoch": 14.64185110663984,
      "grad_norm": 0.4991389513015747,
      "learning_rate": 0.0001707334741925747,
      "loss": 0.4637,
      "step": 7277
    },
    {
      "epoch": 14.643863179074447,
      "grad_norm": 0.5336107015609741,
      "learning_rate": 0.00017072944964282124,
      "loss": 0.4949,
      "step": 7278
    },
    {
      "epoch": 14.645875251509054,
      "grad_norm": 0.5491235852241516,
      "learning_rate": 0.00017072542509306772,
      "loss": 0.4762,
      "step": 7279
    },
    {
      "epoch": 14.647887323943662,
      "grad_norm": 0.47340071201324463,
      "learning_rate": 0.00017072140054331424,
      "loss": 0.4592,
      "step": 7280
    },
    {
      "epoch": 14.64989939637827,
      "grad_norm": 0.4699586033821106,
      "learning_rate": 0.00017071737599356072,
      "loss": 0.4765,
      "step": 7281
    },
    {
      "epoch": 14.651911468812877,
      "grad_norm": 0.47341257333755493,
      "learning_rate": 0.00017071335144380723,
      "loss": 0.4696,
      "step": 7282
    },
    {
      "epoch": 14.653923541247485,
      "grad_norm": 0.48204275965690613,
      "learning_rate": 0.00017070932689405375,
      "loss": 0.4921,
      "step": 7283
    },
    {
      "epoch": 14.655935613682093,
      "grad_norm": 0.5146089196205139,
      "learning_rate": 0.00017070530234430026,
      "loss": 0.4897,
      "step": 7284
    },
    {
      "epoch": 14.6579476861167,
      "grad_norm": 0.5182571411132812,
      "learning_rate": 0.00017070127779454674,
      "loss": 0.4762,
      "step": 7285
    },
    {
      "epoch": 14.659959758551308,
      "grad_norm": 0.5460187196731567,
      "learning_rate": 0.00017069725324479325,
      "loss": 0.486,
      "step": 7286
    },
    {
      "epoch": 14.661971830985916,
      "grad_norm": 0.5127114057540894,
      "learning_rate": 0.00017069322869503974,
      "loss": 0.5002,
      "step": 7287
    },
    {
      "epoch": 14.663983903420522,
      "grad_norm": 0.48119091987609863,
      "learning_rate": 0.00017068920414528625,
      "loss": 0.4965,
      "step": 7288
    },
    {
      "epoch": 14.66599597585513,
      "grad_norm": 0.4753376543521881,
      "learning_rate": 0.00017068517959553276,
      "loss": 0.4732,
      "step": 7289
    },
    {
      "epoch": 14.668008048289739,
      "grad_norm": 0.48214611411094666,
      "learning_rate": 0.00017068115504577925,
      "loss": 0.4834,
      "step": 7290
    },
    {
      "epoch": 14.670020120724345,
      "grad_norm": 0.4677960276603699,
      "learning_rate": 0.00017067713049602576,
      "loss": 0.4706,
      "step": 7291
    },
    {
      "epoch": 14.672032193158953,
      "grad_norm": 0.4794676601886749,
      "learning_rate": 0.00017067310594627227,
      "loss": 0.4555,
      "step": 7292
    },
    {
      "epoch": 14.674044265593562,
      "grad_norm": 0.4800911247730255,
      "learning_rate": 0.00017066908139651879,
      "loss": 0.4437,
      "step": 7293
    },
    {
      "epoch": 14.676056338028168,
      "grad_norm": 0.5231866240501404,
      "learning_rate": 0.00017066505684676527,
      "loss": 0.4793,
      "step": 7294
    },
    {
      "epoch": 14.678068410462776,
      "grad_norm": 0.505473256111145,
      "learning_rate": 0.00017066103229701178,
      "loss": 0.4512,
      "step": 7295
    },
    {
      "epoch": 14.680080482897385,
      "grad_norm": 0.46990451216697693,
      "learning_rate": 0.00017065700774725827,
      "loss": 0.4919,
      "step": 7296
    },
    {
      "epoch": 14.682092555331993,
      "grad_norm": 0.4746188819408417,
      "learning_rate": 0.00017065298319750478,
      "loss": 0.45,
      "step": 7297
    },
    {
      "epoch": 14.6841046277666,
      "grad_norm": 0.4712839126586914,
      "learning_rate": 0.0001706489586477513,
      "loss": 0.4874,
      "step": 7298
    },
    {
      "epoch": 14.686116700201207,
      "grad_norm": 0.4712502956390381,
      "learning_rate": 0.0001706449340979978,
      "loss": 0.4446,
      "step": 7299
    },
    {
      "epoch": 14.688128772635816,
      "grad_norm": 0.4957609474658966,
      "learning_rate": 0.0001706409095482443,
      "loss": 0.4518,
      "step": 7300
    },
    {
      "epoch": 14.690140845070422,
      "grad_norm": 0.48485463857650757,
      "learning_rate": 0.0001706368849984908,
      "loss": 0.4412,
      "step": 7301
    },
    {
      "epoch": 14.69215291750503,
      "grad_norm": 0.5246092081069946,
      "learning_rate": 0.0001706328604487373,
      "loss": 0.4988,
      "step": 7302
    },
    {
      "epoch": 14.694164989939638,
      "grad_norm": 0.5307023525238037,
      "learning_rate": 0.00017062883589898383,
      "loss": 0.444,
      "step": 7303
    },
    {
      "epoch": 14.696177062374245,
      "grad_norm": 0.4640370309352875,
      "learning_rate": 0.0001706248113492303,
      "loss": 0.4473,
      "step": 7304
    },
    {
      "epoch": 14.698189134808853,
      "grad_norm": 0.4794773459434509,
      "learning_rate": 0.00017062078679947682,
      "loss": 0.4546,
      "step": 7305
    },
    {
      "epoch": 14.700201207243461,
      "grad_norm": 0.47474873065948486,
      "learning_rate": 0.0001706167622497233,
      "loss": 0.4405,
      "step": 7306
    },
    {
      "epoch": 14.702213279678068,
      "grad_norm": 0.5185892581939697,
      "learning_rate": 0.00017061273769996982,
      "loss": 0.4661,
      "step": 7307
    },
    {
      "epoch": 14.704225352112676,
      "grad_norm": 0.5422029495239258,
      "learning_rate": 0.00017060871315021633,
      "loss": 0.4358,
      "step": 7308
    },
    {
      "epoch": 14.706237424547284,
      "grad_norm": 0.4777674973011017,
      "learning_rate": 0.00017060468860046285,
      "loss": 0.4611,
      "step": 7309
    },
    {
      "epoch": 14.70824949698189,
      "grad_norm": 0.47735491394996643,
      "learning_rate": 0.00017060066405070933,
      "loss": 0.4711,
      "step": 7310
    },
    {
      "epoch": 14.710261569416499,
      "grad_norm": 0.48784205317497253,
      "learning_rate": 0.00017059663950095584,
      "loss": 0.4626,
      "step": 7311
    },
    {
      "epoch": 14.712273641851107,
      "grad_norm": 0.4740467071533203,
      "learning_rate": 0.00017059261495120233,
      "loss": 0.511,
      "step": 7312
    },
    {
      "epoch": 14.714285714285714,
      "grad_norm": 0.4550381004810333,
      "learning_rate": 0.00017058859040144887,
      "loss": 0.469,
      "step": 7313
    },
    {
      "epoch": 14.716297786720322,
      "grad_norm": 0.4757554531097412,
      "learning_rate": 0.00017058456585169535,
      "loss": 0.4637,
      "step": 7314
    },
    {
      "epoch": 14.71830985915493,
      "grad_norm": 0.4791288375854492,
      "learning_rate": 0.00017058054130194186,
      "loss": 0.4522,
      "step": 7315
    },
    {
      "epoch": 14.720321931589538,
      "grad_norm": 0.4726046621799469,
      "learning_rate": 0.00017057651675218835,
      "loss": 0.4571,
      "step": 7316
    },
    {
      "epoch": 14.722334004024145,
      "grad_norm": 0.5136378407478333,
      "learning_rate": 0.00017057249220243486,
      "loss": 0.4888,
      "step": 7317
    },
    {
      "epoch": 14.724346076458753,
      "grad_norm": 0.4823870360851288,
      "learning_rate": 0.00017056846765268137,
      "loss": 0.4721,
      "step": 7318
    },
    {
      "epoch": 14.726358148893361,
      "grad_norm": 0.5150127410888672,
      "learning_rate": 0.0001705644431029279,
      "loss": 0.4354,
      "step": 7319
    },
    {
      "epoch": 14.728370221327967,
      "grad_norm": 0.49680691957473755,
      "learning_rate": 0.00017056041855317437,
      "loss": 0.4729,
      "step": 7320
    },
    {
      "epoch": 14.730382293762576,
      "grad_norm": 0.45487329363822937,
      "learning_rate": 0.00017055639400342088,
      "loss": 0.4543,
      "step": 7321
    },
    {
      "epoch": 14.732394366197184,
      "grad_norm": 0.4682653546333313,
      "learning_rate": 0.00017055236945366737,
      "loss": 0.4521,
      "step": 7322
    },
    {
      "epoch": 14.73440643863179,
      "grad_norm": 0.4869842827320099,
      "learning_rate": 0.00017054834490391388,
      "loss": 0.4855,
      "step": 7323
    },
    {
      "epoch": 14.736418511066399,
      "grad_norm": 0.4844290316104889,
      "learning_rate": 0.0001705443203541604,
      "loss": 0.4903,
      "step": 7324
    },
    {
      "epoch": 14.738430583501007,
      "grad_norm": 0.4662761986255646,
      "learning_rate": 0.00017054029580440688,
      "loss": 0.4644,
      "step": 7325
    },
    {
      "epoch": 14.740442655935613,
      "grad_norm": 0.488138347864151,
      "learning_rate": 0.0001705362712546534,
      "loss": 0.4773,
      "step": 7326
    },
    {
      "epoch": 14.742454728370221,
      "grad_norm": 0.47221964597702026,
      "learning_rate": 0.00017053224670489988,
      "loss": 0.4772,
      "step": 7327
    },
    {
      "epoch": 14.74446680080483,
      "grad_norm": 0.46579840779304504,
      "learning_rate": 0.00017052822215514642,
      "loss": 0.4763,
      "step": 7328
    },
    {
      "epoch": 14.746478873239436,
      "grad_norm": 0.4943198263645172,
      "learning_rate": 0.0001705241976053929,
      "loss": 0.471,
      "step": 7329
    },
    {
      "epoch": 14.748490945674044,
      "grad_norm": 0.46697500348091125,
      "learning_rate": 0.0001705201730556394,
      "loss": 0.4555,
      "step": 7330
    },
    {
      "epoch": 14.750503018108652,
      "grad_norm": 0.46933186054229736,
      "learning_rate": 0.0001705161485058859,
      "loss": 0.4951,
      "step": 7331
    },
    {
      "epoch": 14.752515090543259,
      "grad_norm": 0.48962682485580444,
      "learning_rate": 0.0001705121239561324,
      "loss": 0.513,
      "step": 7332
    },
    {
      "epoch": 14.754527162977867,
      "grad_norm": 0.5062024593353271,
      "learning_rate": 0.00017050809940637892,
      "loss": 0.4985,
      "step": 7333
    },
    {
      "epoch": 14.756539235412475,
      "grad_norm": 0.5112875699996948,
      "learning_rate": 0.00017050407485662543,
      "loss": 0.5043,
      "step": 7334
    },
    {
      "epoch": 14.758551307847082,
      "grad_norm": 0.454423189163208,
      "learning_rate": 0.00017050005030687192,
      "loss": 0.4189,
      "step": 7335
    },
    {
      "epoch": 14.76056338028169,
      "grad_norm": 0.48095494508743286,
      "learning_rate": 0.00017049602575711843,
      "loss": 0.4815,
      "step": 7336
    },
    {
      "epoch": 14.762575452716298,
      "grad_norm": 0.4644845426082611,
      "learning_rate": 0.00017049200120736492,
      "loss": 0.4558,
      "step": 7337
    },
    {
      "epoch": 14.764587525150905,
      "grad_norm": 0.4702209532260895,
      "learning_rate": 0.00017048797665761146,
      "loss": 0.5004,
      "step": 7338
    },
    {
      "epoch": 14.766599597585513,
      "grad_norm": 0.49205613136291504,
      "learning_rate": 0.00017048395210785794,
      "loss": 0.5215,
      "step": 7339
    },
    {
      "epoch": 14.768611670020121,
      "grad_norm": 0.4866369068622589,
      "learning_rate": 0.00017047992755810445,
      "loss": 0.4676,
      "step": 7340
    },
    {
      "epoch": 14.770623742454728,
      "grad_norm": 0.49812591075897217,
      "learning_rate": 0.00017047590300835094,
      "loss": 0.4587,
      "step": 7341
    },
    {
      "epoch": 14.772635814889336,
      "grad_norm": 0.45319002866744995,
      "learning_rate": 0.00017047187845859745,
      "loss": 0.4135,
      "step": 7342
    },
    {
      "epoch": 14.774647887323944,
      "grad_norm": 0.4646562337875366,
      "learning_rate": 0.00017046785390884396,
      "loss": 0.463,
      "step": 7343
    },
    {
      "epoch": 14.77665995975855,
      "grad_norm": 0.5124536752700806,
      "learning_rate": 0.00017046382935909048,
      "loss": 0.4656,
      "step": 7344
    },
    {
      "epoch": 14.778672032193159,
      "grad_norm": 0.4847065806388855,
      "learning_rate": 0.00017045980480933696,
      "loss": 0.469,
      "step": 7345
    },
    {
      "epoch": 14.780684104627767,
      "grad_norm": 0.45286813378334045,
      "learning_rate": 0.00017045578025958347,
      "loss": 0.504,
      "step": 7346
    },
    {
      "epoch": 14.782696177062375,
      "grad_norm": 0.4360538721084595,
      "learning_rate": 0.00017045175570982996,
      "loss": 0.4491,
      "step": 7347
    },
    {
      "epoch": 14.784708249496981,
      "grad_norm": 0.4825563430786133,
      "learning_rate": 0.0001704477311600765,
      "loss": 0.4861,
      "step": 7348
    },
    {
      "epoch": 14.78672032193159,
      "grad_norm": 0.5169808268547058,
      "learning_rate": 0.00017044370661032298,
      "loss": 0.4737,
      "step": 7349
    },
    {
      "epoch": 14.788732394366198,
      "grad_norm": 0.48624035716056824,
      "learning_rate": 0.0001704396820605695,
      "loss": 0.4926,
      "step": 7350
    },
    {
      "epoch": 14.790744466800804,
      "grad_norm": 0.5032358765602112,
      "learning_rate": 0.00017043565751081598,
      "loss": 0.4897,
      "step": 7351
    },
    {
      "epoch": 14.792756539235413,
      "grad_norm": 0.4896725118160248,
      "learning_rate": 0.0001704316329610625,
      "loss": 0.4767,
      "step": 7352
    },
    {
      "epoch": 14.79476861167002,
      "grad_norm": 0.4940537214279175,
      "learning_rate": 0.000170427608411309,
      "loss": 0.4749,
      "step": 7353
    },
    {
      "epoch": 14.796780684104627,
      "grad_norm": 0.4884050488471985,
      "learning_rate": 0.0001704235838615555,
      "loss": 0.4854,
      "step": 7354
    },
    {
      "epoch": 14.798792756539235,
      "grad_norm": 0.504562258720398,
      "learning_rate": 0.000170419559311802,
      "loss": 0.4765,
      "step": 7355
    },
    {
      "epoch": 14.800804828973844,
      "grad_norm": 0.4694264233112335,
      "learning_rate": 0.0001704155347620485,
      "loss": 0.4936,
      "step": 7356
    },
    {
      "epoch": 14.80281690140845,
      "grad_norm": 0.4933534860610962,
      "learning_rate": 0.000170411510212295,
      "loss": 0.4902,
      "step": 7357
    },
    {
      "epoch": 14.804828973843058,
      "grad_norm": 0.46657001972198486,
      "learning_rate": 0.0001704074856625415,
      "loss": 0.4553,
      "step": 7358
    },
    {
      "epoch": 14.806841046277667,
      "grad_norm": 0.4580262005329132,
      "learning_rate": 0.00017040346111278802,
      "loss": 0.4776,
      "step": 7359
    },
    {
      "epoch": 14.808853118712273,
      "grad_norm": 0.4962771236896515,
      "learning_rate": 0.0001703994365630345,
      "loss": 0.4998,
      "step": 7360
    },
    {
      "epoch": 14.810865191146881,
      "grad_norm": 0.49149227142333984,
      "learning_rate": 0.00017039541201328102,
      "loss": 0.4682,
      "step": 7361
    },
    {
      "epoch": 14.81287726358149,
      "grad_norm": 0.49069929122924805,
      "learning_rate": 0.0001703913874635275,
      "loss": 0.4822,
      "step": 7362
    },
    {
      "epoch": 14.814889336016096,
      "grad_norm": 0.5225918889045715,
      "learning_rate": 0.00017038736291377404,
      "loss": 0.5123,
      "step": 7363
    },
    {
      "epoch": 14.816901408450704,
      "grad_norm": 0.45756423473358154,
      "learning_rate": 0.00017038333836402053,
      "loss": 0.4845,
      "step": 7364
    },
    {
      "epoch": 14.818913480885312,
      "grad_norm": 0.4634217917919159,
      "learning_rate": 0.00017037931381426704,
      "loss": 0.4837,
      "step": 7365
    },
    {
      "epoch": 14.82092555331992,
      "grad_norm": 0.4520902633666992,
      "learning_rate": 0.00017037528926451353,
      "loss": 0.4674,
      "step": 7366
    },
    {
      "epoch": 14.822937625754527,
      "grad_norm": 0.46139270067214966,
      "learning_rate": 0.00017037126471476004,
      "loss": 0.477,
      "step": 7367
    },
    {
      "epoch": 14.824949698189135,
      "grad_norm": 0.4695266783237457,
      "learning_rate": 0.00017036724016500655,
      "loss": 0.4829,
      "step": 7368
    },
    {
      "epoch": 14.826961770623743,
      "grad_norm": 0.5242515802383423,
      "learning_rate": 0.00017036321561525306,
      "loss": 0.5103,
      "step": 7369
    },
    {
      "epoch": 14.82897384305835,
      "grad_norm": 0.45522916316986084,
      "learning_rate": 0.00017035919106549955,
      "loss": 0.4524,
      "step": 7370
    },
    {
      "epoch": 14.830985915492958,
      "grad_norm": 0.46637091040611267,
      "learning_rate": 0.00017035516651574606,
      "loss": 0.4626,
      "step": 7371
    },
    {
      "epoch": 14.832997987927566,
      "grad_norm": 0.4913887679576874,
      "learning_rate": 0.00017035114196599255,
      "loss": 0.4889,
      "step": 7372
    },
    {
      "epoch": 14.835010060362173,
      "grad_norm": 0.5074562430381775,
      "learning_rate": 0.00017034711741623909,
      "loss": 0.458,
      "step": 7373
    },
    {
      "epoch": 14.83702213279678,
      "grad_norm": 0.45732611417770386,
      "learning_rate": 0.00017034309286648557,
      "loss": 0.4516,
      "step": 7374
    },
    {
      "epoch": 14.839034205231389,
      "grad_norm": 0.4533694386482239,
      "learning_rate": 0.00017033906831673208,
      "loss": 0.4364,
      "step": 7375
    },
    {
      "epoch": 14.841046277665995,
      "grad_norm": 0.5156704783439636,
      "learning_rate": 0.00017033504376697857,
      "loss": 0.5011,
      "step": 7376
    },
    {
      "epoch": 14.843058350100604,
      "grad_norm": 0.4715847373008728,
      "learning_rate": 0.00017033101921722508,
      "loss": 0.4556,
      "step": 7377
    },
    {
      "epoch": 14.845070422535212,
      "grad_norm": 0.45607712864875793,
      "learning_rate": 0.0001703269946674716,
      "loss": 0.4569,
      "step": 7378
    },
    {
      "epoch": 14.847082494969818,
      "grad_norm": 0.4673972427845001,
      "learning_rate": 0.0001703229701177181,
      "loss": 0.4515,
      "step": 7379
    },
    {
      "epoch": 14.849094567404427,
      "grad_norm": 0.46723857522010803,
      "learning_rate": 0.0001703189455679646,
      "loss": 0.4972,
      "step": 7380
    },
    {
      "epoch": 14.851106639839035,
      "grad_norm": 0.4876701235771179,
      "learning_rate": 0.0001703149210182111,
      "loss": 0.4759,
      "step": 7381
    },
    {
      "epoch": 14.853118712273641,
      "grad_norm": 0.48121410608291626,
      "learning_rate": 0.0001703108964684576,
      "loss": 0.4436,
      "step": 7382
    },
    {
      "epoch": 14.85513078470825,
      "grad_norm": 0.45426422357559204,
      "learning_rate": 0.00017030687191870413,
      "loss": 0.455,
      "step": 7383
    },
    {
      "epoch": 14.857142857142858,
      "grad_norm": 0.472022145986557,
      "learning_rate": 0.0001703028473689506,
      "loss": 0.4724,
      "step": 7384
    },
    {
      "epoch": 14.859154929577464,
      "grad_norm": 0.5137026309967041,
      "learning_rate": 0.00017029882281919712,
      "loss": 0.538,
      "step": 7385
    },
    {
      "epoch": 14.861167002012072,
      "grad_norm": 0.4679546356201172,
      "learning_rate": 0.0001702947982694436,
      "loss": 0.4804,
      "step": 7386
    },
    {
      "epoch": 14.86317907444668,
      "grad_norm": 0.46134433150291443,
      "learning_rate": 0.00017029077371969012,
      "loss": 0.4777,
      "step": 7387
    },
    {
      "epoch": 14.865191146881287,
      "grad_norm": 0.46980997920036316,
      "learning_rate": 0.00017028674916993663,
      "loss": 0.5009,
      "step": 7388
    },
    {
      "epoch": 14.867203219315895,
      "grad_norm": 0.4452959895133972,
      "learning_rate": 0.00017028272462018312,
      "loss": 0.431,
      "step": 7389
    },
    {
      "epoch": 14.869215291750503,
      "grad_norm": 0.47314631938934326,
      "learning_rate": 0.00017027870007042963,
      "loss": 0.4724,
      "step": 7390
    },
    {
      "epoch": 14.87122736418511,
      "grad_norm": 0.469730943441391,
      "learning_rate": 0.00017027467552067614,
      "loss": 0.468,
      "step": 7391
    },
    {
      "epoch": 14.873239436619718,
      "grad_norm": 0.4753684997558594,
      "learning_rate": 0.00017027065097092263,
      "loss": 0.4724,
      "step": 7392
    },
    {
      "epoch": 14.875251509054326,
      "grad_norm": 0.47201094031333923,
      "learning_rate": 0.00017026662642116914,
      "loss": 0.4995,
      "step": 7393
    },
    {
      "epoch": 14.877263581488933,
      "grad_norm": 0.4967862069606781,
      "learning_rate": 0.00017026260187141565,
      "loss": 0.4348,
      "step": 7394
    },
    {
      "epoch": 14.879275653923541,
      "grad_norm": 0.49486082792282104,
      "learning_rate": 0.00017025857732166214,
      "loss": 0.4587,
      "step": 7395
    },
    {
      "epoch": 14.88128772635815,
      "grad_norm": 0.4689985513687134,
      "learning_rate": 0.00017025455277190865,
      "loss": 0.4781,
      "step": 7396
    },
    {
      "epoch": 14.883299798792757,
      "grad_norm": 0.4759601652622223,
      "learning_rate": 0.00017025052822215513,
      "loss": 0.4978,
      "step": 7397
    },
    {
      "epoch": 14.885311871227364,
      "grad_norm": 0.47391659021377563,
      "learning_rate": 0.00017024650367240167,
      "loss": 0.4879,
      "step": 7398
    },
    {
      "epoch": 14.887323943661972,
      "grad_norm": 0.4657449424266815,
      "learning_rate": 0.00017024247912264816,
      "loss": 0.5141,
      "step": 7399
    },
    {
      "epoch": 14.88933601609658,
      "grad_norm": 0.49311143159866333,
      "learning_rate": 0.00017023845457289467,
      "loss": 0.5007,
      "step": 7400
    },
    {
      "epoch": 14.891348088531187,
      "grad_norm": 0.46880829334259033,
      "learning_rate": 0.00017023443002314116,
      "loss": 0.4839,
      "step": 7401
    },
    {
      "epoch": 14.893360160965795,
      "grad_norm": 0.4804859459400177,
      "learning_rate": 0.00017023040547338767,
      "loss": 0.4821,
      "step": 7402
    },
    {
      "epoch": 14.895372233400403,
      "grad_norm": 0.4806179404258728,
      "learning_rate": 0.00017022638092363415,
      "loss": 0.4618,
      "step": 7403
    },
    {
      "epoch": 14.89738430583501,
      "grad_norm": 0.4750444293022156,
      "learning_rate": 0.0001702223563738807,
      "loss": 0.4819,
      "step": 7404
    },
    {
      "epoch": 14.899396378269618,
      "grad_norm": 0.47218582034111023,
      "learning_rate": 0.00017021833182412718,
      "loss": 0.513,
      "step": 7405
    },
    {
      "epoch": 14.901408450704226,
      "grad_norm": 0.47269290685653687,
      "learning_rate": 0.0001702143072743737,
      "loss": 0.485,
      "step": 7406
    },
    {
      "epoch": 14.903420523138832,
      "grad_norm": 0.4742385149002075,
      "learning_rate": 0.00017021028272462018,
      "loss": 0.4573,
      "step": 7407
    },
    {
      "epoch": 14.90543259557344,
      "grad_norm": 0.46290844678878784,
      "learning_rate": 0.0001702062581748667,
      "loss": 0.4658,
      "step": 7408
    },
    {
      "epoch": 14.907444668008049,
      "grad_norm": 0.4874623715877533,
      "learning_rate": 0.0001702022336251132,
      "loss": 0.4808,
      "step": 7409
    },
    {
      "epoch": 14.909456740442655,
      "grad_norm": 0.43714091181755066,
      "learning_rate": 0.0001701982090753597,
      "loss": 0.4613,
      "step": 7410
    },
    {
      "epoch": 14.911468812877263,
      "grad_norm": 0.48845234513282776,
      "learning_rate": 0.0001701941845256062,
      "loss": 0.4854,
      "step": 7411
    },
    {
      "epoch": 14.913480885311872,
      "grad_norm": 0.45835089683532715,
      "learning_rate": 0.0001701901599758527,
      "loss": 0.4908,
      "step": 7412
    },
    {
      "epoch": 14.915492957746478,
      "grad_norm": 0.46951618790626526,
      "learning_rate": 0.0001701861354260992,
      "loss": 0.4362,
      "step": 7413
    },
    {
      "epoch": 14.917505030181086,
      "grad_norm": 0.5007352232933044,
      "learning_rate": 0.00017018211087634573,
      "loss": 0.506,
      "step": 7414
    },
    {
      "epoch": 14.919517102615695,
      "grad_norm": 0.4740090072154999,
      "learning_rate": 0.00017017808632659222,
      "loss": 0.4928,
      "step": 7415
    },
    {
      "epoch": 14.921529175050303,
      "grad_norm": 0.47714537382125854,
      "learning_rate": 0.00017017406177683873,
      "loss": 0.473,
      "step": 7416
    },
    {
      "epoch": 14.92354124748491,
      "grad_norm": 0.4751487076282501,
      "learning_rate": 0.00017017003722708522,
      "loss": 0.4876,
      "step": 7417
    },
    {
      "epoch": 14.925553319919517,
      "grad_norm": 0.4652894139289856,
      "learning_rate": 0.00017016601267733173,
      "loss": 0.4541,
      "step": 7418
    },
    {
      "epoch": 14.927565392354126,
      "grad_norm": 0.48226335644721985,
      "learning_rate": 0.00017016198812757824,
      "loss": 0.4813,
      "step": 7419
    },
    {
      "epoch": 14.929577464788732,
      "grad_norm": 0.4486831724643707,
      "learning_rate": 0.00017015796357782475,
      "loss": 0.4489,
      "step": 7420
    },
    {
      "epoch": 14.93158953722334,
      "grad_norm": 0.48988601565361023,
      "learning_rate": 0.00017015393902807124,
      "loss": 0.4564,
      "step": 7421
    },
    {
      "epoch": 14.933601609657948,
      "grad_norm": 0.49211713671684265,
      "learning_rate": 0.00017014991447831775,
      "loss": 0.4888,
      "step": 7422
    },
    {
      "epoch": 14.935613682092555,
      "grad_norm": 0.5112444758415222,
      "learning_rate": 0.00017014588992856424,
      "loss": 0.4752,
      "step": 7423
    },
    {
      "epoch": 14.937625754527163,
      "grad_norm": 0.48322591185569763,
      "learning_rate": 0.00017014186537881075,
      "loss": 0.4899,
      "step": 7424
    },
    {
      "epoch": 14.939637826961771,
      "grad_norm": 0.45268714427948,
      "learning_rate": 0.00017013784082905726,
      "loss": 0.4484,
      "step": 7425
    },
    {
      "epoch": 14.941649899396378,
      "grad_norm": 0.5032428503036499,
      "learning_rate": 0.00017013381627930377,
      "loss": 0.4948,
      "step": 7426
    },
    {
      "epoch": 14.943661971830986,
      "grad_norm": 0.445130318403244,
      "learning_rate": 0.00017012979172955026,
      "loss": 0.4422,
      "step": 7427
    },
    {
      "epoch": 14.945674044265594,
      "grad_norm": 0.45518794655799866,
      "learning_rate": 0.00017012576717979677,
      "loss": 0.4882,
      "step": 7428
    },
    {
      "epoch": 14.9476861167002,
      "grad_norm": 0.48621508479118347,
      "learning_rate": 0.00017012174263004328,
      "loss": 0.5136,
      "step": 7429
    },
    {
      "epoch": 14.949698189134809,
      "grad_norm": 0.47675174474716187,
      "learning_rate": 0.00017011771808028977,
      "loss": 0.4686,
      "step": 7430
    },
    {
      "epoch": 14.951710261569417,
      "grad_norm": 0.4818372130393982,
      "learning_rate": 0.00017011369353053628,
      "loss": 0.4555,
      "step": 7431
    },
    {
      "epoch": 14.953722334004024,
      "grad_norm": 0.4782576560974121,
      "learning_rate": 0.00017010966898078276,
      "loss": 0.4952,
      "step": 7432
    },
    {
      "epoch": 14.955734406438632,
      "grad_norm": 0.47457483410835266,
      "learning_rate": 0.00017010564443102928,
      "loss": 0.4429,
      "step": 7433
    },
    {
      "epoch": 14.95774647887324,
      "grad_norm": 0.46042609214782715,
      "learning_rate": 0.0001701016198812758,
      "loss": 0.4884,
      "step": 7434
    },
    {
      "epoch": 14.959758551307846,
      "grad_norm": 0.4632030725479126,
      "learning_rate": 0.0001700975953315223,
      "loss": 0.488,
      "step": 7435
    },
    {
      "epoch": 14.961770623742455,
      "grad_norm": 0.47390666604042053,
      "learning_rate": 0.00017009357078176879,
      "loss": 0.465,
      "step": 7436
    },
    {
      "epoch": 14.963782696177063,
      "grad_norm": 0.5266963243484497,
      "learning_rate": 0.0001700895462320153,
      "loss": 0.4898,
      "step": 7437
    },
    {
      "epoch": 14.96579476861167,
      "grad_norm": 0.47941720485687256,
      "learning_rate": 0.00017008552168226178,
      "loss": 0.4631,
      "step": 7438
    },
    {
      "epoch": 14.967806841046277,
      "grad_norm": 0.47243228554725647,
      "learning_rate": 0.00017008149713250832,
      "loss": 0.5061,
      "step": 7439
    },
    {
      "epoch": 14.969818913480886,
      "grad_norm": 0.4661211669445038,
      "learning_rate": 0.0001700774725827548,
      "loss": 0.4852,
      "step": 7440
    },
    {
      "epoch": 14.971830985915492,
      "grad_norm": 0.47818201780319214,
      "learning_rate": 0.00017007344803300132,
      "loss": 0.5024,
      "step": 7441
    },
    {
      "epoch": 14.9738430583501,
      "grad_norm": 0.4891548752784729,
      "learning_rate": 0.0001700694234832478,
      "loss": 0.4542,
      "step": 7442
    },
    {
      "epoch": 14.975855130784709,
      "grad_norm": 0.4770699143409729,
      "learning_rate": 0.00017006539893349432,
      "loss": 0.4855,
      "step": 7443
    },
    {
      "epoch": 14.977867203219315,
      "grad_norm": 0.4634748697280884,
      "learning_rate": 0.00017006137438374083,
      "loss": 0.483,
      "step": 7444
    },
    {
      "epoch": 14.979879275653923,
      "grad_norm": 0.4503646790981293,
      "learning_rate": 0.00017005734983398734,
      "loss": 0.4749,
      "step": 7445
    },
    {
      "epoch": 14.981891348088531,
      "grad_norm": 0.4810425937175751,
      "learning_rate": 0.00017005332528423383,
      "loss": 0.4948,
      "step": 7446
    },
    {
      "epoch": 14.98390342052314,
      "grad_norm": 0.47644537687301636,
      "learning_rate": 0.00017004930073448034,
      "loss": 0.4562,
      "step": 7447
    },
    {
      "epoch": 14.985915492957746,
      "grad_norm": 0.4915469288825989,
      "learning_rate": 0.00017004527618472682,
      "loss": 0.5087,
      "step": 7448
    },
    {
      "epoch": 14.987927565392354,
      "grad_norm": 0.4717346131801605,
      "learning_rate": 0.00017004125163497336,
      "loss": 0.4843,
      "step": 7449
    },
    {
      "epoch": 14.989939637826962,
      "grad_norm": 0.46236690878868103,
      "learning_rate": 0.00017003722708521985,
      "loss": 0.4835,
      "step": 7450
    },
    {
      "epoch": 14.991951710261569,
      "grad_norm": 0.4563940465450287,
      "learning_rate": 0.00017003320253546636,
      "loss": 0.4909,
      "step": 7451
    },
    {
      "epoch": 14.993963782696177,
      "grad_norm": 0.4659237861633301,
      "learning_rate": 0.00017002917798571285,
      "loss": 0.4999,
      "step": 7452
    },
    {
      "epoch": 14.995975855130785,
      "grad_norm": 0.47168347239494324,
      "learning_rate": 0.00017002515343595936,
      "loss": 0.497,
      "step": 7453
    },
    {
      "epoch": 14.997987927565392,
      "grad_norm": 0.4622129499912262,
      "learning_rate": 0.00017002112888620587,
      "loss": 0.5059,
      "step": 7454
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.5040631890296936,
      "learning_rate": 0.00017001710433645238,
      "loss": 0.4751,
      "step": 7455
    },
    {
      "epoch": 15.0,
      "eval_loss": 0.8355227112770081,
      "eval_runtime": 49.8096,
      "eval_samples_per_second": 19.916,
      "eval_steps_per_second": 2.489,
      "step": 7455
    },
    {
      "epoch": 15.002012072434608,
      "grad_norm": 0.4332890808582306,
      "learning_rate": 0.00017001307978669887,
      "loss": 0.4158,
      "step": 7456
    },
    {
      "epoch": 15.004024144869215,
      "grad_norm": 0.42157453298568726,
      "learning_rate": 0.00017000905523694538,
      "loss": 0.3827,
      "step": 7457
    },
    {
      "epoch": 15.006036217303823,
      "grad_norm": 0.4555249810218811,
      "learning_rate": 0.00017000503068719187,
      "loss": 0.4032,
      "step": 7458
    },
    {
      "epoch": 15.008048289738431,
      "grad_norm": 0.46033430099487305,
      "learning_rate": 0.00017000100613743838,
      "loss": 0.4189,
      "step": 7459
    },
    {
      "epoch": 15.010060362173038,
      "grad_norm": 0.496770977973938,
      "learning_rate": 0.0001699969815876849,
      "loss": 0.4091,
      "step": 7460
    },
    {
      "epoch": 15.012072434607646,
      "grad_norm": 0.5451329946517944,
      "learning_rate": 0.0001699929570379314,
      "loss": 0.4536,
      "step": 7461
    },
    {
      "epoch": 15.014084507042254,
      "grad_norm": 0.5143526196479797,
      "learning_rate": 0.0001699889324881779,
      "loss": 0.3668,
      "step": 7462
    },
    {
      "epoch": 15.01609657947686,
      "grad_norm": 0.4762038290500641,
      "learning_rate": 0.0001699849079384244,
      "loss": 0.4048,
      "step": 7463
    },
    {
      "epoch": 15.018108651911469,
      "grad_norm": 0.5066518187522888,
      "learning_rate": 0.0001699808833886709,
      "loss": 0.4192,
      "step": 7464
    },
    {
      "epoch": 15.020120724346077,
      "grad_norm": 0.4875001907348633,
      "learning_rate": 0.0001699768588389174,
      "loss": 0.4734,
      "step": 7465
    },
    {
      "epoch": 15.022132796780683,
      "grad_norm": 0.44194167852401733,
      "learning_rate": 0.0001699728342891639,
      "loss": 0.4051,
      "step": 7466
    },
    {
      "epoch": 15.024144869215291,
      "grad_norm": 0.47037333250045776,
      "learning_rate": 0.0001699688097394104,
      "loss": 0.4184,
      "step": 7467
    },
    {
      "epoch": 15.0261569416499,
      "grad_norm": 0.47007516026496887,
      "learning_rate": 0.0001699647851896569,
      "loss": 0.4309,
      "step": 7468
    },
    {
      "epoch": 15.028169014084508,
      "grad_norm": 0.48639827966690063,
      "learning_rate": 0.00016996076063990342,
      "loss": 0.4289,
      "step": 7469
    },
    {
      "epoch": 15.030181086519114,
      "grad_norm": 0.4908677935600281,
      "learning_rate": 0.00016995673609014993,
      "loss": 0.378,
      "step": 7470
    },
    {
      "epoch": 15.032193158953723,
      "grad_norm": 0.5118030309677124,
      "learning_rate": 0.00016995271154039642,
      "loss": 0.4242,
      "step": 7471
    },
    {
      "epoch": 15.03420523138833,
      "grad_norm": 0.4788207709789276,
      "learning_rate": 0.00016994868699064293,
      "loss": 0.4172,
      "step": 7472
    },
    {
      "epoch": 15.036217303822937,
      "grad_norm": 0.49792981147766113,
      "learning_rate": 0.0001699446624408894,
      "loss": 0.4069,
      "step": 7473
    },
    {
      "epoch": 15.038229376257545,
      "grad_norm": 0.4840322732925415,
      "learning_rate": 0.00016994063789113595,
      "loss": 0.4185,
      "step": 7474
    },
    {
      "epoch": 15.040241448692154,
      "grad_norm": 0.46515920758247375,
      "learning_rate": 0.00016993661334138244,
      "loss": 0.4111,
      "step": 7475
    },
    {
      "epoch": 15.04225352112676,
      "grad_norm": 0.4796484410762787,
      "learning_rate": 0.00016993258879162895,
      "loss": 0.4287,
      "step": 7476
    },
    {
      "epoch": 15.044265593561368,
      "grad_norm": 0.5075982809066772,
      "learning_rate": 0.00016992856424187543,
      "loss": 0.4222,
      "step": 7477
    },
    {
      "epoch": 15.046277665995976,
      "grad_norm": 0.4769703447818756,
      "learning_rate": 0.00016992453969212195,
      "loss": 0.4571,
      "step": 7478
    },
    {
      "epoch": 15.048289738430583,
      "grad_norm": 0.4712477922439575,
      "learning_rate": 0.00016992051514236846,
      "loss": 0.4003,
      "step": 7479
    },
    {
      "epoch": 15.050301810865191,
      "grad_norm": 0.47434863448143005,
      "learning_rate": 0.00016991649059261497,
      "loss": 0.4067,
      "step": 7480
    },
    {
      "epoch": 15.0523138832998,
      "grad_norm": 0.47673165798187256,
      "learning_rate": 0.00016991246604286146,
      "loss": 0.4202,
      "step": 7481
    },
    {
      "epoch": 15.054325955734406,
      "grad_norm": 0.48142144083976746,
      "learning_rate": 0.00016990844149310797,
      "loss": 0.4217,
      "step": 7482
    },
    {
      "epoch": 15.056338028169014,
      "grad_norm": 0.46888434886932373,
      "learning_rate": 0.00016990441694335445,
      "loss": 0.426,
      "step": 7483
    },
    {
      "epoch": 15.058350100603622,
      "grad_norm": 0.5247043967247009,
      "learning_rate": 0.000169900392393601,
      "loss": 0.4075,
      "step": 7484
    },
    {
      "epoch": 15.060362173038229,
      "grad_norm": 0.48827704787254333,
      "learning_rate": 0.00016989636784384748,
      "loss": 0.4279,
      "step": 7485
    },
    {
      "epoch": 15.062374245472837,
      "grad_norm": 0.4582808315753937,
      "learning_rate": 0.000169892343294094,
      "loss": 0.3921,
      "step": 7486
    },
    {
      "epoch": 15.064386317907445,
      "grad_norm": 0.4876296818256378,
      "learning_rate": 0.00016988831874434048,
      "loss": 0.3739,
      "step": 7487
    },
    {
      "epoch": 15.066398390342052,
      "grad_norm": 0.4789912700653076,
      "learning_rate": 0.000169884294194587,
      "loss": 0.3913,
      "step": 7488
    },
    {
      "epoch": 15.06841046277666,
      "grad_norm": 0.5074466466903687,
      "learning_rate": 0.0001698802696448335,
      "loss": 0.4351,
      "step": 7489
    },
    {
      "epoch": 15.070422535211268,
      "grad_norm": 0.47807833552360535,
      "learning_rate": 0.00016987624509508,
      "loss": 0.4302,
      "step": 7490
    },
    {
      "epoch": 15.072434607645874,
      "grad_norm": 0.4864090383052826,
      "learning_rate": 0.0001698722205453265,
      "loss": 0.4322,
      "step": 7491
    },
    {
      "epoch": 15.074446680080483,
      "grad_norm": 0.48673439025878906,
      "learning_rate": 0.000169868195995573,
      "loss": 0.4141,
      "step": 7492
    },
    {
      "epoch": 15.07645875251509,
      "grad_norm": 0.49452587962150574,
      "learning_rate": 0.0001698641714458195,
      "loss": 0.4271,
      "step": 7493
    },
    {
      "epoch": 15.078470824949699,
      "grad_norm": 0.48028311133384705,
      "learning_rate": 0.000169860146896066,
      "loss": 0.418,
      "step": 7494
    },
    {
      "epoch": 15.080482897384305,
      "grad_norm": 0.4607810974121094,
      "learning_rate": 0.00016985612234631252,
      "loss": 0.4088,
      "step": 7495
    },
    {
      "epoch": 15.082494969818914,
      "grad_norm": 0.48705023527145386,
      "learning_rate": 0.000169852097796559,
      "loss": 0.4258,
      "step": 7496
    },
    {
      "epoch": 15.084507042253522,
      "grad_norm": 0.4900676906108856,
      "learning_rate": 0.00016984807324680552,
      "loss": 0.3943,
      "step": 7497
    },
    {
      "epoch": 15.086519114688128,
      "grad_norm": 0.47175711393356323,
      "learning_rate": 0.00016984404869705203,
      "loss": 0.3938,
      "step": 7498
    },
    {
      "epoch": 15.088531187122737,
      "grad_norm": 0.5146054625511169,
      "learning_rate": 0.00016984002414729854,
      "loss": 0.4142,
      "step": 7499
    },
    {
      "epoch": 15.090543259557345,
      "grad_norm": 0.5367429256439209,
      "learning_rate": 0.00016983599959754503,
      "loss": 0.4364,
      "step": 7500
    },
    {
      "epoch": 15.092555331991951,
      "grad_norm": 0.5237500667572021,
      "learning_rate": 0.00016983197504779154,
      "loss": 0.4602,
      "step": 7501
    },
    {
      "epoch": 15.09456740442656,
      "grad_norm": 0.5069774985313416,
      "learning_rate": 0.00016982795049803802,
      "loss": 0.4386,
      "step": 7502
    },
    {
      "epoch": 15.096579476861168,
      "grad_norm": 0.49674585461616516,
      "learning_rate": 0.00016982392594828454,
      "loss": 0.4336,
      "step": 7503
    },
    {
      "epoch": 15.098591549295774,
      "grad_norm": 0.49986889958381653,
      "learning_rate": 0.00016981990139853105,
      "loss": 0.4462,
      "step": 7504
    },
    {
      "epoch": 15.100603621730382,
      "grad_norm": 0.49489814043045044,
      "learning_rate": 0.00016981587684877756,
      "loss": 0.4293,
      "step": 7505
    },
    {
      "epoch": 15.10261569416499,
      "grad_norm": 0.49005091190338135,
      "learning_rate": 0.00016981185229902404,
      "loss": 0.4378,
      "step": 7506
    },
    {
      "epoch": 15.104627766599597,
      "grad_norm": 0.49197936058044434,
      "learning_rate": 0.00016980782774927056,
      "loss": 0.4209,
      "step": 7507
    },
    {
      "epoch": 15.106639839034205,
      "grad_norm": 0.5065981149673462,
      "learning_rate": 0.00016980380319951704,
      "loss": 0.4386,
      "step": 7508
    },
    {
      "epoch": 15.108651911468813,
      "grad_norm": 0.5327794551849365,
      "learning_rate": 0.00016979977864976358,
      "loss": 0.4366,
      "step": 7509
    },
    {
      "epoch": 15.11066398390342,
      "grad_norm": 0.4907625913619995,
      "learning_rate": 0.00016979575410001007,
      "loss": 0.4267,
      "step": 7510
    },
    {
      "epoch": 15.112676056338028,
      "grad_norm": 0.4829227030277252,
      "learning_rate": 0.00016979172955025658,
      "loss": 0.3892,
      "step": 7511
    },
    {
      "epoch": 15.114688128772636,
      "grad_norm": 0.49807068705558777,
      "learning_rate": 0.00016978770500050306,
      "loss": 0.4048,
      "step": 7512
    },
    {
      "epoch": 15.116700201207243,
      "grad_norm": 0.5101960301399231,
      "learning_rate": 0.00016978368045074958,
      "loss": 0.4153,
      "step": 7513
    },
    {
      "epoch": 15.11871227364185,
      "grad_norm": 0.48974689841270447,
      "learning_rate": 0.0001697796559009961,
      "loss": 0.4028,
      "step": 7514
    },
    {
      "epoch": 15.120724346076459,
      "grad_norm": 0.48026278614997864,
      "learning_rate": 0.0001697756313512426,
      "loss": 0.4078,
      "step": 7515
    },
    {
      "epoch": 15.122736418511066,
      "grad_norm": 0.4870600700378418,
      "learning_rate": 0.00016977160680148909,
      "loss": 0.4363,
      "step": 7516
    },
    {
      "epoch": 15.124748490945674,
      "grad_norm": 0.520037055015564,
      "learning_rate": 0.0001697675822517356,
      "loss": 0.425,
      "step": 7517
    },
    {
      "epoch": 15.126760563380282,
      "grad_norm": 0.4806642532348633,
      "learning_rate": 0.00016976355770198208,
      "loss": 0.4298,
      "step": 7518
    },
    {
      "epoch": 15.12877263581489,
      "grad_norm": 0.5183860063552856,
      "learning_rate": 0.00016975953315222862,
      "loss": 0.446,
      "step": 7519
    },
    {
      "epoch": 15.130784708249497,
      "grad_norm": 0.49470242857933044,
      "learning_rate": 0.0001697555086024751,
      "loss": 0.4153,
      "step": 7520
    },
    {
      "epoch": 15.132796780684105,
      "grad_norm": 0.5203485488891602,
      "learning_rate": 0.00016975148405272162,
      "loss": 0.4262,
      "step": 7521
    },
    {
      "epoch": 15.134808853118713,
      "grad_norm": 0.516602098941803,
      "learning_rate": 0.0001697474595029681,
      "loss": 0.455,
      "step": 7522
    },
    {
      "epoch": 15.13682092555332,
      "grad_norm": 0.4776056706905365,
      "learning_rate": 0.00016974343495321462,
      "loss": 0.3818,
      "step": 7523
    },
    {
      "epoch": 15.138832997987928,
      "grad_norm": 0.4814186692237854,
      "learning_rate": 0.00016973941040346113,
      "loss": 0.3994,
      "step": 7524
    },
    {
      "epoch": 15.140845070422536,
      "grad_norm": 0.5041159987449646,
      "learning_rate": 0.00016973538585370764,
      "loss": 0.419,
      "step": 7525
    },
    {
      "epoch": 15.142857142857142,
      "grad_norm": 0.48204541206359863,
      "learning_rate": 0.00016973136130395413,
      "loss": 0.4059,
      "step": 7526
    },
    {
      "epoch": 15.14486921529175,
      "grad_norm": 0.46488770842552185,
      "learning_rate": 0.00016972733675420064,
      "loss": 0.394,
      "step": 7527
    },
    {
      "epoch": 15.146881287726359,
      "grad_norm": 0.4856666624546051,
      "learning_rate": 0.00016972331220444712,
      "loss": 0.4269,
      "step": 7528
    },
    {
      "epoch": 15.148893360160965,
      "grad_norm": 0.48984962701797485,
      "learning_rate": 0.00016971928765469364,
      "loss": 0.4195,
      "step": 7529
    },
    {
      "epoch": 15.150905432595573,
      "grad_norm": 0.5215373039245605,
      "learning_rate": 0.00016971526310494015,
      "loss": 0.4331,
      "step": 7530
    },
    {
      "epoch": 15.152917505030182,
      "grad_norm": 0.5224244594573975,
      "learning_rate": 0.00016971123855518663,
      "loss": 0.3969,
      "step": 7531
    },
    {
      "epoch": 15.154929577464788,
      "grad_norm": 0.5061846971511841,
      "learning_rate": 0.00016970721400543315,
      "loss": 0.4371,
      "step": 7532
    },
    {
      "epoch": 15.156941649899396,
      "grad_norm": 0.4877033233642578,
      "learning_rate": 0.00016970318945567966,
      "loss": 0.3941,
      "step": 7533
    },
    {
      "epoch": 15.158953722334005,
      "grad_norm": 0.5217934250831604,
      "learning_rate": 0.00016969916490592617,
      "loss": 0.4412,
      "step": 7534
    },
    {
      "epoch": 15.160965794768611,
      "grad_norm": 0.5061248540878296,
      "learning_rate": 0.00016969514035617266,
      "loss": 0.4411,
      "step": 7535
    },
    {
      "epoch": 15.16297786720322,
      "grad_norm": 0.4952588975429535,
      "learning_rate": 0.00016969111580641917,
      "loss": 0.4442,
      "step": 7536
    },
    {
      "epoch": 15.164989939637827,
      "grad_norm": 0.5224772095680237,
      "learning_rate": 0.00016968709125666565,
      "loss": 0.4444,
      "step": 7537
    },
    {
      "epoch": 15.167002012072434,
      "grad_norm": 0.4900480806827545,
      "learning_rate": 0.00016968306670691216,
      "loss": 0.4222,
      "step": 7538
    },
    {
      "epoch": 15.169014084507042,
      "grad_norm": 0.5372161865234375,
      "learning_rate": 0.00016967904215715868,
      "loss": 0.3994,
      "step": 7539
    },
    {
      "epoch": 15.17102615694165,
      "grad_norm": 0.5302976369857788,
      "learning_rate": 0.0001696750176074052,
      "loss": 0.4648,
      "step": 7540
    },
    {
      "epoch": 15.173038229376257,
      "grad_norm": 0.5156477093696594,
      "learning_rate": 0.00016967099305765167,
      "loss": 0.4229,
      "step": 7541
    },
    {
      "epoch": 15.175050301810865,
      "grad_norm": 0.4964781403541565,
      "learning_rate": 0.0001696669685078982,
      "loss": 0.3987,
      "step": 7542
    },
    {
      "epoch": 15.177062374245473,
      "grad_norm": 0.5104294419288635,
      "learning_rate": 0.00016966294395814467,
      "loss": 0.4559,
      "step": 7543
    },
    {
      "epoch": 15.179074446680081,
      "grad_norm": 0.49602174758911133,
      "learning_rate": 0.0001696589194083912,
      "loss": 0.4238,
      "step": 7544
    },
    {
      "epoch": 15.181086519114688,
      "grad_norm": 0.49715161323547363,
      "learning_rate": 0.0001696548948586377,
      "loss": 0.4333,
      "step": 7545
    },
    {
      "epoch": 15.183098591549296,
      "grad_norm": 0.5017226338386536,
      "learning_rate": 0.0001696508703088842,
      "loss": 0.4367,
      "step": 7546
    },
    {
      "epoch": 15.185110663983904,
      "grad_norm": 0.49304571747779846,
      "learning_rate": 0.0001696468457591307,
      "loss": 0.4607,
      "step": 7547
    },
    {
      "epoch": 15.18712273641851,
      "grad_norm": 0.49066656827926636,
      "learning_rate": 0.0001696428212093772,
      "loss": 0.4304,
      "step": 7548
    },
    {
      "epoch": 15.189134808853119,
      "grad_norm": 0.4925135672092438,
      "learning_rate": 0.00016963879665962372,
      "loss": 0.4395,
      "step": 7549
    },
    {
      "epoch": 15.191146881287727,
      "grad_norm": 0.4733823239803314,
      "learning_rate": 0.00016963477210987023,
      "loss": 0.4184,
      "step": 7550
    },
    {
      "epoch": 15.193158953722333,
      "grad_norm": 0.5020893216133118,
      "learning_rate": 0.00016963074756011672,
      "loss": 0.4647,
      "step": 7551
    },
    {
      "epoch": 15.195171026156942,
      "grad_norm": 0.5165642499923706,
      "learning_rate": 0.00016962672301036323,
      "loss": 0.4495,
      "step": 7552
    },
    {
      "epoch": 15.19718309859155,
      "grad_norm": 0.48867639899253845,
      "learning_rate": 0.0001696226984606097,
      "loss": 0.3989,
      "step": 7553
    },
    {
      "epoch": 15.199195171026156,
      "grad_norm": 0.5040362477302551,
      "learning_rate": 0.00016961867391085625,
      "loss": 0.4338,
      "step": 7554
    },
    {
      "epoch": 15.201207243460765,
      "grad_norm": 0.5056672692298889,
      "learning_rate": 0.00016961464936110274,
      "loss": 0.4143,
      "step": 7555
    },
    {
      "epoch": 15.203219315895373,
      "grad_norm": 0.5058921575546265,
      "learning_rate": 0.00016961062481134925,
      "loss": 0.4371,
      "step": 7556
    },
    {
      "epoch": 15.20523138832998,
      "grad_norm": 0.49244198203086853,
      "learning_rate": 0.00016960660026159573,
      "loss": 0.4409,
      "step": 7557
    },
    {
      "epoch": 15.207243460764587,
      "grad_norm": 0.48269888758659363,
      "learning_rate": 0.00016960257571184225,
      "loss": 0.4159,
      "step": 7558
    },
    {
      "epoch": 15.209255533199196,
      "grad_norm": 0.4944550395011902,
      "learning_rate": 0.00016959855116208876,
      "loss": 0.4439,
      "step": 7559
    },
    {
      "epoch": 15.211267605633802,
      "grad_norm": 0.49042028188705444,
      "learning_rate": 0.00016959452661233527,
      "loss": 0.4011,
      "step": 7560
    },
    {
      "epoch": 15.21327967806841,
      "grad_norm": 0.4794044494628906,
      "learning_rate": 0.00016959050206258176,
      "loss": 0.4427,
      "step": 7561
    },
    {
      "epoch": 15.215291750503019,
      "grad_norm": 0.46741029620170593,
      "learning_rate": 0.00016958647751282827,
      "loss": 0.3753,
      "step": 7562
    },
    {
      "epoch": 15.217303822937625,
      "grad_norm": 0.524686872959137,
      "learning_rate": 0.00016958245296307475,
      "loss": 0.4065,
      "step": 7563
    },
    {
      "epoch": 15.219315895372233,
      "grad_norm": 0.48796969652175903,
      "learning_rate": 0.00016957842841332127,
      "loss": 0.4106,
      "step": 7564
    },
    {
      "epoch": 15.221327967806841,
      "grad_norm": 0.48300182819366455,
      "learning_rate": 0.00016957440386356778,
      "loss": 0.3911,
      "step": 7565
    },
    {
      "epoch": 15.223340040241448,
      "grad_norm": 0.5167627930641174,
      "learning_rate": 0.00016957037931381426,
      "loss": 0.424,
      "step": 7566
    },
    {
      "epoch": 15.225352112676056,
      "grad_norm": 0.5392266511917114,
      "learning_rate": 0.00016956635476406078,
      "loss": 0.4759,
      "step": 7567
    },
    {
      "epoch": 15.227364185110664,
      "grad_norm": 0.5020685791969299,
      "learning_rate": 0.0001695623302143073,
      "loss": 0.4489,
      "step": 7568
    },
    {
      "epoch": 15.229376257545272,
      "grad_norm": 0.4945300817489624,
      "learning_rate": 0.0001695583056645538,
      "loss": 0.419,
      "step": 7569
    },
    {
      "epoch": 15.231388329979879,
      "grad_norm": 0.4939459562301636,
      "learning_rate": 0.00016955428111480028,
      "loss": 0.4488,
      "step": 7570
    },
    {
      "epoch": 15.233400402414487,
      "grad_norm": 0.5334357023239136,
      "learning_rate": 0.0001695502565650468,
      "loss": 0.4497,
      "step": 7571
    },
    {
      "epoch": 15.235412474849095,
      "grad_norm": 0.5298027396202087,
      "learning_rate": 0.00016954623201529328,
      "loss": 0.4493,
      "step": 7572
    },
    {
      "epoch": 15.237424547283702,
      "grad_norm": 0.566977322101593,
      "learning_rate": 0.0001695422074655398,
      "loss": 0.4356,
      "step": 7573
    },
    {
      "epoch": 15.23943661971831,
      "grad_norm": 0.488613486289978,
      "learning_rate": 0.0001695381829157863,
      "loss": 0.4258,
      "step": 7574
    },
    {
      "epoch": 15.241448692152918,
      "grad_norm": 0.4974837899208069,
      "learning_rate": 0.00016953415836603282,
      "loss": 0.4133,
      "step": 7575
    },
    {
      "epoch": 15.243460764587525,
      "grad_norm": 0.49411070346832275,
      "learning_rate": 0.0001695301338162793,
      "loss": 0.4317,
      "step": 7576
    },
    {
      "epoch": 15.245472837022133,
      "grad_norm": 0.46944329142570496,
      "learning_rate": 0.00016952610926652582,
      "loss": 0.4237,
      "step": 7577
    },
    {
      "epoch": 15.247484909456741,
      "grad_norm": 0.51591557264328,
      "learning_rate": 0.0001695220847167723,
      "loss": 0.4247,
      "step": 7578
    },
    {
      "epoch": 15.249496981891348,
      "grad_norm": 0.543759286403656,
      "learning_rate": 0.00016951806016701884,
      "loss": 0.4565,
      "step": 7579
    },
    {
      "epoch": 15.251509054325956,
      "grad_norm": 0.5072899460792542,
      "learning_rate": 0.00016951403561726533,
      "loss": 0.4506,
      "step": 7580
    },
    {
      "epoch": 15.253521126760564,
      "grad_norm": 0.49544602632522583,
      "learning_rate": 0.00016951001106751184,
      "loss": 0.4427,
      "step": 7581
    },
    {
      "epoch": 15.25553319919517,
      "grad_norm": 0.49684280157089233,
      "learning_rate": 0.00016950598651775832,
      "loss": 0.4136,
      "step": 7582
    },
    {
      "epoch": 15.257545271629779,
      "grad_norm": 0.4934804141521454,
      "learning_rate": 0.00016950196196800484,
      "loss": 0.3996,
      "step": 7583
    },
    {
      "epoch": 15.259557344064387,
      "grad_norm": 0.5123341679573059,
      "learning_rate": 0.00016949793741825135,
      "loss": 0.4335,
      "step": 7584
    },
    {
      "epoch": 15.261569416498993,
      "grad_norm": 0.4977627396583557,
      "learning_rate": 0.00016949391286849786,
      "loss": 0.4077,
      "step": 7585
    },
    {
      "epoch": 15.263581488933601,
      "grad_norm": 0.5175750255584717,
      "learning_rate": 0.00016948988831874434,
      "loss": 0.4398,
      "step": 7586
    },
    {
      "epoch": 15.26559356136821,
      "grad_norm": 0.4986690878868103,
      "learning_rate": 0.00016948586376899086,
      "loss": 0.431,
      "step": 7587
    },
    {
      "epoch": 15.267605633802816,
      "grad_norm": 0.5257612466812134,
      "learning_rate": 0.00016948183921923734,
      "loss": 0.4195,
      "step": 7588
    },
    {
      "epoch": 15.269617706237424,
      "grad_norm": 0.49883225560188293,
      "learning_rate": 0.00016947781466948388,
      "loss": 0.4341,
      "step": 7589
    },
    {
      "epoch": 15.271629778672033,
      "grad_norm": 0.4989248812198639,
      "learning_rate": 0.00016947379011973037,
      "loss": 0.4457,
      "step": 7590
    },
    {
      "epoch": 15.273641851106639,
      "grad_norm": 0.5069971680641174,
      "learning_rate": 0.00016946976556997688,
      "loss": 0.4022,
      "step": 7591
    },
    {
      "epoch": 15.275653923541247,
      "grad_norm": 0.559155285358429,
      "learning_rate": 0.00016946574102022336,
      "loss": 0.4506,
      "step": 7592
    },
    {
      "epoch": 15.277665995975855,
      "grad_norm": 0.5026329755783081,
      "learning_rate": 0.00016946171647046988,
      "loss": 0.4256,
      "step": 7593
    },
    {
      "epoch": 15.279678068410464,
      "grad_norm": 0.5014466047286987,
      "learning_rate": 0.0001694576919207164,
      "loss": 0.4548,
      "step": 7594
    },
    {
      "epoch": 15.28169014084507,
      "grad_norm": 0.4888767600059509,
      "learning_rate": 0.0001694536673709629,
      "loss": 0.4338,
      "step": 7595
    },
    {
      "epoch": 15.283702213279678,
      "grad_norm": 0.510970950126648,
      "learning_rate": 0.00016944964282120939,
      "loss": 0.4325,
      "step": 7596
    },
    {
      "epoch": 15.285714285714286,
      "grad_norm": 0.47403424978256226,
      "learning_rate": 0.0001694456182714559,
      "loss": 0.4472,
      "step": 7597
    },
    {
      "epoch": 15.287726358148893,
      "grad_norm": 0.5090665221214294,
      "learning_rate": 0.00016944159372170238,
      "loss": 0.4408,
      "step": 7598
    },
    {
      "epoch": 15.289738430583501,
      "grad_norm": 0.5206389427185059,
      "learning_rate": 0.0001694375691719489,
      "loss": 0.4403,
      "step": 7599
    },
    {
      "epoch": 15.29175050301811,
      "grad_norm": 0.5370677709579468,
      "learning_rate": 0.0001694335446221954,
      "loss": 0.4272,
      "step": 7600
    },
    {
      "epoch": 15.293762575452716,
      "grad_norm": 0.5588595867156982,
      "learning_rate": 0.0001694295200724419,
      "loss": 0.4361,
      "step": 7601
    },
    {
      "epoch": 15.295774647887324,
      "grad_norm": 0.5454331636428833,
      "learning_rate": 0.0001694254955226884,
      "loss": 0.4509,
      "step": 7602
    },
    {
      "epoch": 15.297786720321932,
      "grad_norm": 0.488187700510025,
      "learning_rate": 0.00016942147097293492,
      "loss": 0.4292,
      "step": 7603
    },
    {
      "epoch": 15.299798792756539,
      "grad_norm": 0.5092560648918152,
      "learning_rate": 0.00016941744642318143,
      "loss": 0.4532,
      "step": 7604
    },
    {
      "epoch": 15.301810865191147,
      "grad_norm": 0.5078082084655762,
      "learning_rate": 0.00016941342187342791,
      "loss": 0.4487,
      "step": 7605
    },
    {
      "epoch": 15.303822937625755,
      "grad_norm": 0.5079867243766785,
      "learning_rate": 0.00016940939732367443,
      "loss": 0.4779,
      "step": 7606
    },
    {
      "epoch": 15.305835010060362,
      "grad_norm": 0.5109944343566895,
      "learning_rate": 0.0001694053727739209,
      "loss": 0.4644,
      "step": 7607
    },
    {
      "epoch": 15.30784708249497,
      "grad_norm": 0.49137428402900696,
      "learning_rate": 0.00016940134822416742,
      "loss": 0.4451,
      "step": 7608
    },
    {
      "epoch": 15.309859154929578,
      "grad_norm": 0.47570574283599854,
      "learning_rate": 0.00016939732367441394,
      "loss": 0.4176,
      "step": 7609
    },
    {
      "epoch": 15.311871227364184,
      "grad_norm": 0.5096982717514038,
      "learning_rate": 0.00016939329912466045,
      "loss": 0.425,
      "step": 7610
    },
    {
      "epoch": 15.313883299798793,
      "grad_norm": 0.5309371948242188,
      "learning_rate": 0.00016938927457490693,
      "loss": 0.4651,
      "step": 7611
    },
    {
      "epoch": 15.3158953722334,
      "grad_norm": 0.5091227889060974,
      "learning_rate": 0.00016938525002515345,
      "loss": 0.4353,
      "step": 7612
    },
    {
      "epoch": 15.317907444668007,
      "grad_norm": 0.5250053405761719,
      "learning_rate": 0.00016938122547539993,
      "loss": 0.4527,
      "step": 7613
    },
    {
      "epoch": 15.319919517102615,
      "grad_norm": 0.5076091289520264,
      "learning_rate": 0.00016937720092564647,
      "loss": 0.4368,
      "step": 7614
    },
    {
      "epoch": 15.321931589537224,
      "grad_norm": 0.49540796875953674,
      "learning_rate": 0.00016937317637589296,
      "loss": 0.4594,
      "step": 7615
    },
    {
      "epoch": 15.323943661971832,
      "grad_norm": 0.49898943305015564,
      "learning_rate": 0.00016936915182613947,
      "loss": 0.4122,
      "step": 7616
    },
    {
      "epoch": 15.325955734406438,
      "grad_norm": 0.49625998735427856,
      "learning_rate": 0.00016936512727638595,
      "loss": 0.4518,
      "step": 7617
    },
    {
      "epoch": 15.327967806841047,
      "grad_norm": 0.5091490149497986,
      "learning_rate": 0.00016936110272663246,
      "loss": 0.4297,
      "step": 7618
    },
    {
      "epoch": 15.329979879275655,
      "grad_norm": 0.5054764747619629,
      "learning_rate": 0.00016935707817687898,
      "loss": 0.419,
      "step": 7619
    },
    {
      "epoch": 15.331991951710261,
      "grad_norm": 0.5006480813026428,
      "learning_rate": 0.0001693530536271255,
      "loss": 0.4459,
      "step": 7620
    },
    {
      "epoch": 15.33400402414487,
      "grad_norm": 0.5101351737976074,
      "learning_rate": 0.00016934902907737197,
      "loss": 0.4446,
      "step": 7621
    },
    {
      "epoch": 15.336016096579478,
      "grad_norm": 0.48805734515190125,
      "learning_rate": 0.00016934500452761849,
      "loss": 0.4453,
      "step": 7622
    },
    {
      "epoch": 15.338028169014084,
      "grad_norm": 0.5048765540122986,
      "learning_rate": 0.00016934097997786497,
      "loss": 0.4484,
      "step": 7623
    },
    {
      "epoch": 15.340040241448692,
      "grad_norm": 0.5014535784721375,
      "learning_rate": 0.0001693369554281115,
      "loss": 0.4651,
      "step": 7624
    },
    {
      "epoch": 15.3420523138833,
      "grad_norm": 0.5210752487182617,
      "learning_rate": 0.000169332930878358,
      "loss": 0.4478,
      "step": 7625
    },
    {
      "epoch": 15.344064386317907,
      "grad_norm": 0.523148238658905,
      "learning_rate": 0.0001693289063286045,
      "loss": 0.465,
      "step": 7626
    },
    {
      "epoch": 15.346076458752515,
      "grad_norm": 0.494437038898468,
      "learning_rate": 0.000169324881778851,
      "loss": 0.4261,
      "step": 7627
    },
    {
      "epoch": 15.348088531187123,
      "grad_norm": 0.4813792407512665,
      "learning_rate": 0.0001693208572290975,
      "loss": 0.4339,
      "step": 7628
    },
    {
      "epoch": 15.35010060362173,
      "grad_norm": 0.5036506056785583,
      "learning_rate": 0.00016931683267934402,
      "loss": 0.4459,
      "step": 7629
    },
    {
      "epoch": 15.352112676056338,
      "grad_norm": 0.4941537082195282,
      "learning_rate": 0.00016931280812959053,
      "loss": 0.454,
      "step": 7630
    },
    {
      "epoch": 15.354124748490946,
      "grad_norm": 0.5099425911903381,
      "learning_rate": 0.00016930878357983702,
      "loss": 0.4166,
      "step": 7631
    },
    {
      "epoch": 15.356136820925553,
      "grad_norm": 0.5205479860305786,
      "learning_rate": 0.00016930475903008353,
      "loss": 0.4368,
      "step": 7632
    },
    {
      "epoch": 15.35814889336016,
      "grad_norm": 0.5112753510475159,
      "learning_rate": 0.00016930073448033,
      "loss": 0.3972,
      "step": 7633
    },
    {
      "epoch": 15.360160965794769,
      "grad_norm": 0.5252617597579956,
      "learning_rate": 0.00016929670993057652,
      "loss": 0.466,
      "step": 7634
    },
    {
      "epoch": 15.362173038229376,
      "grad_norm": 0.5198681354522705,
      "learning_rate": 0.00016929268538082304,
      "loss": 0.4196,
      "step": 7635
    },
    {
      "epoch": 15.364185110663984,
      "grad_norm": 0.48486509919166565,
      "learning_rate": 0.00016928866083106952,
      "loss": 0.4274,
      "step": 7636
    },
    {
      "epoch": 15.366197183098592,
      "grad_norm": 0.49399909377098083,
      "learning_rate": 0.00016928463628131603,
      "loss": 0.4358,
      "step": 7637
    },
    {
      "epoch": 15.368209255533198,
      "grad_norm": 0.5135356783866882,
      "learning_rate": 0.00016928061173156255,
      "loss": 0.4359,
      "step": 7638
    },
    {
      "epoch": 15.370221327967807,
      "grad_norm": 0.48436108231544495,
      "learning_rate": 0.00016927658718180906,
      "loss": 0.3938,
      "step": 7639
    },
    {
      "epoch": 15.372233400402415,
      "grad_norm": 0.507854163646698,
      "learning_rate": 0.00016927256263205554,
      "loss": 0.4358,
      "step": 7640
    },
    {
      "epoch": 15.374245472837021,
      "grad_norm": 0.5003666281700134,
      "learning_rate": 0.00016926853808230206,
      "loss": 0.4381,
      "step": 7641
    },
    {
      "epoch": 15.37625754527163,
      "grad_norm": 0.47772014141082764,
      "learning_rate": 0.00016926451353254854,
      "loss": 0.4195,
      "step": 7642
    },
    {
      "epoch": 15.378269617706238,
      "grad_norm": 0.4959203004837036,
      "learning_rate": 0.00016926048898279505,
      "loss": 0.4263,
      "step": 7643
    },
    {
      "epoch": 15.380281690140846,
      "grad_norm": 0.5019150972366333,
      "learning_rate": 0.00016925646443304154,
      "loss": 0.435,
      "step": 7644
    },
    {
      "epoch": 15.382293762575452,
      "grad_norm": 0.5192366242408752,
      "learning_rate": 0.00016925243988328808,
      "loss": 0.4345,
      "step": 7645
    },
    {
      "epoch": 15.38430583501006,
      "grad_norm": 0.5177675485610962,
      "learning_rate": 0.00016924841533353456,
      "loss": 0.4524,
      "step": 7646
    },
    {
      "epoch": 15.386317907444669,
      "grad_norm": 0.49831652641296387,
      "learning_rate": 0.00016924439078378107,
      "loss": 0.4444,
      "step": 7647
    },
    {
      "epoch": 15.388329979879275,
      "grad_norm": 0.5024067759513855,
      "learning_rate": 0.00016924036623402756,
      "loss": 0.4563,
      "step": 7648
    },
    {
      "epoch": 15.390342052313883,
      "grad_norm": 0.5048359036445618,
      "learning_rate": 0.00016923634168427407,
      "loss": 0.4357,
      "step": 7649
    },
    {
      "epoch": 15.392354124748492,
      "grad_norm": 0.5061743855476379,
      "learning_rate": 0.00016923231713452058,
      "loss": 0.4688,
      "step": 7650
    },
    {
      "epoch": 15.394366197183098,
      "grad_norm": 0.5318548083305359,
      "learning_rate": 0.0001692282925847671,
      "loss": 0.4235,
      "step": 7651
    },
    {
      "epoch": 15.396378269617706,
      "grad_norm": 0.507127046585083,
      "learning_rate": 0.00016922426803501358,
      "loss": 0.4515,
      "step": 7652
    },
    {
      "epoch": 15.398390342052314,
      "grad_norm": 0.5028899908065796,
      "learning_rate": 0.0001692202434852601,
      "loss": 0.4478,
      "step": 7653
    },
    {
      "epoch": 15.400402414486921,
      "grad_norm": 0.505164623260498,
      "learning_rate": 0.00016921621893550658,
      "loss": 0.4361,
      "step": 7654
    },
    {
      "epoch": 15.40241448692153,
      "grad_norm": 0.4961088299751282,
      "learning_rate": 0.00016921219438575312,
      "loss": 0.4389,
      "step": 7655
    },
    {
      "epoch": 15.404426559356137,
      "grad_norm": 0.5524977445602417,
      "learning_rate": 0.0001692081698359996,
      "loss": 0.4503,
      "step": 7656
    },
    {
      "epoch": 15.406438631790744,
      "grad_norm": 0.5196980237960815,
      "learning_rate": 0.00016920414528624612,
      "loss": 0.4206,
      "step": 7657
    },
    {
      "epoch": 15.408450704225352,
      "grad_norm": 0.5286099314689636,
      "learning_rate": 0.0001692001207364926,
      "loss": 0.4153,
      "step": 7658
    },
    {
      "epoch": 15.41046277665996,
      "grad_norm": 0.5079070925712585,
      "learning_rate": 0.0001691960961867391,
      "loss": 0.4476,
      "step": 7659
    },
    {
      "epoch": 15.412474849094567,
      "grad_norm": 0.5114506483078003,
      "learning_rate": 0.00016919207163698563,
      "loss": 0.429,
      "step": 7660
    },
    {
      "epoch": 15.414486921529175,
      "grad_norm": 0.5258375406265259,
      "learning_rate": 0.00016918804708723214,
      "loss": 0.4701,
      "step": 7661
    },
    {
      "epoch": 15.416498993963783,
      "grad_norm": 0.5167536735534668,
      "learning_rate": 0.00016918402253747862,
      "loss": 0.4248,
      "step": 7662
    },
    {
      "epoch": 15.41851106639839,
      "grad_norm": 0.5087298154830933,
      "learning_rate": 0.00016917999798772513,
      "loss": 0.4354,
      "step": 7663
    },
    {
      "epoch": 15.420523138832998,
      "grad_norm": 0.5274738669395447,
      "learning_rate": 0.00016917597343797162,
      "loss": 0.4614,
      "step": 7664
    },
    {
      "epoch": 15.422535211267606,
      "grad_norm": 0.47944438457489014,
      "learning_rate": 0.00016917194888821816,
      "loss": 0.4192,
      "step": 7665
    },
    {
      "epoch": 15.424547283702214,
      "grad_norm": 0.5250179171562195,
      "learning_rate": 0.00016916792433846464,
      "loss": 0.4268,
      "step": 7666
    },
    {
      "epoch": 15.42655935613682,
      "grad_norm": 0.4877415895462036,
      "learning_rate": 0.00016916389978871116,
      "loss": 0.4153,
      "step": 7667
    },
    {
      "epoch": 15.428571428571429,
      "grad_norm": 0.5141510963439941,
      "learning_rate": 0.00016915987523895764,
      "loss": 0.4449,
      "step": 7668
    },
    {
      "epoch": 15.430583501006037,
      "grad_norm": 0.5005225539207458,
      "learning_rate": 0.00016915585068920415,
      "loss": 0.4314,
      "step": 7669
    },
    {
      "epoch": 15.432595573440643,
      "grad_norm": 0.5035849213600159,
      "learning_rate": 0.00016915182613945067,
      "loss": 0.47,
      "step": 7670
    },
    {
      "epoch": 15.434607645875252,
      "grad_norm": 0.47526073455810547,
      "learning_rate": 0.00016914780158969715,
      "loss": 0.4327,
      "step": 7671
    },
    {
      "epoch": 15.43661971830986,
      "grad_norm": 0.5099614262580872,
      "learning_rate": 0.00016914377703994366,
      "loss": 0.4255,
      "step": 7672
    },
    {
      "epoch": 15.438631790744466,
      "grad_norm": 0.5004638433456421,
      "learning_rate": 0.00016913975249019015,
      "loss": 0.3888,
      "step": 7673
    },
    {
      "epoch": 15.440643863179075,
      "grad_norm": 0.5285058617591858,
      "learning_rate": 0.00016913572794043666,
      "loss": 0.4532,
      "step": 7674
    },
    {
      "epoch": 15.442655935613683,
      "grad_norm": 0.5325120687484741,
      "learning_rate": 0.00016913170339068317,
      "loss": 0.4487,
      "step": 7675
    },
    {
      "epoch": 15.44466800804829,
      "grad_norm": 0.49453532695770264,
      "learning_rate": 0.00016912767884092969,
      "loss": 0.4521,
      "step": 7676
    },
    {
      "epoch": 15.446680080482897,
      "grad_norm": 0.4859258830547333,
      "learning_rate": 0.00016912365429117617,
      "loss": 0.452,
      "step": 7677
    },
    {
      "epoch": 15.448692152917506,
      "grad_norm": 0.49842292070388794,
      "learning_rate": 0.00016911962974142268,
      "loss": 0.4687,
      "step": 7678
    },
    {
      "epoch": 15.450704225352112,
      "grad_norm": 0.5743808746337891,
      "learning_rate": 0.00016911560519166917,
      "loss": 0.4525,
      "step": 7679
    },
    {
      "epoch": 15.45271629778672,
      "grad_norm": 0.4981480836868286,
      "learning_rate": 0.0001691115806419157,
      "loss": 0.4018,
      "step": 7680
    },
    {
      "epoch": 15.454728370221329,
      "grad_norm": 0.4917253851890564,
      "learning_rate": 0.0001691075560921622,
      "loss": 0.438,
      "step": 7681
    },
    {
      "epoch": 15.456740442655935,
      "grad_norm": 0.49021515250205994,
      "learning_rate": 0.0001691035315424087,
      "loss": 0.4359,
      "step": 7682
    },
    {
      "epoch": 15.458752515090543,
      "grad_norm": 0.5349649786949158,
      "learning_rate": 0.0001690995069926552,
      "loss": 0.457,
      "step": 7683
    },
    {
      "epoch": 15.460764587525151,
      "grad_norm": 0.5141707062721252,
      "learning_rate": 0.0001690954824429017,
      "loss": 0.442,
      "step": 7684
    },
    {
      "epoch": 15.462776659959758,
      "grad_norm": 0.507101833820343,
      "learning_rate": 0.00016909145789314821,
      "loss": 0.469,
      "step": 7685
    },
    {
      "epoch": 15.464788732394366,
      "grad_norm": 0.4748305678367615,
      "learning_rate": 0.00016908743334339473,
      "loss": 0.4517,
      "step": 7686
    },
    {
      "epoch": 15.466800804828974,
      "grad_norm": 0.5297138690948486,
      "learning_rate": 0.0001690834087936412,
      "loss": 0.4621,
      "step": 7687
    },
    {
      "epoch": 15.46881287726358,
      "grad_norm": 0.5138043165206909,
      "learning_rate": 0.00016907938424388772,
      "loss": 0.466,
      "step": 7688
    },
    {
      "epoch": 15.470824949698189,
      "grad_norm": 0.5008231997489929,
      "learning_rate": 0.0001690753596941342,
      "loss": 0.4062,
      "step": 7689
    },
    {
      "epoch": 15.472837022132797,
      "grad_norm": 0.5299946665763855,
      "learning_rate": 0.00016907133514438075,
      "loss": 0.4511,
      "step": 7690
    },
    {
      "epoch": 15.474849094567404,
      "grad_norm": 0.5035672783851624,
      "learning_rate": 0.00016906731059462723,
      "loss": 0.4485,
      "step": 7691
    },
    {
      "epoch": 15.476861167002012,
      "grad_norm": 0.504811704158783,
      "learning_rate": 0.00016906328604487375,
      "loss": 0.4774,
      "step": 7692
    },
    {
      "epoch": 15.47887323943662,
      "grad_norm": 0.5037270188331604,
      "learning_rate": 0.00016905926149512023,
      "loss": 0.4588,
      "step": 7693
    },
    {
      "epoch": 15.480885311871228,
      "grad_norm": 0.49137550592422485,
      "learning_rate": 0.00016905523694536674,
      "loss": 0.4652,
      "step": 7694
    },
    {
      "epoch": 15.482897384305835,
      "grad_norm": 0.4789624512195587,
      "learning_rate": 0.00016905121239561325,
      "loss": 0.4532,
      "step": 7695
    },
    {
      "epoch": 15.484909456740443,
      "grad_norm": 0.5040173530578613,
      "learning_rate": 0.00016904718784585977,
      "loss": 0.4408,
      "step": 7696
    },
    {
      "epoch": 15.486921529175051,
      "grad_norm": 0.5425840616226196,
      "learning_rate": 0.00016904316329610625,
      "loss": 0.4488,
      "step": 7697
    },
    {
      "epoch": 15.488933601609657,
      "grad_norm": 0.49704912304878235,
      "learning_rate": 0.00016903913874635276,
      "loss": 0.4226,
      "step": 7698
    },
    {
      "epoch": 15.490945674044266,
      "grad_norm": 0.5024357438087463,
      "learning_rate": 0.00016903511419659925,
      "loss": 0.4327,
      "step": 7699
    },
    {
      "epoch": 15.492957746478874,
      "grad_norm": 0.5013020634651184,
      "learning_rate": 0.00016903108964684576,
      "loss": 0.4586,
      "step": 7700
    },
    {
      "epoch": 15.49496981891348,
      "grad_norm": 0.4964025318622589,
      "learning_rate": 0.00016902706509709227,
      "loss": 0.4405,
      "step": 7701
    },
    {
      "epoch": 15.496981891348089,
      "grad_norm": 0.5007280111312866,
      "learning_rate": 0.00016902304054733879,
      "loss": 0.4593,
      "step": 7702
    },
    {
      "epoch": 15.498993963782697,
      "grad_norm": 0.5029367804527283,
      "learning_rate": 0.00016901901599758527,
      "loss": 0.4313,
      "step": 7703
    },
    {
      "epoch": 15.501006036217303,
      "grad_norm": 0.49129578471183777,
      "learning_rate": 0.00016901499144783178,
      "loss": 0.4359,
      "step": 7704
    },
    {
      "epoch": 15.503018108651911,
      "grad_norm": 0.48671582341194153,
      "learning_rate": 0.0001690109668980783,
      "loss": 0.4494,
      "step": 7705
    },
    {
      "epoch": 15.50503018108652,
      "grad_norm": 0.5164615511894226,
      "learning_rate": 0.00016900694234832478,
      "loss": 0.4538,
      "step": 7706
    },
    {
      "epoch": 15.507042253521126,
      "grad_norm": 0.5075557827949524,
      "learning_rate": 0.0001690029177985713,
      "loss": 0.4157,
      "step": 7707
    },
    {
      "epoch": 15.509054325955734,
      "grad_norm": 0.49878939986228943,
      "learning_rate": 0.00016899889324881778,
      "loss": 0.4372,
      "step": 7708
    },
    {
      "epoch": 15.511066398390343,
      "grad_norm": 0.48781755566596985,
      "learning_rate": 0.0001689948686990643,
      "loss": 0.4091,
      "step": 7709
    },
    {
      "epoch": 15.513078470824949,
      "grad_norm": 0.5421662330627441,
      "learning_rate": 0.0001689908441493108,
      "loss": 0.5041,
      "step": 7710
    },
    {
      "epoch": 15.515090543259557,
      "grad_norm": 0.5159633755683899,
      "learning_rate": 0.00016898681959955731,
      "loss": 0.4548,
      "step": 7711
    },
    {
      "epoch": 15.517102615694165,
      "grad_norm": 0.5004734992980957,
      "learning_rate": 0.0001689827950498038,
      "loss": 0.4446,
      "step": 7712
    },
    {
      "epoch": 15.519114688128772,
      "grad_norm": 0.548531174659729,
      "learning_rate": 0.0001689787705000503,
      "loss": 0.4433,
      "step": 7713
    },
    {
      "epoch": 15.52112676056338,
      "grad_norm": 0.5099679231643677,
      "learning_rate": 0.0001689747459502968,
      "loss": 0.4196,
      "step": 7714
    },
    {
      "epoch": 15.523138832997988,
      "grad_norm": 0.5153651833534241,
      "learning_rate": 0.00016897072140054334,
      "loss": 0.4488,
      "step": 7715
    },
    {
      "epoch": 15.525150905432596,
      "grad_norm": 0.48603060841560364,
      "learning_rate": 0.00016896669685078982,
      "loss": 0.4552,
      "step": 7716
    },
    {
      "epoch": 15.527162977867203,
      "grad_norm": 0.5198943018913269,
      "learning_rate": 0.00016896267230103633,
      "loss": 0.4439,
      "step": 7717
    },
    {
      "epoch": 15.529175050301811,
      "grad_norm": 0.520496129989624,
      "learning_rate": 0.00016895864775128282,
      "loss": 0.4555,
      "step": 7718
    },
    {
      "epoch": 15.53118712273642,
      "grad_norm": 0.49143433570861816,
      "learning_rate": 0.00016895462320152933,
      "loss": 0.4518,
      "step": 7719
    },
    {
      "epoch": 15.533199195171026,
      "grad_norm": 0.5043001174926758,
      "learning_rate": 0.00016895059865177584,
      "loss": 0.4586,
      "step": 7720
    },
    {
      "epoch": 15.535211267605634,
      "grad_norm": 0.5014125108718872,
      "learning_rate": 0.00016894657410202236,
      "loss": 0.4565,
      "step": 7721
    },
    {
      "epoch": 15.537223340040242,
      "grad_norm": 0.5309052467346191,
      "learning_rate": 0.00016894254955226884,
      "loss": 0.4674,
      "step": 7722
    },
    {
      "epoch": 15.539235412474849,
      "grad_norm": 0.5296851992607117,
      "learning_rate": 0.00016893852500251535,
      "loss": 0.4881,
      "step": 7723
    },
    {
      "epoch": 15.541247484909457,
      "grad_norm": 0.513879120349884,
      "learning_rate": 0.00016893450045276184,
      "loss": 0.4917,
      "step": 7724
    },
    {
      "epoch": 15.543259557344065,
      "grad_norm": 0.48949262499809265,
      "learning_rate": 0.00016893047590300838,
      "loss": 0.4346,
      "step": 7725
    },
    {
      "epoch": 15.545271629778671,
      "grad_norm": 0.4983674883842468,
      "learning_rate": 0.00016892645135325486,
      "loss": 0.433,
      "step": 7726
    },
    {
      "epoch": 15.54728370221328,
      "grad_norm": 0.5015702247619629,
      "learning_rate": 0.00016892242680350137,
      "loss": 0.428,
      "step": 7727
    },
    {
      "epoch": 15.549295774647888,
      "grad_norm": 0.556959867477417,
      "learning_rate": 0.00016891840225374786,
      "loss": 0.4725,
      "step": 7728
    },
    {
      "epoch": 15.551307847082494,
      "grad_norm": 0.48436403274536133,
      "learning_rate": 0.00016891437770399437,
      "loss": 0.4326,
      "step": 7729
    },
    {
      "epoch": 15.553319919517103,
      "grad_norm": 0.5026112198829651,
      "learning_rate": 0.00016891035315424088,
      "loss": 0.4309,
      "step": 7730
    },
    {
      "epoch": 15.55533199195171,
      "grad_norm": 0.5059593319892883,
      "learning_rate": 0.0001689063286044874,
      "loss": 0.4685,
      "step": 7731
    },
    {
      "epoch": 15.557344064386317,
      "grad_norm": 0.5274390578269958,
      "learning_rate": 0.00016890230405473388,
      "loss": 0.5005,
      "step": 7732
    },
    {
      "epoch": 15.559356136820925,
      "grad_norm": 0.49017512798309326,
      "learning_rate": 0.0001688982795049804,
      "loss": 0.4676,
      "step": 7733
    },
    {
      "epoch": 15.561368209255534,
      "grad_norm": 0.4949088394641876,
      "learning_rate": 0.00016889425495522688,
      "loss": 0.4288,
      "step": 7734
    },
    {
      "epoch": 15.56338028169014,
      "grad_norm": 0.48037511110305786,
      "learning_rate": 0.0001688902304054734,
      "loss": 0.4401,
      "step": 7735
    },
    {
      "epoch": 15.565392354124748,
      "grad_norm": 0.5319424867630005,
      "learning_rate": 0.0001688862058557199,
      "loss": 0.4547,
      "step": 7736
    },
    {
      "epoch": 15.567404426559357,
      "grad_norm": 0.5182598829269409,
      "learning_rate": 0.00016888218130596642,
      "loss": 0.464,
      "step": 7737
    },
    {
      "epoch": 15.569416498993963,
      "grad_norm": 0.47555992007255554,
      "learning_rate": 0.0001688781567562129,
      "loss": 0.4359,
      "step": 7738
    },
    {
      "epoch": 15.571428571428571,
      "grad_norm": 0.49169453978538513,
      "learning_rate": 0.0001688741322064594,
      "loss": 0.4212,
      "step": 7739
    },
    {
      "epoch": 15.57344064386318,
      "grad_norm": 0.5084099769592285,
      "learning_rate": 0.00016887010765670593,
      "loss": 0.4517,
      "step": 7740
    },
    {
      "epoch": 15.575452716297786,
      "grad_norm": 0.5177004933357239,
      "learning_rate": 0.0001688660831069524,
      "loss": 0.4474,
      "step": 7741
    },
    {
      "epoch": 15.577464788732394,
      "grad_norm": 0.5298421382904053,
      "learning_rate": 0.00016886205855719892,
      "loss": 0.4815,
      "step": 7742
    },
    {
      "epoch": 15.579476861167002,
      "grad_norm": 0.4916333556175232,
      "learning_rate": 0.0001688580340074454,
      "loss": 0.4689,
      "step": 7743
    },
    {
      "epoch": 15.58148893360161,
      "grad_norm": 0.5203611850738525,
      "learning_rate": 0.00016885400945769192,
      "loss": 0.4628,
      "step": 7744
    },
    {
      "epoch": 15.583501006036217,
      "grad_norm": 0.5166781544685364,
      "learning_rate": 0.00016884998490793843,
      "loss": 0.4389,
      "step": 7745
    },
    {
      "epoch": 15.585513078470825,
      "grad_norm": 0.5155678391456604,
      "learning_rate": 0.00016884596035818494,
      "loss": 0.4724,
      "step": 7746
    },
    {
      "epoch": 15.587525150905433,
      "grad_norm": 0.5285922884941101,
      "learning_rate": 0.00016884193580843143,
      "loss": 0.4409,
      "step": 7747
    },
    {
      "epoch": 15.58953722334004,
      "grad_norm": 0.5117175579071045,
      "learning_rate": 0.00016883791125867794,
      "loss": 0.466,
      "step": 7748
    },
    {
      "epoch": 15.591549295774648,
      "grad_norm": 0.5220500230789185,
      "learning_rate": 0.00016883388670892443,
      "loss": 0.4319,
      "step": 7749
    },
    {
      "epoch": 15.593561368209256,
      "grad_norm": 0.5030242204666138,
      "learning_rate": 0.00016882986215917097,
      "loss": 0.4733,
      "step": 7750
    },
    {
      "epoch": 15.595573440643863,
      "grad_norm": 0.5736940503120422,
      "learning_rate": 0.00016882583760941745,
      "loss": 0.464,
      "step": 7751
    },
    {
      "epoch": 15.59758551307847,
      "grad_norm": 0.476451575756073,
      "learning_rate": 0.00016882181305966396,
      "loss": 0.4259,
      "step": 7752
    },
    {
      "epoch": 15.599597585513079,
      "grad_norm": 0.5278083682060242,
      "learning_rate": 0.00016881778850991045,
      "loss": 0.4702,
      "step": 7753
    },
    {
      "epoch": 15.601609657947686,
      "grad_norm": 0.5228379368782043,
      "learning_rate": 0.00016881376396015696,
      "loss": 0.4855,
      "step": 7754
    },
    {
      "epoch": 15.603621730382294,
      "grad_norm": 0.4989911913871765,
      "learning_rate": 0.00016880973941040347,
      "loss": 0.4439,
      "step": 7755
    },
    {
      "epoch": 15.605633802816902,
      "grad_norm": 0.5198857188224792,
      "learning_rate": 0.00016880571486064999,
      "loss": 0.4552,
      "step": 7756
    },
    {
      "epoch": 15.607645875251508,
      "grad_norm": 0.5093660354614258,
      "learning_rate": 0.00016880169031089647,
      "loss": 0.4821,
      "step": 7757
    },
    {
      "epoch": 15.609657947686117,
      "grad_norm": 0.49330300092697144,
      "learning_rate": 0.00016879766576114298,
      "loss": 0.4661,
      "step": 7758
    },
    {
      "epoch": 15.611670020120725,
      "grad_norm": 0.5141468048095703,
      "learning_rate": 0.00016879364121138947,
      "loss": 0.4404,
      "step": 7759
    },
    {
      "epoch": 15.613682092555331,
      "grad_norm": 0.5126877427101135,
      "learning_rate": 0.000168789616661636,
      "loss": 0.4821,
      "step": 7760
    },
    {
      "epoch": 15.61569416498994,
      "grad_norm": 0.520789623260498,
      "learning_rate": 0.0001687855921118825,
      "loss": 0.4548,
      "step": 7761
    },
    {
      "epoch": 15.617706237424548,
      "grad_norm": 0.5360652804374695,
      "learning_rate": 0.000168781567562129,
      "loss": 0.4851,
      "step": 7762
    },
    {
      "epoch": 15.619718309859154,
      "grad_norm": 0.5458500981330872,
      "learning_rate": 0.0001687775430123755,
      "loss": 0.4403,
      "step": 7763
    },
    {
      "epoch": 15.621730382293762,
      "grad_norm": 0.5408787131309509,
      "learning_rate": 0.000168773518462622,
      "loss": 0.4243,
      "step": 7764
    },
    {
      "epoch": 15.62374245472837,
      "grad_norm": 0.5187947750091553,
      "learning_rate": 0.00016876949391286851,
      "loss": 0.4435,
      "step": 7765
    },
    {
      "epoch": 15.625754527162979,
      "grad_norm": 0.517946720123291,
      "learning_rate": 0.00016876546936311503,
      "loss": 0.4395,
      "step": 7766
    },
    {
      "epoch": 15.627766599597585,
      "grad_norm": 0.523222029209137,
      "learning_rate": 0.0001687614448133615,
      "loss": 0.4886,
      "step": 7767
    },
    {
      "epoch": 15.629778672032193,
      "grad_norm": 0.5116997361183167,
      "learning_rate": 0.00016875742026360802,
      "loss": 0.5027,
      "step": 7768
    },
    {
      "epoch": 15.631790744466802,
      "grad_norm": 0.5394707322120667,
      "learning_rate": 0.0001687533957138545,
      "loss": 0.4697,
      "step": 7769
    },
    {
      "epoch": 15.633802816901408,
      "grad_norm": 0.5013561248779297,
      "learning_rate": 0.00016874937116410102,
      "loss": 0.4758,
      "step": 7770
    },
    {
      "epoch": 15.635814889336016,
      "grad_norm": 0.5128323435783386,
      "learning_rate": 0.00016874534661434753,
      "loss": 0.4502,
      "step": 7771
    },
    {
      "epoch": 15.637826961770624,
      "grad_norm": 0.508775532245636,
      "learning_rate": 0.00016874132206459404,
      "loss": 0.476,
      "step": 7772
    },
    {
      "epoch": 15.639839034205231,
      "grad_norm": 0.5186518430709839,
      "learning_rate": 0.00016873729751484053,
      "loss": 0.4422,
      "step": 7773
    },
    {
      "epoch": 15.64185110663984,
      "grad_norm": 0.5190256834030151,
      "learning_rate": 0.00016873327296508704,
      "loss": 0.4347,
      "step": 7774
    },
    {
      "epoch": 15.643863179074447,
      "grad_norm": 0.512051522731781,
      "learning_rate": 0.00016872924841533355,
      "loss": 0.4292,
      "step": 7775
    },
    {
      "epoch": 15.645875251509054,
      "grad_norm": 0.5234336256980896,
      "learning_rate": 0.00016872522386558004,
      "loss": 0.4657,
      "step": 7776
    },
    {
      "epoch": 15.647887323943662,
      "grad_norm": 0.5114213228225708,
      "learning_rate": 0.00016872119931582655,
      "loss": 0.4211,
      "step": 7777
    },
    {
      "epoch": 15.64989939637827,
      "grad_norm": 0.4918782413005829,
      "learning_rate": 0.00016871717476607304,
      "loss": 0.4262,
      "step": 7778
    },
    {
      "epoch": 15.651911468812877,
      "grad_norm": 0.49652329087257385,
      "learning_rate": 0.00016871315021631955,
      "loss": 0.4745,
      "step": 7779
    },
    {
      "epoch": 15.653923541247485,
      "grad_norm": 0.5044300556182861,
      "learning_rate": 0.00016870912566656606,
      "loss": 0.5007,
      "step": 7780
    },
    {
      "epoch": 15.655935613682093,
      "grad_norm": 0.5291843414306641,
      "learning_rate": 0.00016870510111681257,
      "loss": 0.4441,
      "step": 7781
    },
    {
      "epoch": 15.6579476861167,
      "grad_norm": 0.49083277583122253,
      "learning_rate": 0.00016870107656705906,
      "loss": 0.4129,
      "step": 7782
    },
    {
      "epoch": 15.659959758551308,
      "grad_norm": 0.493310809135437,
      "learning_rate": 0.00016869705201730557,
      "loss": 0.4214,
      "step": 7783
    },
    {
      "epoch": 15.661971830985916,
      "grad_norm": 0.5019686818122864,
      "learning_rate": 0.00016869302746755206,
      "loss": 0.4322,
      "step": 7784
    },
    {
      "epoch": 15.663983903420522,
      "grad_norm": 0.5058107376098633,
      "learning_rate": 0.0001686890029177986,
      "loss": 0.4724,
      "step": 7785
    },
    {
      "epoch": 15.66599597585513,
      "grad_norm": 0.5099115967750549,
      "learning_rate": 0.00016868497836804508,
      "loss": 0.4542,
      "step": 7786
    },
    {
      "epoch": 15.668008048289739,
      "grad_norm": 0.495449423789978,
      "learning_rate": 0.0001686809538182916,
      "loss": 0.4731,
      "step": 7787
    },
    {
      "epoch": 15.670020120724345,
      "grad_norm": 0.5211751461029053,
      "learning_rate": 0.00016867692926853808,
      "loss": 0.4795,
      "step": 7788
    },
    {
      "epoch": 15.672032193158953,
      "grad_norm": 0.5103089809417725,
      "learning_rate": 0.0001686729047187846,
      "loss": 0.4382,
      "step": 7789
    },
    {
      "epoch": 15.674044265593562,
      "grad_norm": 0.4811759293079376,
      "learning_rate": 0.0001686688801690311,
      "loss": 0.4335,
      "step": 7790
    },
    {
      "epoch": 15.676056338028168,
      "grad_norm": 0.4830407500267029,
      "learning_rate": 0.00016866485561927761,
      "loss": 0.4493,
      "step": 7791
    },
    {
      "epoch": 15.678068410462776,
      "grad_norm": 0.512697160243988,
      "learning_rate": 0.0001686608310695241,
      "loss": 0.435,
      "step": 7792
    },
    {
      "epoch": 15.680080482897385,
      "grad_norm": 0.4855569899082184,
      "learning_rate": 0.0001686568065197706,
      "loss": 0.4377,
      "step": 7793
    },
    {
      "epoch": 15.682092555331993,
      "grad_norm": 0.5160130262374878,
      "learning_rate": 0.0001686527819700171,
      "loss": 0.4483,
      "step": 7794
    },
    {
      "epoch": 15.6841046277666,
      "grad_norm": 0.5693573355674744,
      "learning_rate": 0.00016864875742026364,
      "loss": 0.4778,
      "step": 7795
    },
    {
      "epoch": 15.686116700201207,
      "grad_norm": 0.5464813709259033,
      "learning_rate": 0.00016864473287051012,
      "loss": 0.4563,
      "step": 7796
    },
    {
      "epoch": 15.688128772635816,
      "grad_norm": 0.5116639733314514,
      "learning_rate": 0.00016864070832075663,
      "loss": 0.4768,
      "step": 7797
    },
    {
      "epoch": 15.690140845070422,
      "grad_norm": 0.5082698464393616,
      "learning_rate": 0.00016863668377100312,
      "loss": 0.4567,
      "step": 7798
    },
    {
      "epoch": 15.69215291750503,
      "grad_norm": 0.5105946660041809,
      "learning_rate": 0.00016863265922124963,
      "loss": 0.457,
      "step": 7799
    },
    {
      "epoch": 15.694164989939638,
      "grad_norm": 0.5781680345535278,
      "learning_rate": 0.00016862863467149614,
      "loss": 0.4403,
      "step": 7800
    },
    {
      "epoch": 15.696177062374245,
      "grad_norm": 0.5024398565292358,
      "learning_rate": 0.00016862461012174266,
      "loss": 0.4458,
      "step": 7801
    },
    {
      "epoch": 15.698189134808853,
      "grad_norm": 0.5317608118057251,
      "learning_rate": 0.00016862058557198914,
      "loss": 0.408,
      "step": 7802
    },
    {
      "epoch": 15.700201207243461,
      "grad_norm": 0.5403968095779419,
      "learning_rate": 0.00016861656102223565,
      "loss": 0.449,
      "step": 7803
    },
    {
      "epoch": 15.702213279678068,
      "grad_norm": 0.5389165878295898,
      "learning_rate": 0.00016861253647248214,
      "loss": 0.4976,
      "step": 7804
    },
    {
      "epoch": 15.704225352112676,
      "grad_norm": 0.5113919973373413,
      "learning_rate": 0.00016860851192272865,
      "loss": 0.4483,
      "step": 7805
    },
    {
      "epoch": 15.706237424547284,
      "grad_norm": 0.5672174692153931,
      "learning_rate": 0.00016860448737297516,
      "loss": 0.4805,
      "step": 7806
    },
    {
      "epoch": 15.70824949698189,
      "grad_norm": 0.493490070104599,
      "learning_rate": 0.00016860046282322167,
      "loss": 0.4283,
      "step": 7807
    },
    {
      "epoch": 15.710261569416499,
      "grad_norm": 0.5158017873764038,
      "learning_rate": 0.00016859643827346816,
      "loss": 0.4491,
      "step": 7808
    },
    {
      "epoch": 15.712273641851107,
      "grad_norm": 0.5312477350234985,
      "learning_rate": 0.00016859241372371467,
      "loss": 0.4451,
      "step": 7809
    },
    {
      "epoch": 15.714285714285714,
      "grad_norm": 0.47378286719322205,
      "learning_rate": 0.00016858838917396118,
      "loss": 0.4334,
      "step": 7810
    },
    {
      "epoch": 15.716297786720322,
      "grad_norm": 0.5098953247070312,
      "learning_rate": 0.00016858436462420767,
      "loss": 0.4957,
      "step": 7811
    },
    {
      "epoch": 15.71830985915493,
      "grad_norm": 0.49639666080474854,
      "learning_rate": 0.00016858034007445418,
      "loss": 0.4761,
      "step": 7812
    },
    {
      "epoch": 15.720321931589538,
      "grad_norm": 0.5330116152763367,
      "learning_rate": 0.00016857631552470067,
      "loss": 0.4462,
      "step": 7813
    },
    {
      "epoch": 15.722334004024145,
      "grad_norm": 0.5129598379135132,
      "learning_rate": 0.00016857229097494718,
      "loss": 0.4497,
      "step": 7814
    },
    {
      "epoch": 15.724346076458753,
      "grad_norm": 0.4867461621761322,
      "learning_rate": 0.0001685682664251937,
      "loss": 0.4454,
      "step": 7815
    },
    {
      "epoch": 15.726358148893361,
      "grad_norm": 0.5024351477622986,
      "learning_rate": 0.0001685642418754402,
      "loss": 0.456,
      "step": 7816
    },
    {
      "epoch": 15.728370221327967,
      "grad_norm": 0.5033055543899536,
      "learning_rate": 0.0001685602173256867,
      "loss": 0.4393,
      "step": 7817
    },
    {
      "epoch": 15.730382293762576,
      "grad_norm": 0.5003187656402588,
      "learning_rate": 0.0001685561927759332,
      "loss": 0.4437,
      "step": 7818
    },
    {
      "epoch": 15.732394366197184,
      "grad_norm": 0.5030065178871155,
      "learning_rate": 0.00016855216822617969,
      "loss": 0.4675,
      "step": 7819
    },
    {
      "epoch": 15.73440643863179,
      "grad_norm": 0.5111952424049377,
      "learning_rate": 0.00016854814367642622,
      "loss": 0.4733,
      "step": 7820
    },
    {
      "epoch": 15.736418511066399,
      "grad_norm": 0.5297138094902039,
      "learning_rate": 0.0001685441191266727,
      "loss": 0.4874,
      "step": 7821
    },
    {
      "epoch": 15.738430583501007,
      "grad_norm": 0.5007362961769104,
      "learning_rate": 0.00016854009457691922,
      "loss": 0.4577,
      "step": 7822
    },
    {
      "epoch": 15.740442655935613,
      "grad_norm": 0.5000082850456238,
      "learning_rate": 0.0001685360700271657,
      "loss": 0.4433,
      "step": 7823
    },
    {
      "epoch": 15.742454728370221,
      "grad_norm": 0.49520137906074524,
      "learning_rate": 0.00016853204547741222,
      "loss": 0.4632,
      "step": 7824
    },
    {
      "epoch": 15.74446680080483,
      "grad_norm": 0.512241005897522,
      "learning_rate": 0.00016852802092765873,
      "loss": 0.4326,
      "step": 7825
    },
    {
      "epoch": 15.746478873239436,
      "grad_norm": 0.5421081185340881,
      "learning_rate": 0.00016852399637790524,
      "loss": 0.4362,
      "step": 7826
    },
    {
      "epoch": 15.748490945674044,
      "grad_norm": 0.5132197737693787,
      "learning_rate": 0.00016851997182815173,
      "loss": 0.4253,
      "step": 7827
    },
    {
      "epoch": 15.750503018108652,
      "grad_norm": 0.5245163440704346,
      "learning_rate": 0.00016851594727839824,
      "loss": 0.4837,
      "step": 7828
    },
    {
      "epoch": 15.752515090543259,
      "grad_norm": 0.5279178023338318,
      "learning_rate": 0.00016851192272864473,
      "loss": 0.4935,
      "step": 7829
    },
    {
      "epoch": 15.754527162977867,
      "grad_norm": 0.5123958587646484,
      "learning_rate": 0.00016850789817889127,
      "loss": 0.4724,
      "step": 7830
    },
    {
      "epoch": 15.756539235412475,
      "grad_norm": 0.4967256188392639,
      "learning_rate": 0.00016850387362913775,
      "loss": 0.4402,
      "step": 7831
    },
    {
      "epoch": 15.758551307847082,
      "grad_norm": 0.5188573598861694,
      "learning_rate": 0.00016849984907938426,
      "loss": 0.459,
      "step": 7832
    },
    {
      "epoch": 15.76056338028169,
      "grad_norm": 0.5040131211280823,
      "learning_rate": 0.00016849582452963075,
      "loss": 0.4686,
      "step": 7833
    },
    {
      "epoch": 15.762575452716298,
      "grad_norm": 0.5263273119926453,
      "learning_rate": 0.00016849179997987726,
      "loss": 0.4707,
      "step": 7834
    },
    {
      "epoch": 15.764587525150905,
      "grad_norm": 0.513550877571106,
      "learning_rate": 0.00016848777543012377,
      "loss": 0.4874,
      "step": 7835
    },
    {
      "epoch": 15.766599597585513,
      "grad_norm": 0.5110759735107422,
      "learning_rate": 0.00016848375088037028,
      "loss": 0.4481,
      "step": 7836
    },
    {
      "epoch": 15.768611670020121,
      "grad_norm": 0.5071537494659424,
      "learning_rate": 0.00016847972633061677,
      "loss": 0.4665,
      "step": 7837
    },
    {
      "epoch": 15.770623742454728,
      "grad_norm": 0.516696572303772,
      "learning_rate": 0.00016847570178086328,
      "loss": 0.4861,
      "step": 7838
    },
    {
      "epoch": 15.772635814889336,
      "grad_norm": 0.5445382595062256,
      "learning_rate": 0.00016847167723110977,
      "loss": 0.4988,
      "step": 7839
    },
    {
      "epoch": 15.774647887323944,
      "grad_norm": 0.48999103903770447,
      "learning_rate": 0.00016846765268135628,
      "loss": 0.4497,
      "step": 7840
    },
    {
      "epoch": 15.77665995975855,
      "grad_norm": 0.5495129823684692,
      "learning_rate": 0.0001684636281316028,
      "loss": 0.491,
      "step": 7841
    },
    {
      "epoch": 15.778672032193159,
      "grad_norm": 0.5029159188270569,
      "learning_rate": 0.00016845960358184928,
      "loss": 0.4449,
      "step": 7842
    },
    {
      "epoch": 15.780684104627767,
      "grad_norm": 0.5131246447563171,
      "learning_rate": 0.0001684555790320958,
      "loss": 0.451,
      "step": 7843
    },
    {
      "epoch": 15.782696177062375,
      "grad_norm": 0.49343857169151306,
      "learning_rate": 0.0001684515544823423,
      "loss": 0.4743,
      "step": 7844
    },
    {
      "epoch": 15.784708249496981,
      "grad_norm": 0.5439685583114624,
      "learning_rate": 0.0001684475299325888,
      "loss": 0.4353,
      "step": 7845
    },
    {
      "epoch": 15.78672032193159,
      "grad_norm": 0.510363757610321,
      "learning_rate": 0.0001684435053828353,
      "loss": 0.4862,
      "step": 7846
    },
    {
      "epoch": 15.788732394366198,
      "grad_norm": 0.507755696773529,
      "learning_rate": 0.0001684394808330818,
      "loss": 0.4712,
      "step": 7847
    },
    {
      "epoch": 15.790744466800804,
      "grad_norm": 0.5276859998703003,
      "learning_rate": 0.0001684354562833283,
      "loss": 0.4811,
      "step": 7848
    },
    {
      "epoch": 15.792756539235413,
      "grad_norm": 0.49279117584228516,
      "learning_rate": 0.0001684314317335748,
      "loss": 0.4944,
      "step": 7849
    },
    {
      "epoch": 15.79476861167002,
      "grad_norm": 0.5122255682945251,
      "learning_rate": 0.00016842740718382132,
      "loss": 0.4754,
      "step": 7850
    },
    {
      "epoch": 15.796780684104627,
      "grad_norm": 0.49167317152023315,
      "learning_rate": 0.00016842338263406783,
      "loss": 0.4406,
      "step": 7851
    },
    {
      "epoch": 15.798792756539235,
      "grad_norm": 0.5002931356430054,
      "learning_rate": 0.00016841935808431432,
      "loss": 0.4643,
      "step": 7852
    },
    {
      "epoch": 15.800804828973844,
      "grad_norm": 0.475087434053421,
      "learning_rate": 0.00016841533353456083,
      "loss": 0.4677,
      "step": 7853
    },
    {
      "epoch": 15.80281690140845,
      "grad_norm": 0.5278864502906799,
      "learning_rate": 0.00016841130898480731,
      "loss": 0.4319,
      "step": 7854
    },
    {
      "epoch": 15.804828973843058,
      "grad_norm": 0.48231571912765503,
      "learning_rate": 0.00016840728443505385,
      "loss": 0.4204,
      "step": 7855
    },
    {
      "epoch": 15.806841046277667,
      "grad_norm": 0.5104852914810181,
      "learning_rate": 0.00016840325988530034,
      "loss": 0.4774,
      "step": 7856
    },
    {
      "epoch": 15.808853118712273,
      "grad_norm": 0.4889902174472809,
      "learning_rate": 0.00016839923533554685,
      "loss": 0.437,
      "step": 7857
    },
    {
      "epoch": 15.810865191146881,
      "grad_norm": 0.5137041807174683,
      "learning_rate": 0.00016839521078579334,
      "loss": 0.4649,
      "step": 7858
    },
    {
      "epoch": 15.81287726358149,
      "grad_norm": 0.5144931674003601,
      "learning_rate": 0.00016839118623603985,
      "loss": 0.419,
      "step": 7859
    },
    {
      "epoch": 15.814889336016096,
      "grad_norm": 0.5081955194473267,
      "learning_rate": 0.00016838716168628636,
      "loss": 0.4691,
      "step": 7860
    },
    {
      "epoch": 15.816901408450704,
      "grad_norm": 0.4946002960205078,
      "learning_rate": 0.00016838313713653287,
      "loss": 0.4417,
      "step": 7861
    },
    {
      "epoch": 15.818913480885312,
      "grad_norm": 0.5006065964698792,
      "learning_rate": 0.00016837911258677936,
      "loss": 0.4588,
      "step": 7862
    },
    {
      "epoch": 15.82092555331992,
      "grad_norm": 0.5076594948768616,
      "learning_rate": 0.00016837508803702587,
      "loss": 0.4622,
      "step": 7863
    },
    {
      "epoch": 15.822937625754527,
      "grad_norm": 0.49840596318244934,
      "learning_rate": 0.00016837106348727236,
      "loss": 0.4448,
      "step": 7864
    },
    {
      "epoch": 15.824949698189135,
      "grad_norm": 0.53608238697052,
      "learning_rate": 0.0001683670389375189,
      "loss": 0.4851,
      "step": 7865
    },
    {
      "epoch": 15.826961770623743,
      "grad_norm": 0.4942934811115265,
      "learning_rate": 0.00016836301438776538,
      "loss": 0.4481,
      "step": 7866
    },
    {
      "epoch": 15.82897384305835,
      "grad_norm": 0.5264964699745178,
      "learning_rate": 0.0001683589898380119,
      "loss": 0.4475,
      "step": 7867
    },
    {
      "epoch": 15.830985915492958,
      "grad_norm": 0.5158607363700867,
      "learning_rate": 0.00016835496528825838,
      "loss": 0.4561,
      "step": 7868
    },
    {
      "epoch": 15.832997987927566,
      "grad_norm": 0.524071216583252,
      "learning_rate": 0.0001683509407385049,
      "loss": 0.4706,
      "step": 7869
    },
    {
      "epoch": 15.835010060362173,
      "grad_norm": 0.5143025517463684,
      "learning_rate": 0.0001683469161887514,
      "loss": 0.4462,
      "step": 7870
    },
    {
      "epoch": 15.83702213279678,
      "grad_norm": 0.48047712445259094,
      "learning_rate": 0.00016834289163899791,
      "loss": 0.4435,
      "step": 7871
    },
    {
      "epoch": 15.839034205231389,
      "grad_norm": 0.5226343870162964,
      "learning_rate": 0.0001683388670892444,
      "loss": 0.4544,
      "step": 7872
    },
    {
      "epoch": 15.841046277665995,
      "grad_norm": 0.5158110857009888,
      "learning_rate": 0.0001683348425394909,
      "loss": 0.4427,
      "step": 7873
    },
    {
      "epoch": 15.843058350100604,
      "grad_norm": 0.5338292121887207,
      "learning_rate": 0.0001683308179897374,
      "loss": 0.4634,
      "step": 7874
    },
    {
      "epoch": 15.845070422535212,
      "grad_norm": 0.5078652501106262,
      "learning_rate": 0.0001683267934399839,
      "loss": 0.4811,
      "step": 7875
    },
    {
      "epoch": 15.847082494969818,
      "grad_norm": 0.5138689279556274,
      "learning_rate": 0.00016832276889023042,
      "loss": 0.4267,
      "step": 7876
    },
    {
      "epoch": 15.849094567404427,
      "grad_norm": 0.4959292709827423,
      "learning_rate": 0.0001683187443404769,
      "loss": 0.4274,
      "step": 7877
    },
    {
      "epoch": 15.851106639839035,
      "grad_norm": 0.5224047303199768,
      "learning_rate": 0.00016831471979072342,
      "loss": 0.4682,
      "step": 7878
    },
    {
      "epoch": 15.853118712273641,
      "grad_norm": 0.5308578014373779,
      "learning_rate": 0.00016831069524096993,
      "loss": 0.4756,
      "step": 7879
    },
    {
      "epoch": 15.85513078470825,
      "grad_norm": 0.5263919830322266,
      "learning_rate": 0.00016830667069121644,
      "loss": 0.4637,
      "step": 7880
    },
    {
      "epoch": 15.857142857142858,
      "grad_norm": 0.48774078488349915,
      "learning_rate": 0.00016830264614146293,
      "loss": 0.457,
      "step": 7881
    },
    {
      "epoch": 15.859154929577464,
      "grad_norm": 0.48994022607803345,
      "learning_rate": 0.00016829862159170944,
      "loss": 0.4572,
      "step": 7882
    },
    {
      "epoch": 15.861167002012072,
      "grad_norm": 0.4857282042503357,
      "learning_rate": 0.00016829459704195593,
      "loss": 0.453,
      "step": 7883
    },
    {
      "epoch": 15.86317907444668,
      "grad_norm": 0.4845757484436035,
      "learning_rate": 0.00016829057249220244,
      "loss": 0.4665,
      "step": 7884
    },
    {
      "epoch": 15.865191146881287,
      "grad_norm": 0.5068399310112,
      "learning_rate": 0.00016828654794244892,
      "loss": 0.4692,
      "step": 7885
    },
    {
      "epoch": 15.867203219315895,
      "grad_norm": 0.5283025503158569,
      "learning_rate": 0.00016828252339269546,
      "loss": 0.4891,
      "step": 7886
    },
    {
      "epoch": 15.869215291750503,
      "grad_norm": 0.500862181186676,
      "learning_rate": 0.00016827849884294195,
      "loss": 0.4808,
      "step": 7887
    },
    {
      "epoch": 15.87122736418511,
      "grad_norm": 0.519413411617279,
      "learning_rate": 0.00016827447429318846,
      "loss": 0.4945,
      "step": 7888
    },
    {
      "epoch": 15.873239436619718,
      "grad_norm": 0.5004993081092834,
      "learning_rate": 0.00016827044974343494,
      "loss": 0.4756,
      "step": 7889
    },
    {
      "epoch": 15.875251509054326,
      "grad_norm": 0.505955159664154,
      "learning_rate": 0.00016826642519368146,
      "loss": 0.4652,
      "step": 7890
    },
    {
      "epoch": 15.877263581488933,
      "grad_norm": 0.49506065249443054,
      "learning_rate": 0.00016826240064392797,
      "loss": 0.4586,
      "step": 7891
    },
    {
      "epoch": 15.879275653923541,
      "grad_norm": 0.49705636501312256,
      "learning_rate": 0.00016825837609417448,
      "loss": 0.4318,
      "step": 7892
    },
    {
      "epoch": 15.88128772635815,
      "grad_norm": 0.5129302144050598,
      "learning_rate": 0.00016825435154442097,
      "loss": 0.4739,
      "step": 7893
    },
    {
      "epoch": 15.883299798792757,
      "grad_norm": 0.5167077779769897,
      "learning_rate": 0.00016825032699466748,
      "loss": 0.4841,
      "step": 7894
    },
    {
      "epoch": 15.885311871227364,
      "grad_norm": 0.49895063042640686,
      "learning_rate": 0.00016824630244491396,
      "loss": 0.4568,
      "step": 7895
    },
    {
      "epoch": 15.887323943661972,
      "grad_norm": 0.4770817160606384,
      "learning_rate": 0.0001682422778951605,
      "loss": 0.4348,
      "step": 7896
    },
    {
      "epoch": 15.88933601609658,
      "grad_norm": 0.4802795648574829,
      "learning_rate": 0.000168238253345407,
      "loss": 0.4728,
      "step": 7897
    },
    {
      "epoch": 15.891348088531187,
      "grad_norm": 0.5048949122428894,
      "learning_rate": 0.0001682342287956535,
      "loss": 0.4858,
      "step": 7898
    },
    {
      "epoch": 15.893360160965795,
      "grad_norm": 0.4993433654308319,
      "learning_rate": 0.00016823020424589999,
      "loss": 0.4674,
      "step": 7899
    },
    {
      "epoch": 15.895372233400403,
      "grad_norm": 0.5009642839431763,
      "learning_rate": 0.0001682261796961465,
      "loss": 0.4135,
      "step": 7900
    },
    {
      "epoch": 15.89738430583501,
      "grad_norm": 0.48894450068473816,
      "learning_rate": 0.000168222155146393,
      "loss": 0.4501,
      "step": 7901
    },
    {
      "epoch": 15.899396378269618,
      "grad_norm": 0.4988841414451599,
      "learning_rate": 0.00016821813059663952,
      "loss": 0.482,
      "step": 7902
    },
    {
      "epoch": 15.901408450704226,
      "grad_norm": 0.5438306331634521,
      "learning_rate": 0.000168214106046886,
      "loss": 0.4547,
      "step": 7903
    },
    {
      "epoch": 15.903420523138832,
      "grad_norm": 0.4905514419078827,
      "learning_rate": 0.00016821008149713252,
      "loss": 0.437,
      "step": 7904
    },
    {
      "epoch": 15.90543259557344,
      "grad_norm": 0.5120090246200562,
      "learning_rate": 0.000168206056947379,
      "loss": 0.4989,
      "step": 7905
    },
    {
      "epoch": 15.907444668008049,
      "grad_norm": 0.49290338158607483,
      "learning_rate": 0.00016820203239762554,
      "loss": 0.4425,
      "step": 7906
    },
    {
      "epoch": 15.909456740442655,
      "grad_norm": 0.500934362411499,
      "learning_rate": 0.00016819800784787203,
      "loss": 0.4688,
      "step": 7907
    },
    {
      "epoch": 15.911468812877263,
      "grad_norm": 0.495865136384964,
      "learning_rate": 0.00016819398329811854,
      "loss": 0.4449,
      "step": 7908
    },
    {
      "epoch": 15.913480885311872,
      "grad_norm": 0.5498356819152832,
      "learning_rate": 0.00016818995874836503,
      "loss": 0.4472,
      "step": 7909
    },
    {
      "epoch": 15.915492957746478,
      "grad_norm": 0.5004912614822388,
      "learning_rate": 0.00016818593419861154,
      "loss": 0.4349,
      "step": 7910
    },
    {
      "epoch": 15.917505030181086,
      "grad_norm": 0.5001692175865173,
      "learning_rate": 0.00016818190964885805,
      "loss": 0.4479,
      "step": 7911
    },
    {
      "epoch": 15.919517102615695,
      "grad_norm": 0.5037207007408142,
      "learning_rate": 0.00016817788509910454,
      "loss": 0.4868,
      "step": 7912
    },
    {
      "epoch": 15.921529175050303,
      "grad_norm": 0.5092029571533203,
      "learning_rate": 0.00016817386054935105,
      "loss": 0.446,
      "step": 7913
    },
    {
      "epoch": 15.92354124748491,
      "grad_norm": 0.5108270645141602,
      "learning_rate": 0.00016816983599959756,
      "loss": 0.4585,
      "step": 7914
    },
    {
      "epoch": 15.925553319919517,
      "grad_norm": 0.4876290261745453,
      "learning_rate": 0.00016816581144984405,
      "loss": 0.4751,
      "step": 7915
    },
    {
      "epoch": 15.927565392354126,
      "grad_norm": 0.4908892810344696,
      "learning_rate": 0.00016816178690009056,
      "loss": 0.4627,
      "step": 7916
    },
    {
      "epoch": 15.929577464788732,
      "grad_norm": 0.49369993805885315,
      "learning_rate": 0.00016815776235033707,
      "loss": 0.4957,
      "step": 7917
    },
    {
      "epoch": 15.93158953722334,
      "grad_norm": 0.4964750409126282,
      "learning_rate": 0.00016815373780058355,
      "loss": 0.447,
      "step": 7918
    },
    {
      "epoch": 15.933601609657948,
      "grad_norm": 0.5169289112091064,
      "learning_rate": 0.00016814971325083007,
      "loss": 0.4605,
      "step": 7919
    },
    {
      "epoch": 15.935613682092555,
      "grad_norm": 0.5117095708847046,
      "learning_rate": 0.00016814568870107655,
      "loss": 0.4499,
      "step": 7920
    },
    {
      "epoch": 15.937625754527163,
      "grad_norm": 0.5329343676567078,
      "learning_rate": 0.0001681416641513231,
      "loss": 0.5115,
      "step": 7921
    },
    {
      "epoch": 15.939637826961771,
      "grad_norm": 0.4923439919948578,
      "learning_rate": 0.00016813763960156958,
      "loss": 0.4192,
      "step": 7922
    },
    {
      "epoch": 15.941649899396378,
      "grad_norm": 0.5225313901901245,
      "learning_rate": 0.0001681336150518161,
      "loss": 0.4618,
      "step": 7923
    },
    {
      "epoch": 15.943661971830986,
      "grad_norm": 0.5433551669120789,
      "learning_rate": 0.00016812959050206257,
      "loss": 0.4655,
      "step": 7924
    },
    {
      "epoch": 15.945674044265594,
      "grad_norm": 0.4979327917098999,
      "learning_rate": 0.00016812556595230909,
      "loss": 0.4279,
      "step": 7925
    },
    {
      "epoch": 15.9476861167002,
      "grad_norm": 0.51621413230896,
      "learning_rate": 0.0001681215414025556,
      "loss": 0.4711,
      "step": 7926
    },
    {
      "epoch": 15.949698189134809,
      "grad_norm": 0.5084158182144165,
      "learning_rate": 0.0001681175168528021,
      "loss": 0.453,
      "step": 7927
    },
    {
      "epoch": 15.951710261569417,
      "grad_norm": 0.5135614275932312,
      "learning_rate": 0.0001681134923030486,
      "loss": 0.4131,
      "step": 7928
    },
    {
      "epoch": 15.953722334004024,
      "grad_norm": 0.5226110219955444,
      "learning_rate": 0.0001681094677532951,
      "loss": 0.4454,
      "step": 7929
    },
    {
      "epoch": 15.955734406438632,
      "grad_norm": 0.5072236061096191,
      "learning_rate": 0.0001681054432035416,
      "loss": 0.4538,
      "step": 7930
    },
    {
      "epoch": 15.95774647887324,
      "grad_norm": 0.49018731713294983,
      "learning_rate": 0.00016810141865378813,
      "loss": 0.4603,
      "step": 7931
    },
    {
      "epoch": 15.959758551307846,
      "grad_norm": 0.5338764786720276,
      "learning_rate": 0.00016809739410403462,
      "loss": 0.4808,
      "step": 7932
    },
    {
      "epoch": 15.961770623742455,
      "grad_norm": 0.4976906180381775,
      "learning_rate": 0.00016809336955428113,
      "loss": 0.4298,
      "step": 7933
    },
    {
      "epoch": 15.963782696177063,
      "grad_norm": 0.5118398666381836,
      "learning_rate": 0.00016808934500452761,
      "loss": 0.446,
      "step": 7934
    },
    {
      "epoch": 15.96579476861167,
      "grad_norm": 0.5130042433738708,
      "learning_rate": 0.00016808532045477413,
      "loss": 0.4871,
      "step": 7935
    },
    {
      "epoch": 15.967806841046277,
      "grad_norm": 0.5074149966239929,
      "learning_rate": 0.00016808129590502064,
      "loss": 0.43,
      "step": 7936
    },
    {
      "epoch": 15.969818913480886,
      "grad_norm": 0.5195584893226624,
      "learning_rate": 0.00016807727135526715,
      "loss": 0.4284,
      "step": 7937
    },
    {
      "epoch": 15.971830985915492,
      "grad_norm": 0.4987911581993103,
      "learning_rate": 0.00016807324680551364,
      "loss": 0.4341,
      "step": 7938
    },
    {
      "epoch": 15.9738430583501,
      "grad_norm": 0.48740264773368835,
      "learning_rate": 0.00016806922225576015,
      "loss": 0.4384,
      "step": 7939
    },
    {
      "epoch": 15.975855130784709,
      "grad_norm": 0.5141028165817261,
      "learning_rate": 0.00016806519770600663,
      "loss": 0.4508,
      "step": 7940
    },
    {
      "epoch": 15.977867203219315,
      "grad_norm": 0.4978245198726654,
      "learning_rate": 0.00016806117315625317,
      "loss": 0.4599,
      "step": 7941
    },
    {
      "epoch": 15.979879275653923,
      "grad_norm": 0.5083931088447571,
      "learning_rate": 0.00016805714860649966,
      "loss": 0.4765,
      "step": 7942
    },
    {
      "epoch": 15.981891348088531,
      "grad_norm": 0.5108939409255981,
      "learning_rate": 0.00016805312405674617,
      "loss": 0.4612,
      "step": 7943
    },
    {
      "epoch": 15.98390342052314,
      "grad_norm": 0.47724270820617676,
      "learning_rate": 0.00016804909950699266,
      "loss": 0.4936,
      "step": 7944
    },
    {
      "epoch": 15.985915492957746,
      "grad_norm": 0.49120163917541504,
      "learning_rate": 0.00016804507495723917,
      "loss": 0.4569,
      "step": 7945
    },
    {
      "epoch": 15.987927565392354,
      "grad_norm": 0.4978744685649872,
      "learning_rate": 0.00016804105040748568,
      "loss": 0.4599,
      "step": 7946
    },
    {
      "epoch": 15.989939637826962,
      "grad_norm": 0.5326465368270874,
      "learning_rate": 0.00016803702585773217,
      "loss": 0.4254,
      "step": 7947
    },
    {
      "epoch": 15.991951710261569,
      "grad_norm": 0.5177255272865295,
      "learning_rate": 0.00016803300130797868,
      "loss": 0.4439,
      "step": 7948
    },
    {
      "epoch": 15.993963782696177,
      "grad_norm": 0.5061835646629333,
      "learning_rate": 0.0001680289767582252,
      "loss": 0.4944,
      "step": 7949
    },
    {
      "epoch": 15.995975855130785,
      "grad_norm": 0.5107339024543762,
      "learning_rate": 0.00016802495220847167,
      "loss": 0.4764,
      "step": 7950
    },
    {
      "epoch": 15.997987927565392,
      "grad_norm": 0.48907074332237244,
      "learning_rate": 0.0001680209276587182,
      "loss": 0.4289,
      "step": 7951
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.5576322674751282,
      "learning_rate": 0.0001680169031089647,
      "loss": 0.4523,
      "step": 7952
    },
    {
      "epoch": 16.0,
      "eval_loss": 0.8590829968452454,
      "eval_runtime": 49.8215,
      "eval_samples_per_second": 19.911,
      "eval_steps_per_second": 2.489,
      "step": 7952
    },
    {
      "epoch": 16.002012072434606,
      "grad_norm": 0.4744953513145447,
      "learning_rate": 0.00016801287855921118,
      "loss": 0.4004,
      "step": 7953
    },
    {
      "epoch": 16.004024144869216,
      "grad_norm": 0.45690086483955383,
      "learning_rate": 0.0001680088540094577,
      "loss": 0.4427,
      "step": 7954
    },
    {
      "epoch": 16.006036217303823,
      "grad_norm": 0.47007444500923157,
      "learning_rate": 0.00016800482945970418,
      "loss": 0.3574,
      "step": 7955
    },
    {
      "epoch": 16.00804828973843,
      "grad_norm": 0.5286073088645935,
      "learning_rate": 0.00016800080490995072,
      "loss": 0.3855,
      "step": 7956
    },
    {
      "epoch": 16.01006036217304,
      "grad_norm": 0.5239047408103943,
      "learning_rate": 0.0001679967803601972,
      "loss": 0.3717,
      "step": 7957
    },
    {
      "epoch": 16.012072434607646,
      "grad_norm": 0.5202406048774719,
      "learning_rate": 0.00016799275581044372,
      "loss": 0.4033,
      "step": 7958
    },
    {
      "epoch": 16.014084507042252,
      "grad_norm": 0.6180210113525391,
      "learning_rate": 0.0001679887312606902,
      "loss": 0.4166,
      "step": 7959
    },
    {
      "epoch": 16.016096579476862,
      "grad_norm": 0.5995789766311646,
      "learning_rate": 0.00016798470671093672,
      "loss": 0.3888,
      "step": 7960
    },
    {
      "epoch": 16.01810865191147,
      "grad_norm": 0.5022232532501221,
      "learning_rate": 0.00016798068216118323,
      "loss": 0.3876,
      "step": 7961
    },
    {
      "epoch": 16.020120724346075,
      "grad_norm": 0.5081400871276855,
      "learning_rate": 0.00016797665761142974,
      "loss": 0.3949,
      "step": 7962
    },
    {
      "epoch": 16.022132796780685,
      "grad_norm": 0.4763205051422119,
      "learning_rate": 0.00016797263306167622,
      "loss": 0.3988,
      "step": 7963
    },
    {
      "epoch": 16.02414486921529,
      "grad_norm": 0.502478301525116,
      "learning_rate": 0.00016796860851192274,
      "loss": 0.4113,
      "step": 7964
    },
    {
      "epoch": 16.026156941649898,
      "grad_norm": 0.48953136801719666,
      "learning_rate": 0.00016796458396216922,
      "loss": 0.3955,
      "step": 7965
    },
    {
      "epoch": 16.028169014084508,
      "grad_norm": 0.5003037452697754,
      "learning_rate": 0.00016796055941241576,
      "loss": 0.3714,
      "step": 7966
    },
    {
      "epoch": 16.030181086519114,
      "grad_norm": 0.520625650882721,
      "learning_rate": 0.00016795653486266225,
      "loss": 0.3859,
      "step": 7967
    },
    {
      "epoch": 16.03219315895372,
      "grad_norm": 0.5035104155540466,
      "learning_rate": 0.00016795251031290876,
      "loss": 0.3762,
      "step": 7968
    },
    {
      "epoch": 16.03420523138833,
      "grad_norm": 0.5583209991455078,
      "learning_rate": 0.00016794848576315524,
      "loss": 0.3999,
      "step": 7969
    },
    {
      "epoch": 16.036217303822937,
      "grad_norm": 0.5267172455787659,
      "learning_rate": 0.00016794446121340176,
      "loss": 0.3919,
      "step": 7970
    },
    {
      "epoch": 16.038229376257544,
      "grad_norm": 0.4920119047164917,
      "learning_rate": 0.00016794043666364827,
      "loss": 0.3886,
      "step": 7971
    },
    {
      "epoch": 16.040241448692154,
      "grad_norm": 0.492087721824646,
      "learning_rate": 0.00016793641211389478,
      "loss": 0.3922,
      "step": 7972
    },
    {
      "epoch": 16.04225352112676,
      "grad_norm": 0.5242359638214111,
      "learning_rate": 0.00016793238756414127,
      "loss": 0.3877,
      "step": 7973
    },
    {
      "epoch": 16.044265593561367,
      "grad_norm": 0.4521026313304901,
      "learning_rate": 0.00016792836301438778,
      "loss": 0.3659,
      "step": 7974
    },
    {
      "epoch": 16.046277665995976,
      "grad_norm": 0.5289317965507507,
      "learning_rate": 0.00016792433846463426,
      "loss": 0.4304,
      "step": 7975
    },
    {
      "epoch": 16.048289738430583,
      "grad_norm": 0.5170983076095581,
      "learning_rate": 0.0001679203139148808,
      "loss": 0.3859,
      "step": 7976
    },
    {
      "epoch": 16.050301810865193,
      "grad_norm": 0.4868698716163635,
      "learning_rate": 0.0001679162893651273,
      "loss": 0.3678,
      "step": 7977
    },
    {
      "epoch": 16.0523138832998,
      "grad_norm": 0.5042306184768677,
      "learning_rate": 0.0001679122648153738,
      "loss": 0.4033,
      "step": 7978
    },
    {
      "epoch": 16.054325955734406,
      "grad_norm": 0.4990048408508301,
      "learning_rate": 0.00016790824026562028,
      "loss": 0.3835,
      "step": 7979
    },
    {
      "epoch": 16.056338028169016,
      "grad_norm": 0.5130856037139893,
      "learning_rate": 0.0001679042157158668,
      "loss": 0.3891,
      "step": 7980
    },
    {
      "epoch": 16.058350100603622,
      "grad_norm": 0.503444492816925,
      "learning_rate": 0.0001679001911661133,
      "loss": 0.3966,
      "step": 7981
    },
    {
      "epoch": 16.06036217303823,
      "grad_norm": 0.4983840882778168,
      "learning_rate": 0.0001678961666163598,
      "loss": 0.4135,
      "step": 7982
    },
    {
      "epoch": 16.06237424547284,
      "grad_norm": 0.5116901397705078,
      "learning_rate": 0.0001678921420666063,
      "loss": 0.3957,
      "step": 7983
    },
    {
      "epoch": 16.064386317907445,
      "grad_norm": 0.48933109641075134,
      "learning_rate": 0.0001678881175168528,
      "loss": 0.3954,
      "step": 7984
    },
    {
      "epoch": 16.06639839034205,
      "grad_norm": 0.48865264654159546,
      "learning_rate": 0.0001678840929670993,
      "loss": 0.406,
      "step": 7985
    },
    {
      "epoch": 16.06841046277666,
      "grad_norm": 0.514672577381134,
      "learning_rate": 0.00016788006841734582,
      "loss": 0.4139,
      "step": 7986
    },
    {
      "epoch": 16.070422535211268,
      "grad_norm": 0.4964143931865692,
      "learning_rate": 0.00016787604386759233,
      "loss": 0.4134,
      "step": 7987
    },
    {
      "epoch": 16.072434607645874,
      "grad_norm": 0.522787868976593,
      "learning_rate": 0.00016787201931783881,
      "loss": 0.4124,
      "step": 7988
    },
    {
      "epoch": 16.074446680080484,
      "grad_norm": 0.5478188991546631,
      "learning_rate": 0.00016786799476808533,
      "loss": 0.4301,
      "step": 7989
    },
    {
      "epoch": 16.07645875251509,
      "grad_norm": 0.5013301372528076,
      "learning_rate": 0.0001678639702183318,
      "loss": 0.3827,
      "step": 7990
    },
    {
      "epoch": 16.078470824949697,
      "grad_norm": 0.5253791213035583,
      "learning_rate": 0.00016785994566857835,
      "loss": 0.4145,
      "step": 7991
    },
    {
      "epoch": 16.080482897384307,
      "grad_norm": 0.529547393321991,
      "learning_rate": 0.00016785592111882484,
      "loss": 0.4077,
      "step": 7992
    },
    {
      "epoch": 16.082494969818914,
      "grad_norm": 0.508427083492279,
      "learning_rate": 0.00016785189656907135,
      "loss": 0.3708,
      "step": 7993
    },
    {
      "epoch": 16.08450704225352,
      "grad_norm": 0.5120671987533569,
      "learning_rate": 0.00016784787201931783,
      "loss": 0.3841,
      "step": 7994
    },
    {
      "epoch": 16.08651911468813,
      "grad_norm": 0.5099191069602966,
      "learning_rate": 0.00016784384746956434,
      "loss": 0.3856,
      "step": 7995
    },
    {
      "epoch": 16.088531187122737,
      "grad_norm": 0.49002906680107117,
      "learning_rate": 0.00016783982291981086,
      "loss": 0.4099,
      "step": 7996
    },
    {
      "epoch": 16.090543259557343,
      "grad_norm": 0.5338720679283142,
      "learning_rate": 0.00016783579837005737,
      "loss": 0.3993,
      "step": 7997
    },
    {
      "epoch": 16.092555331991953,
      "grad_norm": 0.49325284361839294,
      "learning_rate": 0.00016783177382030385,
      "loss": 0.3919,
      "step": 7998
    },
    {
      "epoch": 16.09456740442656,
      "grad_norm": 0.5281168818473816,
      "learning_rate": 0.00016782774927055037,
      "loss": 0.4082,
      "step": 7999
    },
    {
      "epoch": 16.096579476861166,
      "grad_norm": 0.5369515419006348,
      "learning_rate": 0.00016782372472079685,
      "loss": 0.4334,
      "step": 8000
    },
    {
      "epoch": 16.098591549295776,
      "grad_norm": 0.5036776065826416,
      "learning_rate": 0.0001678197001710434,
      "loss": 0.4088,
      "step": 8001
    },
    {
      "epoch": 16.100603621730382,
      "grad_norm": 0.5199558734893799,
      "learning_rate": 0.00016781567562128988,
      "loss": 0.4099,
      "step": 8002
    },
    {
      "epoch": 16.10261569416499,
      "grad_norm": 0.5116767883300781,
      "learning_rate": 0.0001678116510715364,
      "loss": 0.4007,
      "step": 8003
    },
    {
      "epoch": 16.1046277665996,
      "grad_norm": 0.5132052898406982,
      "learning_rate": 0.00016780762652178287,
      "loss": 0.4055,
      "step": 8004
    },
    {
      "epoch": 16.106639839034205,
      "grad_norm": 0.5452735424041748,
      "learning_rate": 0.00016780360197202939,
      "loss": 0.3896,
      "step": 8005
    },
    {
      "epoch": 16.10865191146881,
      "grad_norm": 0.5138320922851562,
      "learning_rate": 0.0001677995774222759,
      "loss": 0.4237,
      "step": 8006
    },
    {
      "epoch": 16.11066398390342,
      "grad_norm": 0.4881821870803833,
      "learning_rate": 0.0001677955528725224,
      "loss": 0.3917,
      "step": 8007
    },
    {
      "epoch": 16.112676056338028,
      "grad_norm": 0.5496751070022583,
      "learning_rate": 0.0001677915283227689,
      "loss": 0.3943,
      "step": 8008
    },
    {
      "epoch": 16.114688128772634,
      "grad_norm": 0.5023056268692017,
      "learning_rate": 0.0001677875037730154,
      "loss": 0.3507,
      "step": 8009
    },
    {
      "epoch": 16.116700201207244,
      "grad_norm": 0.5711005926132202,
      "learning_rate": 0.0001677834792232619,
      "loss": 0.4144,
      "step": 8010
    },
    {
      "epoch": 16.11871227364185,
      "grad_norm": 0.5225479006767273,
      "learning_rate": 0.0001677794546735084,
      "loss": 0.4029,
      "step": 8011
    },
    {
      "epoch": 16.120724346076457,
      "grad_norm": 0.5359032154083252,
      "learning_rate": 0.00016777543012375492,
      "loss": 0.412,
      "step": 8012
    },
    {
      "epoch": 16.122736418511067,
      "grad_norm": 0.5492992401123047,
      "learning_rate": 0.00016777140557400143,
      "loss": 0.4187,
      "step": 8013
    },
    {
      "epoch": 16.124748490945674,
      "grad_norm": 0.5034955739974976,
      "learning_rate": 0.00016776738102424791,
      "loss": 0.3952,
      "step": 8014
    },
    {
      "epoch": 16.12676056338028,
      "grad_norm": 0.5254913568496704,
      "learning_rate": 0.00016776335647449443,
      "loss": 0.402,
      "step": 8015
    },
    {
      "epoch": 16.12877263581489,
      "grad_norm": 0.5246506929397583,
      "learning_rate": 0.00016775933192474094,
      "loss": 0.4183,
      "step": 8016
    },
    {
      "epoch": 16.130784708249497,
      "grad_norm": 0.5510525703430176,
      "learning_rate": 0.00016775530737498742,
      "loss": 0.4323,
      "step": 8017
    },
    {
      "epoch": 16.132796780684103,
      "grad_norm": 0.5256684422492981,
      "learning_rate": 0.00016775128282523394,
      "loss": 0.3934,
      "step": 8018
    },
    {
      "epoch": 16.134808853118713,
      "grad_norm": 0.5073411464691162,
      "learning_rate": 0.00016774725827548042,
      "loss": 0.3974,
      "step": 8019
    },
    {
      "epoch": 16.13682092555332,
      "grad_norm": 0.5286937952041626,
      "learning_rate": 0.00016774323372572693,
      "loss": 0.4406,
      "step": 8020
    },
    {
      "epoch": 16.138832997987926,
      "grad_norm": 0.5183370113372803,
      "learning_rate": 0.00016773920917597345,
      "loss": 0.4108,
      "step": 8021
    },
    {
      "epoch": 16.140845070422536,
      "grad_norm": 0.5073530673980713,
      "learning_rate": 0.00016773518462621996,
      "loss": 0.3814,
      "step": 8022
    },
    {
      "epoch": 16.142857142857142,
      "grad_norm": 0.5027155876159668,
      "learning_rate": 0.00016773116007646644,
      "loss": 0.3628,
      "step": 8023
    },
    {
      "epoch": 16.14486921529175,
      "grad_norm": 0.51706463098526,
      "learning_rate": 0.00016772713552671296,
      "loss": 0.4043,
      "step": 8024
    },
    {
      "epoch": 16.14688128772636,
      "grad_norm": 0.5479816198348999,
      "learning_rate": 0.00016772311097695944,
      "loss": 0.4139,
      "step": 8025
    },
    {
      "epoch": 16.148893360160965,
      "grad_norm": 0.5109592080116272,
      "learning_rate": 0.00016771908642720598,
      "loss": 0.3761,
      "step": 8026
    },
    {
      "epoch": 16.15090543259557,
      "grad_norm": 0.4971437454223633,
      "learning_rate": 0.00016771506187745246,
      "loss": 0.3904,
      "step": 8027
    },
    {
      "epoch": 16.15291750503018,
      "grad_norm": 0.523307740688324,
      "learning_rate": 0.00016771103732769898,
      "loss": 0.3947,
      "step": 8028
    },
    {
      "epoch": 16.154929577464788,
      "grad_norm": 0.5112143754959106,
      "learning_rate": 0.00016770701277794546,
      "loss": 0.3972,
      "step": 8029
    },
    {
      "epoch": 16.156941649899398,
      "grad_norm": 0.5405992865562439,
      "learning_rate": 0.00016770298822819197,
      "loss": 0.423,
      "step": 8030
    },
    {
      "epoch": 16.158953722334005,
      "grad_norm": 0.49682319164276123,
      "learning_rate": 0.0001676989636784385,
      "loss": 0.4073,
      "step": 8031
    },
    {
      "epoch": 16.16096579476861,
      "grad_norm": 0.5566872954368591,
      "learning_rate": 0.000167694939128685,
      "loss": 0.3898,
      "step": 8032
    },
    {
      "epoch": 16.16297786720322,
      "grad_norm": 0.5144420862197876,
      "learning_rate": 0.00016769091457893148,
      "loss": 0.3953,
      "step": 8033
    },
    {
      "epoch": 16.164989939637827,
      "grad_norm": 0.518581748008728,
      "learning_rate": 0.000167686890029178,
      "loss": 0.424,
      "step": 8034
    },
    {
      "epoch": 16.167002012072434,
      "grad_norm": 0.5389595031738281,
      "learning_rate": 0.00016768286547942448,
      "loss": 0.3969,
      "step": 8035
    },
    {
      "epoch": 16.169014084507044,
      "grad_norm": 0.5214425921440125,
      "learning_rate": 0.00016767884092967102,
      "loss": 0.4136,
      "step": 8036
    },
    {
      "epoch": 16.17102615694165,
      "grad_norm": 0.5375459790229797,
      "learning_rate": 0.0001676748163799175,
      "loss": 0.4334,
      "step": 8037
    },
    {
      "epoch": 16.173038229376257,
      "grad_norm": 0.5338336229324341,
      "learning_rate": 0.00016767079183016402,
      "loss": 0.4358,
      "step": 8038
    },
    {
      "epoch": 16.175050301810867,
      "grad_norm": 0.5232363939285278,
      "learning_rate": 0.0001676667672804105,
      "loss": 0.4148,
      "step": 8039
    },
    {
      "epoch": 16.177062374245473,
      "grad_norm": 0.515491783618927,
      "learning_rate": 0.00016766274273065702,
      "loss": 0.4279,
      "step": 8040
    },
    {
      "epoch": 16.17907444668008,
      "grad_norm": 0.5401437878608704,
      "learning_rate": 0.00016765871818090353,
      "loss": 0.4394,
      "step": 8041
    },
    {
      "epoch": 16.18108651911469,
      "grad_norm": 0.5143731236457825,
      "learning_rate": 0.00016765469363115004,
      "loss": 0.4032,
      "step": 8042
    },
    {
      "epoch": 16.183098591549296,
      "grad_norm": 0.4906373620033264,
      "learning_rate": 0.00016765066908139652,
      "loss": 0.3627,
      "step": 8043
    },
    {
      "epoch": 16.185110663983902,
      "grad_norm": 0.5385865569114685,
      "learning_rate": 0.00016764664453164304,
      "loss": 0.425,
      "step": 8044
    },
    {
      "epoch": 16.187122736418512,
      "grad_norm": 0.5498288869857788,
      "learning_rate": 0.00016764261998188952,
      "loss": 0.4522,
      "step": 8045
    },
    {
      "epoch": 16.18913480885312,
      "grad_norm": 0.5397444367408752,
      "learning_rate": 0.00016763859543213603,
      "loss": 0.4052,
      "step": 8046
    },
    {
      "epoch": 16.191146881287725,
      "grad_norm": 0.5249761343002319,
      "learning_rate": 0.00016763457088238255,
      "loss": 0.4437,
      "step": 8047
    },
    {
      "epoch": 16.193158953722335,
      "grad_norm": 0.5350421071052551,
      "learning_rate": 0.00016763054633262906,
      "loss": 0.4222,
      "step": 8048
    },
    {
      "epoch": 16.19517102615694,
      "grad_norm": 0.5279663801193237,
      "learning_rate": 0.00016762652178287554,
      "loss": 0.3932,
      "step": 8049
    },
    {
      "epoch": 16.197183098591548,
      "grad_norm": 0.5178400874137878,
      "learning_rate": 0.00016762249723312206,
      "loss": 0.4022,
      "step": 8050
    },
    {
      "epoch": 16.199195171026158,
      "grad_norm": 0.4972500205039978,
      "learning_rate": 0.00016761847268336857,
      "loss": 0.4064,
      "step": 8051
    },
    {
      "epoch": 16.201207243460765,
      "grad_norm": 0.5314501523971558,
      "learning_rate": 0.00016761444813361505,
      "loss": 0.4195,
      "step": 8052
    },
    {
      "epoch": 16.20321931589537,
      "grad_norm": 0.5534835457801819,
      "learning_rate": 0.00016761042358386157,
      "loss": 0.4146,
      "step": 8053
    },
    {
      "epoch": 16.20523138832998,
      "grad_norm": 0.5319278836250305,
      "learning_rate": 0.00016760639903410805,
      "loss": 0.4167,
      "step": 8054
    },
    {
      "epoch": 16.207243460764587,
      "grad_norm": 0.5483934283256531,
      "learning_rate": 0.00016760237448435456,
      "loss": 0.4015,
      "step": 8055
    },
    {
      "epoch": 16.209255533199194,
      "grad_norm": 0.5344535708427429,
      "learning_rate": 0.00016759834993460108,
      "loss": 0.4245,
      "step": 8056
    },
    {
      "epoch": 16.211267605633804,
      "grad_norm": 0.5230979323387146,
      "learning_rate": 0.0001675943253848476,
      "loss": 0.4049,
      "step": 8057
    },
    {
      "epoch": 16.21327967806841,
      "grad_norm": 0.5247936248779297,
      "learning_rate": 0.00016759030083509407,
      "loss": 0.3785,
      "step": 8058
    },
    {
      "epoch": 16.215291750503017,
      "grad_norm": 0.5187656879425049,
      "learning_rate": 0.00016758627628534058,
      "loss": 0.3891,
      "step": 8059
    },
    {
      "epoch": 16.217303822937627,
      "grad_norm": 0.5387548804283142,
      "learning_rate": 0.00016758225173558707,
      "loss": 0.4629,
      "step": 8060
    },
    {
      "epoch": 16.219315895372233,
      "grad_norm": 0.50065016746521,
      "learning_rate": 0.0001675782271858336,
      "loss": 0.4108,
      "step": 8061
    },
    {
      "epoch": 16.22132796780684,
      "grad_norm": 0.5198221206665039,
      "learning_rate": 0.0001675742026360801,
      "loss": 0.44,
      "step": 8062
    },
    {
      "epoch": 16.22334004024145,
      "grad_norm": 0.5379105806350708,
      "learning_rate": 0.0001675701780863266,
      "loss": 0.4337,
      "step": 8063
    },
    {
      "epoch": 16.225352112676056,
      "grad_norm": 0.5064715147018433,
      "learning_rate": 0.0001675661535365731,
      "loss": 0.3958,
      "step": 8064
    },
    {
      "epoch": 16.227364185110662,
      "grad_norm": 0.52653968334198,
      "learning_rate": 0.0001675621289868196,
      "loss": 0.4216,
      "step": 8065
    },
    {
      "epoch": 16.229376257545272,
      "grad_norm": 0.5203180313110352,
      "learning_rate": 0.00016755810443706612,
      "loss": 0.434,
      "step": 8066
    },
    {
      "epoch": 16.23138832997988,
      "grad_norm": 0.5812638401985168,
      "learning_rate": 0.00016755407988731263,
      "loss": 0.4056,
      "step": 8067
    },
    {
      "epoch": 16.233400402414485,
      "grad_norm": 0.5375671982765198,
      "learning_rate": 0.0001675500553375591,
      "loss": 0.4185,
      "step": 8068
    },
    {
      "epoch": 16.235412474849095,
      "grad_norm": 0.5276609063148499,
      "learning_rate": 0.00016754603078780563,
      "loss": 0.4184,
      "step": 8069
    },
    {
      "epoch": 16.2374245472837,
      "grad_norm": 0.5741297602653503,
      "learning_rate": 0.0001675420062380521,
      "loss": 0.435,
      "step": 8070
    },
    {
      "epoch": 16.239436619718308,
      "grad_norm": 0.516117513179779,
      "learning_rate": 0.00016753798168829865,
      "loss": 0.4022,
      "step": 8071
    },
    {
      "epoch": 16.241448692152918,
      "grad_norm": 0.5111129283905029,
      "learning_rate": 0.00016753395713854514,
      "loss": 0.4017,
      "step": 8072
    },
    {
      "epoch": 16.243460764587525,
      "grad_norm": 0.56895911693573,
      "learning_rate": 0.00016752993258879165,
      "loss": 0.4364,
      "step": 8073
    },
    {
      "epoch": 16.24547283702213,
      "grad_norm": 0.5577505230903625,
      "learning_rate": 0.00016752590803903813,
      "loss": 0.4419,
      "step": 8074
    },
    {
      "epoch": 16.24748490945674,
      "grad_norm": 0.5271437168121338,
      "learning_rate": 0.00016752188348928464,
      "loss": 0.4491,
      "step": 8075
    },
    {
      "epoch": 16.249496981891348,
      "grad_norm": 0.5115094184875488,
      "learning_rate": 0.00016751785893953116,
      "loss": 0.413,
      "step": 8076
    },
    {
      "epoch": 16.251509054325957,
      "grad_norm": 0.5346812009811401,
      "learning_rate": 0.00016751383438977767,
      "loss": 0.393,
      "step": 8077
    },
    {
      "epoch": 16.253521126760564,
      "grad_norm": 0.5366100072860718,
      "learning_rate": 0.00016750980984002415,
      "loss": 0.4093,
      "step": 8078
    },
    {
      "epoch": 16.25553319919517,
      "grad_norm": 0.5438676476478577,
      "learning_rate": 0.00016750578529027067,
      "loss": 0.4056,
      "step": 8079
    },
    {
      "epoch": 16.25754527162978,
      "grad_norm": 0.5194693803787231,
      "learning_rate": 0.00016750176074051715,
      "loss": 0.4115,
      "step": 8080
    },
    {
      "epoch": 16.259557344064387,
      "grad_norm": 0.5290639996528625,
      "learning_rate": 0.00016749773619076366,
      "loss": 0.4258,
      "step": 8081
    },
    {
      "epoch": 16.261569416498993,
      "grad_norm": 0.5543432831764221,
      "learning_rate": 0.00016749371164101018,
      "loss": 0.4179,
      "step": 8082
    },
    {
      "epoch": 16.263581488933603,
      "grad_norm": 0.5217109322547913,
      "learning_rate": 0.0001674896870912567,
      "loss": 0.4149,
      "step": 8083
    },
    {
      "epoch": 16.26559356136821,
      "grad_norm": 0.5429089665412903,
      "learning_rate": 0.00016748566254150317,
      "loss": 0.4081,
      "step": 8084
    },
    {
      "epoch": 16.267605633802816,
      "grad_norm": 0.5251086950302124,
      "learning_rate": 0.00016748163799174969,
      "loss": 0.4157,
      "step": 8085
    },
    {
      "epoch": 16.269617706237426,
      "grad_norm": 0.5371167063713074,
      "learning_rate": 0.0001674776134419962,
      "loss": 0.4158,
      "step": 8086
    },
    {
      "epoch": 16.271629778672033,
      "grad_norm": 0.5420867800712585,
      "learning_rate": 0.00016747358889224268,
      "loss": 0.4142,
      "step": 8087
    },
    {
      "epoch": 16.27364185110664,
      "grad_norm": 0.5500564575195312,
      "learning_rate": 0.0001674695643424892,
      "loss": 0.4445,
      "step": 8088
    },
    {
      "epoch": 16.27565392354125,
      "grad_norm": 0.5289689898490906,
      "learning_rate": 0.00016746553979273568,
      "loss": 0.4103,
      "step": 8089
    },
    {
      "epoch": 16.277665995975855,
      "grad_norm": 0.5680335760116577,
      "learning_rate": 0.0001674615152429822,
      "loss": 0.405,
      "step": 8090
    },
    {
      "epoch": 16.279678068410462,
      "grad_norm": 0.5573604106903076,
      "learning_rate": 0.0001674574906932287,
      "loss": 0.4287,
      "step": 8091
    },
    {
      "epoch": 16.281690140845072,
      "grad_norm": 0.527027428150177,
      "learning_rate": 0.00016745346614347522,
      "loss": 0.403,
      "step": 8092
    },
    {
      "epoch": 16.28370221327968,
      "grad_norm": 0.5459323525428772,
      "learning_rate": 0.0001674494415937217,
      "loss": 0.4553,
      "step": 8093
    },
    {
      "epoch": 16.285714285714285,
      "grad_norm": 0.5258840918540955,
      "learning_rate": 0.00016744541704396821,
      "loss": 0.4202,
      "step": 8094
    },
    {
      "epoch": 16.287726358148895,
      "grad_norm": 0.5099782347679138,
      "learning_rate": 0.0001674413924942147,
      "loss": 0.4238,
      "step": 8095
    },
    {
      "epoch": 16.2897384305835,
      "grad_norm": 0.524816632270813,
      "learning_rate": 0.00016743736794446124,
      "loss": 0.4255,
      "step": 8096
    },
    {
      "epoch": 16.291750503018108,
      "grad_norm": 0.528440535068512,
      "learning_rate": 0.00016743334339470772,
      "loss": 0.4117,
      "step": 8097
    },
    {
      "epoch": 16.293762575452718,
      "grad_norm": 0.5059061050415039,
      "learning_rate": 0.00016742931884495424,
      "loss": 0.4096,
      "step": 8098
    },
    {
      "epoch": 16.295774647887324,
      "grad_norm": 0.5380798578262329,
      "learning_rate": 0.00016742529429520072,
      "loss": 0.422,
      "step": 8099
    },
    {
      "epoch": 16.29778672032193,
      "grad_norm": 0.566204309463501,
      "learning_rate": 0.00016742126974544723,
      "loss": 0.3931,
      "step": 8100
    },
    {
      "epoch": 16.29979879275654,
      "grad_norm": 0.5497180223464966,
      "learning_rate": 0.00016741724519569375,
      "loss": 0.4167,
      "step": 8101
    },
    {
      "epoch": 16.301810865191147,
      "grad_norm": 0.5708010792732239,
      "learning_rate": 0.00016741322064594026,
      "loss": 0.3956,
      "step": 8102
    },
    {
      "epoch": 16.303822937625753,
      "grad_norm": 0.5305092930793762,
      "learning_rate": 0.00016740919609618674,
      "loss": 0.4495,
      "step": 8103
    },
    {
      "epoch": 16.305835010060363,
      "grad_norm": 0.5348930954933167,
      "learning_rate": 0.00016740517154643325,
      "loss": 0.3933,
      "step": 8104
    },
    {
      "epoch": 16.30784708249497,
      "grad_norm": 0.5523884892463684,
      "learning_rate": 0.00016740114699667974,
      "loss": 0.3945,
      "step": 8105
    },
    {
      "epoch": 16.309859154929576,
      "grad_norm": 0.5697917938232422,
      "learning_rate": 0.00016739712244692628,
      "loss": 0.421,
      "step": 8106
    },
    {
      "epoch": 16.311871227364186,
      "grad_norm": 0.5411065816879272,
      "learning_rate": 0.00016739309789717276,
      "loss": 0.4439,
      "step": 8107
    },
    {
      "epoch": 16.313883299798793,
      "grad_norm": 0.5384547114372253,
      "learning_rate": 0.00016738907334741928,
      "loss": 0.4109,
      "step": 8108
    },
    {
      "epoch": 16.3158953722334,
      "grad_norm": 0.5300272107124329,
      "learning_rate": 0.00016738504879766576,
      "loss": 0.4257,
      "step": 8109
    },
    {
      "epoch": 16.31790744466801,
      "grad_norm": 0.533491313457489,
      "learning_rate": 0.00016738102424791227,
      "loss": 0.4341,
      "step": 8110
    },
    {
      "epoch": 16.319919517102615,
      "grad_norm": 0.5501510500907898,
      "learning_rate": 0.00016737699969815879,
      "loss": 0.4045,
      "step": 8111
    },
    {
      "epoch": 16.321931589537222,
      "grad_norm": 0.5296778678894043,
      "learning_rate": 0.0001673729751484053,
      "loss": 0.436,
      "step": 8112
    },
    {
      "epoch": 16.323943661971832,
      "grad_norm": 0.5593113303184509,
      "learning_rate": 0.00016736895059865178,
      "loss": 0.4328,
      "step": 8113
    },
    {
      "epoch": 16.32595573440644,
      "grad_norm": 0.5014079213142395,
      "learning_rate": 0.0001673649260488983,
      "loss": 0.4031,
      "step": 8114
    },
    {
      "epoch": 16.327967806841045,
      "grad_norm": 0.5784081816673279,
      "learning_rate": 0.00016736090149914478,
      "loss": 0.4539,
      "step": 8115
    },
    {
      "epoch": 16.329979879275655,
      "grad_norm": 0.5179831981658936,
      "learning_rate": 0.0001673568769493913,
      "loss": 0.4197,
      "step": 8116
    },
    {
      "epoch": 16.33199195171026,
      "grad_norm": 0.5280986428260803,
      "learning_rate": 0.0001673528523996378,
      "loss": 0.4129,
      "step": 8117
    },
    {
      "epoch": 16.334004024144868,
      "grad_norm": 0.5427863001823425,
      "learning_rate": 0.00016734882784988432,
      "loss": 0.4656,
      "step": 8118
    },
    {
      "epoch": 16.336016096579478,
      "grad_norm": 0.51956707239151,
      "learning_rate": 0.0001673448033001308,
      "loss": 0.4112,
      "step": 8119
    },
    {
      "epoch": 16.338028169014084,
      "grad_norm": 0.5712661147117615,
      "learning_rate": 0.00016734077875037731,
      "loss": 0.4284,
      "step": 8120
    },
    {
      "epoch": 16.34004024144869,
      "grad_norm": 0.5128258466720581,
      "learning_rate": 0.00016733675420062383,
      "loss": 0.4596,
      "step": 8121
    },
    {
      "epoch": 16.3420523138833,
      "grad_norm": 0.5589656829833984,
      "learning_rate": 0.0001673327296508703,
      "loss": 0.4252,
      "step": 8122
    },
    {
      "epoch": 16.344064386317907,
      "grad_norm": 0.5202420353889465,
      "learning_rate": 0.00016732870510111682,
      "loss": 0.4188,
      "step": 8123
    },
    {
      "epoch": 16.346076458752513,
      "grad_norm": 0.5419149398803711,
      "learning_rate": 0.0001673246805513633,
      "loss": 0.4343,
      "step": 8124
    },
    {
      "epoch": 16.348088531187123,
      "grad_norm": 0.5162330865859985,
      "learning_rate": 0.00016732065600160982,
      "loss": 0.4019,
      "step": 8125
    },
    {
      "epoch": 16.35010060362173,
      "grad_norm": 0.5461097359657288,
      "learning_rate": 0.00016731663145185633,
      "loss": 0.4329,
      "step": 8126
    },
    {
      "epoch": 16.352112676056336,
      "grad_norm": 0.5794872045516968,
      "learning_rate": 0.00016731260690210285,
      "loss": 0.4106,
      "step": 8127
    },
    {
      "epoch": 16.354124748490946,
      "grad_norm": 0.5255016088485718,
      "learning_rate": 0.00016730858235234933,
      "loss": 0.4254,
      "step": 8128
    },
    {
      "epoch": 16.356136820925553,
      "grad_norm": 0.5663505792617798,
      "learning_rate": 0.00016730455780259584,
      "loss": 0.4514,
      "step": 8129
    },
    {
      "epoch": 16.358148893360163,
      "grad_norm": 0.5438891649246216,
      "learning_rate": 0.00016730053325284233,
      "loss": 0.427,
      "step": 8130
    },
    {
      "epoch": 16.36016096579477,
      "grad_norm": 0.5474292039871216,
      "learning_rate": 0.00016729650870308884,
      "loss": 0.4054,
      "step": 8131
    },
    {
      "epoch": 16.362173038229376,
      "grad_norm": 0.5583478808403015,
      "learning_rate": 0.00016729248415333535,
      "loss": 0.436,
      "step": 8132
    },
    {
      "epoch": 16.364185110663986,
      "grad_norm": 0.5387612581253052,
      "learning_rate": 0.00016728845960358187,
      "loss": 0.3996,
      "step": 8133
    },
    {
      "epoch": 16.366197183098592,
      "grad_norm": 0.5308371186256409,
      "learning_rate": 0.00016728443505382835,
      "loss": 0.4017,
      "step": 8134
    },
    {
      "epoch": 16.3682092555332,
      "grad_norm": 0.5214492082595825,
      "learning_rate": 0.00016728041050407486,
      "loss": 0.4066,
      "step": 8135
    },
    {
      "epoch": 16.37022132796781,
      "grad_norm": 0.5491047501564026,
      "learning_rate": 0.00016727638595432135,
      "loss": 0.3977,
      "step": 8136
    },
    {
      "epoch": 16.372233400402415,
      "grad_norm": 0.5561590790748596,
      "learning_rate": 0.0001672723614045679,
      "loss": 0.4267,
      "step": 8137
    },
    {
      "epoch": 16.37424547283702,
      "grad_norm": 0.5269911289215088,
      "learning_rate": 0.00016726833685481437,
      "loss": 0.4444,
      "step": 8138
    },
    {
      "epoch": 16.37625754527163,
      "grad_norm": 0.582344114780426,
      "learning_rate": 0.00016726431230506088,
      "loss": 0.4248,
      "step": 8139
    },
    {
      "epoch": 16.378269617706238,
      "grad_norm": 0.5328061580657959,
      "learning_rate": 0.00016726028775530737,
      "loss": 0.4505,
      "step": 8140
    },
    {
      "epoch": 16.380281690140844,
      "grad_norm": 0.523064136505127,
      "learning_rate": 0.00016725626320555388,
      "loss": 0.4152,
      "step": 8141
    },
    {
      "epoch": 16.382293762575454,
      "grad_norm": 0.5251511931419373,
      "learning_rate": 0.0001672522386558004,
      "loss": 0.4413,
      "step": 8142
    },
    {
      "epoch": 16.38430583501006,
      "grad_norm": 0.5375210642814636,
      "learning_rate": 0.0001672482141060469,
      "loss": 0.429,
      "step": 8143
    },
    {
      "epoch": 16.386317907444667,
      "grad_norm": 0.512190580368042,
      "learning_rate": 0.0001672441895562934,
      "loss": 0.4119,
      "step": 8144
    },
    {
      "epoch": 16.388329979879277,
      "grad_norm": 0.5410854816436768,
      "learning_rate": 0.0001672401650065399,
      "loss": 0.4297,
      "step": 8145
    },
    {
      "epoch": 16.390342052313883,
      "grad_norm": 0.5640282034873962,
      "learning_rate": 0.0001672361404567864,
      "loss": 0.4649,
      "step": 8146
    },
    {
      "epoch": 16.39235412474849,
      "grad_norm": 0.5596483945846558,
      "learning_rate": 0.00016723211590703293,
      "loss": 0.4373,
      "step": 8147
    },
    {
      "epoch": 16.3943661971831,
      "grad_norm": 0.5275404453277588,
      "learning_rate": 0.0001672280913572794,
      "loss": 0.4135,
      "step": 8148
    },
    {
      "epoch": 16.396378269617706,
      "grad_norm": 0.5429463386535645,
      "learning_rate": 0.00016722406680752593,
      "loss": 0.4588,
      "step": 8149
    },
    {
      "epoch": 16.398390342052313,
      "grad_norm": 0.5215045213699341,
      "learning_rate": 0.0001672200422577724,
      "loss": 0.4004,
      "step": 8150
    },
    {
      "epoch": 16.400402414486923,
      "grad_norm": 0.5681933164596558,
      "learning_rate": 0.00016721601770801892,
      "loss": 0.4448,
      "step": 8151
    },
    {
      "epoch": 16.40241448692153,
      "grad_norm": 0.518327534198761,
      "learning_rate": 0.00016721199315826543,
      "loss": 0.4266,
      "step": 8152
    },
    {
      "epoch": 16.404426559356136,
      "grad_norm": 0.5484693646430969,
      "learning_rate": 0.00016720796860851195,
      "loss": 0.4268,
      "step": 8153
    },
    {
      "epoch": 16.406438631790746,
      "grad_norm": 0.5731998682022095,
      "learning_rate": 0.00016720394405875843,
      "loss": 0.4102,
      "step": 8154
    },
    {
      "epoch": 16.408450704225352,
      "grad_norm": 0.5463446378707886,
      "learning_rate": 0.00016719991950900494,
      "loss": 0.4388,
      "step": 8155
    },
    {
      "epoch": 16.41046277665996,
      "grad_norm": 0.5442410707473755,
      "learning_rate": 0.00016719589495925143,
      "loss": 0.4298,
      "step": 8156
    },
    {
      "epoch": 16.41247484909457,
      "grad_norm": 0.552082359790802,
      "learning_rate": 0.00016719187040949794,
      "loss": 0.4066,
      "step": 8157
    },
    {
      "epoch": 16.414486921529175,
      "grad_norm": 0.5620719790458679,
      "learning_rate": 0.00016718784585974445,
      "loss": 0.422,
      "step": 8158
    },
    {
      "epoch": 16.41649899396378,
      "grad_norm": 0.5234250426292419,
      "learning_rate": 0.00016718382130999094,
      "loss": 0.4252,
      "step": 8159
    },
    {
      "epoch": 16.41851106639839,
      "grad_norm": 0.5920541882514954,
      "learning_rate": 0.00016717979676023745,
      "loss": 0.4214,
      "step": 8160
    },
    {
      "epoch": 16.420523138832998,
      "grad_norm": 0.5387206077575684,
      "learning_rate": 0.00016717577221048394,
      "loss": 0.4202,
      "step": 8161
    },
    {
      "epoch": 16.422535211267604,
      "grad_norm": 0.5745570659637451,
      "learning_rate": 0.00016717174766073048,
      "loss": 0.4436,
      "step": 8162
    },
    {
      "epoch": 16.424547283702214,
      "grad_norm": 0.5452447533607483,
      "learning_rate": 0.00016716772311097696,
      "loss": 0.4337,
      "step": 8163
    },
    {
      "epoch": 16.42655935613682,
      "grad_norm": 0.5566335320472717,
      "learning_rate": 0.00016716369856122347,
      "loss": 0.4283,
      "step": 8164
    },
    {
      "epoch": 16.428571428571427,
      "grad_norm": 0.5576951503753662,
      "learning_rate": 0.00016715967401146996,
      "loss": 0.4051,
      "step": 8165
    },
    {
      "epoch": 16.430583501006037,
      "grad_norm": 0.5498246550559998,
      "learning_rate": 0.00016715564946171647,
      "loss": 0.4483,
      "step": 8166
    },
    {
      "epoch": 16.432595573440643,
      "grad_norm": 0.5264835953712463,
      "learning_rate": 0.00016715162491196298,
      "loss": 0.4639,
      "step": 8167
    },
    {
      "epoch": 16.43460764587525,
      "grad_norm": 0.513726532459259,
      "learning_rate": 0.0001671476003622095,
      "loss": 0.4281,
      "step": 8168
    },
    {
      "epoch": 16.43661971830986,
      "grad_norm": 0.5385086536407471,
      "learning_rate": 0.00016714357581245598,
      "loss": 0.4479,
      "step": 8169
    },
    {
      "epoch": 16.438631790744466,
      "grad_norm": 0.5225390791893005,
      "learning_rate": 0.0001671395512627025,
      "loss": 0.4462,
      "step": 8170
    },
    {
      "epoch": 16.440643863179073,
      "grad_norm": 0.6277555823326111,
      "learning_rate": 0.00016713552671294898,
      "loss": 0.4282,
      "step": 8171
    },
    {
      "epoch": 16.442655935613683,
      "grad_norm": 0.5654388070106506,
      "learning_rate": 0.00016713150216319552,
      "loss": 0.4307,
      "step": 8172
    },
    {
      "epoch": 16.44466800804829,
      "grad_norm": 0.5409928560256958,
      "learning_rate": 0.000167127477613442,
      "loss": 0.3607,
      "step": 8173
    },
    {
      "epoch": 16.446680080482896,
      "grad_norm": 0.5472803115844727,
      "learning_rate": 0.00016712345306368851,
      "loss": 0.4412,
      "step": 8174
    },
    {
      "epoch": 16.448692152917506,
      "grad_norm": 0.5498549342155457,
      "learning_rate": 0.000167119428513935,
      "loss": 0.4221,
      "step": 8175
    },
    {
      "epoch": 16.450704225352112,
      "grad_norm": 0.5128883123397827,
      "learning_rate": 0.0001671154039641815,
      "loss": 0.4019,
      "step": 8176
    },
    {
      "epoch": 16.452716297786722,
      "grad_norm": 0.5308584570884705,
      "learning_rate": 0.00016711137941442802,
      "loss": 0.4439,
      "step": 8177
    },
    {
      "epoch": 16.45472837022133,
      "grad_norm": 0.5480022430419922,
      "learning_rate": 0.00016710735486467454,
      "loss": 0.4472,
      "step": 8178
    },
    {
      "epoch": 16.456740442655935,
      "grad_norm": 0.5368757843971252,
      "learning_rate": 0.00016710333031492102,
      "loss": 0.3966,
      "step": 8179
    },
    {
      "epoch": 16.458752515090545,
      "grad_norm": 0.5310755372047424,
      "learning_rate": 0.00016709930576516753,
      "loss": 0.4471,
      "step": 8180
    },
    {
      "epoch": 16.46076458752515,
      "grad_norm": 0.5286740660667419,
      "learning_rate": 0.00016709528121541402,
      "loss": 0.4085,
      "step": 8181
    },
    {
      "epoch": 16.462776659959758,
      "grad_norm": 0.5439414381980896,
      "learning_rate": 0.00016709125666566056,
      "loss": 0.4383,
      "step": 8182
    },
    {
      "epoch": 16.464788732394368,
      "grad_norm": 0.5297099351882935,
      "learning_rate": 0.00016708723211590704,
      "loss": 0.4183,
      "step": 8183
    },
    {
      "epoch": 16.466800804828974,
      "grad_norm": 0.5559848546981812,
      "learning_rate": 0.00016708320756615355,
      "loss": 0.4203,
      "step": 8184
    },
    {
      "epoch": 16.46881287726358,
      "grad_norm": 0.4978627860546112,
      "learning_rate": 0.00016707918301640004,
      "loss": 0.4437,
      "step": 8185
    },
    {
      "epoch": 16.47082494969819,
      "grad_norm": 0.5315319299697876,
      "learning_rate": 0.00016707515846664655,
      "loss": 0.3944,
      "step": 8186
    },
    {
      "epoch": 16.472837022132797,
      "grad_norm": 0.5442990064620972,
      "learning_rate": 0.00016707113391689306,
      "loss": 0.4234,
      "step": 8187
    },
    {
      "epoch": 16.474849094567404,
      "grad_norm": 0.5329136848449707,
      "learning_rate": 0.00016706710936713955,
      "loss": 0.4183,
      "step": 8188
    },
    {
      "epoch": 16.476861167002014,
      "grad_norm": 0.5596719980239868,
      "learning_rate": 0.00016706308481738606,
      "loss": 0.4307,
      "step": 8189
    },
    {
      "epoch": 16.47887323943662,
      "grad_norm": 0.5631299614906311,
      "learning_rate": 0.00016705906026763257,
      "loss": 0.4566,
      "step": 8190
    },
    {
      "epoch": 16.480885311871226,
      "grad_norm": 0.522439181804657,
      "learning_rate": 0.00016705503571787906,
      "loss": 0.4144,
      "step": 8191
    },
    {
      "epoch": 16.482897384305836,
      "grad_norm": 0.5190043449401855,
      "learning_rate": 0.00016705101116812557,
      "loss": 0.4188,
      "step": 8192
    },
    {
      "epoch": 16.484909456740443,
      "grad_norm": 0.5534198880195618,
      "learning_rate": 0.00016704698661837208,
      "loss": 0.4124,
      "step": 8193
    },
    {
      "epoch": 16.48692152917505,
      "grad_norm": 0.57365882396698,
      "learning_rate": 0.00016704296206861857,
      "loss": 0.4588,
      "step": 8194
    },
    {
      "epoch": 16.48893360160966,
      "grad_norm": 0.563774585723877,
      "learning_rate": 0.00016703893751886508,
      "loss": 0.4298,
      "step": 8195
    },
    {
      "epoch": 16.490945674044266,
      "grad_norm": 0.5344448089599609,
      "learning_rate": 0.00016703491296911157,
      "loss": 0.4569,
      "step": 8196
    },
    {
      "epoch": 16.492957746478872,
      "grad_norm": 0.5283566117286682,
      "learning_rate": 0.0001670308884193581,
      "loss": 0.4262,
      "step": 8197
    },
    {
      "epoch": 16.494969818913482,
      "grad_norm": 0.5370575189590454,
      "learning_rate": 0.0001670268638696046,
      "loss": 0.4345,
      "step": 8198
    },
    {
      "epoch": 16.49698189134809,
      "grad_norm": 0.5797678828239441,
      "learning_rate": 0.0001670228393198511,
      "loss": 0.4077,
      "step": 8199
    },
    {
      "epoch": 16.498993963782695,
      "grad_norm": 0.536772608757019,
      "learning_rate": 0.0001670188147700976,
      "loss": 0.4576,
      "step": 8200
    },
    {
      "epoch": 16.501006036217305,
      "grad_norm": 0.5526216626167297,
      "learning_rate": 0.0001670147902203441,
      "loss": 0.4411,
      "step": 8201
    },
    {
      "epoch": 16.50301810865191,
      "grad_norm": 0.5506181716918945,
      "learning_rate": 0.0001670107656705906,
      "loss": 0.4417,
      "step": 8202
    },
    {
      "epoch": 16.505030181086518,
      "grad_norm": 0.5157212018966675,
      "learning_rate": 0.00016700674112083712,
      "loss": 0.4072,
      "step": 8203
    },
    {
      "epoch": 16.507042253521128,
      "grad_norm": 0.5366633534431458,
      "learning_rate": 0.0001670027165710836,
      "loss": 0.4509,
      "step": 8204
    },
    {
      "epoch": 16.509054325955734,
      "grad_norm": 0.523926854133606,
      "learning_rate": 0.00016699869202133012,
      "loss": 0.4402,
      "step": 8205
    },
    {
      "epoch": 16.51106639839034,
      "grad_norm": 0.5489174127578735,
      "learning_rate": 0.0001669946674715766,
      "loss": 0.4354,
      "step": 8206
    },
    {
      "epoch": 16.51307847082495,
      "grad_norm": 0.5420361161231995,
      "learning_rate": 0.00016699064292182315,
      "loss": 0.4241,
      "step": 8207
    },
    {
      "epoch": 16.515090543259557,
      "grad_norm": 0.5316653251647949,
      "learning_rate": 0.00016698661837206963,
      "loss": 0.4292,
      "step": 8208
    },
    {
      "epoch": 16.517102615694164,
      "grad_norm": 0.5441012978553772,
      "learning_rate": 0.00016698259382231614,
      "loss": 0.4253,
      "step": 8209
    },
    {
      "epoch": 16.519114688128774,
      "grad_norm": 0.5301831364631653,
      "learning_rate": 0.00016697856927256263,
      "loss": 0.4135,
      "step": 8210
    },
    {
      "epoch": 16.52112676056338,
      "grad_norm": 0.5467122793197632,
      "learning_rate": 0.00016697454472280914,
      "loss": 0.419,
      "step": 8211
    },
    {
      "epoch": 16.523138832997986,
      "grad_norm": 0.4942273795604706,
      "learning_rate": 0.00016697052017305565,
      "loss": 0.4033,
      "step": 8212
    },
    {
      "epoch": 16.525150905432596,
      "grad_norm": 0.5305325984954834,
      "learning_rate": 0.00016696649562330217,
      "loss": 0.4416,
      "step": 8213
    },
    {
      "epoch": 16.527162977867203,
      "grad_norm": 0.5312239527702332,
      "learning_rate": 0.00016696247107354865,
      "loss": 0.4749,
      "step": 8214
    },
    {
      "epoch": 16.52917505030181,
      "grad_norm": 0.5579188466072083,
      "learning_rate": 0.00016695844652379516,
      "loss": 0.4646,
      "step": 8215
    },
    {
      "epoch": 16.53118712273642,
      "grad_norm": 0.5183202624320984,
      "learning_rate": 0.00016695442197404165,
      "loss": 0.4572,
      "step": 8216
    },
    {
      "epoch": 16.533199195171026,
      "grad_norm": 0.531262218952179,
      "learning_rate": 0.0001669503974242882,
      "loss": 0.4233,
      "step": 8217
    },
    {
      "epoch": 16.535211267605632,
      "grad_norm": 0.5175822377204895,
      "learning_rate": 0.00016694637287453467,
      "loss": 0.3842,
      "step": 8218
    },
    {
      "epoch": 16.537223340040242,
      "grad_norm": 0.5201115012168884,
      "learning_rate": 0.00016694234832478118,
      "loss": 0.4303,
      "step": 8219
    },
    {
      "epoch": 16.53923541247485,
      "grad_norm": 0.5370665788650513,
      "learning_rate": 0.00016693832377502767,
      "loss": 0.4272,
      "step": 8220
    },
    {
      "epoch": 16.541247484909455,
      "grad_norm": 0.5221143364906311,
      "learning_rate": 0.00016693429922527418,
      "loss": 0.4362,
      "step": 8221
    },
    {
      "epoch": 16.543259557344065,
      "grad_norm": 0.5216938853263855,
      "learning_rate": 0.0001669302746755207,
      "loss": 0.434,
      "step": 8222
    },
    {
      "epoch": 16.54527162977867,
      "grad_norm": 0.5651117563247681,
      "learning_rate": 0.00016692625012576718,
      "loss": 0.447,
      "step": 8223
    },
    {
      "epoch": 16.547283702213278,
      "grad_norm": 0.5515978336334229,
      "learning_rate": 0.0001669222255760137,
      "loss": 0.4505,
      "step": 8224
    },
    {
      "epoch": 16.549295774647888,
      "grad_norm": 0.5330206751823425,
      "learning_rate": 0.0001669182010262602,
      "loss": 0.4324,
      "step": 8225
    },
    {
      "epoch": 16.551307847082494,
      "grad_norm": 0.5242815017700195,
      "learning_rate": 0.0001669141764765067,
      "loss": 0.4074,
      "step": 8226
    },
    {
      "epoch": 16.5533199195171,
      "grad_norm": 0.5114128589630127,
      "learning_rate": 0.0001669101519267532,
      "loss": 0.4164,
      "step": 8227
    },
    {
      "epoch": 16.55533199195171,
      "grad_norm": 0.5184831023216248,
      "learning_rate": 0.0001669061273769997,
      "loss": 0.4149,
      "step": 8228
    },
    {
      "epoch": 16.557344064386317,
      "grad_norm": 0.5103088021278381,
      "learning_rate": 0.0001669021028272462,
      "loss": 0.4231,
      "step": 8229
    },
    {
      "epoch": 16.559356136820927,
      "grad_norm": 0.5709244608879089,
      "learning_rate": 0.0001668980782774927,
      "loss": 0.4311,
      "step": 8230
    },
    {
      "epoch": 16.561368209255534,
      "grad_norm": 0.5147923827171326,
      "learning_rate": 0.0001668940537277392,
      "loss": 0.4423,
      "step": 8231
    },
    {
      "epoch": 16.56338028169014,
      "grad_norm": 0.5155372619628906,
      "learning_rate": 0.00016689002917798573,
      "loss": 0.4082,
      "step": 8232
    },
    {
      "epoch": 16.56539235412475,
      "grad_norm": 0.5206677317619324,
      "learning_rate": 0.00016688600462823222,
      "loss": 0.4187,
      "step": 8233
    },
    {
      "epoch": 16.567404426559357,
      "grad_norm": 0.5543162822723389,
      "learning_rate": 0.00016688198007847873,
      "loss": 0.4234,
      "step": 8234
    },
    {
      "epoch": 16.569416498993963,
      "grad_norm": 0.5459465384483337,
      "learning_rate": 0.00016687795552872522,
      "loss": 0.4361,
      "step": 8235
    },
    {
      "epoch": 16.571428571428573,
      "grad_norm": 0.5617706775665283,
      "learning_rate": 0.00016687393097897173,
      "loss": 0.4455,
      "step": 8236
    },
    {
      "epoch": 16.57344064386318,
      "grad_norm": 0.5199425220489502,
      "learning_rate": 0.00016686990642921824,
      "loss": 0.431,
      "step": 8237
    },
    {
      "epoch": 16.575452716297786,
      "grad_norm": 0.5549023747444153,
      "learning_rate": 0.00016686588187946475,
      "loss": 0.4411,
      "step": 8238
    },
    {
      "epoch": 16.577464788732396,
      "grad_norm": 0.5206513404846191,
      "learning_rate": 0.00016686185732971124,
      "loss": 0.4109,
      "step": 8239
    },
    {
      "epoch": 16.579476861167002,
      "grad_norm": 0.5320683121681213,
      "learning_rate": 0.00016685783277995775,
      "loss": 0.4152,
      "step": 8240
    },
    {
      "epoch": 16.58148893360161,
      "grad_norm": 0.5659909844398499,
      "learning_rate": 0.00016685380823020424,
      "loss": 0.4382,
      "step": 8241
    },
    {
      "epoch": 16.58350100603622,
      "grad_norm": 0.5396456718444824,
      "learning_rate": 0.00016684978368045078,
      "loss": 0.3872,
      "step": 8242
    },
    {
      "epoch": 16.585513078470825,
      "grad_norm": 0.5403101444244385,
      "learning_rate": 0.00016684575913069726,
      "loss": 0.406,
      "step": 8243
    },
    {
      "epoch": 16.58752515090543,
      "grad_norm": 0.5342304706573486,
      "learning_rate": 0.00016684173458094377,
      "loss": 0.4297,
      "step": 8244
    },
    {
      "epoch": 16.58953722334004,
      "grad_norm": 0.5224475264549255,
      "learning_rate": 0.00016683771003119026,
      "loss": 0.3898,
      "step": 8245
    },
    {
      "epoch": 16.591549295774648,
      "grad_norm": 0.5357260704040527,
      "learning_rate": 0.00016683368548143677,
      "loss": 0.4084,
      "step": 8246
    },
    {
      "epoch": 16.593561368209254,
      "grad_norm": 0.5474728345870972,
      "learning_rate": 0.00016682966093168328,
      "loss": 0.4589,
      "step": 8247
    },
    {
      "epoch": 16.595573440643864,
      "grad_norm": 0.5524539947509766,
      "learning_rate": 0.0001668256363819298,
      "loss": 0.4678,
      "step": 8248
    },
    {
      "epoch": 16.59758551307847,
      "grad_norm": 0.5503667593002319,
      "learning_rate": 0.00016682161183217628,
      "loss": 0.4607,
      "step": 8249
    },
    {
      "epoch": 16.599597585513077,
      "grad_norm": 0.5532060265541077,
      "learning_rate": 0.0001668175872824228,
      "loss": 0.4382,
      "step": 8250
    },
    {
      "epoch": 16.601609657947687,
      "grad_norm": 0.523230791091919,
      "learning_rate": 0.00016681356273266928,
      "loss": 0.4257,
      "step": 8251
    },
    {
      "epoch": 16.603621730382294,
      "grad_norm": 0.5388711094856262,
      "learning_rate": 0.00016680953818291582,
      "loss": 0.4283,
      "step": 8252
    },
    {
      "epoch": 16.6056338028169,
      "grad_norm": 0.5547934174537659,
      "learning_rate": 0.0001668055136331623,
      "loss": 0.4212,
      "step": 8253
    },
    {
      "epoch": 16.60764587525151,
      "grad_norm": 0.5148617029190063,
      "learning_rate": 0.0001668014890834088,
      "loss": 0.4297,
      "step": 8254
    },
    {
      "epoch": 16.609657947686117,
      "grad_norm": 0.5410566329956055,
      "learning_rate": 0.0001667974645336553,
      "loss": 0.4344,
      "step": 8255
    },
    {
      "epoch": 16.611670020120723,
      "grad_norm": 0.5450095534324646,
      "learning_rate": 0.0001667934399839018,
      "loss": 0.4495,
      "step": 8256
    },
    {
      "epoch": 16.613682092555333,
      "grad_norm": 0.5045039057731628,
      "learning_rate": 0.00016678941543414832,
      "loss": 0.4141,
      "step": 8257
    },
    {
      "epoch": 16.61569416498994,
      "grad_norm": 0.5494322776794434,
      "learning_rate": 0.0001667853908843948,
      "loss": 0.4231,
      "step": 8258
    },
    {
      "epoch": 16.617706237424546,
      "grad_norm": 0.5383175015449524,
      "learning_rate": 0.00016678136633464132,
      "loss": 0.4474,
      "step": 8259
    },
    {
      "epoch": 16.619718309859156,
      "grad_norm": 0.581785261631012,
      "learning_rate": 0.00016677734178488783,
      "loss": 0.4549,
      "step": 8260
    },
    {
      "epoch": 16.621730382293762,
      "grad_norm": 0.5173441767692566,
      "learning_rate": 0.00016677331723513432,
      "loss": 0.4304,
      "step": 8261
    },
    {
      "epoch": 16.62374245472837,
      "grad_norm": 0.5566946864128113,
      "learning_rate": 0.00016676929268538083,
      "loss": 0.4198,
      "step": 8262
    },
    {
      "epoch": 16.62575452716298,
      "grad_norm": 0.5379316806793213,
      "learning_rate": 0.00016676526813562734,
      "loss": 0.4108,
      "step": 8263
    },
    {
      "epoch": 16.627766599597585,
      "grad_norm": 0.5414530038833618,
      "learning_rate": 0.00016676124358587383,
      "loss": 0.441,
      "step": 8264
    },
    {
      "epoch": 16.62977867203219,
      "grad_norm": 0.531470537185669,
      "learning_rate": 0.00016675721903612034,
      "loss": 0.4526,
      "step": 8265
    },
    {
      "epoch": 16.6317907444668,
      "grad_norm": 0.5174546837806702,
      "learning_rate": 0.00016675319448636682,
      "loss": 0.4259,
      "step": 8266
    },
    {
      "epoch": 16.633802816901408,
      "grad_norm": 0.5133635997772217,
      "learning_rate": 0.00016674916993661336,
      "loss": 0.4014,
      "step": 8267
    },
    {
      "epoch": 16.635814889336014,
      "grad_norm": 0.5518127679824829,
      "learning_rate": 0.00016674514538685985,
      "loss": 0.4699,
      "step": 8268
    },
    {
      "epoch": 16.637826961770624,
      "grad_norm": 0.5164405703544617,
      "learning_rate": 0.00016674112083710636,
      "loss": 0.4357,
      "step": 8269
    },
    {
      "epoch": 16.63983903420523,
      "grad_norm": 0.5283989310264587,
      "learning_rate": 0.00016673709628735285,
      "loss": 0.442,
      "step": 8270
    },
    {
      "epoch": 16.641851106639837,
      "grad_norm": 0.5443073511123657,
      "learning_rate": 0.00016673307173759936,
      "loss": 0.4463,
      "step": 8271
    },
    {
      "epoch": 16.643863179074447,
      "grad_norm": 0.5426802039146423,
      "learning_rate": 0.00016672904718784587,
      "loss": 0.435,
      "step": 8272
    },
    {
      "epoch": 16.645875251509054,
      "grad_norm": 0.5720906853675842,
      "learning_rate": 0.00016672502263809238,
      "loss": 0.4482,
      "step": 8273
    },
    {
      "epoch": 16.647887323943664,
      "grad_norm": 0.5117337703704834,
      "learning_rate": 0.00016672099808833887,
      "loss": 0.4374,
      "step": 8274
    },
    {
      "epoch": 16.64989939637827,
      "grad_norm": 0.5931544303894043,
      "learning_rate": 0.00016671697353858538,
      "loss": 0.4404,
      "step": 8275
    },
    {
      "epoch": 16.651911468812877,
      "grad_norm": 0.5325303077697754,
      "learning_rate": 0.00016671294898883187,
      "loss": 0.4508,
      "step": 8276
    },
    {
      "epoch": 16.653923541247487,
      "grad_norm": 0.5158862471580505,
      "learning_rate": 0.0001667089244390784,
      "loss": 0.4395,
      "step": 8277
    },
    {
      "epoch": 16.655935613682093,
      "grad_norm": 0.5344072580337524,
      "learning_rate": 0.0001667048998893249,
      "loss": 0.4382,
      "step": 8278
    },
    {
      "epoch": 16.6579476861167,
      "grad_norm": 0.5583759546279907,
      "learning_rate": 0.0001667008753395714,
      "loss": 0.4303,
      "step": 8279
    },
    {
      "epoch": 16.65995975855131,
      "grad_norm": 0.534706175327301,
      "learning_rate": 0.0001666968507898179,
      "loss": 0.4068,
      "step": 8280
    },
    {
      "epoch": 16.661971830985916,
      "grad_norm": 0.5609119534492493,
      "learning_rate": 0.0001666928262400644,
      "loss": 0.4376,
      "step": 8281
    },
    {
      "epoch": 16.663983903420522,
      "grad_norm": 0.5446752309799194,
      "learning_rate": 0.0001666888016903109,
      "loss": 0.4167,
      "step": 8282
    },
    {
      "epoch": 16.665995975855132,
      "grad_norm": 0.5180515050888062,
      "learning_rate": 0.00016668477714055742,
      "loss": 0.4165,
      "step": 8283
    },
    {
      "epoch": 16.66800804828974,
      "grad_norm": 0.5412532091140747,
      "learning_rate": 0.0001666807525908039,
      "loss": 0.4355,
      "step": 8284
    },
    {
      "epoch": 16.670020120724345,
      "grad_norm": 0.5324536561965942,
      "learning_rate": 0.00016667672804105042,
      "loss": 0.4439,
      "step": 8285
    },
    {
      "epoch": 16.672032193158955,
      "grad_norm": 0.5318152904510498,
      "learning_rate": 0.0001666727034912969,
      "loss": 0.4386,
      "step": 8286
    },
    {
      "epoch": 16.67404426559356,
      "grad_norm": 0.5243274569511414,
      "learning_rate": 0.00016666867894154345,
      "loss": 0.4555,
      "step": 8287
    },
    {
      "epoch": 16.676056338028168,
      "grad_norm": 0.5260266661643982,
      "learning_rate": 0.00016666465439178993,
      "loss": 0.4409,
      "step": 8288
    },
    {
      "epoch": 16.678068410462778,
      "grad_norm": 0.5716272592544556,
      "learning_rate": 0.00016666062984203644,
      "loss": 0.4736,
      "step": 8289
    },
    {
      "epoch": 16.680080482897385,
      "grad_norm": 0.5214449167251587,
      "learning_rate": 0.00016665660529228293,
      "loss": 0.4143,
      "step": 8290
    },
    {
      "epoch": 16.68209255533199,
      "grad_norm": 0.5729954242706299,
      "learning_rate": 0.00016665258074252944,
      "loss": 0.458,
      "step": 8291
    },
    {
      "epoch": 16.6841046277666,
      "grad_norm": 0.5345441699028015,
      "learning_rate": 0.00016664855619277595,
      "loss": 0.4502,
      "step": 8292
    },
    {
      "epoch": 16.686116700201207,
      "grad_norm": 0.5440582036972046,
      "learning_rate": 0.00016664453164302244,
      "loss": 0.4225,
      "step": 8293
    },
    {
      "epoch": 16.688128772635814,
      "grad_norm": 0.5610960125923157,
      "learning_rate": 0.00016664050709326895,
      "loss": 0.444,
      "step": 8294
    },
    {
      "epoch": 16.690140845070424,
      "grad_norm": 0.5442938208580017,
      "learning_rate": 0.00016663648254351546,
      "loss": 0.4399,
      "step": 8295
    },
    {
      "epoch": 16.69215291750503,
      "grad_norm": 0.5326491594314575,
      "learning_rate": 0.00016663245799376195,
      "loss": 0.4661,
      "step": 8296
    },
    {
      "epoch": 16.694164989939637,
      "grad_norm": 0.5348320007324219,
      "learning_rate": 0.00016662843344400846,
      "loss": 0.471,
      "step": 8297
    },
    {
      "epoch": 16.696177062374247,
      "grad_norm": 0.5234320759773254,
      "learning_rate": 0.00016662440889425497,
      "loss": 0.4563,
      "step": 8298
    },
    {
      "epoch": 16.698189134808853,
      "grad_norm": 0.5455676913261414,
      "learning_rate": 0.00016662038434450146,
      "loss": 0.425,
      "step": 8299
    },
    {
      "epoch": 16.70020120724346,
      "grad_norm": 0.5589350461959839,
      "learning_rate": 0.00016661635979474797,
      "loss": 0.4224,
      "step": 8300
    },
    {
      "epoch": 16.70221327967807,
      "grad_norm": 0.5299139618873596,
      "learning_rate": 0.00016661233524499445,
      "loss": 0.416,
      "step": 8301
    },
    {
      "epoch": 16.704225352112676,
      "grad_norm": 0.5322353839874268,
      "learning_rate": 0.000166608310695241,
      "loss": 0.4534,
      "step": 8302
    },
    {
      "epoch": 16.706237424547282,
      "grad_norm": 0.5460196137428284,
      "learning_rate": 0.00016660428614548748,
      "loss": 0.468,
      "step": 8303
    },
    {
      "epoch": 16.708249496981892,
      "grad_norm": 0.5328546166419983,
      "learning_rate": 0.000166600261595734,
      "loss": 0.4468,
      "step": 8304
    },
    {
      "epoch": 16.7102615694165,
      "grad_norm": 0.5263974070549011,
      "learning_rate": 0.00016659623704598048,
      "loss": 0.3951,
      "step": 8305
    },
    {
      "epoch": 16.712273641851105,
      "grad_norm": 0.5470567941665649,
      "learning_rate": 0.000166592212496227,
      "loss": 0.4303,
      "step": 8306
    },
    {
      "epoch": 16.714285714285715,
      "grad_norm": 0.5511165857315063,
      "learning_rate": 0.0001665881879464735,
      "loss": 0.4428,
      "step": 8307
    },
    {
      "epoch": 16.71629778672032,
      "grad_norm": 0.5727799534797668,
      "learning_rate": 0.00016658416339672,
      "loss": 0.4445,
      "step": 8308
    },
    {
      "epoch": 16.718309859154928,
      "grad_norm": 0.5203011631965637,
      "learning_rate": 0.0001665801388469665,
      "loss": 0.4239,
      "step": 8309
    },
    {
      "epoch": 16.720321931589538,
      "grad_norm": 0.5242851376533508,
      "learning_rate": 0.000166576114297213,
      "loss": 0.4358,
      "step": 8310
    },
    {
      "epoch": 16.722334004024145,
      "grad_norm": 0.5185617208480835,
      "learning_rate": 0.0001665720897474595,
      "loss": 0.4308,
      "step": 8311
    },
    {
      "epoch": 16.72434607645875,
      "grad_norm": 0.5625302791595459,
      "learning_rate": 0.00016656806519770603,
      "loss": 0.4211,
      "step": 8312
    },
    {
      "epoch": 16.72635814889336,
      "grad_norm": 0.5468012094497681,
      "learning_rate": 0.00016656404064795252,
      "loss": 0.4478,
      "step": 8313
    },
    {
      "epoch": 16.728370221327967,
      "grad_norm": 0.52022385597229,
      "learning_rate": 0.00016656001609819903,
      "loss": 0.4335,
      "step": 8314
    },
    {
      "epoch": 16.730382293762574,
      "grad_norm": 0.5742027163505554,
      "learning_rate": 0.00016655599154844552,
      "loss": 0.4599,
      "step": 8315
    },
    {
      "epoch": 16.732394366197184,
      "grad_norm": 0.4997040331363678,
      "learning_rate": 0.00016655196699869203,
      "loss": 0.4175,
      "step": 8316
    },
    {
      "epoch": 16.73440643863179,
      "grad_norm": 0.5372480154037476,
      "learning_rate": 0.00016654794244893854,
      "loss": 0.4553,
      "step": 8317
    },
    {
      "epoch": 16.736418511066397,
      "grad_norm": 0.5169543623924255,
      "learning_rate": 0.00016654391789918505,
      "loss": 0.4241,
      "step": 8318
    },
    {
      "epoch": 16.738430583501007,
      "grad_norm": 0.530720591545105,
      "learning_rate": 0.00016653989334943154,
      "loss": 0.4407,
      "step": 8319
    },
    {
      "epoch": 16.740442655935613,
      "grad_norm": 0.521166205406189,
      "learning_rate": 0.00016653586879967805,
      "loss": 0.4461,
      "step": 8320
    },
    {
      "epoch": 16.74245472837022,
      "grad_norm": 0.5191617012023926,
      "learning_rate": 0.00016653184424992454,
      "loss": 0.4054,
      "step": 8321
    },
    {
      "epoch": 16.74446680080483,
      "grad_norm": 0.5339537262916565,
      "learning_rate": 0.00016652781970017108,
      "loss": 0.4295,
      "step": 8322
    },
    {
      "epoch": 16.746478873239436,
      "grad_norm": 0.5277564525604248,
      "learning_rate": 0.00016652379515041756,
      "loss": 0.4665,
      "step": 8323
    },
    {
      "epoch": 16.748490945674043,
      "grad_norm": 0.5292893648147583,
      "learning_rate": 0.00016651977060066407,
      "loss": 0.4279,
      "step": 8324
    },
    {
      "epoch": 16.750503018108652,
      "grad_norm": 0.5568022131919861,
      "learning_rate": 0.00016651574605091056,
      "loss": 0.4478,
      "step": 8325
    },
    {
      "epoch": 16.75251509054326,
      "grad_norm": 0.5446447730064392,
      "learning_rate": 0.00016651172150115707,
      "loss": 0.4509,
      "step": 8326
    },
    {
      "epoch": 16.754527162977865,
      "grad_norm": 0.5559377670288086,
      "learning_rate": 0.00016650769695140358,
      "loss": 0.4193,
      "step": 8327
    },
    {
      "epoch": 16.756539235412475,
      "grad_norm": 0.5428972840309143,
      "learning_rate": 0.00016650367240165007,
      "loss": 0.4389,
      "step": 8328
    },
    {
      "epoch": 16.758551307847082,
      "grad_norm": 0.5402886271476746,
      "learning_rate": 0.00016649964785189658,
      "loss": 0.415,
      "step": 8329
    },
    {
      "epoch": 16.760563380281692,
      "grad_norm": 0.5439505577087402,
      "learning_rate": 0.00016649562330214306,
      "loss": 0.4808,
      "step": 8330
    },
    {
      "epoch": 16.7625754527163,
      "grad_norm": 0.5423354506492615,
      "learning_rate": 0.00016649159875238958,
      "loss": 0.4396,
      "step": 8331
    },
    {
      "epoch": 16.764587525150905,
      "grad_norm": 0.5235763192176819,
      "learning_rate": 0.0001664875742026361,
      "loss": 0.4112,
      "step": 8332
    },
    {
      "epoch": 16.766599597585515,
      "grad_norm": 0.5230134129524231,
      "learning_rate": 0.0001664835496528826,
      "loss": 0.4608,
      "step": 8333
    },
    {
      "epoch": 16.76861167002012,
      "grad_norm": 0.5479276180267334,
      "learning_rate": 0.00016647952510312909,
      "loss": 0.4684,
      "step": 8334
    },
    {
      "epoch": 16.770623742454728,
      "grad_norm": 0.5037106871604919,
      "learning_rate": 0.0001664755005533756,
      "loss": 0.454,
      "step": 8335
    },
    {
      "epoch": 16.772635814889338,
      "grad_norm": 0.5128544569015503,
      "learning_rate": 0.00016647147600362208,
      "loss": 0.4401,
      "step": 8336
    },
    {
      "epoch": 16.774647887323944,
      "grad_norm": 0.5631580352783203,
      "learning_rate": 0.00016646745145386862,
      "loss": 0.466,
      "step": 8337
    },
    {
      "epoch": 16.77665995975855,
      "grad_norm": 0.5261943936347961,
      "learning_rate": 0.0001664634269041151,
      "loss": 0.4565,
      "step": 8338
    },
    {
      "epoch": 16.77867203219316,
      "grad_norm": 0.5402885675430298,
      "learning_rate": 0.00016645940235436162,
      "loss": 0.4334,
      "step": 8339
    },
    {
      "epoch": 16.780684104627767,
      "grad_norm": 0.509727418422699,
      "learning_rate": 0.0001664553778046081,
      "loss": 0.4137,
      "step": 8340
    },
    {
      "epoch": 16.782696177062373,
      "grad_norm": 0.5328602194786072,
      "learning_rate": 0.00016645135325485462,
      "loss": 0.4255,
      "step": 8341
    },
    {
      "epoch": 16.784708249496983,
      "grad_norm": 0.5195904970169067,
      "learning_rate": 0.00016644732870510113,
      "loss": 0.4286,
      "step": 8342
    },
    {
      "epoch": 16.78672032193159,
      "grad_norm": 0.5442083477973938,
      "learning_rate": 0.00016644330415534764,
      "loss": 0.4585,
      "step": 8343
    },
    {
      "epoch": 16.788732394366196,
      "grad_norm": 0.5315159559249878,
      "learning_rate": 0.00016643927960559413,
      "loss": 0.4511,
      "step": 8344
    },
    {
      "epoch": 16.790744466800806,
      "grad_norm": 0.5066776275634766,
      "learning_rate": 0.00016643525505584064,
      "loss": 0.4275,
      "step": 8345
    },
    {
      "epoch": 16.792756539235413,
      "grad_norm": 0.5272502899169922,
      "learning_rate": 0.00016643123050608712,
      "loss": 0.4596,
      "step": 8346
    },
    {
      "epoch": 16.79476861167002,
      "grad_norm": 0.510519802570343,
      "learning_rate": 0.00016642720595633366,
      "loss": 0.4549,
      "step": 8347
    },
    {
      "epoch": 16.79678068410463,
      "grad_norm": 0.5308730006217957,
      "learning_rate": 0.00016642318140658015,
      "loss": 0.4456,
      "step": 8348
    },
    {
      "epoch": 16.798792756539235,
      "grad_norm": 0.5374624729156494,
      "learning_rate": 0.00016641915685682666,
      "loss": 0.4282,
      "step": 8349
    },
    {
      "epoch": 16.800804828973842,
      "grad_norm": 0.5661358833312988,
      "learning_rate": 0.00016641513230707315,
      "loss": 0.4365,
      "step": 8350
    },
    {
      "epoch": 16.802816901408452,
      "grad_norm": 0.5312092304229736,
      "learning_rate": 0.00016641110775731966,
      "loss": 0.4519,
      "step": 8351
    },
    {
      "epoch": 16.80482897384306,
      "grad_norm": 0.5242884159088135,
      "learning_rate": 0.00016640708320756617,
      "loss": 0.4252,
      "step": 8352
    },
    {
      "epoch": 16.806841046277665,
      "grad_norm": 0.5351622104644775,
      "learning_rate": 0.00016640305865781268,
      "loss": 0.4699,
      "step": 8353
    },
    {
      "epoch": 16.808853118712275,
      "grad_norm": 0.5358599424362183,
      "learning_rate": 0.00016639903410805917,
      "loss": 0.4488,
      "step": 8354
    },
    {
      "epoch": 16.81086519114688,
      "grad_norm": 0.5581740140914917,
      "learning_rate": 0.00016639500955830568,
      "loss": 0.4983,
      "step": 8355
    },
    {
      "epoch": 16.812877263581488,
      "grad_norm": 0.5027979016304016,
      "learning_rate": 0.00016639098500855217,
      "loss": 0.4517,
      "step": 8356
    },
    {
      "epoch": 16.814889336016098,
      "grad_norm": 0.5231884121894836,
      "learning_rate": 0.00016638696045879868,
      "loss": 0.4609,
      "step": 8357
    },
    {
      "epoch": 16.816901408450704,
      "grad_norm": 0.49517911672592163,
      "learning_rate": 0.0001663829359090452,
      "loss": 0.426,
      "step": 8358
    },
    {
      "epoch": 16.81891348088531,
      "grad_norm": 0.5367566347122192,
      "learning_rate": 0.0001663789113592917,
      "loss": 0.4543,
      "step": 8359
    },
    {
      "epoch": 16.82092555331992,
      "grad_norm": 0.5194041728973389,
      "learning_rate": 0.0001663748868095382,
      "loss": 0.4323,
      "step": 8360
    },
    {
      "epoch": 16.822937625754527,
      "grad_norm": 0.5262913703918457,
      "learning_rate": 0.0001663708622597847,
      "loss": 0.4251,
      "step": 8361
    },
    {
      "epoch": 16.824949698189133,
      "grad_norm": 0.5210273265838623,
      "learning_rate": 0.0001663668377100312,
      "loss": 0.4283,
      "step": 8362
    },
    {
      "epoch": 16.826961770623743,
      "grad_norm": 0.5435473322868347,
      "learning_rate": 0.0001663628131602777,
      "loss": 0.4408,
      "step": 8363
    },
    {
      "epoch": 16.82897384305835,
      "grad_norm": 0.5318405032157898,
      "learning_rate": 0.0001663587886105242,
      "loss": 0.4143,
      "step": 8364
    },
    {
      "epoch": 16.830985915492956,
      "grad_norm": 0.5429338216781616,
      "learning_rate": 0.0001663547640607707,
      "loss": 0.4487,
      "step": 8365
    },
    {
      "epoch": 16.832997987927566,
      "grad_norm": 0.5265556573867798,
      "learning_rate": 0.0001663507395110172,
      "loss": 0.4509,
      "step": 8366
    },
    {
      "epoch": 16.835010060362173,
      "grad_norm": 0.5189302563667297,
      "learning_rate": 0.00016634671496126372,
      "loss": 0.434,
      "step": 8367
    },
    {
      "epoch": 16.83702213279678,
      "grad_norm": 0.5264939069747925,
      "learning_rate": 0.00016634269041151023,
      "loss": 0.4342,
      "step": 8368
    },
    {
      "epoch": 16.83903420523139,
      "grad_norm": 0.5223227143287659,
      "learning_rate": 0.00016633866586175672,
      "loss": 0.4482,
      "step": 8369
    },
    {
      "epoch": 16.841046277665995,
      "grad_norm": 0.518015444278717,
      "learning_rate": 0.00016633464131200323,
      "loss": 0.4411,
      "step": 8370
    },
    {
      "epoch": 16.843058350100602,
      "grad_norm": 0.5554770231246948,
      "learning_rate": 0.0001663306167622497,
      "loss": 0.4336,
      "step": 8371
    },
    {
      "epoch": 16.845070422535212,
      "grad_norm": 0.5286738872528076,
      "learning_rate": 0.00016632659221249623,
      "loss": 0.4497,
      "step": 8372
    },
    {
      "epoch": 16.84708249496982,
      "grad_norm": 0.5653141140937805,
      "learning_rate": 0.00016632256766274274,
      "loss": 0.4835,
      "step": 8373
    },
    {
      "epoch": 16.84909456740443,
      "grad_norm": 0.522362470626831,
      "learning_rate": 0.00016631854311298925,
      "loss": 0.4232,
      "step": 8374
    },
    {
      "epoch": 16.851106639839035,
      "grad_norm": 0.5228455662727356,
      "learning_rate": 0.00016631451856323573,
      "loss": 0.4452,
      "step": 8375
    },
    {
      "epoch": 16.85311871227364,
      "grad_norm": 0.5487924218177795,
      "learning_rate": 0.00016631049401348225,
      "loss": 0.4762,
      "step": 8376
    },
    {
      "epoch": 16.85513078470825,
      "grad_norm": 0.5235661864280701,
      "learning_rate": 0.00016630646946372873,
      "loss": 0.3933,
      "step": 8377
    },
    {
      "epoch": 16.857142857142858,
      "grad_norm": 0.5243000984191895,
      "learning_rate": 0.00016630244491397527,
      "loss": 0.421,
      "step": 8378
    },
    {
      "epoch": 16.859154929577464,
      "grad_norm": 0.5166539549827576,
      "learning_rate": 0.00016629842036422176,
      "loss": 0.4213,
      "step": 8379
    },
    {
      "epoch": 16.861167002012074,
      "grad_norm": 0.5554929971694946,
      "learning_rate": 0.00016629439581446827,
      "loss": 0.4657,
      "step": 8380
    },
    {
      "epoch": 16.86317907444668,
      "grad_norm": 0.5191907286643982,
      "learning_rate": 0.00016629037126471475,
      "loss": 0.4398,
      "step": 8381
    },
    {
      "epoch": 16.865191146881287,
      "grad_norm": 0.5229232907295227,
      "learning_rate": 0.00016628634671496127,
      "loss": 0.429,
      "step": 8382
    },
    {
      "epoch": 16.867203219315897,
      "grad_norm": 0.5188505053520203,
      "learning_rate": 0.00016628232216520778,
      "loss": 0.4461,
      "step": 8383
    },
    {
      "epoch": 16.869215291750503,
      "grad_norm": 0.5109816193580627,
      "learning_rate": 0.0001662782976154543,
      "loss": 0.4124,
      "step": 8384
    },
    {
      "epoch": 16.87122736418511,
      "grad_norm": 0.5180532932281494,
      "learning_rate": 0.00016627427306570078,
      "loss": 0.4292,
      "step": 8385
    },
    {
      "epoch": 16.87323943661972,
      "grad_norm": 0.5253850221633911,
      "learning_rate": 0.0001662702485159473,
      "loss": 0.4625,
      "step": 8386
    },
    {
      "epoch": 16.875251509054326,
      "grad_norm": 0.49937793612480164,
      "learning_rate": 0.00016626622396619377,
      "loss": 0.449,
      "step": 8387
    },
    {
      "epoch": 16.877263581488933,
      "grad_norm": 0.526924729347229,
      "learning_rate": 0.0001662621994164403,
      "loss": 0.4274,
      "step": 8388
    },
    {
      "epoch": 16.879275653923543,
      "grad_norm": 0.5122800469398499,
      "learning_rate": 0.0001662581748666868,
      "loss": 0.4139,
      "step": 8389
    },
    {
      "epoch": 16.88128772635815,
      "grad_norm": 0.5534771084785461,
      "learning_rate": 0.0001662541503169333,
      "loss": 0.4694,
      "step": 8390
    },
    {
      "epoch": 16.883299798792756,
      "grad_norm": 0.5363740921020508,
      "learning_rate": 0.0001662501257671798,
      "loss": 0.4247,
      "step": 8391
    },
    {
      "epoch": 16.885311871227366,
      "grad_norm": 0.5495151281356812,
      "learning_rate": 0.0001662461012174263,
      "loss": 0.4537,
      "step": 8392
    },
    {
      "epoch": 16.887323943661972,
      "grad_norm": 0.5181635618209839,
      "learning_rate": 0.00016624207666767282,
      "loss": 0.4535,
      "step": 8393
    },
    {
      "epoch": 16.88933601609658,
      "grad_norm": 0.5283640027046204,
      "learning_rate": 0.00016623805211791933,
      "loss": 0.4342,
      "step": 8394
    },
    {
      "epoch": 16.89134808853119,
      "grad_norm": 0.5390255451202393,
      "learning_rate": 0.00016623402756816582,
      "loss": 0.437,
      "step": 8395
    },
    {
      "epoch": 16.893360160965795,
      "grad_norm": 0.561251699924469,
      "learning_rate": 0.00016623000301841233,
      "loss": 0.4677,
      "step": 8396
    },
    {
      "epoch": 16.8953722334004,
      "grad_norm": 0.5632472038269043,
      "learning_rate": 0.00016622597846865881,
      "loss": 0.4152,
      "step": 8397
    },
    {
      "epoch": 16.89738430583501,
      "grad_norm": 0.5400870442390442,
      "learning_rate": 0.00016622195391890533,
      "loss": 0.4634,
      "step": 8398
    },
    {
      "epoch": 16.899396378269618,
      "grad_norm": 0.5215831398963928,
      "learning_rate": 0.00016621792936915184,
      "loss": 0.4255,
      "step": 8399
    },
    {
      "epoch": 16.901408450704224,
      "grad_norm": 0.5297478437423706,
      "learning_rate": 0.00016621390481939832,
      "loss": 0.4795,
      "step": 8400
    },
    {
      "epoch": 16.903420523138834,
      "grad_norm": 0.5216572284698486,
      "learning_rate": 0.00016620988026964484,
      "loss": 0.4407,
      "step": 8401
    },
    {
      "epoch": 16.90543259557344,
      "grad_norm": 0.5232091546058655,
      "learning_rate": 0.00016620585571989135,
      "loss": 0.4326,
      "step": 8402
    },
    {
      "epoch": 16.907444668008047,
      "grad_norm": 0.5585110187530518,
      "learning_rate": 0.00016620183117013786,
      "loss": 0.4343,
      "step": 8403
    },
    {
      "epoch": 16.909456740442657,
      "grad_norm": 0.5292837619781494,
      "learning_rate": 0.00016619780662038435,
      "loss": 0.4404,
      "step": 8404
    },
    {
      "epoch": 16.911468812877263,
      "grad_norm": 0.5345982313156128,
      "learning_rate": 0.00016619378207063086,
      "loss": 0.4509,
      "step": 8405
    },
    {
      "epoch": 16.91348088531187,
      "grad_norm": 0.5183001160621643,
      "learning_rate": 0.00016618975752087734,
      "loss": 0.4245,
      "step": 8406
    },
    {
      "epoch": 16.91549295774648,
      "grad_norm": 0.5473116636276245,
      "learning_rate": 0.00016618573297112385,
      "loss": 0.4509,
      "step": 8407
    },
    {
      "epoch": 16.917505030181086,
      "grad_norm": 0.5654874444007874,
      "learning_rate": 0.00016618170842137037,
      "loss": 0.4255,
      "step": 8408
    },
    {
      "epoch": 16.919517102615693,
      "grad_norm": 0.5141612887382507,
      "learning_rate": 0.00016617768387161688,
      "loss": 0.4426,
      "step": 8409
    },
    {
      "epoch": 16.921529175050303,
      "grad_norm": 0.5548219084739685,
      "learning_rate": 0.00016617365932186336,
      "loss": 0.4738,
      "step": 8410
    },
    {
      "epoch": 16.92354124748491,
      "grad_norm": 0.5787235498428345,
      "learning_rate": 0.00016616963477210988,
      "loss": 0.4498,
      "step": 8411
    },
    {
      "epoch": 16.925553319919516,
      "grad_norm": 0.5517971515655518,
      "learning_rate": 0.00016616561022235636,
      "loss": 0.4431,
      "step": 8412
    },
    {
      "epoch": 16.927565392354126,
      "grad_norm": 0.5540522933006287,
      "learning_rate": 0.0001661615856726029,
      "loss": 0.4219,
      "step": 8413
    },
    {
      "epoch": 16.929577464788732,
      "grad_norm": 0.5666874051094055,
      "learning_rate": 0.00016615756112284939,
      "loss": 0.4507,
      "step": 8414
    },
    {
      "epoch": 16.93158953722334,
      "grad_norm": 0.5218634009361267,
      "learning_rate": 0.0001661535365730959,
      "loss": 0.4483,
      "step": 8415
    },
    {
      "epoch": 16.93360160965795,
      "grad_norm": 0.5953515768051147,
      "learning_rate": 0.00016614951202334238,
      "loss": 0.4553,
      "step": 8416
    },
    {
      "epoch": 16.935613682092555,
      "grad_norm": 0.5670790672302246,
      "learning_rate": 0.0001661454874735889,
      "loss": 0.4256,
      "step": 8417
    },
    {
      "epoch": 16.93762575452716,
      "grad_norm": 0.5610931515693665,
      "learning_rate": 0.0001661414629238354,
      "loss": 0.4287,
      "step": 8418
    },
    {
      "epoch": 16.93963782696177,
      "grad_norm": 0.5580146312713623,
      "learning_rate": 0.00016613743837408192,
      "loss": 0.4446,
      "step": 8419
    },
    {
      "epoch": 16.941649899396378,
      "grad_norm": 0.5300270318984985,
      "learning_rate": 0.0001661334138243284,
      "loss": 0.4555,
      "step": 8420
    },
    {
      "epoch": 16.943661971830984,
      "grad_norm": 0.5498335957527161,
      "learning_rate": 0.00016612938927457492,
      "loss": 0.4573,
      "step": 8421
    },
    {
      "epoch": 16.945674044265594,
      "grad_norm": 0.5238277912139893,
      "learning_rate": 0.0001661253647248214,
      "loss": 0.4092,
      "step": 8422
    },
    {
      "epoch": 16.9476861167002,
      "grad_norm": 0.5295702815055847,
      "learning_rate": 0.00016612134017506794,
      "loss": 0.4709,
      "step": 8423
    },
    {
      "epoch": 16.949698189134807,
      "grad_norm": 0.5562023520469666,
      "learning_rate": 0.00016611731562531443,
      "loss": 0.4185,
      "step": 8424
    },
    {
      "epoch": 16.951710261569417,
      "grad_norm": 0.5266855955123901,
      "learning_rate": 0.00016611329107556094,
      "loss": 0.483,
      "step": 8425
    },
    {
      "epoch": 16.953722334004024,
      "grad_norm": 0.5321042537689209,
      "learning_rate": 0.00016610926652580742,
      "loss": 0.4665,
      "step": 8426
    },
    {
      "epoch": 16.955734406438633,
      "grad_norm": 0.49776792526245117,
      "learning_rate": 0.00016610524197605394,
      "loss": 0.4342,
      "step": 8427
    },
    {
      "epoch": 16.95774647887324,
      "grad_norm": 0.5055845379829407,
      "learning_rate": 0.00016610121742630045,
      "loss": 0.4502,
      "step": 8428
    },
    {
      "epoch": 16.959758551307846,
      "grad_norm": 0.5333277583122253,
      "learning_rate": 0.00016609719287654696,
      "loss": 0.4267,
      "step": 8429
    },
    {
      "epoch": 16.961770623742456,
      "grad_norm": 0.5392646193504333,
      "learning_rate": 0.00016609316832679345,
      "loss": 0.4551,
      "step": 8430
    },
    {
      "epoch": 16.963782696177063,
      "grad_norm": 0.5580921173095703,
      "learning_rate": 0.00016608914377703996,
      "loss": 0.4431,
      "step": 8431
    },
    {
      "epoch": 16.96579476861167,
      "grad_norm": 0.5345814228057861,
      "learning_rate": 0.00016608511922728644,
      "loss": 0.4447,
      "step": 8432
    },
    {
      "epoch": 16.96780684104628,
      "grad_norm": 0.5885222554206848,
      "learning_rate": 0.00016608109467753296,
      "loss": 0.4831,
      "step": 8433
    },
    {
      "epoch": 16.969818913480886,
      "grad_norm": 0.5172339677810669,
      "learning_rate": 0.00016607707012777947,
      "loss": 0.4097,
      "step": 8434
    },
    {
      "epoch": 16.971830985915492,
      "grad_norm": 0.5379565954208374,
      "learning_rate": 0.00016607304557802595,
      "loss": 0.4356,
      "step": 8435
    },
    {
      "epoch": 16.973843058350102,
      "grad_norm": 0.5354264378547668,
      "learning_rate": 0.00016606902102827246,
      "loss": 0.4555,
      "step": 8436
    },
    {
      "epoch": 16.97585513078471,
      "grad_norm": 0.5140238404273987,
      "learning_rate": 0.00016606499647851898,
      "loss": 0.4529,
      "step": 8437
    },
    {
      "epoch": 16.977867203219315,
      "grad_norm": 0.5255675911903381,
      "learning_rate": 0.0001660609719287655,
      "loss": 0.4508,
      "step": 8438
    },
    {
      "epoch": 16.979879275653925,
      "grad_norm": 0.5322712063789368,
      "learning_rate": 0.00016605694737901197,
      "loss": 0.4183,
      "step": 8439
    },
    {
      "epoch": 16.98189134808853,
      "grad_norm": 0.5531941056251526,
      "learning_rate": 0.0001660529228292585,
      "loss": 0.4645,
      "step": 8440
    },
    {
      "epoch": 16.983903420523138,
      "grad_norm": 0.5221073627471924,
      "learning_rate": 0.00016604889827950497,
      "loss": 0.4459,
      "step": 8441
    },
    {
      "epoch": 16.985915492957748,
      "grad_norm": 0.5366290211677551,
      "learning_rate": 0.00016604487372975148,
      "loss": 0.4228,
      "step": 8442
    },
    {
      "epoch": 16.987927565392354,
      "grad_norm": 0.5417541265487671,
      "learning_rate": 0.000166040849179998,
      "loss": 0.4888,
      "step": 8443
    },
    {
      "epoch": 16.98993963782696,
      "grad_norm": 0.5177537202835083,
      "learning_rate": 0.0001660368246302445,
      "loss": 0.449,
      "step": 8444
    },
    {
      "epoch": 16.99195171026157,
      "grad_norm": 0.5347569584846497,
      "learning_rate": 0.000166032800080491,
      "loss": 0.4839,
      "step": 8445
    },
    {
      "epoch": 16.993963782696177,
      "grad_norm": 0.5180078744888306,
      "learning_rate": 0.0001660287755307375,
      "loss": 0.409,
      "step": 8446
    },
    {
      "epoch": 16.995975855130784,
      "grad_norm": 0.5389052033424377,
      "learning_rate": 0.000166024750980984,
      "loss": 0.4168,
      "step": 8447
    },
    {
      "epoch": 16.997987927565394,
      "grad_norm": 0.5549967885017395,
      "learning_rate": 0.00016602072643123053,
      "loss": 0.4528,
      "step": 8448
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.5534797310829163,
      "learning_rate": 0.00016601670188147702,
      "loss": 0.439,
      "step": 8449
    },
    {
      "epoch": 17.0,
      "eval_loss": 0.8858647346496582,
      "eval_runtime": 49.8062,
      "eval_samples_per_second": 19.917,
      "eval_steps_per_second": 2.49,
      "step": 8449
    },
    {
      "epoch": 17.002012072434606,
      "grad_norm": 0.48028457164764404,
      "learning_rate": 0.00016601267733172353,
      "loss": 0.3658,
      "step": 8450
    },
    {
      "epoch": 17.004024144869216,
      "grad_norm": 0.5010720491409302,
      "learning_rate": 0.00016600865278197,
      "loss": 0.3736,
      "step": 8451
    },
    {
      "epoch": 17.006036217303823,
      "grad_norm": 0.4984036087989807,
      "learning_rate": 0.00016600462823221652,
      "loss": 0.3752,
      "step": 8452
    },
    {
      "epoch": 17.00804828973843,
      "grad_norm": 0.5427587628364563,
      "learning_rate": 0.00016600060368246304,
      "loss": 0.3877,
      "step": 8453
    },
    {
      "epoch": 17.01006036217304,
      "grad_norm": 0.5609927773475647,
      "learning_rate": 0.00016599657913270955,
      "loss": 0.3852,
      "step": 8454
    },
    {
      "epoch": 17.012072434607646,
      "grad_norm": 0.5852439999580383,
      "learning_rate": 0.00016599255458295603,
      "loss": 0.3878,
      "step": 8455
    },
    {
      "epoch": 17.014084507042252,
      "grad_norm": 0.5299956202507019,
      "learning_rate": 0.00016598853003320255,
      "loss": 0.3787,
      "step": 8456
    },
    {
      "epoch": 17.016096579476862,
      "grad_norm": 0.5354337692260742,
      "learning_rate": 0.00016598450548344903,
      "loss": 0.3758,
      "step": 8457
    },
    {
      "epoch": 17.01810865191147,
      "grad_norm": 0.50471031665802,
      "learning_rate": 0.00016598048093369557,
      "loss": 0.3492,
      "step": 8458
    },
    {
      "epoch": 17.020120724346075,
      "grad_norm": 0.48838236927986145,
      "learning_rate": 0.00016597645638394206,
      "loss": 0.3623,
      "step": 8459
    },
    {
      "epoch": 17.022132796780685,
      "grad_norm": 0.5225790739059448,
      "learning_rate": 0.00016597243183418857,
      "loss": 0.3762,
      "step": 8460
    },
    {
      "epoch": 17.02414486921529,
      "grad_norm": 0.5309543013572693,
      "learning_rate": 0.00016596840728443505,
      "loss": 0.4119,
      "step": 8461
    },
    {
      "epoch": 17.026156941649898,
      "grad_norm": 0.5524566769599915,
      "learning_rate": 0.00016596438273468157,
      "loss": 0.4144,
      "step": 8462
    },
    {
      "epoch": 17.028169014084508,
      "grad_norm": 0.5193338990211487,
      "learning_rate": 0.00016596035818492808,
      "loss": 0.3764,
      "step": 8463
    },
    {
      "epoch": 17.030181086519114,
      "grad_norm": 0.5675846338272095,
      "learning_rate": 0.0001659563336351746,
      "loss": 0.3873,
      "step": 8464
    },
    {
      "epoch": 17.03219315895372,
      "grad_norm": 0.5401807427406311,
      "learning_rate": 0.00016595230908542108,
      "loss": 0.3975,
      "step": 8465
    },
    {
      "epoch": 17.03420523138833,
      "grad_norm": 0.5434258580207825,
      "learning_rate": 0.0001659482845356676,
      "loss": 0.3865,
      "step": 8466
    },
    {
      "epoch": 17.036217303822937,
      "grad_norm": 0.48135218024253845,
      "learning_rate": 0.00016594425998591407,
      "loss": 0.3468,
      "step": 8467
    },
    {
      "epoch": 17.038229376257544,
      "grad_norm": 0.5416181087493896,
      "learning_rate": 0.00016594023543616058,
      "loss": 0.4093,
      "step": 8468
    },
    {
      "epoch": 17.040241448692154,
      "grad_norm": 0.5226427912712097,
      "learning_rate": 0.0001659362108864071,
      "loss": 0.3767,
      "step": 8469
    },
    {
      "epoch": 17.04225352112676,
      "grad_norm": 0.5494763851165771,
      "learning_rate": 0.00016593218633665358,
      "loss": 0.3941,
      "step": 8470
    },
    {
      "epoch": 17.044265593561367,
      "grad_norm": 0.5406976342201233,
      "learning_rate": 0.0001659281617869001,
      "loss": 0.3959,
      "step": 8471
    },
    {
      "epoch": 17.046277665995976,
      "grad_norm": 0.5278927683830261,
      "learning_rate": 0.00016592413723714658,
      "loss": 0.3727,
      "step": 8472
    },
    {
      "epoch": 17.048289738430583,
      "grad_norm": 0.5831350684165955,
      "learning_rate": 0.00016592011268739312,
      "loss": 0.403,
      "step": 8473
    },
    {
      "epoch": 17.050301810865193,
      "grad_norm": 0.5070232152938843,
      "learning_rate": 0.0001659160881376396,
      "loss": 0.3589,
      "step": 8474
    },
    {
      "epoch": 17.0523138832998,
      "grad_norm": 0.5226315855979919,
      "learning_rate": 0.00016591206358788612,
      "loss": 0.3929,
      "step": 8475
    },
    {
      "epoch": 17.054325955734406,
      "grad_norm": 0.5570104718208313,
      "learning_rate": 0.0001659080390381326,
      "loss": 0.4025,
      "step": 8476
    },
    {
      "epoch": 17.056338028169016,
      "grad_norm": 0.5144354701042175,
      "learning_rate": 0.0001659040144883791,
      "loss": 0.3463,
      "step": 8477
    },
    {
      "epoch": 17.058350100603622,
      "grad_norm": 0.5685533285140991,
      "learning_rate": 0.00016589998993862563,
      "loss": 0.3977,
      "step": 8478
    },
    {
      "epoch": 17.06036217303823,
      "grad_norm": 0.5371354818344116,
      "learning_rate": 0.00016589596538887214,
      "loss": 0.3756,
      "step": 8479
    },
    {
      "epoch": 17.06237424547284,
      "grad_norm": 0.5392351746559143,
      "learning_rate": 0.00016589194083911862,
      "loss": 0.4004,
      "step": 8480
    },
    {
      "epoch": 17.064386317907445,
      "grad_norm": 0.5444599390029907,
      "learning_rate": 0.00016588791628936514,
      "loss": 0.3872,
      "step": 8481
    },
    {
      "epoch": 17.06639839034205,
      "grad_norm": 0.5231419205665588,
      "learning_rate": 0.00016588389173961162,
      "loss": 0.3741,
      "step": 8482
    },
    {
      "epoch": 17.06841046277666,
      "grad_norm": 0.5387289524078369,
      "learning_rate": 0.00016587986718985816,
      "loss": 0.3802,
      "step": 8483
    },
    {
      "epoch": 17.070422535211268,
      "grad_norm": 0.4908547103404999,
      "learning_rate": 0.00016587584264010464,
      "loss": 0.3355,
      "step": 8484
    },
    {
      "epoch": 17.072434607645874,
      "grad_norm": 0.5230743288993835,
      "learning_rate": 0.00016587181809035116,
      "loss": 0.3815,
      "step": 8485
    },
    {
      "epoch": 17.074446680080484,
      "grad_norm": 0.5330308079719543,
      "learning_rate": 0.00016586779354059764,
      "loss": 0.3798,
      "step": 8486
    },
    {
      "epoch": 17.07645875251509,
      "grad_norm": 0.5348541140556335,
      "learning_rate": 0.00016586376899084415,
      "loss": 0.3717,
      "step": 8487
    },
    {
      "epoch": 17.078470824949697,
      "grad_norm": 0.5588256120681763,
      "learning_rate": 0.00016585974444109067,
      "loss": 0.407,
      "step": 8488
    },
    {
      "epoch": 17.080482897384307,
      "grad_norm": 0.5478912591934204,
      "learning_rate": 0.00016585571989133718,
      "loss": 0.3772,
      "step": 8489
    },
    {
      "epoch": 17.082494969818914,
      "grad_norm": 0.5514009594917297,
      "learning_rate": 0.00016585169534158366,
      "loss": 0.3891,
      "step": 8490
    },
    {
      "epoch": 17.08450704225352,
      "grad_norm": 0.5502551198005676,
      "learning_rate": 0.00016584767079183018,
      "loss": 0.3769,
      "step": 8491
    },
    {
      "epoch": 17.08651911468813,
      "grad_norm": 0.5172996520996094,
      "learning_rate": 0.00016584364624207666,
      "loss": 0.4034,
      "step": 8492
    },
    {
      "epoch": 17.088531187122737,
      "grad_norm": 0.541657030582428,
      "learning_rate": 0.0001658396216923232,
      "loss": 0.4031,
      "step": 8493
    },
    {
      "epoch": 17.090543259557343,
      "grad_norm": 0.5261183977127075,
      "learning_rate": 0.00016583559714256969,
      "loss": 0.4045,
      "step": 8494
    },
    {
      "epoch": 17.092555331991953,
      "grad_norm": 0.520682692527771,
      "learning_rate": 0.0001658315725928162,
      "loss": 0.3764,
      "step": 8495
    },
    {
      "epoch": 17.09456740442656,
      "grad_norm": 0.5621131062507629,
      "learning_rate": 0.00016582754804306268,
      "loss": 0.3568,
      "step": 8496
    },
    {
      "epoch": 17.096579476861166,
      "grad_norm": 0.5491740703582764,
      "learning_rate": 0.0001658235234933092,
      "loss": 0.3799,
      "step": 8497
    },
    {
      "epoch": 17.098591549295776,
      "grad_norm": 0.5308569073677063,
      "learning_rate": 0.0001658194989435557,
      "loss": 0.4103,
      "step": 8498
    },
    {
      "epoch": 17.100603621730382,
      "grad_norm": 0.5014280080795288,
      "learning_rate": 0.0001658154743938022,
      "loss": 0.3643,
      "step": 8499
    },
    {
      "epoch": 17.10261569416499,
      "grad_norm": 0.5717155337333679,
      "learning_rate": 0.0001658114498440487,
      "loss": 0.3936,
      "step": 8500
    },
    {
      "epoch": 17.1046277665996,
      "grad_norm": 0.5753148198127747,
      "learning_rate": 0.00016580742529429522,
      "loss": 0.3815,
      "step": 8501
    },
    {
      "epoch": 17.106639839034205,
      "grad_norm": 0.5439361929893494,
      "learning_rate": 0.0001658034007445417,
      "loss": 0.3551,
      "step": 8502
    },
    {
      "epoch": 17.10865191146881,
      "grad_norm": 0.5206843614578247,
      "learning_rate": 0.00016579937619478821,
      "loss": 0.3597,
      "step": 8503
    },
    {
      "epoch": 17.11066398390342,
      "grad_norm": 0.5537779331207275,
      "learning_rate": 0.00016579535164503473,
      "loss": 0.3711,
      "step": 8504
    },
    {
      "epoch": 17.112676056338028,
      "grad_norm": 0.5220082402229309,
      "learning_rate": 0.0001657913270952812,
      "loss": 0.3969,
      "step": 8505
    },
    {
      "epoch": 17.114688128772634,
      "grad_norm": 0.555476188659668,
      "learning_rate": 0.00016578730254552772,
      "loss": 0.3924,
      "step": 8506
    },
    {
      "epoch": 17.116700201207244,
      "grad_norm": 0.5375513434410095,
      "learning_rate": 0.0001657832779957742,
      "loss": 0.3779,
      "step": 8507
    },
    {
      "epoch": 17.11871227364185,
      "grad_norm": 0.5361893177032471,
      "learning_rate": 0.00016577925344602075,
      "loss": 0.3798,
      "step": 8508
    },
    {
      "epoch": 17.120724346076457,
      "grad_norm": 0.5595827102661133,
      "learning_rate": 0.00016577522889626723,
      "loss": 0.3669,
      "step": 8509
    },
    {
      "epoch": 17.122736418511067,
      "grad_norm": 0.6041492819786072,
      "learning_rate": 0.00016577120434651375,
      "loss": 0.3937,
      "step": 8510
    },
    {
      "epoch": 17.124748490945674,
      "grad_norm": 0.5443927645683289,
      "learning_rate": 0.00016576717979676023,
      "loss": 0.3929,
      "step": 8511
    },
    {
      "epoch": 17.12676056338028,
      "grad_norm": 0.5535230040550232,
      "learning_rate": 0.00016576315524700674,
      "loss": 0.4133,
      "step": 8512
    },
    {
      "epoch": 17.12877263581489,
      "grad_norm": 0.5496939420700073,
      "learning_rate": 0.00016575913069725326,
      "loss": 0.3989,
      "step": 8513
    },
    {
      "epoch": 17.130784708249497,
      "grad_norm": 0.5456663966178894,
      "learning_rate": 0.00016575510614749977,
      "loss": 0.3754,
      "step": 8514
    },
    {
      "epoch": 17.132796780684103,
      "grad_norm": 0.545090913772583,
      "learning_rate": 0.00016575108159774625,
      "loss": 0.3786,
      "step": 8515
    },
    {
      "epoch": 17.134808853118713,
      "grad_norm": 0.5493759512901306,
      "learning_rate": 0.00016574705704799276,
      "loss": 0.367,
      "step": 8516
    },
    {
      "epoch": 17.13682092555332,
      "grad_norm": 0.5903661847114563,
      "learning_rate": 0.00016574303249823925,
      "loss": 0.3535,
      "step": 8517
    },
    {
      "epoch": 17.138832997987926,
      "grad_norm": 0.5710644125938416,
      "learning_rate": 0.0001657390079484858,
      "loss": 0.3989,
      "step": 8518
    },
    {
      "epoch": 17.140845070422536,
      "grad_norm": 0.5356944799423218,
      "learning_rate": 0.00016573498339873227,
      "loss": 0.3595,
      "step": 8519
    },
    {
      "epoch": 17.142857142857142,
      "grad_norm": 0.5640385150909424,
      "learning_rate": 0.00016573095884897879,
      "loss": 0.3993,
      "step": 8520
    },
    {
      "epoch": 17.14486921529175,
      "grad_norm": 0.5471977591514587,
      "learning_rate": 0.00016572693429922527,
      "loss": 0.3828,
      "step": 8521
    },
    {
      "epoch": 17.14688128772636,
      "grad_norm": 0.5613815784454346,
      "learning_rate": 0.00016572290974947178,
      "loss": 0.3978,
      "step": 8522
    },
    {
      "epoch": 17.148893360160965,
      "grad_norm": 0.5369795560836792,
      "learning_rate": 0.0001657188851997183,
      "loss": 0.3868,
      "step": 8523
    },
    {
      "epoch": 17.15090543259557,
      "grad_norm": 0.5645793080329895,
      "learning_rate": 0.0001657148606499648,
      "loss": 0.3864,
      "step": 8524
    },
    {
      "epoch": 17.15291750503018,
      "grad_norm": 0.522429883480072,
      "learning_rate": 0.0001657108361002113,
      "loss": 0.4119,
      "step": 8525
    },
    {
      "epoch": 17.154929577464788,
      "grad_norm": 0.5618031024932861,
      "learning_rate": 0.0001657068115504578,
      "loss": 0.413,
      "step": 8526
    },
    {
      "epoch": 17.156941649899398,
      "grad_norm": 0.5619937181472778,
      "learning_rate": 0.0001657027870007043,
      "loss": 0.3731,
      "step": 8527
    },
    {
      "epoch": 17.158953722334005,
      "grad_norm": 0.531996488571167,
      "learning_rate": 0.00016569876245095083,
      "loss": 0.3729,
      "step": 8528
    },
    {
      "epoch": 17.16096579476861,
      "grad_norm": 0.5447020530700684,
      "learning_rate": 0.00016569473790119732,
      "loss": 0.3995,
      "step": 8529
    },
    {
      "epoch": 17.16297786720322,
      "grad_norm": 0.5556278824806213,
      "learning_rate": 0.00016569071335144383,
      "loss": 0.4088,
      "step": 8530
    },
    {
      "epoch": 17.164989939637827,
      "grad_norm": 0.5535686016082764,
      "learning_rate": 0.0001656866888016903,
      "loss": 0.4036,
      "step": 8531
    },
    {
      "epoch": 17.167002012072434,
      "grad_norm": 0.5359050035476685,
      "learning_rate": 0.00016568266425193682,
      "loss": 0.3797,
      "step": 8532
    },
    {
      "epoch": 17.169014084507044,
      "grad_norm": 0.505867063999176,
      "learning_rate": 0.00016567863970218334,
      "loss": 0.3607,
      "step": 8533
    },
    {
      "epoch": 17.17102615694165,
      "grad_norm": 0.5556027293205261,
      "learning_rate": 0.00016567461515242982,
      "loss": 0.4137,
      "step": 8534
    },
    {
      "epoch": 17.173038229376257,
      "grad_norm": 0.5111607909202576,
      "learning_rate": 0.00016567059060267633,
      "loss": 0.3834,
      "step": 8535
    },
    {
      "epoch": 17.175050301810867,
      "grad_norm": 0.5470276474952698,
      "learning_rate": 0.00016566656605292285,
      "loss": 0.3975,
      "step": 8536
    },
    {
      "epoch": 17.177062374245473,
      "grad_norm": 0.5591427087783813,
      "learning_rate": 0.00016566254150316933,
      "loss": 0.3889,
      "step": 8537
    },
    {
      "epoch": 17.17907444668008,
      "grad_norm": 0.5594227910041809,
      "learning_rate": 0.00016565851695341584,
      "loss": 0.3651,
      "step": 8538
    },
    {
      "epoch": 17.18108651911469,
      "grad_norm": 0.5663492679595947,
      "learning_rate": 0.00016565449240366236,
      "loss": 0.3978,
      "step": 8539
    },
    {
      "epoch": 17.183098591549296,
      "grad_norm": 0.5878180265426636,
      "learning_rate": 0.00016565046785390884,
      "loss": 0.4112,
      "step": 8540
    },
    {
      "epoch": 17.185110663983902,
      "grad_norm": 0.5475003123283386,
      "learning_rate": 0.00016564644330415535,
      "loss": 0.3892,
      "step": 8541
    },
    {
      "epoch": 17.187122736418512,
      "grad_norm": 0.536757230758667,
      "learning_rate": 0.00016564241875440184,
      "loss": 0.4185,
      "step": 8542
    },
    {
      "epoch": 17.18913480885312,
      "grad_norm": 0.5374740958213806,
      "learning_rate": 0.00016563839420464838,
      "loss": 0.4049,
      "step": 8543
    },
    {
      "epoch": 17.191146881287725,
      "grad_norm": 0.5461058020591736,
      "learning_rate": 0.00016563436965489486,
      "loss": 0.3852,
      "step": 8544
    },
    {
      "epoch": 17.193158953722335,
      "grad_norm": 0.5423621535301208,
      "learning_rate": 0.00016563034510514137,
      "loss": 0.4081,
      "step": 8545
    },
    {
      "epoch": 17.19517102615694,
      "grad_norm": 0.548451840877533,
      "learning_rate": 0.00016562632055538786,
      "loss": 0.3587,
      "step": 8546
    },
    {
      "epoch": 17.197183098591548,
      "grad_norm": 0.5592080950737,
      "learning_rate": 0.00016562229600563437,
      "loss": 0.3884,
      "step": 8547
    },
    {
      "epoch": 17.199195171026158,
      "grad_norm": 0.5431942343711853,
      "learning_rate": 0.00016561827145588088,
      "loss": 0.3879,
      "step": 8548
    },
    {
      "epoch": 17.201207243460765,
      "grad_norm": 0.5316081643104553,
      "learning_rate": 0.0001656142469061274,
      "loss": 0.3883,
      "step": 8549
    },
    {
      "epoch": 17.20321931589537,
      "grad_norm": 0.5934295654296875,
      "learning_rate": 0.00016561022235637388,
      "loss": 0.4101,
      "step": 8550
    },
    {
      "epoch": 17.20523138832998,
      "grad_norm": 0.5392793416976929,
      "learning_rate": 0.0001656061978066204,
      "loss": 0.3607,
      "step": 8551
    },
    {
      "epoch": 17.207243460764587,
      "grad_norm": 0.5593065619468689,
      "learning_rate": 0.00016560217325686688,
      "loss": 0.3766,
      "step": 8552
    },
    {
      "epoch": 17.209255533199194,
      "grad_norm": 0.5676193833351135,
      "learning_rate": 0.00016559814870711342,
      "loss": 0.4122,
      "step": 8553
    },
    {
      "epoch": 17.211267605633804,
      "grad_norm": 0.5475620031356812,
      "learning_rate": 0.0001655941241573599,
      "loss": 0.421,
      "step": 8554
    },
    {
      "epoch": 17.21327967806841,
      "grad_norm": 0.5375148057937622,
      "learning_rate": 0.00016559009960760642,
      "loss": 0.3967,
      "step": 8555
    },
    {
      "epoch": 17.215291750503017,
      "grad_norm": 0.5580316781997681,
      "learning_rate": 0.0001655860750578529,
      "loss": 0.4146,
      "step": 8556
    },
    {
      "epoch": 17.217303822937627,
      "grad_norm": 0.5540512800216675,
      "learning_rate": 0.0001655820505080994,
      "loss": 0.3867,
      "step": 8557
    },
    {
      "epoch": 17.219315895372233,
      "grad_norm": 0.5319512486457825,
      "learning_rate": 0.00016557802595834593,
      "loss": 0.3717,
      "step": 8558
    },
    {
      "epoch": 17.22132796780684,
      "grad_norm": 0.5822701454162598,
      "learning_rate": 0.00016557400140859244,
      "loss": 0.4077,
      "step": 8559
    },
    {
      "epoch": 17.22334004024145,
      "grad_norm": 0.511451005935669,
      "learning_rate": 0.00016556997685883892,
      "loss": 0.382,
      "step": 8560
    },
    {
      "epoch": 17.225352112676056,
      "grad_norm": 0.5312104225158691,
      "learning_rate": 0.00016556595230908543,
      "loss": 0.4139,
      "step": 8561
    },
    {
      "epoch": 17.227364185110662,
      "grad_norm": 0.5561350584030151,
      "learning_rate": 0.00016556192775933192,
      "loss": 0.3957,
      "step": 8562
    },
    {
      "epoch": 17.229376257545272,
      "grad_norm": 0.5533721446990967,
      "learning_rate": 0.00016555790320957846,
      "loss": 0.3952,
      "step": 8563
    },
    {
      "epoch": 17.23138832997988,
      "grad_norm": 0.531985342502594,
      "learning_rate": 0.00016555387865982494,
      "loss": 0.3795,
      "step": 8564
    },
    {
      "epoch": 17.233400402414485,
      "grad_norm": 0.5603903532028198,
      "learning_rate": 0.00016554985411007146,
      "loss": 0.3897,
      "step": 8565
    },
    {
      "epoch": 17.235412474849095,
      "grad_norm": 0.5995194911956787,
      "learning_rate": 0.00016554582956031794,
      "loss": 0.423,
      "step": 8566
    },
    {
      "epoch": 17.2374245472837,
      "grad_norm": 0.5660512447357178,
      "learning_rate": 0.00016554180501056445,
      "loss": 0.4127,
      "step": 8567
    },
    {
      "epoch": 17.239436619718308,
      "grad_norm": 0.5460759997367859,
      "learning_rate": 0.00016553778046081097,
      "loss": 0.3954,
      "step": 8568
    },
    {
      "epoch": 17.241448692152918,
      "grad_norm": 0.5449640154838562,
      "learning_rate": 0.00016553375591105745,
      "loss": 0.3885,
      "step": 8569
    },
    {
      "epoch": 17.243460764587525,
      "grad_norm": 0.5509340763092041,
      "learning_rate": 0.00016552973136130396,
      "loss": 0.4065,
      "step": 8570
    },
    {
      "epoch": 17.24547283702213,
      "grad_norm": 0.5730670690536499,
      "learning_rate": 0.00016552570681155048,
      "loss": 0.3949,
      "step": 8571
    },
    {
      "epoch": 17.24748490945674,
      "grad_norm": 0.5923105478286743,
      "learning_rate": 0.00016552168226179696,
      "loss": 0.408,
      "step": 8572
    },
    {
      "epoch": 17.249496981891348,
      "grad_norm": 0.5330700874328613,
      "learning_rate": 0.00016551765771204347,
      "loss": 0.3566,
      "step": 8573
    },
    {
      "epoch": 17.251509054325957,
      "grad_norm": 0.5927950739860535,
      "learning_rate": 0.00016551363316228999,
      "loss": 0.43,
      "step": 8574
    },
    {
      "epoch": 17.253521126760564,
      "grad_norm": 0.5469219088554382,
      "learning_rate": 0.00016550960861253647,
      "loss": 0.403,
      "step": 8575
    },
    {
      "epoch": 17.25553319919517,
      "grad_norm": 0.5455608367919922,
      "learning_rate": 0.00016550558406278298,
      "loss": 0.4143,
      "step": 8576
    },
    {
      "epoch": 17.25754527162978,
      "grad_norm": 0.523487389087677,
      "learning_rate": 0.00016550155951302947,
      "loss": 0.4003,
      "step": 8577
    },
    {
      "epoch": 17.259557344064387,
      "grad_norm": 0.5564989447593689,
      "learning_rate": 0.000165497534963276,
      "loss": 0.3881,
      "step": 8578
    },
    {
      "epoch": 17.261569416498993,
      "grad_norm": 0.5439208745956421,
      "learning_rate": 0.0001654935104135225,
      "loss": 0.4188,
      "step": 8579
    },
    {
      "epoch": 17.263581488933603,
      "grad_norm": 0.5517742037773132,
      "learning_rate": 0.000165489485863769,
      "loss": 0.405,
      "step": 8580
    },
    {
      "epoch": 17.26559356136821,
      "grad_norm": 0.5721126198768616,
      "learning_rate": 0.0001654854613140155,
      "loss": 0.3738,
      "step": 8581
    },
    {
      "epoch": 17.267605633802816,
      "grad_norm": 0.5307433605194092,
      "learning_rate": 0.000165481436764262,
      "loss": 0.3957,
      "step": 8582
    },
    {
      "epoch": 17.269617706237426,
      "grad_norm": 0.5758151412010193,
      "learning_rate": 0.00016547741221450851,
      "loss": 0.3996,
      "step": 8583
    },
    {
      "epoch": 17.271629778672033,
      "grad_norm": 0.5657637119293213,
      "learning_rate": 0.00016547338766475503,
      "loss": 0.4388,
      "step": 8584
    },
    {
      "epoch": 17.27364185110664,
      "grad_norm": 0.5494128465652466,
      "learning_rate": 0.0001654693631150015,
      "loss": 0.4191,
      "step": 8585
    },
    {
      "epoch": 17.27565392354125,
      "grad_norm": 0.5908313989639282,
      "learning_rate": 0.00016546533856524802,
      "loss": 0.3711,
      "step": 8586
    },
    {
      "epoch": 17.277665995975855,
      "grad_norm": 0.5634462833404541,
      "learning_rate": 0.0001654613140154945,
      "loss": 0.399,
      "step": 8587
    },
    {
      "epoch": 17.279678068410462,
      "grad_norm": 0.5339594483375549,
      "learning_rate": 0.00016545728946574105,
      "loss": 0.3707,
      "step": 8588
    },
    {
      "epoch": 17.281690140845072,
      "grad_norm": 0.604367196559906,
      "learning_rate": 0.00016545326491598753,
      "loss": 0.394,
      "step": 8589
    },
    {
      "epoch": 17.28370221327968,
      "grad_norm": 0.5577888488769531,
      "learning_rate": 0.00016544924036623405,
      "loss": 0.3618,
      "step": 8590
    },
    {
      "epoch": 17.285714285714285,
      "grad_norm": 0.540186882019043,
      "learning_rate": 0.00016544521581648053,
      "loss": 0.4003,
      "step": 8591
    },
    {
      "epoch": 17.287726358148895,
      "grad_norm": 0.5829858183860779,
      "learning_rate": 0.00016544119126672704,
      "loss": 0.4172,
      "step": 8592
    },
    {
      "epoch": 17.2897384305835,
      "grad_norm": 0.5680457949638367,
      "learning_rate": 0.00016543716671697355,
      "loss": 0.4023,
      "step": 8593
    },
    {
      "epoch": 17.291750503018108,
      "grad_norm": 0.558462381362915,
      "learning_rate": 0.00016543314216722007,
      "loss": 0.4048,
      "step": 8594
    },
    {
      "epoch": 17.293762575452718,
      "grad_norm": 0.5563470125198364,
      "learning_rate": 0.00016542911761746655,
      "loss": 0.4173,
      "step": 8595
    },
    {
      "epoch": 17.295774647887324,
      "grad_norm": 0.5524364709854126,
      "learning_rate": 0.00016542509306771306,
      "loss": 0.3704,
      "step": 8596
    },
    {
      "epoch": 17.29778672032193,
      "grad_norm": 0.5602121353149414,
      "learning_rate": 0.00016542106851795955,
      "loss": 0.4027,
      "step": 8597
    },
    {
      "epoch": 17.29979879275654,
      "grad_norm": 0.5696565508842468,
      "learning_rate": 0.0001654170439682061,
      "loss": 0.3898,
      "step": 8598
    },
    {
      "epoch": 17.301810865191147,
      "grad_norm": 0.5667974948883057,
      "learning_rate": 0.00016541301941845257,
      "loss": 0.3898,
      "step": 8599
    },
    {
      "epoch": 17.303822937625753,
      "grad_norm": 0.5460132360458374,
      "learning_rate": 0.00016540899486869909,
      "loss": 0.4078,
      "step": 8600
    },
    {
      "epoch": 17.305835010060363,
      "grad_norm": 0.5863747596740723,
      "learning_rate": 0.00016540497031894557,
      "loss": 0.4404,
      "step": 8601
    },
    {
      "epoch": 17.30784708249497,
      "grad_norm": 0.5972887873649597,
      "learning_rate": 0.00016540094576919208,
      "loss": 0.4066,
      "step": 8602
    },
    {
      "epoch": 17.309859154929576,
      "grad_norm": 0.5636903643608093,
      "learning_rate": 0.0001653969212194386,
      "loss": 0.3894,
      "step": 8603
    },
    {
      "epoch": 17.311871227364186,
      "grad_norm": 0.5596282482147217,
      "learning_rate": 0.00016539289666968508,
      "loss": 0.4165,
      "step": 8604
    },
    {
      "epoch": 17.313883299798793,
      "grad_norm": 0.5513330101966858,
      "learning_rate": 0.0001653888721199316,
      "loss": 0.3886,
      "step": 8605
    },
    {
      "epoch": 17.3158953722334,
      "grad_norm": 0.5480008721351624,
      "learning_rate": 0.0001653848475701781,
      "loss": 0.4112,
      "step": 8606
    },
    {
      "epoch": 17.31790744466801,
      "grad_norm": 0.5643220543861389,
      "learning_rate": 0.0001653808230204246,
      "loss": 0.4043,
      "step": 8607
    },
    {
      "epoch": 17.319919517102615,
      "grad_norm": 0.5484504699707031,
      "learning_rate": 0.0001653767984706711,
      "loss": 0.3946,
      "step": 8608
    },
    {
      "epoch": 17.321931589537222,
      "grad_norm": 0.558332622051239,
      "learning_rate": 0.00016537277392091761,
      "loss": 0.3737,
      "step": 8609
    },
    {
      "epoch": 17.323943661971832,
      "grad_norm": 0.551024854183197,
      "learning_rate": 0.0001653687493711641,
      "loss": 0.3943,
      "step": 8610
    },
    {
      "epoch": 17.32595573440644,
      "grad_norm": 0.5310310125350952,
      "learning_rate": 0.0001653647248214106,
      "loss": 0.3784,
      "step": 8611
    },
    {
      "epoch": 17.327967806841045,
      "grad_norm": 0.5547109842300415,
      "learning_rate": 0.0001653607002716571,
      "loss": 0.3686,
      "step": 8612
    },
    {
      "epoch": 17.329979879275655,
      "grad_norm": 0.5539204478263855,
      "learning_rate": 0.0001653566757219036,
      "loss": 0.3931,
      "step": 8613
    },
    {
      "epoch": 17.33199195171026,
      "grad_norm": 0.541782021522522,
      "learning_rate": 0.00016535265117215012,
      "loss": 0.3852,
      "step": 8614
    },
    {
      "epoch": 17.334004024144868,
      "grad_norm": 0.573689341545105,
      "learning_rate": 0.00016534862662239663,
      "loss": 0.3923,
      "step": 8615
    },
    {
      "epoch": 17.336016096579478,
      "grad_norm": 0.5747799873352051,
      "learning_rate": 0.00016534460207264312,
      "loss": 0.3885,
      "step": 8616
    },
    {
      "epoch": 17.338028169014084,
      "grad_norm": 0.5115594863891602,
      "learning_rate": 0.00016534057752288963,
      "loss": 0.4026,
      "step": 8617
    },
    {
      "epoch": 17.34004024144869,
      "grad_norm": 0.5167754888534546,
      "learning_rate": 0.00016533655297313612,
      "loss": 0.379,
      "step": 8618
    },
    {
      "epoch": 17.3420523138833,
      "grad_norm": 0.554755687713623,
      "learning_rate": 0.00016533252842338266,
      "loss": 0.3991,
      "step": 8619
    },
    {
      "epoch": 17.344064386317907,
      "grad_norm": 0.541639506816864,
      "learning_rate": 0.00016532850387362914,
      "loss": 0.4217,
      "step": 8620
    },
    {
      "epoch": 17.346076458752513,
      "grad_norm": 0.5666613578796387,
      "learning_rate": 0.00016532447932387565,
      "loss": 0.4095,
      "step": 8621
    },
    {
      "epoch": 17.348088531187123,
      "grad_norm": 0.5665249824523926,
      "learning_rate": 0.00016532045477412214,
      "loss": 0.4292,
      "step": 8622
    },
    {
      "epoch": 17.35010060362173,
      "grad_norm": 0.5656888484954834,
      "learning_rate": 0.00016531643022436865,
      "loss": 0.4303,
      "step": 8623
    },
    {
      "epoch": 17.352112676056336,
      "grad_norm": 0.5827404856681824,
      "learning_rate": 0.00016531240567461516,
      "loss": 0.4274,
      "step": 8624
    },
    {
      "epoch": 17.354124748490946,
      "grad_norm": 0.577610969543457,
      "learning_rate": 0.00016530838112486167,
      "loss": 0.4146,
      "step": 8625
    },
    {
      "epoch": 17.356136820925553,
      "grad_norm": 0.5400705933570862,
      "learning_rate": 0.00016530435657510816,
      "loss": 0.3952,
      "step": 8626
    },
    {
      "epoch": 17.358148893360163,
      "grad_norm": 0.6006993651390076,
      "learning_rate": 0.00016530033202535467,
      "loss": 0.397,
      "step": 8627
    },
    {
      "epoch": 17.36016096579477,
      "grad_norm": 0.5855292081832886,
      "learning_rate": 0.00016529630747560116,
      "loss": 0.3972,
      "step": 8628
    },
    {
      "epoch": 17.362173038229376,
      "grad_norm": 0.5638655424118042,
      "learning_rate": 0.0001652922829258477,
      "loss": 0.3994,
      "step": 8629
    },
    {
      "epoch": 17.364185110663986,
      "grad_norm": 0.563572347164154,
      "learning_rate": 0.00016528825837609418,
      "loss": 0.3915,
      "step": 8630
    },
    {
      "epoch": 17.366197183098592,
      "grad_norm": 0.5763830542564392,
      "learning_rate": 0.0001652842338263407,
      "loss": 0.4197,
      "step": 8631
    },
    {
      "epoch": 17.3682092555332,
      "grad_norm": 0.5594809651374817,
      "learning_rate": 0.00016528020927658718,
      "loss": 0.3878,
      "step": 8632
    },
    {
      "epoch": 17.37022132796781,
      "grad_norm": 0.5595607757568359,
      "learning_rate": 0.0001652761847268337,
      "loss": 0.3953,
      "step": 8633
    },
    {
      "epoch": 17.372233400402415,
      "grad_norm": 0.5199443101882935,
      "learning_rate": 0.0001652721601770802,
      "loss": 0.4111,
      "step": 8634
    },
    {
      "epoch": 17.37424547283702,
      "grad_norm": 0.5689365267753601,
      "learning_rate": 0.00016526813562732672,
      "loss": 0.4095,
      "step": 8635
    },
    {
      "epoch": 17.37625754527163,
      "grad_norm": 0.5440441966056824,
      "learning_rate": 0.0001652641110775732,
      "loss": 0.4179,
      "step": 8636
    },
    {
      "epoch": 17.378269617706238,
      "grad_norm": 0.5607497692108154,
      "learning_rate": 0.0001652600865278197,
      "loss": 0.4204,
      "step": 8637
    },
    {
      "epoch": 17.380281690140844,
      "grad_norm": 0.5458343029022217,
      "learning_rate": 0.0001652560619780662,
      "loss": 0.4233,
      "step": 8638
    },
    {
      "epoch": 17.382293762575454,
      "grad_norm": 0.549127995967865,
      "learning_rate": 0.0001652520374283127,
      "loss": 0.4117,
      "step": 8639
    },
    {
      "epoch": 17.38430583501006,
      "grad_norm": 0.5673529505729675,
      "learning_rate": 0.00016524801287855922,
      "loss": 0.4105,
      "step": 8640
    },
    {
      "epoch": 17.386317907444667,
      "grad_norm": 0.534760057926178,
      "learning_rate": 0.00016524398832880573,
      "loss": 0.3971,
      "step": 8641
    },
    {
      "epoch": 17.388329979879277,
      "grad_norm": 0.5865880846977234,
      "learning_rate": 0.00016523996377905222,
      "loss": 0.3992,
      "step": 8642
    },
    {
      "epoch": 17.390342052313883,
      "grad_norm": 0.5553528666496277,
      "learning_rate": 0.00016523593922929873,
      "loss": 0.4319,
      "step": 8643
    },
    {
      "epoch": 17.39235412474849,
      "grad_norm": 0.5728515386581421,
      "learning_rate": 0.00016523191467954524,
      "loss": 0.401,
      "step": 8644
    },
    {
      "epoch": 17.3943661971831,
      "grad_norm": 0.5754272937774658,
      "learning_rate": 0.00016522789012979173,
      "loss": 0.4202,
      "step": 8645
    },
    {
      "epoch": 17.396378269617706,
      "grad_norm": 0.565901517868042,
      "learning_rate": 0.00016522386558003824,
      "loss": 0.4059,
      "step": 8646
    },
    {
      "epoch": 17.398390342052313,
      "grad_norm": 0.5378036499023438,
      "learning_rate": 0.00016521984103028473,
      "loss": 0.3443,
      "step": 8647
    },
    {
      "epoch": 17.400402414486923,
      "grad_norm": 0.5885022878646851,
      "learning_rate": 0.00016521581648053124,
      "loss": 0.4063,
      "step": 8648
    },
    {
      "epoch": 17.40241448692153,
      "grad_norm": 0.5761702656745911,
      "learning_rate": 0.00016521179193077775,
      "loss": 0.4231,
      "step": 8649
    },
    {
      "epoch": 17.404426559356136,
      "grad_norm": 0.5802910923957825,
      "learning_rate": 0.00016520776738102426,
      "loss": 0.409,
      "step": 8650
    },
    {
      "epoch": 17.406438631790746,
      "grad_norm": 0.5591008067131042,
      "learning_rate": 0.00016520374283127075,
      "loss": 0.4117,
      "step": 8651
    },
    {
      "epoch": 17.408450704225352,
      "grad_norm": 0.5757259130477905,
      "learning_rate": 0.00016519971828151726,
      "loss": 0.4273,
      "step": 8652
    },
    {
      "epoch": 17.41046277665996,
      "grad_norm": 0.5445660352706909,
      "learning_rate": 0.00016519569373176375,
      "loss": 0.4127,
      "step": 8653
    },
    {
      "epoch": 17.41247484909457,
      "grad_norm": 0.5322484374046326,
      "learning_rate": 0.00016519166918201029,
      "loss": 0.4024,
      "step": 8654
    },
    {
      "epoch": 17.414486921529175,
      "grad_norm": 0.5867998003959656,
      "learning_rate": 0.00016518764463225677,
      "loss": 0.4264,
      "step": 8655
    },
    {
      "epoch": 17.41649899396378,
      "grad_norm": 0.5650105476379395,
      "learning_rate": 0.00016518362008250328,
      "loss": 0.366,
      "step": 8656
    },
    {
      "epoch": 17.41851106639839,
      "grad_norm": 0.5574730634689331,
      "learning_rate": 0.00016517959553274977,
      "loss": 0.3892,
      "step": 8657
    },
    {
      "epoch": 17.420523138832998,
      "grad_norm": 0.556342601776123,
      "learning_rate": 0.00016517557098299628,
      "loss": 0.3931,
      "step": 8658
    },
    {
      "epoch": 17.422535211267604,
      "grad_norm": 0.5723679661750793,
      "learning_rate": 0.0001651715464332428,
      "loss": 0.3914,
      "step": 8659
    },
    {
      "epoch": 17.424547283702214,
      "grad_norm": 0.5434498190879822,
      "learning_rate": 0.0001651675218834893,
      "loss": 0.3953,
      "step": 8660
    },
    {
      "epoch": 17.42655935613682,
      "grad_norm": 0.5512425303459167,
      "learning_rate": 0.0001651634973337358,
      "loss": 0.3973,
      "step": 8661
    },
    {
      "epoch": 17.428571428571427,
      "grad_norm": 0.5875496864318848,
      "learning_rate": 0.0001651594727839823,
      "loss": 0.4234,
      "step": 8662
    },
    {
      "epoch": 17.430583501006037,
      "grad_norm": 0.5676082968711853,
      "learning_rate": 0.0001651554482342288,
      "loss": 0.3995,
      "step": 8663
    },
    {
      "epoch": 17.432595573440643,
      "grad_norm": 0.5715339183807373,
      "learning_rate": 0.00016515142368447533,
      "loss": 0.3762,
      "step": 8664
    },
    {
      "epoch": 17.43460764587525,
      "grad_norm": 0.5794601440429688,
      "learning_rate": 0.0001651473991347218,
      "loss": 0.4062,
      "step": 8665
    },
    {
      "epoch": 17.43661971830986,
      "grad_norm": 0.5608410835266113,
      "learning_rate": 0.00016514337458496832,
      "loss": 0.4201,
      "step": 8666
    },
    {
      "epoch": 17.438631790744466,
      "grad_norm": 0.5886169672012329,
      "learning_rate": 0.0001651393500352148,
      "loss": 0.3891,
      "step": 8667
    },
    {
      "epoch": 17.440643863179073,
      "grad_norm": 0.5492422580718994,
      "learning_rate": 0.00016513532548546132,
      "loss": 0.4072,
      "step": 8668
    },
    {
      "epoch": 17.442655935613683,
      "grad_norm": 0.5625385046005249,
      "learning_rate": 0.00016513130093570783,
      "loss": 0.4139,
      "step": 8669
    },
    {
      "epoch": 17.44466800804829,
      "grad_norm": 0.5687307715415955,
      "learning_rate": 0.00016512727638595435,
      "loss": 0.4187,
      "step": 8670
    },
    {
      "epoch": 17.446680080482896,
      "grad_norm": 0.5590882301330566,
      "learning_rate": 0.00016512325183620083,
      "loss": 0.4145,
      "step": 8671
    },
    {
      "epoch": 17.448692152917506,
      "grad_norm": 0.5766814947128296,
      "learning_rate": 0.00016511922728644734,
      "loss": 0.4395,
      "step": 8672
    },
    {
      "epoch": 17.450704225352112,
      "grad_norm": 0.5951476693153381,
      "learning_rate": 0.00016511520273669383,
      "loss": 0.4118,
      "step": 8673
    },
    {
      "epoch": 17.452716297786722,
      "grad_norm": 0.5836796760559082,
      "learning_rate": 0.00016511117818694034,
      "loss": 0.3812,
      "step": 8674
    },
    {
      "epoch": 17.45472837022133,
      "grad_norm": 0.586395263671875,
      "learning_rate": 0.00016510715363718685,
      "loss": 0.4221,
      "step": 8675
    },
    {
      "epoch": 17.456740442655935,
      "grad_norm": 0.5632692575454712,
      "learning_rate": 0.00016510312908743334,
      "loss": 0.3853,
      "step": 8676
    },
    {
      "epoch": 17.458752515090545,
      "grad_norm": 0.5671746134757996,
      "learning_rate": 0.00016509910453767985,
      "loss": 0.4004,
      "step": 8677
    },
    {
      "epoch": 17.46076458752515,
      "grad_norm": 0.5769646763801575,
      "learning_rate": 0.00016509507998792636,
      "loss": 0.3817,
      "step": 8678
    },
    {
      "epoch": 17.462776659959758,
      "grad_norm": 0.5380567908287048,
      "learning_rate": 0.00016509105543817287,
      "loss": 0.4175,
      "step": 8679
    },
    {
      "epoch": 17.464788732394368,
      "grad_norm": 0.5691074728965759,
      "learning_rate": 0.00016508703088841936,
      "loss": 0.4282,
      "step": 8680
    },
    {
      "epoch": 17.466800804828974,
      "grad_norm": 0.5630508661270142,
      "learning_rate": 0.00016508300633866587,
      "loss": 0.4064,
      "step": 8681
    },
    {
      "epoch": 17.46881287726358,
      "grad_norm": 0.5874820351600647,
      "learning_rate": 0.00016507898178891236,
      "loss": 0.4208,
      "step": 8682
    },
    {
      "epoch": 17.47082494969819,
      "grad_norm": 0.5844193696975708,
      "learning_rate": 0.00016507495723915887,
      "loss": 0.4249,
      "step": 8683
    },
    {
      "epoch": 17.472837022132797,
      "grad_norm": 0.5312048196792603,
      "learning_rate": 0.00016507093268940538,
      "loss": 0.4019,
      "step": 8684
    },
    {
      "epoch": 17.474849094567404,
      "grad_norm": 0.6095119714736938,
      "learning_rate": 0.0001650669081396519,
      "loss": 0.4338,
      "step": 8685
    },
    {
      "epoch": 17.476861167002014,
      "grad_norm": 0.577025830745697,
      "learning_rate": 0.00016506288358989838,
      "loss": 0.3966,
      "step": 8686
    },
    {
      "epoch": 17.47887323943662,
      "grad_norm": 0.6142449975013733,
      "learning_rate": 0.0001650588590401449,
      "loss": 0.4059,
      "step": 8687
    },
    {
      "epoch": 17.480885311871226,
      "grad_norm": 0.571210503578186,
      "learning_rate": 0.00016505483449039138,
      "loss": 0.4209,
      "step": 8688
    },
    {
      "epoch": 17.482897384305836,
      "grad_norm": 0.5797219276428223,
      "learning_rate": 0.00016505080994063791,
      "loss": 0.3992,
      "step": 8689
    },
    {
      "epoch": 17.484909456740443,
      "grad_norm": 0.5837699770927429,
      "learning_rate": 0.0001650467853908844,
      "loss": 0.4452,
      "step": 8690
    },
    {
      "epoch": 17.48692152917505,
      "grad_norm": 0.5600553750991821,
      "learning_rate": 0.0001650427608411309,
      "loss": 0.3976,
      "step": 8691
    },
    {
      "epoch": 17.48893360160966,
      "grad_norm": 0.5463100075721741,
      "learning_rate": 0.0001650387362913774,
      "loss": 0.4207,
      "step": 8692
    },
    {
      "epoch": 17.490945674044266,
      "grad_norm": 0.5214964151382446,
      "learning_rate": 0.0001650347117416239,
      "loss": 0.393,
      "step": 8693
    },
    {
      "epoch": 17.492957746478872,
      "grad_norm": 0.6225323677062988,
      "learning_rate": 0.00016503068719187042,
      "loss": 0.445,
      "step": 8694
    },
    {
      "epoch": 17.494969818913482,
      "grad_norm": 0.549436628818512,
      "learning_rate": 0.00016502666264211693,
      "loss": 0.3906,
      "step": 8695
    },
    {
      "epoch": 17.49698189134809,
      "grad_norm": 0.5792858600616455,
      "learning_rate": 0.00016502263809236342,
      "loss": 0.4332,
      "step": 8696
    },
    {
      "epoch": 17.498993963782695,
      "grad_norm": 0.5646266341209412,
      "learning_rate": 0.00016501861354260993,
      "loss": 0.4355,
      "step": 8697
    },
    {
      "epoch": 17.501006036217305,
      "grad_norm": 0.5623303055763245,
      "learning_rate": 0.00016501458899285642,
      "loss": 0.3996,
      "step": 8698
    },
    {
      "epoch": 17.50301810865191,
      "grad_norm": 0.5658946633338928,
      "learning_rate": 0.00016501056444310296,
      "loss": 0.4072,
      "step": 8699
    },
    {
      "epoch": 17.505030181086518,
      "grad_norm": 0.6166298389434814,
      "learning_rate": 0.00016500653989334944,
      "loss": 0.4217,
      "step": 8700
    },
    {
      "epoch": 17.507042253521128,
      "grad_norm": 0.5367369055747986,
      "learning_rate": 0.00016500251534359595,
      "loss": 0.4291,
      "step": 8701
    },
    {
      "epoch": 17.509054325955734,
      "grad_norm": 0.5484903454780579,
      "learning_rate": 0.00016499849079384244,
      "loss": 0.3736,
      "step": 8702
    },
    {
      "epoch": 17.51106639839034,
      "grad_norm": 0.6007542014122009,
      "learning_rate": 0.00016499446624408895,
      "loss": 0.4321,
      "step": 8703
    },
    {
      "epoch": 17.51307847082495,
      "grad_norm": 0.5944948792457581,
      "learning_rate": 0.00016499044169433546,
      "loss": 0.4194,
      "step": 8704
    },
    {
      "epoch": 17.515090543259557,
      "grad_norm": 0.5584394335746765,
      "learning_rate": 0.00016498641714458197,
      "loss": 0.4347,
      "step": 8705
    },
    {
      "epoch": 17.517102615694164,
      "grad_norm": 0.5850406289100647,
      "learning_rate": 0.00016498239259482846,
      "loss": 0.4089,
      "step": 8706
    },
    {
      "epoch": 17.519114688128774,
      "grad_norm": 0.5594474077224731,
      "learning_rate": 0.00016497836804507497,
      "loss": 0.4159,
      "step": 8707
    },
    {
      "epoch": 17.52112676056338,
      "grad_norm": 0.5490145087242126,
      "learning_rate": 0.00016497434349532146,
      "loss": 0.4188,
      "step": 8708
    },
    {
      "epoch": 17.523138832997986,
      "grad_norm": 0.5699928402900696,
      "learning_rate": 0.00016497031894556797,
      "loss": 0.4299,
      "step": 8709
    },
    {
      "epoch": 17.525150905432596,
      "grad_norm": 0.5623758435249329,
      "learning_rate": 0.00016496629439581448,
      "loss": 0.3922,
      "step": 8710
    },
    {
      "epoch": 17.527162977867203,
      "grad_norm": 0.580379843711853,
      "learning_rate": 0.00016496226984606097,
      "loss": 0.4408,
      "step": 8711
    },
    {
      "epoch": 17.52917505030181,
      "grad_norm": 0.5690356492996216,
      "learning_rate": 0.00016495824529630748,
      "loss": 0.4181,
      "step": 8712
    },
    {
      "epoch": 17.53118712273642,
      "grad_norm": 0.6056306958198547,
      "learning_rate": 0.000164954220746554,
      "loss": 0.418,
      "step": 8713
    },
    {
      "epoch": 17.533199195171026,
      "grad_norm": 0.5897178649902344,
      "learning_rate": 0.0001649501961968005,
      "loss": 0.4284,
      "step": 8714
    },
    {
      "epoch": 17.535211267605632,
      "grad_norm": 0.5523934960365295,
      "learning_rate": 0.000164946171647047,
      "loss": 0.4055,
      "step": 8715
    },
    {
      "epoch": 17.537223340040242,
      "grad_norm": 0.5709959864616394,
      "learning_rate": 0.0001649421470972935,
      "loss": 0.4551,
      "step": 8716
    },
    {
      "epoch": 17.53923541247485,
      "grad_norm": 0.565028190612793,
      "learning_rate": 0.00016493812254753999,
      "loss": 0.4526,
      "step": 8717
    },
    {
      "epoch": 17.541247484909455,
      "grad_norm": 0.5508039593696594,
      "learning_rate": 0.0001649340979977865,
      "loss": 0.4282,
      "step": 8718
    },
    {
      "epoch": 17.543259557344065,
      "grad_norm": 0.5644857287406921,
      "learning_rate": 0.000164930073448033,
      "loss": 0.4113,
      "step": 8719
    },
    {
      "epoch": 17.54527162977867,
      "grad_norm": 0.602058470249176,
      "learning_rate": 0.00016492604889827952,
      "loss": 0.4198,
      "step": 8720
    },
    {
      "epoch": 17.547283702213278,
      "grad_norm": 0.5691399574279785,
      "learning_rate": 0.000164922024348526,
      "loss": 0.399,
      "step": 8721
    },
    {
      "epoch": 17.549295774647888,
      "grad_norm": 0.5892807841300964,
      "learning_rate": 0.00016491799979877252,
      "loss": 0.3897,
      "step": 8722
    },
    {
      "epoch": 17.551307847082494,
      "grad_norm": 0.5813594460487366,
      "learning_rate": 0.000164913975249019,
      "loss": 0.4293,
      "step": 8723
    },
    {
      "epoch": 17.5533199195171,
      "grad_norm": 0.532751202583313,
      "learning_rate": 0.00016490995069926554,
      "loss": 0.3986,
      "step": 8724
    },
    {
      "epoch": 17.55533199195171,
      "grad_norm": 0.5410788059234619,
      "learning_rate": 0.00016490592614951203,
      "loss": 0.4172,
      "step": 8725
    },
    {
      "epoch": 17.557344064386317,
      "grad_norm": 0.5347887277603149,
      "learning_rate": 0.00016490190159975854,
      "loss": 0.4001,
      "step": 8726
    },
    {
      "epoch": 17.559356136820927,
      "grad_norm": 0.575410783290863,
      "learning_rate": 0.00016489787705000503,
      "loss": 0.3956,
      "step": 8727
    },
    {
      "epoch": 17.561368209255534,
      "grad_norm": 0.5910652875900269,
      "learning_rate": 0.00016489385250025154,
      "loss": 0.4193,
      "step": 8728
    },
    {
      "epoch": 17.56338028169014,
      "grad_norm": 0.5917092561721802,
      "learning_rate": 0.00016488982795049805,
      "loss": 0.4352,
      "step": 8729
    },
    {
      "epoch": 17.56539235412475,
      "grad_norm": 0.598503589630127,
      "learning_rate": 0.00016488580340074456,
      "loss": 0.4202,
      "step": 8730
    },
    {
      "epoch": 17.567404426559357,
      "grad_norm": 0.5645903944969177,
      "learning_rate": 0.00016488177885099105,
      "loss": 0.4183,
      "step": 8731
    },
    {
      "epoch": 17.569416498993963,
      "grad_norm": 0.5464613437652588,
      "learning_rate": 0.00016487775430123756,
      "loss": 0.4166,
      "step": 8732
    },
    {
      "epoch": 17.571428571428573,
      "grad_norm": 0.5688885450363159,
      "learning_rate": 0.00016487372975148405,
      "loss": 0.4542,
      "step": 8733
    },
    {
      "epoch": 17.57344064386318,
      "grad_norm": 0.5398103594779968,
      "learning_rate": 0.00016486970520173058,
      "loss": 0.4202,
      "step": 8734
    },
    {
      "epoch": 17.575452716297786,
      "grad_norm": 0.5849635004997253,
      "learning_rate": 0.00016486568065197707,
      "loss": 0.4337,
      "step": 8735
    },
    {
      "epoch": 17.577464788732396,
      "grad_norm": 0.5720505118370056,
      "learning_rate": 0.00016486165610222358,
      "loss": 0.3929,
      "step": 8736
    },
    {
      "epoch": 17.579476861167002,
      "grad_norm": 0.5454169511795044,
      "learning_rate": 0.00016485763155247007,
      "loss": 0.391,
      "step": 8737
    },
    {
      "epoch": 17.58148893360161,
      "grad_norm": 0.5558194518089294,
      "learning_rate": 0.00016485360700271658,
      "loss": 0.4483,
      "step": 8738
    },
    {
      "epoch": 17.58350100603622,
      "grad_norm": 0.5871989130973816,
      "learning_rate": 0.0001648495824529631,
      "loss": 0.4381,
      "step": 8739
    },
    {
      "epoch": 17.585513078470825,
      "grad_norm": 0.580240786075592,
      "learning_rate": 0.0001648455579032096,
      "loss": 0.4369,
      "step": 8740
    },
    {
      "epoch": 17.58752515090543,
      "grad_norm": 0.565034806728363,
      "learning_rate": 0.0001648415333534561,
      "loss": 0.4149,
      "step": 8741
    },
    {
      "epoch": 17.58953722334004,
      "grad_norm": 0.5423715114593506,
      "learning_rate": 0.0001648375088037026,
      "loss": 0.4172,
      "step": 8742
    },
    {
      "epoch": 17.591549295774648,
      "grad_norm": 0.5641193985939026,
      "learning_rate": 0.00016483348425394909,
      "loss": 0.41,
      "step": 8743
    },
    {
      "epoch": 17.593561368209254,
      "grad_norm": 0.5891560316085815,
      "learning_rate": 0.0001648294597041956,
      "loss": 0.4065,
      "step": 8744
    },
    {
      "epoch": 17.595573440643864,
      "grad_norm": 0.5618254542350769,
      "learning_rate": 0.0001648254351544421,
      "loss": 0.4067,
      "step": 8745
    },
    {
      "epoch": 17.59758551307847,
      "grad_norm": 0.5660226941108704,
      "learning_rate": 0.0001648214106046886,
      "loss": 0.4089,
      "step": 8746
    },
    {
      "epoch": 17.599597585513077,
      "grad_norm": 0.5537588000297546,
      "learning_rate": 0.0001648173860549351,
      "loss": 0.4105,
      "step": 8747
    },
    {
      "epoch": 17.601609657947687,
      "grad_norm": 0.5796710848808289,
      "learning_rate": 0.00016481336150518162,
      "loss": 0.4293,
      "step": 8748
    },
    {
      "epoch": 17.603621730382294,
      "grad_norm": 0.5910065770149231,
      "learning_rate": 0.00016480933695542813,
      "loss": 0.4133,
      "step": 8749
    },
    {
      "epoch": 17.6056338028169,
      "grad_norm": 0.5691804885864258,
      "learning_rate": 0.00016480531240567462,
      "loss": 0.4003,
      "step": 8750
    },
    {
      "epoch": 17.60764587525151,
      "grad_norm": 0.5532642602920532,
      "learning_rate": 0.00016480128785592113,
      "loss": 0.3928,
      "step": 8751
    },
    {
      "epoch": 17.609657947686117,
      "grad_norm": 0.572750449180603,
      "learning_rate": 0.00016479726330616761,
      "loss": 0.4177,
      "step": 8752
    },
    {
      "epoch": 17.611670020120723,
      "grad_norm": 0.6142052412033081,
      "learning_rate": 0.00016479323875641413,
      "loss": 0.4307,
      "step": 8753
    },
    {
      "epoch": 17.613682092555333,
      "grad_norm": 0.5772892832756042,
      "learning_rate": 0.00016478921420666064,
      "loss": 0.4239,
      "step": 8754
    },
    {
      "epoch": 17.61569416498994,
      "grad_norm": 0.5412014722824097,
      "learning_rate": 0.00016478518965690715,
      "loss": 0.4381,
      "step": 8755
    },
    {
      "epoch": 17.617706237424546,
      "grad_norm": 0.5736757516860962,
      "learning_rate": 0.00016478116510715364,
      "loss": 0.424,
      "step": 8756
    },
    {
      "epoch": 17.619718309859156,
      "grad_norm": 0.551824688911438,
      "learning_rate": 0.00016477714055740015,
      "loss": 0.4008,
      "step": 8757
    },
    {
      "epoch": 17.621730382293762,
      "grad_norm": 0.558897078037262,
      "learning_rate": 0.00016477311600764663,
      "loss": 0.4412,
      "step": 8758
    },
    {
      "epoch": 17.62374245472837,
      "grad_norm": 0.5414736270904541,
      "learning_rate": 0.00016476909145789317,
      "loss": 0.4174,
      "step": 8759
    },
    {
      "epoch": 17.62575452716298,
      "grad_norm": 0.5895729064941406,
      "learning_rate": 0.00016476506690813966,
      "loss": 0.4333,
      "step": 8760
    },
    {
      "epoch": 17.627766599597585,
      "grad_norm": 0.5625490546226501,
      "learning_rate": 0.00016476104235838617,
      "loss": 0.408,
      "step": 8761
    },
    {
      "epoch": 17.62977867203219,
      "grad_norm": 0.5876058340072632,
      "learning_rate": 0.00016475701780863266,
      "loss": 0.4173,
      "step": 8762
    },
    {
      "epoch": 17.6317907444668,
      "grad_norm": 0.5525316596031189,
      "learning_rate": 0.00016475299325887917,
      "loss": 0.4474,
      "step": 8763
    },
    {
      "epoch": 17.633802816901408,
      "grad_norm": 0.545070230960846,
      "learning_rate": 0.00016474896870912568,
      "loss": 0.3999,
      "step": 8764
    },
    {
      "epoch": 17.635814889336014,
      "grad_norm": 0.5505632162094116,
      "learning_rate": 0.0001647449441593722,
      "loss": 0.4036,
      "step": 8765
    },
    {
      "epoch": 17.637826961770624,
      "grad_norm": 0.5483461022377014,
      "learning_rate": 0.00016474091960961868,
      "loss": 0.4148,
      "step": 8766
    },
    {
      "epoch": 17.63983903420523,
      "grad_norm": 0.594372570514679,
      "learning_rate": 0.0001647368950598652,
      "loss": 0.445,
      "step": 8767
    },
    {
      "epoch": 17.641851106639837,
      "grad_norm": 0.5894476175308228,
      "learning_rate": 0.00016473287051011167,
      "loss": 0.4515,
      "step": 8768
    },
    {
      "epoch": 17.643863179074447,
      "grad_norm": 0.5551332235336304,
      "learning_rate": 0.00016472884596035821,
      "loss": 0.4108,
      "step": 8769
    },
    {
      "epoch": 17.645875251509054,
      "grad_norm": 0.5491825938224792,
      "learning_rate": 0.0001647248214106047,
      "loss": 0.4253,
      "step": 8770
    },
    {
      "epoch": 17.647887323943664,
      "grad_norm": 0.5798887014389038,
      "learning_rate": 0.0001647207968608512,
      "loss": 0.4465,
      "step": 8771
    },
    {
      "epoch": 17.64989939637827,
      "grad_norm": 0.5506928563117981,
      "learning_rate": 0.0001647167723110977,
      "loss": 0.4346,
      "step": 8772
    },
    {
      "epoch": 17.651911468812877,
      "grad_norm": 0.5494695901870728,
      "learning_rate": 0.0001647127477613442,
      "loss": 0.433,
      "step": 8773
    },
    {
      "epoch": 17.653923541247487,
      "grad_norm": 0.5632100105285645,
      "learning_rate": 0.00016470872321159072,
      "loss": 0.4265,
      "step": 8774
    },
    {
      "epoch": 17.655935613682093,
      "grad_norm": 0.5400708913803101,
      "learning_rate": 0.00016470469866183723,
      "loss": 0.4153,
      "step": 8775
    },
    {
      "epoch": 17.6579476861167,
      "grad_norm": 0.5533589124679565,
      "learning_rate": 0.00016470067411208372,
      "loss": 0.4299,
      "step": 8776
    },
    {
      "epoch": 17.65995975855131,
      "grad_norm": 0.5772716403007507,
      "learning_rate": 0.00016469664956233023,
      "loss": 0.4147,
      "step": 8777
    },
    {
      "epoch": 17.661971830985916,
      "grad_norm": 0.5726171731948853,
      "learning_rate": 0.00016469262501257672,
      "loss": 0.4434,
      "step": 8778
    },
    {
      "epoch": 17.663983903420522,
      "grad_norm": 0.5655131340026855,
      "learning_rate": 0.00016468860046282323,
      "loss": 0.408,
      "step": 8779
    },
    {
      "epoch": 17.665995975855132,
      "grad_norm": 0.5783680081367493,
      "learning_rate": 0.00016468457591306974,
      "loss": 0.4324,
      "step": 8780
    },
    {
      "epoch": 17.66800804828974,
      "grad_norm": 0.583268404006958,
      "learning_rate": 0.00016468055136331623,
      "loss": 0.4425,
      "step": 8781
    },
    {
      "epoch": 17.670020120724345,
      "grad_norm": 0.5598602294921875,
      "learning_rate": 0.00016467652681356274,
      "loss": 0.4158,
      "step": 8782
    },
    {
      "epoch": 17.672032193158955,
      "grad_norm": 0.563902735710144,
      "learning_rate": 0.00016467250226380925,
      "loss": 0.4317,
      "step": 8783
    },
    {
      "epoch": 17.67404426559356,
      "grad_norm": 0.5513167977333069,
      "learning_rate": 0.00016466847771405576,
      "loss": 0.4404,
      "step": 8784
    },
    {
      "epoch": 17.676056338028168,
      "grad_norm": 0.5552837252616882,
      "learning_rate": 0.00016466445316430225,
      "loss": 0.4129,
      "step": 8785
    },
    {
      "epoch": 17.678068410462778,
      "grad_norm": 0.5405049920082092,
      "learning_rate": 0.00016466042861454876,
      "loss": 0.4166,
      "step": 8786
    },
    {
      "epoch": 17.680080482897385,
      "grad_norm": 0.6085816621780396,
      "learning_rate": 0.00016465640406479524,
      "loss": 0.4368,
      "step": 8787
    },
    {
      "epoch": 17.68209255533199,
      "grad_norm": 0.5633537769317627,
      "learning_rate": 0.00016465237951504176,
      "loss": 0.4272,
      "step": 8788
    },
    {
      "epoch": 17.6841046277666,
      "grad_norm": 0.5637220740318298,
      "learning_rate": 0.00016464835496528827,
      "loss": 0.4386,
      "step": 8789
    },
    {
      "epoch": 17.686116700201207,
      "grad_norm": 0.5414266586303711,
      "learning_rate": 0.00016464433041553478,
      "loss": 0.3755,
      "step": 8790
    },
    {
      "epoch": 17.688128772635814,
      "grad_norm": 0.5537069439888,
      "learning_rate": 0.00016464030586578127,
      "loss": 0.4235,
      "step": 8791
    },
    {
      "epoch": 17.690140845070424,
      "grad_norm": 0.5591304898262024,
      "learning_rate": 0.00016463628131602778,
      "loss": 0.4215,
      "step": 8792
    },
    {
      "epoch": 17.69215291750503,
      "grad_norm": 0.5775431990623474,
      "learning_rate": 0.00016463225676627426,
      "loss": 0.4242,
      "step": 8793
    },
    {
      "epoch": 17.694164989939637,
      "grad_norm": 0.5485204458236694,
      "learning_rate": 0.0001646282322165208,
      "loss": 0.4144,
      "step": 8794
    },
    {
      "epoch": 17.696177062374247,
      "grad_norm": 0.5336709022521973,
      "learning_rate": 0.0001646242076667673,
      "loss": 0.4093,
      "step": 8795
    },
    {
      "epoch": 17.698189134808853,
      "grad_norm": 0.5318202376365662,
      "learning_rate": 0.0001646201831170138,
      "loss": 0.4522,
      "step": 8796
    },
    {
      "epoch": 17.70020120724346,
      "grad_norm": 0.5375085473060608,
      "learning_rate": 0.00016461615856726029,
      "loss": 0.403,
      "step": 8797
    },
    {
      "epoch": 17.70221327967807,
      "grad_norm": 0.548237144947052,
      "learning_rate": 0.0001646121340175068,
      "loss": 0.4151,
      "step": 8798
    },
    {
      "epoch": 17.704225352112676,
      "grad_norm": 0.5520310401916504,
      "learning_rate": 0.0001646081094677533,
      "loss": 0.4142,
      "step": 8799
    },
    {
      "epoch": 17.706237424547282,
      "grad_norm": 0.5379915237426758,
      "learning_rate": 0.00016460408491799982,
      "loss": 0.3899,
      "step": 8800
    },
    {
      "epoch": 17.708249496981892,
      "grad_norm": 0.5716817378997803,
      "learning_rate": 0.0001646000603682463,
      "loss": 0.3954,
      "step": 8801
    },
    {
      "epoch": 17.7102615694165,
      "grad_norm": 0.5610567927360535,
      "learning_rate": 0.00016459603581849282,
      "loss": 0.442,
      "step": 8802
    },
    {
      "epoch": 17.712273641851105,
      "grad_norm": 0.5875772833824158,
      "learning_rate": 0.0001645920112687393,
      "loss": 0.4242,
      "step": 8803
    },
    {
      "epoch": 17.714285714285715,
      "grad_norm": 0.6143385767936707,
      "learning_rate": 0.00016458798671898584,
      "loss": 0.4323,
      "step": 8804
    },
    {
      "epoch": 17.71629778672032,
      "grad_norm": 0.5866143703460693,
      "learning_rate": 0.00016458396216923233,
      "loss": 0.4132,
      "step": 8805
    },
    {
      "epoch": 17.718309859154928,
      "grad_norm": 0.5374706387519836,
      "learning_rate": 0.00016457993761947884,
      "loss": 0.4138,
      "step": 8806
    },
    {
      "epoch": 17.720321931589538,
      "grad_norm": 0.5576204657554626,
      "learning_rate": 0.00016457591306972533,
      "loss": 0.4724,
      "step": 8807
    },
    {
      "epoch": 17.722334004024145,
      "grad_norm": 0.5394272804260254,
      "learning_rate": 0.00016457188851997184,
      "loss": 0.4169,
      "step": 8808
    },
    {
      "epoch": 17.72434607645875,
      "grad_norm": 0.584094762802124,
      "learning_rate": 0.00016456786397021835,
      "loss": 0.4218,
      "step": 8809
    },
    {
      "epoch": 17.72635814889336,
      "grad_norm": 0.555735170841217,
      "learning_rate": 0.00016456383942046486,
      "loss": 0.4257,
      "step": 8810
    },
    {
      "epoch": 17.728370221327967,
      "grad_norm": 0.5680844783782959,
      "learning_rate": 0.00016455981487071135,
      "loss": 0.432,
      "step": 8811
    },
    {
      "epoch": 17.730382293762574,
      "grad_norm": 0.5836203694343567,
      "learning_rate": 0.00016455579032095786,
      "loss": 0.4281,
      "step": 8812
    },
    {
      "epoch": 17.732394366197184,
      "grad_norm": 0.5785874724388123,
      "learning_rate": 0.00016455176577120435,
      "loss": 0.3803,
      "step": 8813
    },
    {
      "epoch": 17.73440643863179,
      "grad_norm": 0.6097726225852966,
      "learning_rate": 0.00016454774122145086,
      "loss": 0.4521,
      "step": 8814
    },
    {
      "epoch": 17.736418511066397,
      "grad_norm": 0.5400794744491577,
      "learning_rate": 0.00016454371667169737,
      "loss": 0.4336,
      "step": 8815
    },
    {
      "epoch": 17.738430583501007,
      "grad_norm": 0.5403767228126526,
      "learning_rate": 0.00016453969212194385,
      "loss": 0.4071,
      "step": 8816
    },
    {
      "epoch": 17.740442655935613,
      "grad_norm": 0.5405402183532715,
      "learning_rate": 0.00016453566757219037,
      "loss": 0.4507,
      "step": 8817
    },
    {
      "epoch": 17.74245472837022,
      "grad_norm": 0.5748281478881836,
      "learning_rate": 0.00016453164302243685,
      "loss": 0.4282,
      "step": 8818
    },
    {
      "epoch": 17.74446680080483,
      "grad_norm": 0.5608096122741699,
      "learning_rate": 0.0001645276184726834,
      "loss": 0.4184,
      "step": 8819
    },
    {
      "epoch": 17.746478873239436,
      "grad_norm": 0.5686072111129761,
      "learning_rate": 0.00016452359392292988,
      "loss": 0.4202,
      "step": 8820
    },
    {
      "epoch": 17.748490945674043,
      "grad_norm": 0.561488151550293,
      "learning_rate": 0.0001645195693731764,
      "loss": 0.4289,
      "step": 8821
    },
    {
      "epoch": 17.750503018108652,
      "grad_norm": 0.5668981075286865,
      "learning_rate": 0.00016451554482342287,
      "loss": 0.4051,
      "step": 8822
    },
    {
      "epoch": 17.75251509054326,
      "grad_norm": 0.5383673906326294,
      "learning_rate": 0.00016451152027366939,
      "loss": 0.3968,
      "step": 8823
    },
    {
      "epoch": 17.754527162977865,
      "grad_norm": 0.5515531897544861,
      "learning_rate": 0.0001645074957239159,
      "loss": 0.4127,
      "step": 8824
    },
    {
      "epoch": 17.756539235412475,
      "grad_norm": 0.5703245401382446,
      "learning_rate": 0.0001645034711741624,
      "loss": 0.4195,
      "step": 8825
    },
    {
      "epoch": 17.758551307847082,
      "grad_norm": 0.5547568202018738,
      "learning_rate": 0.0001644994466244089,
      "loss": 0.412,
      "step": 8826
    },
    {
      "epoch": 17.760563380281692,
      "grad_norm": 0.550967812538147,
      "learning_rate": 0.0001644954220746554,
      "loss": 0.4469,
      "step": 8827
    },
    {
      "epoch": 17.7625754527163,
      "grad_norm": 0.5352912545204163,
      "learning_rate": 0.0001644913975249019,
      "loss": 0.4592,
      "step": 8828
    },
    {
      "epoch": 17.764587525150905,
      "grad_norm": 0.5675435662269592,
      "learning_rate": 0.00016448737297514843,
      "loss": 0.4067,
      "step": 8829
    },
    {
      "epoch": 17.766599597585515,
      "grad_norm": 0.5943297743797302,
      "learning_rate": 0.00016448334842539492,
      "loss": 0.444,
      "step": 8830
    },
    {
      "epoch": 17.76861167002012,
      "grad_norm": 0.5928871035575867,
      "learning_rate": 0.00016447932387564143,
      "loss": 0.427,
      "step": 8831
    },
    {
      "epoch": 17.770623742454728,
      "grad_norm": 0.5518651604652405,
      "learning_rate": 0.00016447529932588791,
      "loss": 0.439,
      "step": 8832
    },
    {
      "epoch": 17.772635814889338,
      "grad_norm": 0.5266179442405701,
      "learning_rate": 0.00016447127477613443,
      "loss": 0.4102,
      "step": 8833
    },
    {
      "epoch": 17.774647887323944,
      "grad_norm": 0.550598680973053,
      "learning_rate": 0.00016446725022638094,
      "loss": 0.417,
      "step": 8834
    },
    {
      "epoch": 17.77665995975855,
      "grad_norm": 0.5775871872901917,
      "learning_rate": 0.00016446322567662745,
      "loss": 0.3968,
      "step": 8835
    },
    {
      "epoch": 17.77867203219316,
      "grad_norm": 0.5650926828384399,
      "learning_rate": 0.00016445920112687394,
      "loss": 0.424,
      "step": 8836
    },
    {
      "epoch": 17.780684104627767,
      "grad_norm": 0.5976989269256592,
      "learning_rate": 0.00016445517657712045,
      "loss": 0.4015,
      "step": 8837
    },
    {
      "epoch": 17.782696177062373,
      "grad_norm": 0.53944331407547,
      "learning_rate": 0.00016445115202736693,
      "loss": 0.3865,
      "step": 8838
    },
    {
      "epoch": 17.784708249496983,
      "grad_norm": 0.5354501008987427,
      "learning_rate": 0.00016444712747761347,
      "loss": 0.3926,
      "step": 8839
    },
    {
      "epoch": 17.78672032193159,
      "grad_norm": 0.5689398050308228,
      "learning_rate": 0.00016444310292785996,
      "loss": 0.4398,
      "step": 8840
    },
    {
      "epoch": 17.788732394366196,
      "grad_norm": 0.5414974689483643,
      "learning_rate": 0.00016443907837810647,
      "loss": 0.3809,
      "step": 8841
    },
    {
      "epoch": 17.790744466800806,
      "grad_norm": 0.5701054334640503,
      "learning_rate": 0.00016443505382835296,
      "loss": 0.4006,
      "step": 8842
    },
    {
      "epoch": 17.792756539235413,
      "grad_norm": 0.5473135709762573,
      "learning_rate": 0.00016443102927859947,
      "loss": 0.4365,
      "step": 8843
    },
    {
      "epoch": 17.79476861167002,
      "grad_norm": 0.5499748587608337,
      "learning_rate": 0.00016442700472884598,
      "loss": 0.4122,
      "step": 8844
    },
    {
      "epoch": 17.79678068410463,
      "grad_norm": 0.5660245418548584,
      "learning_rate": 0.00016442298017909247,
      "loss": 0.4402,
      "step": 8845
    },
    {
      "epoch": 17.798792756539235,
      "grad_norm": 0.5590109825134277,
      "learning_rate": 0.00016441895562933898,
      "loss": 0.4421,
      "step": 8846
    },
    {
      "epoch": 17.800804828973842,
      "grad_norm": 0.5753225088119507,
      "learning_rate": 0.0001644149310795855,
      "loss": 0.4346,
      "step": 8847
    },
    {
      "epoch": 17.802816901408452,
      "grad_norm": 0.5757262706756592,
      "learning_rate": 0.00016441090652983197,
      "loss": 0.4285,
      "step": 8848
    },
    {
      "epoch": 17.80482897384306,
      "grad_norm": 0.5490371584892273,
      "learning_rate": 0.0001644068819800785,
      "loss": 0.4027,
      "step": 8849
    },
    {
      "epoch": 17.806841046277665,
      "grad_norm": 0.5995374917984009,
      "learning_rate": 0.000164402857430325,
      "loss": 0.4532,
      "step": 8850
    },
    {
      "epoch": 17.808853118712275,
      "grad_norm": 0.5861923694610596,
      "learning_rate": 0.00016439883288057148,
      "loss": 0.427,
      "step": 8851
    },
    {
      "epoch": 17.81086519114688,
      "grad_norm": 0.5668014287948608,
      "learning_rate": 0.000164394808330818,
      "loss": 0.3822,
      "step": 8852
    },
    {
      "epoch": 17.812877263581488,
      "grad_norm": 0.5643305778503418,
      "learning_rate": 0.00016439078378106448,
      "loss": 0.3982,
      "step": 8853
    },
    {
      "epoch": 17.814889336016098,
      "grad_norm": 0.5763933062553406,
      "learning_rate": 0.000164386759231311,
      "loss": 0.4388,
      "step": 8854
    },
    {
      "epoch": 17.816901408450704,
      "grad_norm": 0.5766167640686035,
      "learning_rate": 0.0001643827346815575,
      "loss": 0.4409,
      "step": 8855
    },
    {
      "epoch": 17.81891348088531,
      "grad_norm": 0.5586464405059814,
      "learning_rate": 0.00016437871013180402,
      "loss": 0.438,
      "step": 8856
    },
    {
      "epoch": 17.82092555331992,
      "grad_norm": 0.5916559100151062,
      "learning_rate": 0.0001643746855820505,
      "loss": 0.4784,
      "step": 8857
    },
    {
      "epoch": 17.822937625754527,
      "grad_norm": 0.5262008905410767,
      "learning_rate": 0.00016437066103229702,
      "loss": 0.4016,
      "step": 8858
    },
    {
      "epoch": 17.824949698189133,
      "grad_norm": 0.5586042404174805,
      "learning_rate": 0.0001643666364825435,
      "loss": 0.4354,
      "step": 8859
    },
    {
      "epoch": 17.826961770623743,
      "grad_norm": 0.5867890119552612,
      "learning_rate": 0.00016436261193279004,
      "loss": 0.4441,
      "step": 8860
    },
    {
      "epoch": 17.82897384305835,
      "grad_norm": 0.5563854575157166,
      "learning_rate": 0.00016435858738303653,
      "loss": 0.4376,
      "step": 8861
    },
    {
      "epoch": 17.830985915492956,
      "grad_norm": 0.5733332633972168,
      "learning_rate": 0.00016435456283328304,
      "loss": 0.4209,
      "step": 8862
    },
    {
      "epoch": 17.832997987927566,
      "grad_norm": 0.5630557537078857,
      "learning_rate": 0.00016435053828352952,
      "loss": 0.4222,
      "step": 8863
    },
    {
      "epoch": 17.835010060362173,
      "grad_norm": 0.5452198386192322,
      "learning_rate": 0.00016434651373377603,
      "loss": 0.4483,
      "step": 8864
    },
    {
      "epoch": 17.83702213279678,
      "grad_norm": 0.5559903979301453,
      "learning_rate": 0.00016434248918402255,
      "loss": 0.4239,
      "step": 8865
    },
    {
      "epoch": 17.83903420523139,
      "grad_norm": 0.5594774484634399,
      "learning_rate": 0.00016433846463426906,
      "loss": 0.4068,
      "step": 8866
    },
    {
      "epoch": 17.841046277665995,
      "grad_norm": 0.5791243314743042,
      "learning_rate": 0.00016433444008451554,
      "loss": 0.4354,
      "step": 8867
    },
    {
      "epoch": 17.843058350100602,
      "grad_norm": 0.5685167908668518,
      "learning_rate": 0.00016433041553476206,
      "loss": 0.4763,
      "step": 8868
    },
    {
      "epoch": 17.845070422535212,
      "grad_norm": 0.551815927028656,
      "learning_rate": 0.00016432639098500854,
      "loss": 0.4467,
      "step": 8869
    },
    {
      "epoch": 17.84708249496982,
      "grad_norm": 0.5344730019569397,
      "learning_rate": 0.00016432236643525508,
      "loss": 0.3998,
      "step": 8870
    },
    {
      "epoch": 17.84909456740443,
      "grad_norm": 0.5691838264465332,
      "learning_rate": 0.00016431834188550157,
      "loss": 0.447,
      "step": 8871
    },
    {
      "epoch": 17.851106639839035,
      "grad_norm": 0.5846884846687317,
      "learning_rate": 0.00016431431733574808,
      "loss": 0.4532,
      "step": 8872
    },
    {
      "epoch": 17.85311871227364,
      "grad_norm": 0.5408501625061035,
      "learning_rate": 0.00016431029278599456,
      "loss": 0.4127,
      "step": 8873
    },
    {
      "epoch": 17.85513078470825,
      "grad_norm": 0.5449231863021851,
      "learning_rate": 0.00016430626823624108,
      "loss": 0.3925,
      "step": 8874
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 0.603708803653717,
      "learning_rate": 0.0001643022436864876,
      "loss": 0.4666,
      "step": 8875
    },
    {
      "epoch": 17.859154929577464,
      "grad_norm": 0.5683003664016724,
      "learning_rate": 0.0001642982191367341,
      "loss": 0.4466,
      "step": 8876
    },
    {
      "epoch": 17.861167002012074,
      "grad_norm": 0.5419557094573975,
      "learning_rate": 0.00016429419458698058,
      "loss": 0.4378,
      "step": 8877
    },
    {
      "epoch": 17.86317907444668,
      "grad_norm": 0.5797192454338074,
      "learning_rate": 0.0001642901700372271,
      "loss": 0.4506,
      "step": 8878
    },
    {
      "epoch": 17.865191146881287,
      "grad_norm": 0.5397324562072754,
      "learning_rate": 0.00016428614548747358,
      "loss": 0.4427,
      "step": 8879
    },
    {
      "epoch": 17.867203219315897,
      "grad_norm": 0.6249297261238098,
      "learning_rate": 0.0001642821209377201,
      "loss": 0.4649,
      "step": 8880
    },
    {
      "epoch": 17.869215291750503,
      "grad_norm": 0.5588142275810242,
      "learning_rate": 0.0001642780963879666,
      "loss": 0.4325,
      "step": 8881
    },
    {
      "epoch": 17.87122736418511,
      "grad_norm": 0.5405838489532471,
      "learning_rate": 0.00016427407183821312,
      "loss": 0.4116,
      "step": 8882
    },
    {
      "epoch": 17.87323943661972,
      "grad_norm": 0.5249367952346802,
      "learning_rate": 0.0001642700472884596,
      "loss": 0.3803,
      "step": 8883
    },
    {
      "epoch": 17.875251509054326,
      "grad_norm": 0.5491893291473389,
      "learning_rate": 0.00016426602273870612,
      "loss": 0.4359,
      "step": 8884
    },
    {
      "epoch": 17.877263581488933,
      "grad_norm": 0.5553982257843018,
      "learning_rate": 0.00016426199818895263,
      "loss": 0.4075,
      "step": 8885
    },
    {
      "epoch": 17.879275653923543,
      "grad_norm": 0.5727673768997192,
      "learning_rate": 0.00016425797363919911,
      "loss": 0.4362,
      "step": 8886
    },
    {
      "epoch": 17.88128772635815,
      "grad_norm": 0.5713834762573242,
      "learning_rate": 0.00016425394908944563,
      "loss": 0.4366,
      "step": 8887
    },
    {
      "epoch": 17.883299798792756,
      "grad_norm": 0.530563473701477,
      "learning_rate": 0.0001642499245396921,
      "loss": 0.4129,
      "step": 8888
    },
    {
      "epoch": 17.885311871227366,
      "grad_norm": 0.540679931640625,
      "learning_rate": 0.00016424589998993862,
      "loss": 0.4078,
      "step": 8889
    },
    {
      "epoch": 17.887323943661972,
      "grad_norm": 0.5447285771369934,
      "learning_rate": 0.00016424187544018514,
      "loss": 0.4414,
      "step": 8890
    },
    {
      "epoch": 17.88933601609658,
      "grad_norm": 0.5926942229270935,
      "learning_rate": 0.00016423785089043165,
      "loss": 0.4195,
      "step": 8891
    },
    {
      "epoch": 17.89134808853119,
      "grad_norm": 0.5600255727767944,
      "learning_rate": 0.00016423382634067813,
      "loss": 0.4057,
      "step": 8892
    },
    {
      "epoch": 17.893360160965795,
      "grad_norm": 0.5636408925056458,
      "learning_rate": 0.00016422980179092464,
      "loss": 0.468,
      "step": 8893
    },
    {
      "epoch": 17.8953722334004,
      "grad_norm": 0.5677143931388855,
      "learning_rate": 0.00016422577724117113,
      "loss": 0.4347,
      "step": 8894
    },
    {
      "epoch": 17.89738430583501,
      "grad_norm": 0.6032279133796692,
      "learning_rate": 0.00016422175269141767,
      "loss": 0.458,
      "step": 8895
    },
    {
      "epoch": 17.899396378269618,
      "grad_norm": 0.5613856911659241,
      "learning_rate": 0.00016421772814166415,
      "loss": 0.4226,
      "step": 8896
    },
    {
      "epoch": 17.901408450704224,
      "grad_norm": 0.5484299063682556,
      "learning_rate": 0.00016421370359191067,
      "loss": 0.4383,
      "step": 8897
    },
    {
      "epoch": 17.903420523138834,
      "grad_norm": 0.5523030757904053,
      "learning_rate": 0.00016420967904215715,
      "loss": 0.3929,
      "step": 8898
    },
    {
      "epoch": 17.90543259557344,
      "grad_norm": 0.5719437003135681,
      "learning_rate": 0.00016420565449240366,
      "loss": 0.4492,
      "step": 8899
    },
    {
      "epoch": 17.907444668008047,
      "grad_norm": 0.5605387091636658,
      "learning_rate": 0.00016420162994265018,
      "loss": 0.4174,
      "step": 8900
    },
    {
      "epoch": 17.909456740442657,
      "grad_norm": 0.5415639281272888,
      "learning_rate": 0.0001641976053928967,
      "loss": 0.4476,
      "step": 8901
    },
    {
      "epoch": 17.911468812877263,
      "grad_norm": 0.5691441893577576,
      "learning_rate": 0.00016419358084314317,
      "loss": 0.4478,
      "step": 8902
    },
    {
      "epoch": 17.91348088531187,
      "grad_norm": 0.5687374472618103,
      "learning_rate": 0.00016418955629338969,
      "loss": 0.4282,
      "step": 8903
    },
    {
      "epoch": 17.91549295774648,
      "grad_norm": 0.545677661895752,
      "learning_rate": 0.00016418553174363617,
      "loss": 0.4123,
      "step": 8904
    },
    {
      "epoch": 17.917505030181086,
      "grad_norm": 0.5715803503990173,
      "learning_rate": 0.0001641815071938827,
      "loss": 0.4417,
      "step": 8905
    },
    {
      "epoch": 17.919517102615693,
      "grad_norm": 0.5652540326118469,
      "learning_rate": 0.0001641774826441292,
      "loss": 0.4397,
      "step": 8906
    },
    {
      "epoch": 17.921529175050303,
      "grad_norm": 0.554521918296814,
      "learning_rate": 0.0001641734580943757,
      "loss": 0.4274,
      "step": 8907
    },
    {
      "epoch": 17.92354124748491,
      "grad_norm": 0.5586979985237122,
      "learning_rate": 0.0001641694335446222,
      "loss": 0.4594,
      "step": 8908
    },
    {
      "epoch": 17.925553319919516,
      "grad_norm": 0.5861747860908508,
      "learning_rate": 0.0001641654089948687,
      "loss": 0.4281,
      "step": 8909
    },
    {
      "epoch": 17.927565392354126,
      "grad_norm": 0.5400349497795105,
      "learning_rate": 0.00016416138444511522,
      "loss": 0.4269,
      "step": 8910
    },
    {
      "epoch": 17.929577464788732,
      "grad_norm": 0.5456300973892212,
      "learning_rate": 0.00016415735989536173,
      "loss": 0.4529,
      "step": 8911
    },
    {
      "epoch": 17.93158953722334,
      "grad_norm": 0.5445125699043274,
      "learning_rate": 0.00016415333534560821,
      "loss": 0.4049,
      "step": 8912
    },
    {
      "epoch": 17.93360160965795,
      "grad_norm": 0.5603550672531128,
      "learning_rate": 0.00016414931079585473,
      "loss": 0.4095,
      "step": 8913
    },
    {
      "epoch": 17.935613682092555,
      "grad_norm": 0.5408795475959778,
      "learning_rate": 0.0001641452862461012,
      "loss": 0.4037,
      "step": 8914
    },
    {
      "epoch": 17.93762575452716,
      "grad_norm": 0.5681116580963135,
      "learning_rate": 0.00016414126169634772,
      "loss": 0.4116,
      "step": 8915
    },
    {
      "epoch": 17.93963782696177,
      "grad_norm": 0.5767181515693665,
      "learning_rate": 0.00016413723714659424,
      "loss": 0.4124,
      "step": 8916
    },
    {
      "epoch": 17.941649899396378,
      "grad_norm": 0.5781808495521545,
      "learning_rate": 0.00016413321259684075,
      "loss": 0.4222,
      "step": 8917
    },
    {
      "epoch": 17.943661971830984,
      "grad_norm": 0.5569828152656555,
      "learning_rate": 0.00016412918804708723,
      "loss": 0.4503,
      "step": 8918
    },
    {
      "epoch": 17.945674044265594,
      "grad_norm": 0.5661766529083252,
      "learning_rate": 0.00016412516349733375,
      "loss": 0.4504,
      "step": 8919
    },
    {
      "epoch": 17.9476861167002,
      "grad_norm": 0.5467649102210999,
      "learning_rate": 0.00016412113894758026,
      "loss": 0.4528,
      "step": 8920
    },
    {
      "epoch": 17.949698189134807,
      "grad_norm": 0.569187343120575,
      "learning_rate": 0.00016411711439782674,
      "loss": 0.4451,
      "step": 8921
    },
    {
      "epoch": 17.951710261569417,
      "grad_norm": 0.5334988832473755,
      "learning_rate": 0.00016411308984807326,
      "loss": 0.4384,
      "step": 8922
    },
    {
      "epoch": 17.953722334004024,
      "grad_norm": 0.5558121204376221,
      "learning_rate": 0.00016410906529831974,
      "loss": 0.438,
      "step": 8923
    },
    {
      "epoch": 17.955734406438633,
      "grad_norm": 0.5484572649002075,
      "learning_rate": 0.00016410504074856625,
      "loss": 0.4394,
      "step": 8924
    },
    {
      "epoch": 17.95774647887324,
      "grad_norm": 0.5503681898117065,
      "learning_rate": 0.00016410101619881276,
      "loss": 0.4035,
      "step": 8925
    },
    {
      "epoch": 17.959758551307846,
      "grad_norm": 0.5782437920570374,
      "learning_rate": 0.00016409699164905928,
      "loss": 0.4324,
      "step": 8926
    },
    {
      "epoch": 17.961770623742456,
      "grad_norm": 0.5273743271827698,
      "learning_rate": 0.00016409296709930576,
      "loss": 0.4177,
      "step": 8927
    },
    {
      "epoch": 17.963782696177063,
      "grad_norm": 0.6095398664474487,
      "learning_rate": 0.00016408894254955227,
      "loss": 0.4211,
      "step": 8928
    },
    {
      "epoch": 17.96579476861167,
      "grad_norm": 0.5986837148666382,
      "learning_rate": 0.00016408491799979876,
      "loss": 0.4456,
      "step": 8929
    },
    {
      "epoch": 17.96780684104628,
      "grad_norm": 0.5705927610397339,
      "learning_rate": 0.0001640808934500453,
      "loss": 0.4296,
      "step": 8930
    },
    {
      "epoch": 17.969818913480886,
      "grad_norm": 0.5502727031707764,
      "learning_rate": 0.00016407686890029178,
      "loss": 0.4206,
      "step": 8931
    },
    {
      "epoch": 17.971830985915492,
      "grad_norm": 0.5807932019233704,
      "learning_rate": 0.0001640728443505383,
      "loss": 0.4409,
      "step": 8932
    },
    {
      "epoch": 17.973843058350102,
      "grad_norm": 0.5631916522979736,
      "learning_rate": 0.00016406881980078478,
      "loss": 0.4152,
      "step": 8933
    },
    {
      "epoch": 17.97585513078471,
      "grad_norm": 0.597772479057312,
      "learning_rate": 0.0001640647952510313,
      "loss": 0.4489,
      "step": 8934
    },
    {
      "epoch": 17.977867203219315,
      "grad_norm": 0.5287572145462036,
      "learning_rate": 0.0001640607707012778,
      "loss": 0.4167,
      "step": 8935
    },
    {
      "epoch": 17.979879275653925,
      "grad_norm": 0.554158091545105,
      "learning_rate": 0.00016405674615152432,
      "loss": 0.3984,
      "step": 8936
    },
    {
      "epoch": 17.98189134808853,
      "grad_norm": 0.5736198425292969,
      "learning_rate": 0.0001640527216017708,
      "loss": 0.4502,
      "step": 8937
    },
    {
      "epoch": 17.983903420523138,
      "grad_norm": 0.5727097988128662,
      "learning_rate": 0.00016404869705201732,
      "loss": 0.4492,
      "step": 8938
    },
    {
      "epoch": 17.985915492957748,
      "grad_norm": 0.5676281452178955,
      "learning_rate": 0.0001640446725022638,
      "loss": 0.4159,
      "step": 8939
    },
    {
      "epoch": 17.987927565392354,
      "grad_norm": 0.5925752520561218,
      "learning_rate": 0.00016404064795251034,
      "loss": 0.4429,
      "step": 8940
    },
    {
      "epoch": 17.98993963782696,
      "grad_norm": 0.5278241038322449,
      "learning_rate": 0.00016403662340275682,
      "loss": 0.4164,
      "step": 8941
    },
    {
      "epoch": 17.99195171026157,
      "grad_norm": 0.552158534526825,
      "learning_rate": 0.00016403259885300334,
      "loss": 0.4405,
      "step": 8942
    },
    {
      "epoch": 17.993963782696177,
      "grad_norm": 0.5806781649589539,
      "learning_rate": 0.00016402857430324982,
      "loss": 0.427,
      "step": 8943
    },
    {
      "epoch": 17.995975855130784,
      "grad_norm": 0.5691760182380676,
      "learning_rate": 0.00016402454975349633,
      "loss": 0.4435,
      "step": 8944
    },
    {
      "epoch": 17.997987927565394,
      "grad_norm": 0.5577759742736816,
      "learning_rate": 0.00016402052520374285,
      "loss": 0.4245,
      "step": 8945
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.5814611315727234,
      "learning_rate": 0.00016401650065398936,
      "loss": 0.4365,
      "step": 8946
    },
    {
      "epoch": 18.0,
      "eval_loss": 0.9089351892471313,
      "eval_runtime": 49.8365,
      "eval_samples_per_second": 19.905,
      "eval_steps_per_second": 2.488,
      "step": 8946
    },
    {
      "epoch": 18.002012072434606,
      "grad_norm": 0.494806706905365,
      "learning_rate": 0.00016401247610423584,
      "loss": 0.3567,
      "step": 8947
    },
    {
      "epoch": 18.004024144869216,
      "grad_norm": 0.5297966599464417,
      "learning_rate": 0.00016400845155448236,
      "loss": 0.371,
      "step": 8948
    },
    {
      "epoch": 18.006036217303823,
      "grad_norm": 0.5304977297782898,
      "learning_rate": 0.00016400442700472884,
      "loss": 0.3834,
      "step": 8949
    },
    {
      "epoch": 18.00804828973843,
      "grad_norm": 0.5487025380134583,
      "learning_rate": 0.00016400040245497535,
      "loss": 0.3607,
      "step": 8950
    },
    {
      "epoch": 18.01006036217304,
      "grad_norm": 0.5826927423477173,
      "learning_rate": 0.00016399637790522187,
      "loss": 0.3584,
      "step": 8951
    },
    {
      "epoch": 18.012072434607646,
      "grad_norm": 0.5693477988243103,
      "learning_rate": 0.00016399235335546838,
      "loss": 0.3648,
      "step": 8952
    },
    {
      "epoch": 18.014084507042252,
      "grad_norm": 0.5477653741836548,
      "learning_rate": 0.00016398832880571486,
      "loss": 0.3842,
      "step": 8953
    },
    {
      "epoch": 18.016096579476862,
      "grad_norm": 0.6140854954719543,
      "learning_rate": 0.00016398430425596138,
      "loss": 0.3801,
      "step": 8954
    },
    {
      "epoch": 18.01810865191147,
      "grad_norm": 0.5099650621414185,
      "learning_rate": 0.0001639802797062079,
      "loss": 0.3551,
      "step": 8955
    },
    {
      "epoch": 18.020120724346075,
      "grad_norm": 0.5260863900184631,
      "learning_rate": 0.00016397625515645437,
      "loss": 0.3647,
      "step": 8956
    },
    {
      "epoch": 18.022132796780685,
      "grad_norm": 0.49573051929473877,
      "learning_rate": 0.00016397223060670088,
      "loss": 0.3519,
      "step": 8957
    },
    {
      "epoch": 18.02414486921529,
      "grad_norm": 0.5270435214042664,
      "learning_rate": 0.00016396820605694737,
      "loss": 0.3861,
      "step": 8958
    },
    {
      "epoch": 18.026156941649898,
      "grad_norm": 0.5506987571716309,
      "learning_rate": 0.00016396418150719388,
      "loss": 0.3548,
      "step": 8959
    },
    {
      "epoch": 18.028169014084508,
      "grad_norm": 0.5669539570808411,
      "learning_rate": 0.0001639601569574404,
      "loss": 0.3889,
      "step": 8960
    },
    {
      "epoch": 18.030181086519114,
      "grad_norm": 0.5231021642684937,
      "learning_rate": 0.0001639561324076869,
      "loss": 0.3328,
      "step": 8961
    },
    {
      "epoch": 18.03219315895372,
      "grad_norm": 0.5874931812286377,
      "learning_rate": 0.0001639521078579334,
      "loss": 0.3937,
      "step": 8962
    },
    {
      "epoch": 18.03420523138833,
      "grad_norm": 0.5769100189208984,
      "learning_rate": 0.0001639480833081799,
      "loss": 0.3583,
      "step": 8963
    },
    {
      "epoch": 18.036217303822937,
      "grad_norm": 0.5454981327056885,
      "learning_rate": 0.0001639440587584264,
      "loss": 0.3478,
      "step": 8964
    },
    {
      "epoch": 18.038229376257544,
      "grad_norm": 0.5481530427932739,
      "learning_rate": 0.00016394003420867293,
      "loss": 0.3795,
      "step": 8965
    },
    {
      "epoch": 18.040241448692154,
      "grad_norm": 0.5573145151138306,
      "learning_rate": 0.0001639360096589194,
      "loss": 0.3734,
      "step": 8966
    },
    {
      "epoch": 18.04225352112676,
      "grad_norm": 0.5271766781806946,
      "learning_rate": 0.00016393198510916593,
      "loss": 0.3562,
      "step": 8967
    },
    {
      "epoch": 18.044265593561367,
      "grad_norm": 0.531055212020874,
      "learning_rate": 0.0001639279605594124,
      "loss": 0.3704,
      "step": 8968
    },
    {
      "epoch": 18.046277665995976,
      "grad_norm": 0.5604196190834045,
      "learning_rate": 0.00016392393600965892,
      "loss": 0.3792,
      "step": 8969
    },
    {
      "epoch": 18.048289738430583,
      "grad_norm": 0.5513765811920166,
      "learning_rate": 0.00016391991145990544,
      "loss": 0.3814,
      "step": 8970
    },
    {
      "epoch": 18.050301810865193,
      "grad_norm": 0.5708245635032654,
      "learning_rate": 0.00016391588691015195,
      "loss": 0.3883,
      "step": 8971
    },
    {
      "epoch": 18.0523138832998,
      "grad_norm": 0.6016436219215393,
      "learning_rate": 0.00016391186236039843,
      "loss": 0.4063,
      "step": 8972
    },
    {
      "epoch": 18.054325955734406,
      "grad_norm": 0.5659144520759583,
      "learning_rate": 0.00016390783781064494,
      "loss": 0.3624,
      "step": 8973
    },
    {
      "epoch": 18.056338028169016,
      "grad_norm": 0.5051155090332031,
      "learning_rate": 0.00016390381326089143,
      "loss": 0.3299,
      "step": 8974
    },
    {
      "epoch": 18.058350100603622,
      "grad_norm": 0.5644978284835815,
      "learning_rate": 0.00016389978871113797,
      "loss": 0.3885,
      "step": 8975
    },
    {
      "epoch": 18.06036217303823,
      "grad_norm": 0.5567057728767395,
      "learning_rate": 0.00016389576416138445,
      "loss": 0.3588,
      "step": 8976
    },
    {
      "epoch": 18.06237424547284,
      "grad_norm": 0.5546706914901733,
      "learning_rate": 0.00016389173961163097,
      "loss": 0.3732,
      "step": 8977
    },
    {
      "epoch": 18.064386317907445,
      "grad_norm": 0.5525645017623901,
      "learning_rate": 0.00016388771506187745,
      "loss": 0.3608,
      "step": 8978
    },
    {
      "epoch": 18.06639839034205,
      "grad_norm": 0.5518618226051331,
      "learning_rate": 0.00016388369051212396,
      "loss": 0.3767,
      "step": 8979
    },
    {
      "epoch": 18.06841046277666,
      "grad_norm": 0.5551290512084961,
      "learning_rate": 0.00016387966596237048,
      "loss": 0.3522,
      "step": 8980
    },
    {
      "epoch": 18.070422535211268,
      "grad_norm": 0.5680411458015442,
      "learning_rate": 0.000163875641412617,
      "loss": 0.3678,
      "step": 8981
    },
    {
      "epoch": 18.072434607645874,
      "grad_norm": 0.5578944683074951,
      "learning_rate": 0.00016387161686286347,
      "loss": 0.3934,
      "step": 8982
    },
    {
      "epoch": 18.074446680080484,
      "grad_norm": 0.5740517973899841,
      "learning_rate": 0.00016386759231310999,
      "loss": 0.3807,
      "step": 8983
    },
    {
      "epoch": 18.07645875251509,
      "grad_norm": 0.5616214871406555,
      "learning_rate": 0.00016386356776335647,
      "loss": 0.4247,
      "step": 8984
    },
    {
      "epoch": 18.078470824949697,
      "grad_norm": 0.5549854636192322,
      "learning_rate": 0.00016385954321360298,
      "loss": 0.3625,
      "step": 8985
    },
    {
      "epoch": 18.080482897384307,
      "grad_norm": 0.5738400816917419,
      "learning_rate": 0.0001638555186638495,
      "loss": 0.352,
      "step": 8986
    },
    {
      "epoch": 18.082494969818914,
      "grad_norm": 0.5603558421134949,
      "learning_rate": 0.00016385149411409598,
      "loss": 0.345,
      "step": 8987
    },
    {
      "epoch": 18.08450704225352,
      "grad_norm": 0.578683614730835,
      "learning_rate": 0.0001638474695643425,
      "loss": 0.369,
      "step": 8988
    },
    {
      "epoch": 18.08651911468813,
      "grad_norm": 0.5428130030632019,
      "learning_rate": 0.000163843445014589,
      "loss": 0.3698,
      "step": 8989
    },
    {
      "epoch": 18.088531187122737,
      "grad_norm": 0.587224006652832,
      "learning_rate": 0.00016383942046483552,
      "loss": 0.3573,
      "step": 8990
    },
    {
      "epoch": 18.090543259557343,
      "grad_norm": 0.5411645174026489,
      "learning_rate": 0.000163835395915082,
      "loss": 0.382,
      "step": 8991
    },
    {
      "epoch": 18.092555331991953,
      "grad_norm": 0.5665229558944702,
      "learning_rate": 0.00016383137136532851,
      "loss": 0.3756,
      "step": 8992
    },
    {
      "epoch": 18.09456740442656,
      "grad_norm": 0.5290595293045044,
      "learning_rate": 0.000163827346815575,
      "loss": 0.3559,
      "step": 8993
    },
    {
      "epoch": 18.096579476861166,
      "grad_norm": 0.5403383374214172,
      "learning_rate": 0.0001638233222658215,
      "loss": 0.3672,
      "step": 8994
    },
    {
      "epoch": 18.098591549295776,
      "grad_norm": 0.536329448223114,
      "learning_rate": 0.00016381929771606802,
      "loss": 0.36,
      "step": 8995
    },
    {
      "epoch": 18.100603621730382,
      "grad_norm": 0.5350825190544128,
      "learning_rate": 0.00016381527316631454,
      "loss": 0.3626,
      "step": 8996
    },
    {
      "epoch": 18.10261569416499,
      "grad_norm": 0.5536033511161804,
      "learning_rate": 0.00016381124861656102,
      "loss": 0.3588,
      "step": 8997
    },
    {
      "epoch": 18.1046277665996,
      "grad_norm": 0.6009944081306458,
      "learning_rate": 0.00016380722406680753,
      "loss": 0.4036,
      "step": 8998
    },
    {
      "epoch": 18.106639839034205,
      "grad_norm": 0.5694246292114258,
      "learning_rate": 0.00016380319951705402,
      "loss": 0.3683,
      "step": 8999
    },
    {
      "epoch": 18.10865191146881,
      "grad_norm": 0.5477612614631653,
      "learning_rate": 0.00016379917496730056,
      "loss": 0.3639,
      "step": 9000
    },
    {
      "epoch": 18.11066398390342,
      "grad_norm": 0.541803240776062,
      "learning_rate": 0.00016379515041754704,
      "loss": 0.3813,
      "step": 9001
    },
    {
      "epoch": 18.112676056338028,
      "grad_norm": 0.5990808010101318,
      "learning_rate": 0.00016379112586779355,
      "loss": 0.395,
      "step": 9002
    },
    {
      "epoch": 18.114688128772634,
      "grad_norm": 0.5416254997253418,
      "learning_rate": 0.00016378710131804004,
      "loss": 0.3732,
      "step": 9003
    },
    {
      "epoch": 18.116700201207244,
      "grad_norm": 0.5906438231468201,
      "learning_rate": 0.00016378307676828655,
      "loss": 0.3806,
      "step": 9004
    },
    {
      "epoch": 18.11871227364185,
      "grad_norm": 0.5730568766593933,
      "learning_rate": 0.00016377905221853306,
      "loss": 0.3752,
      "step": 9005
    },
    {
      "epoch": 18.120724346076457,
      "grad_norm": 0.5855638980865479,
      "learning_rate": 0.00016377502766877958,
      "loss": 0.3727,
      "step": 9006
    },
    {
      "epoch": 18.122736418511067,
      "grad_norm": 0.5784675478935242,
      "learning_rate": 0.00016377100311902606,
      "loss": 0.4062,
      "step": 9007
    },
    {
      "epoch": 18.124748490945674,
      "grad_norm": 0.5723415613174438,
      "learning_rate": 0.00016376697856927257,
      "loss": 0.3618,
      "step": 9008
    },
    {
      "epoch": 18.12676056338028,
      "grad_norm": 0.5447978377342224,
      "learning_rate": 0.00016376295401951906,
      "loss": 0.386,
      "step": 9009
    },
    {
      "epoch": 18.12877263581489,
      "grad_norm": 0.5580431818962097,
      "learning_rate": 0.0001637589294697656,
      "loss": 0.4005,
      "step": 9010
    },
    {
      "epoch": 18.130784708249497,
      "grad_norm": 0.5706266164779663,
      "learning_rate": 0.00016375490492001208,
      "loss": 0.3887,
      "step": 9011
    },
    {
      "epoch": 18.132796780684103,
      "grad_norm": 0.5620419979095459,
      "learning_rate": 0.0001637508803702586,
      "loss": 0.3526,
      "step": 9012
    },
    {
      "epoch": 18.134808853118713,
      "grad_norm": 0.5528864860534668,
      "learning_rate": 0.00016374685582050508,
      "loss": 0.3627,
      "step": 9013
    },
    {
      "epoch": 18.13682092555332,
      "grad_norm": 0.549331784248352,
      "learning_rate": 0.0001637428312707516,
      "loss": 0.3755,
      "step": 9014
    },
    {
      "epoch": 18.138832997987926,
      "grad_norm": 0.5885622501373291,
      "learning_rate": 0.0001637388067209981,
      "loss": 0.3824,
      "step": 9015
    },
    {
      "epoch": 18.140845070422536,
      "grad_norm": 0.5717616081237793,
      "learning_rate": 0.00016373478217124462,
      "loss": 0.3509,
      "step": 9016
    },
    {
      "epoch": 18.142857142857142,
      "grad_norm": 0.5564221739768982,
      "learning_rate": 0.0001637307576214911,
      "loss": 0.3874,
      "step": 9017
    },
    {
      "epoch": 18.14486921529175,
      "grad_norm": 0.5696746110916138,
      "learning_rate": 0.00016372673307173761,
      "loss": 0.3735,
      "step": 9018
    },
    {
      "epoch": 18.14688128772636,
      "grad_norm": 0.5855751037597656,
      "learning_rate": 0.0001637227085219841,
      "loss": 0.3877,
      "step": 9019
    },
    {
      "epoch": 18.148893360160965,
      "grad_norm": 0.5801517963409424,
      "learning_rate": 0.0001637186839722306,
      "loss": 0.3895,
      "step": 9020
    },
    {
      "epoch": 18.15090543259557,
      "grad_norm": 0.5417829751968384,
      "learning_rate": 0.00016371465942247712,
      "loss": 0.3786,
      "step": 9021
    },
    {
      "epoch": 18.15291750503018,
      "grad_norm": 0.558063268661499,
      "learning_rate": 0.0001637106348727236,
      "loss": 0.3808,
      "step": 9022
    },
    {
      "epoch": 18.154929577464788,
      "grad_norm": 0.5282365083694458,
      "learning_rate": 0.00016370661032297012,
      "loss": 0.3396,
      "step": 9023
    },
    {
      "epoch": 18.156941649899398,
      "grad_norm": 0.5678005814552307,
      "learning_rate": 0.00016370258577321663,
      "loss": 0.3938,
      "step": 9024
    },
    {
      "epoch": 18.158953722334005,
      "grad_norm": 0.5884398221969604,
      "learning_rate": 0.00016369856122346315,
      "loss": 0.3593,
      "step": 9025
    },
    {
      "epoch": 18.16096579476861,
      "grad_norm": 0.5779139399528503,
      "learning_rate": 0.00016369453667370963,
      "loss": 0.3767,
      "step": 9026
    },
    {
      "epoch": 18.16297786720322,
      "grad_norm": 0.5421445369720459,
      "learning_rate": 0.00016369051212395614,
      "loss": 0.349,
      "step": 9027
    },
    {
      "epoch": 18.164989939637827,
      "grad_norm": 0.5751093626022339,
      "learning_rate": 0.00016368648757420263,
      "loss": 0.3437,
      "step": 9028
    },
    {
      "epoch": 18.167002012072434,
      "grad_norm": 0.6004547476768494,
      "learning_rate": 0.00016368246302444914,
      "loss": 0.3721,
      "step": 9029
    },
    {
      "epoch": 18.169014084507044,
      "grad_norm": 0.565605640411377,
      "learning_rate": 0.00016367843847469565,
      "loss": 0.3528,
      "step": 9030
    },
    {
      "epoch": 18.17102615694165,
      "grad_norm": 0.5468209981918335,
      "learning_rate": 0.00016367441392494217,
      "loss": 0.3576,
      "step": 9031
    },
    {
      "epoch": 18.173038229376257,
      "grad_norm": 0.5894436836242676,
      "learning_rate": 0.00016367038937518865,
      "loss": 0.3785,
      "step": 9032
    },
    {
      "epoch": 18.175050301810867,
      "grad_norm": 0.5811936855316162,
      "learning_rate": 0.00016366636482543516,
      "loss": 0.3997,
      "step": 9033
    },
    {
      "epoch": 18.177062374245473,
      "grad_norm": 0.6083053946495056,
      "learning_rate": 0.00016366234027568165,
      "loss": 0.4031,
      "step": 9034
    },
    {
      "epoch": 18.17907444668008,
      "grad_norm": 0.5602396726608276,
      "learning_rate": 0.0001636583157259282,
      "loss": 0.3523,
      "step": 9035
    },
    {
      "epoch": 18.18108651911469,
      "grad_norm": 0.5797232985496521,
      "learning_rate": 0.00016365429117617467,
      "loss": 0.3736,
      "step": 9036
    },
    {
      "epoch": 18.183098591549296,
      "grad_norm": 0.5565217137336731,
      "learning_rate": 0.00016365026662642118,
      "loss": 0.3676,
      "step": 9037
    },
    {
      "epoch": 18.185110663983902,
      "grad_norm": 0.5486165881156921,
      "learning_rate": 0.00016364624207666767,
      "loss": 0.3559,
      "step": 9038
    },
    {
      "epoch": 18.187122736418512,
      "grad_norm": 0.5682585835456848,
      "learning_rate": 0.00016364221752691418,
      "loss": 0.3649,
      "step": 9039
    },
    {
      "epoch": 18.18913480885312,
      "grad_norm": 0.5659483671188354,
      "learning_rate": 0.0001636381929771607,
      "loss": 0.4134,
      "step": 9040
    },
    {
      "epoch": 18.191146881287725,
      "grad_norm": 0.541053295135498,
      "learning_rate": 0.0001636341684274072,
      "loss": 0.3612,
      "step": 9041
    },
    {
      "epoch": 18.193158953722335,
      "grad_norm": 0.5752090811729431,
      "learning_rate": 0.0001636301438776537,
      "loss": 0.3847,
      "step": 9042
    },
    {
      "epoch": 18.19517102615694,
      "grad_norm": 0.5860550403594971,
      "learning_rate": 0.0001636261193279002,
      "loss": 0.3968,
      "step": 9043
    },
    {
      "epoch": 18.197183098591548,
      "grad_norm": 0.565396249294281,
      "learning_rate": 0.0001636220947781467,
      "loss": 0.3657,
      "step": 9044
    },
    {
      "epoch": 18.199195171026158,
      "grad_norm": 0.5867307782173157,
      "learning_rate": 0.00016361807022839323,
      "loss": 0.3529,
      "step": 9045
    },
    {
      "epoch": 18.201207243460765,
      "grad_norm": 0.5847316980361938,
      "learning_rate": 0.0001636140456786397,
      "loss": 0.3753,
      "step": 9046
    },
    {
      "epoch": 18.20321931589537,
      "grad_norm": 0.5872698426246643,
      "learning_rate": 0.00016361002112888623,
      "loss": 0.4002,
      "step": 9047
    },
    {
      "epoch": 18.20523138832998,
      "grad_norm": 0.5598208904266357,
      "learning_rate": 0.0001636059965791327,
      "loss": 0.3797,
      "step": 9048
    },
    {
      "epoch": 18.207243460764587,
      "grad_norm": 0.6088394522666931,
      "learning_rate": 0.00016360197202937922,
      "loss": 0.3874,
      "step": 9049
    },
    {
      "epoch": 18.209255533199194,
      "grad_norm": 0.5649147033691406,
      "learning_rate": 0.00016359794747962573,
      "loss": 0.3693,
      "step": 9050
    },
    {
      "epoch": 18.211267605633804,
      "grad_norm": 0.6313520073890686,
      "learning_rate": 0.00016359392292987225,
      "loss": 0.373,
      "step": 9051
    },
    {
      "epoch": 18.21327967806841,
      "grad_norm": 0.547903835773468,
      "learning_rate": 0.00016358989838011873,
      "loss": 0.3786,
      "step": 9052
    },
    {
      "epoch": 18.215291750503017,
      "grad_norm": 0.5679596662521362,
      "learning_rate": 0.00016358587383036524,
      "loss": 0.3594,
      "step": 9053
    },
    {
      "epoch": 18.217303822937627,
      "grad_norm": 0.5561227202415466,
      "learning_rate": 0.00016358184928061173,
      "loss": 0.3573,
      "step": 9054
    },
    {
      "epoch": 18.219315895372233,
      "grad_norm": 0.5862146019935608,
      "learning_rate": 0.00016357782473085824,
      "loss": 0.4022,
      "step": 9055
    },
    {
      "epoch": 18.22132796780684,
      "grad_norm": 0.5549948811531067,
      "learning_rate": 0.00016357380018110475,
      "loss": 0.3727,
      "step": 9056
    },
    {
      "epoch": 18.22334004024145,
      "grad_norm": 0.5946269631385803,
      "learning_rate": 0.00016356977563135124,
      "loss": 0.4384,
      "step": 9057
    },
    {
      "epoch": 18.225352112676056,
      "grad_norm": 0.5887841582298279,
      "learning_rate": 0.00016356575108159775,
      "loss": 0.3841,
      "step": 9058
    },
    {
      "epoch": 18.227364185110662,
      "grad_norm": 0.5685785412788391,
      "learning_rate": 0.00016356172653184426,
      "loss": 0.3814,
      "step": 9059
    },
    {
      "epoch": 18.229376257545272,
      "grad_norm": 0.61187744140625,
      "learning_rate": 0.00016355770198209078,
      "loss": 0.4073,
      "step": 9060
    },
    {
      "epoch": 18.23138832997988,
      "grad_norm": 0.5928771495819092,
      "learning_rate": 0.00016355367743233726,
      "loss": 0.3528,
      "step": 9061
    },
    {
      "epoch": 18.233400402414485,
      "grad_norm": 0.5738223195075989,
      "learning_rate": 0.00016354965288258377,
      "loss": 0.3605,
      "step": 9062
    },
    {
      "epoch": 18.235412474849095,
      "grad_norm": 0.5787851214408875,
      "learning_rate": 0.00016354562833283026,
      "loss": 0.3728,
      "step": 9063
    },
    {
      "epoch": 18.2374245472837,
      "grad_norm": 0.5778416395187378,
      "learning_rate": 0.00016354160378307677,
      "loss": 0.3929,
      "step": 9064
    },
    {
      "epoch": 18.239436619718308,
      "grad_norm": 0.5950304269790649,
      "learning_rate": 0.00016353757923332328,
      "loss": 0.3812,
      "step": 9065
    },
    {
      "epoch": 18.241448692152918,
      "grad_norm": 0.5598801374435425,
      "learning_rate": 0.0001635335546835698,
      "loss": 0.3605,
      "step": 9066
    },
    {
      "epoch": 18.243460764587525,
      "grad_norm": 0.5868468284606934,
      "learning_rate": 0.00016352953013381628,
      "loss": 0.4017,
      "step": 9067
    },
    {
      "epoch": 18.24547283702213,
      "grad_norm": 0.5500618815422058,
      "learning_rate": 0.0001635255055840628,
      "loss": 0.3804,
      "step": 9068
    },
    {
      "epoch": 18.24748490945674,
      "grad_norm": 0.5835221409797668,
      "learning_rate": 0.00016352148103430928,
      "loss": 0.3939,
      "step": 9069
    },
    {
      "epoch": 18.249496981891348,
      "grad_norm": 0.573246419429779,
      "learning_rate": 0.00016351745648455582,
      "loss": 0.3612,
      "step": 9070
    },
    {
      "epoch": 18.251509054325957,
      "grad_norm": 0.5736188292503357,
      "learning_rate": 0.0001635134319348023,
      "loss": 0.3688,
      "step": 9071
    },
    {
      "epoch": 18.253521126760564,
      "grad_norm": 0.56993168592453,
      "learning_rate": 0.00016350940738504881,
      "loss": 0.3685,
      "step": 9072
    },
    {
      "epoch": 18.25553319919517,
      "grad_norm": 0.5863520503044128,
      "learning_rate": 0.0001635053828352953,
      "loss": 0.4096,
      "step": 9073
    },
    {
      "epoch": 18.25754527162978,
      "grad_norm": 0.5700764060020447,
      "learning_rate": 0.0001635013582855418,
      "loss": 0.3772,
      "step": 9074
    },
    {
      "epoch": 18.259557344064387,
      "grad_norm": 0.5775741338729858,
      "learning_rate": 0.00016349733373578832,
      "loss": 0.3724,
      "step": 9075
    },
    {
      "epoch": 18.261569416498993,
      "grad_norm": 0.5865123867988586,
      "learning_rate": 0.00016349330918603484,
      "loss": 0.3946,
      "step": 9076
    },
    {
      "epoch": 18.263581488933603,
      "grad_norm": 0.5674886703491211,
      "learning_rate": 0.00016348928463628132,
      "loss": 0.3779,
      "step": 9077
    },
    {
      "epoch": 18.26559356136821,
      "grad_norm": 0.5567755103111267,
      "learning_rate": 0.00016348526008652783,
      "loss": 0.3736,
      "step": 9078
    },
    {
      "epoch": 18.267605633802816,
      "grad_norm": 0.5462239384651184,
      "learning_rate": 0.00016348123553677432,
      "loss": 0.3646,
      "step": 9079
    },
    {
      "epoch": 18.269617706237426,
      "grad_norm": 0.5847951769828796,
      "learning_rate": 0.00016347721098702086,
      "loss": 0.3684,
      "step": 9080
    },
    {
      "epoch": 18.271629778672033,
      "grad_norm": 0.5825313329696655,
      "learning_rate": 0.00016347318643726734,
      "loss": 0.3811,
      "step": 9081
    },
    {
      "epoch": 18.27364185110664,
      "grad_norm": 0.5538093447685242,
      "learning_rate": 0.00016346916188751385,
      "loss": 0.3716,
      "step": 9082
    },
    {
      "epoch": 18.27565392354125,
      "grad_norm": 0.5590439438819885,
      "learning_rate": 0.00016346513733776034,
      "loss": 0.3766,
      "step": 9083
    },
    {
      "epoch": 18.277665995975855,
      "grad_norm": 0.5834033489227295,
      "learning_rate": 0.00016346111278800685,
      "loss": 0.3975,
      "step": 9084
    },
    {
      "epoch": 18.279678068410462,
      "grad_norm": 0.5609684586524963,
      "learning_rate": 0.00016345708823825336,
      "loss": 0.3577,
      "step": 9085
    },
    {
      "epoch": 18.281690140845072,
      "grad_norm": 0.6032899618148804,
      "learning_rate": 0.00016345306368849988,
      "loss": 0.3852,
      "step": 9086
    },
    {
      "epoch": 18.28370221327968,
      "grad_norm": 0.5453217625617981,
      "learning_rate": 0.00016344903913874636,
      "loss": 0.3485,
      "step": 9087
    },
    {
      "epoch": 18.285714285714285,
      "grad_norm": 0.5964179635047913,
      "learning_rate": 0.00016344501458899287,
      "loss": 0.389,
      "step": 9088
    },
    {
      "epoch": 18.287726358148895,
      "grad_norm": 0.6004152894020081,
      "learning_rate": 0.00016344099003923936,
      "loss": 0.3683,
      "step": 9089
    },
    {
      "epoch": 18.2897384305835,
      "grad_norm": 0.6202361583709717,
      "learning_rate": 0.00016343696548948587,
      "loss": 0.4118,
      "step": 9090
    },
    {
      "epoch": 18.291750503018108,
      "grad_norm": 0.6204843521118164,
      "learning_rate": 0.00016343294093973238,
      "loss": 0.3979,
      "step": 9091
    },
    {
      "epoch": 18.293762575452718,
      "grad_norm": 0.563389241695404,
      "learning_rate": 0.00016342891638997887,
      "loss": 0.3749,
      "step": 9092
    },
    {
      "epoch": 18.295774647887324,
      "grad_norm": 0.6288793087005615,
      "learning_rate": 0.00016342489184022538,
      "loss": 0.3989,
      "step": 9093
    },
    {
      "epoch": 18.29778672032193,
      "grad_norm": 0.6000030636787415,
      "learning_rate": 0.0001634208672904719,
      "loss": 0.393,
      "step": 9094
    },
    {
      "epoch": 18.29979879275654,
      "grad_norm": 0.5981570482254028,
      "learning_rate": 0.0001634168427407184,
      "loss": 0.3571,
      "step": 9095
    },
    {
      "epoch": 18.301810865191147,
      "grad_norm": 0.5721389055252075,
      "learning_rate": 0.0001634128181909649,
      "loss": 0.401,
      "step": 9096
    },
    {
      "epoch": 18.303822937625753,
      "grad_norm": 0.5965300798416138,
      "learning_rate": 0.0001634087936412114,
      "loss": 0.3984,
      "step": 9097
    },
    {
      "epoch": 18.305835010060363,
      "grad_norm": 0.61600661277771,
      "learning_rate": 0.0001634047690914579,
      "loss": 0.3803,
      "step": 9098
    },
    {
      "epoch": 18.30784708249497,
      "grad_norm": 0.5688244104385376,
      "learning_rate": 0.0001634007445417044,
      "loss": 0.3948,
      "step": 9099
    },
    {
      "epoch": 18.309859154929576,
      "grad_norm": 0.5865432024002075,
      "learning_rate": 0.00016339671999195088,
      "loss": 0.365,
      "step": 9100
    },
    {
      "epoch": 18.311871227364186,
      "grad_norm": 0.5773131847381592,
      "learning_rate": 0.00016339269544219742,
      "loss": 0.3761,
      "step": 9101
    },
    {
      "epoch": 18.313883299798793,
      "grad_norm": 0.6031786203384399,
      "learning_rate": 0.0001633886708924439,
      "loss": 0.4006,
      "step": 9102
    },
    {
      "epoch": 18.3158953722334,
      "grad_norm": 0.5735543370246887,
      "learning_rate": 0.00016338464634269042,
      "loss": 0.3588,
      "step": 9103
    },
    {
      "epoch": 18.31790744466801,
      "grad_norm": 0.5806044340133667,
      "learning_rate": 0.0001633806217929369,
      "loss": 0.41,
      "step": 9104
    },
    {
      "epoch": 18.319919517102615,
      "grad_norm": 0.5855614542961121,
      "learning_rate": 0.00016337659724318342,
      "loss": 0.3883,
      "step": 9105
    },
    {
      "epoch": 18.321931589537222,
      "grad_norm": 0.6129940152168274,
      "learning_rate": 0.00016337257269342993,
      "loss": 0.3698,
      "step": 9106
    },
    {
      "epoch": 18.323943661971832,
      "grad_norm": 0.6097202301025391,
      "learning_rate": 0.00016336854814367644,
      "loss": 0.3894,
      "step": 9107
    },
    {
      "epoch": 18.32595573440644,
      "grad_norm": 0.5715931057929993,
      "learning_rate": 0.00016336452359392293,
      "loss": 0.4025,
      "step": 9108
    },
    {
      "epoch": 18.327967806841045,
      "grad_norm": 0.6042793393135071,
      "learning_rate": 0.00016336049904416944,
      "loss": 0.3785,
      "step": 9109
    },
    {
      "epoch": 18.329979879275655,
      "grad_norm": 0.6299771070480347,
      "learning_rate": 0.00016335647449441593,
      "loss": 0.4016,
      "step": 9110
    },
    {
      "epoch": 18.33199195171026,
      "grad_norm": 0.6286770701408386,
      "learning_rate": 0.00016335244994466247,
      "loss": 0.4276,
      "step": 9111
    },
    {
      "epoch": 18.334004024144868,
      "grad_norm": 0.6005508899688721,
      "learning_rate": 0.00016334842539490895,
      "loss": 0.3774,
      "step": 9112
    },
    {
      "epoch": 18.336016096579478,
      "grad_norm": 0.6063216924667358,
      "learning_rate": 0.00016334440084515546,
      "loss": 0.3729,
      "step": 9113
    },
    {
      "epoch": 18.338028169014084,
      "grad_norm": 0.5710132718086243,
      "learning_rate": 0.00016334037629540195,
      "loss": 0.4036,
      "step": 9114
    },
    {
      "epoch": 18.34004024144869,
      "grad_norm": 0.580170214176178,
      "learning_rate": 0.00016333635174564846,
      "loss": 0.3814,
      "step": 9115
    },
    {
      "epoch": 18.3420523138833,
      "grad_norm": 0.5884062647819519,
      "learning_rate": 0.00016333232719589497,
      "loss": 0.401,
      "step": 9116
    },
    {
      "epoch": 18.344064386317907,
      "grad_norm": 0.6033430695533752,
      "learning_rate": 0.00016332830264614148,
      "loss": 0.3885,
      "step": 9117
    },
    {
      "epoch": 18.346076458752513,
      "grad_norm": 0.5916641354560852,
      "learning_rate": 0.00016332427809638797,
      "loss": 0.3999,
      "step": 9118
    },
    {
      "epoch": 18.348088531187123,
      "grad_norm": 0.5775458216667175,
      "learning_rate": 0.00016332025354663448,
      "loss": 0.3825,
      "step": 9119
    },
    {
      "epoch": 18.35010060362173,
      "grad_norm": 0.5694368481636047,
      "learning_rate": 0.00016331622899688097,
      "loss": 0.3684,
      "step": 9120
    },
    {
      "epoch": 18.352112676056336,
      "grad_norm": 0.5773151516914368,
      "learning_rate": 0.0001633122044471275,
      "loss": 0.3777,
      "step": 9121
    },
    {
      "epoch": 18.354124748490946,
      "grad_norm": 0.5725519061088562,
      "learning_rate": 0.000163308179897374,
      "loss": 0.3532,
      "step": 9122
    },
    {
      "epoch": 18.356136820925553,
      "grad_norm": 0.627939760684967,
      "learning_rate": 0.0001633041553476205,
      "loss": 0.3815,
      "step": 9123
    },
    {
      "epoch": 18.358148893360163,
      "grad_norm": 0.6363005638122559,
      "learning_rate": 0.000163300130797867,
      "loss": 0.4401,
      "step": 9124
    },
    {
      "epoch": 18.36016096579477,
      "grad_norm": 0.62423175573349,
      "learning_rate": 0.0001632961062481135,
      "loss": 0.4005,
      "step": 9125
    },
    {
      "epoch": 18.362173038229376,
      "grad_norm": 0.5768870711326599,
      "learning_rate": 0.00016329208169836,
      "loss": 0.3607,
      "step": 9126
    },
    {
      "epoch": 18.364185110663986,
      "grad_norm": 0.6021906137466431,
      "learning_rate": 0.0001632880571486065,
      "loss": 0.4056,
      "step": 9127
    },
    {
      "epoch": 18.366197183098592,
      "grad_norm": 0.6132479906082153,
      "learning_rate": 0.000163284032598853,
      "loss": 0.3852,
      "step": 9128
    },
    {
      "epoch": 18.3682092555332,
      "grad_norm": 0.596083402633667,
      "learning_rate": 0.0001632800080490995,
      "loss": 0.3922,
      "step": 9129
    },
    {
      "epoch": 18.37022132796781,
      "grad_norm": 0.5676183700561523,
      "learning_rate": 0.000163275983499346,
      "loss": 0.3933,
      "step": 9130
    },
    {
      "epoch": 18.372233400402415,
      "grad_norm": 0.5620925426483154,
      "learning_rate": 0.00016327195894959252,
      "loss": 0.3901,
      "step": 9131
    },
    {
      "epoch": 18.37424547283702,
      "grad_norm": 0.5817111730575562,
      "learning_rate": 0.00016326793439983903,
      "loss": 0.4189,
      "step": 9132
    },
    {
      "epoch": 18.37625754527163,
      "grad_norm": 0.5996440649032593,
      "learning_rate": 0.00016326390985008552,
      "loss": 0.3953,
      "step": 9133
    },
    {
      "epoch": 18.378269617706238,
      "grad_norm": 0.5943081378936768,
      "learning_rate": 0.00016325988530033203,
      "loss": 0.3961,
      "step": 9134
    },
    {
      "epoch": 18.380281690140844,
      "grad_norm": 0.5911170840263367,
      "learning_rate": 0.00016325586075057851,
      "loss": 0.4302,
      "step": 9135
    },
    {
      "epoch": 18.382293762575454,
      "grad_norm": 0.5839877724647522,
      "learning_rate": 0.00016325183620082505,
      "loss": 0.3816,
      "step": 9136
    },
    {
      "epoch": 18.38430583501006,
      "grad_norm": 0.5580029487609863,
      "learning_rate": 0.00016324781165107154,
      "loss": 0.4026,
      "step": 9137
    },
    {
      "epoch": 18.386317907444667,
      "grad_norm": 0.5766119360923767,
      "learning_rate": 0.00016324378710131805,
      "loss": 0.3758,
      "step": 9138
    },
    {
      "epoch": 18.388329979879277,
      "grad_norm": 0.5876514315605164,
      "learning_rate": 0.00016323976255156454,
      "loss": 0.4136,
      "step": 9139
    },
    {
      "epoch": 18.390342052313883,
      "grad_norm": 0.6022106409072876,
      "learning_rate": 0.00016323573800181105,
      "loss": 0.4249,
      "step": 9140
    },
    {
      "epoch": 18.39235412474849,
      "grad_norm": 0.5971959233283997,
      "learning_rate": 0.00016323171345205756,
      "loss": 0.4003,
      "step": 9141
    },
    {
      "epoch": 18.3943661971831,
      "grad_norm": 0.5921735763549805,
      "learning_rate": 0.00016322768890230407,
      "loss": 0.4063,
      "step": 9142
    },
    {
      "epoch": 18.396378269617706,
      "grad_norm": 0.5899782180786133,
      "learning_rate": 0.00016322366435255056,
      "loss": 0.398,
      "step": 9143
    },
    {
      "epoch": 18.398390342052313,
      "grad_norm": 0.6016472578048706,
      "learning_rate": 0.00016321963980279707,
      "loss": 0.4378,
      "step": 9144
    },
    {
      "epoch": 18.400402414486923,
      "grad_norm": 0.6249744892120361,
      "learning_rate": 0.00016321561525304356,
      "loss": 0.4127,
      "step": 9145
    },
    {
      "epoch": 18.40241448692153,
      "grad_norm": 0.615990936756134,
      "learning_rate": 0.0001632115907032901,
      "loss": 0.3907,
      "step": 9146
    },
    {
      "epoch": 18.404426559356136,
      "grad_norm": 0.5815645456314087,
      "learning_rate": 0.00016320756615353658,
      "loss": 0.3723,
      "step": 9147
    },
    {
      "epoch": 18.406438631790746,
      "grad_norm": 0.5868332982063293,
      "learning_rate": 0.0001632035416037831,
      "loss": 0.3755,
      "step": 9148
    },
    {
      "epoch": 18.408450704225352,
      "grad_norm": 0.6019871234893799,
      "learning_rate": 0.00016319951705402958,
      "loss": 0.4187,
      "step": 9149
    },
    {
      "epoch": 18.41046277665996,
      "grad_norm": 0.5902378559112549,
      "learning_rate": 0.0001631954925042761,
      "loss": 0.402,
      "step": 9150
    },
    {
      "epoch": 18.41247484909457,
      "grad_norm": 0.6020281314849854,
      "learning_rate": 0.0001631914679545226,
      "loss": 0.4009,
      "step": 9151
    },
    {
      "epoch": 18.414486921529175,
      "grad_norm": 0.5753500461578369,
      "learning_rate": 0.00016318744340476911,
      "loss": 0.3883,
      "step": 9152
    },
    {
      "epoch": 18.41649899396378,
      "grad_norm": 0.5596521496772766,
      "learning_rate": 0.0001631834188550156,
      "loss": 0.3638,
      "step": 9153
    },
    {
      "epoch": 18.41851106639839,
      "grad_norm": 0.6322688460350037,
      "learning_rate": 0.0001631793943052621,
      "loss": 0.4063,
      "step": 9154
    },
    {
      "epoch": 18.420523138832998,
      "grad_norm": 0.6044135689735413,
      "learning_rate": 0.0001631753697555086,
      "loss": 0.4239,
      "step": 9155
    },
    {
      "epoch": 18.422535211267604,
      "grad_norm": 0.6309214234352112,
      "learning_rate": 0.0001631713452057551,
      "loss": 0.4038,
      "step": 9156
    },
    {
      "epoch": 18.424547283702214,
      "grad_norm": 0.6267197132110596,
      "learning_rate": 0.00016316732065600162,
      "loss": 0.4041,
      "step": 9157
    },
    {
      "epoch": 18.42655935613682,
      "grad_norm": 0.6621496677398682,
      "learning_rate": 0.00016316329610624813,
      "loss": 0.4147,
      "step": 9158
    },
    {
      "epoch": 18.428571428571427,
      "grad_norm": 0.6323847770690918,
      "learning_rate": 0.00016315927155649462,
      "loss": 0.3666,
      "step": 9159
    },
    {
      "epoch": 18.430583501006037,
      "grad_norm": 0.5916774868965149,
      "learning_rate": 0.00016315524700674113,
      "loss": 0.4042,
      "step": 9160
    },
    {
      "epoch": 18.432595573440643,
      "grad_norm": 0.5763185024261475,
      "learning_rate": 0.00016315122245698764,
      "loss": 0.3957,
      "step": 9161
    },
    {
      "epoch": 18.43460764587525,
      "grad_norm": 0.5685632824897766,
      "learning_rate": 0.00016314719790723413,
      "loss": 0.3694,
      "step": 9162
    },
    {
      "epoch": 18.43661971830986,
      "grad_norm": 0.5943077802658081,
      "learning_rate": 0.00016314317335748064,
      "loss": 0.3918,
      "step": 9163
    },
    {
      "epoch": 18.438631790744466,
      "grad_norm": 0.573413610458374,
      "learning_rate": 0.00016313914880772712,
      "loss": 0.4176,
      "step": 9164
    },
    {
      "epoch": 18.440643863179073,
      "grad_norm": 0.5954806804656982,
      "learning_rate": 0.00016313512425797364,
      "loss": 0.3988,
      "step": 9165
    },
    {
      "epoch": 18.442655935613683,
      "grad_norm": 0.5942266583442688,
      "learning_rate": 0.00016313109970822015,
      "loss": 0.3799,
      "step": 9166
    },
    {
      "epoch": 18.44466800804829,
      "grad_norm": 0.5902665257453918,
      "learning_rate": 0.00016312707515846666,
      "loss": 0.4051,
      "step": 9167
    },
    {
      "epoch": 18.446680080482896,
      "grad_norm": 0.6088894009590149,
      "learning_rate": 0.00016312305060871315,
      "loss": 0.4264,
      "step": 9168
    },
    {
      "epoch": 18.448692152917506,
      "grad_norm": 0.6044578552246094,
      "learning_rate": 0.00016311902605895966,
      "loss": 0.3955,
      "step": 9169
    },
    {
      "epoch": 18.450704225352112,
      "grad_norm": 0.6220475435256958,
      "learning_rate": 0.00016311500150920614,
      "loss": 0.4033,
      "step": 9170
    },
    {
      "epoch": 18.452716297786722,
      "grad_norm": 0.6063637733459473,
      "learning_rate": 0.00016311097695945268,
      "loss": 0.3471,
      "step": 9171
    },
    {
      "epoch": 18.45472837022133,
      "grad_norm": 0.5756960511207581,
      "learning_rate": 0.00016310695240969917,
      "loss": 0.4015,
      "step": 9172
    },
    {
      "epoch": 18.456740442655935,
      "grad_norm": 0.5817150473594666,
      "learning_rate": 0.00016310292785994568,
      "loss": 0.3938,
      "step": 9173
    },
    {
      "epoch": 18.458752515090545,
      "grad_norm": 0.5614199042320251,
      "learning_rate": 0.00016309890331019217,
      "loss": 0.3988,
      "step": 9174
    },
    {
      "epoch": 18.46076458752515,
      "grad_norm": 0.5516934394836426,
      "learning_rate": 0.00016309487876043868,
      "loss": 0.3798,
      "step": 9175
    },
    {
      "epoch": 18.462776659959758,
      "grad_norm": 0.5776560306549072,
      "learning_rate": 0.0001630908542106852,
      "loss": 0.4034,
      "step": 9176
    },
    {
      "epoch": 18.464788732394368,
      "grad_norm": 0.5766623616218567,
      "learning_rate": 0.0001630868296609317,
      "loss": 0.3894,
      "step": 9177
    },
    {
      "epoch": 18.466800804828974,
      "grad_norm": 0.5906175971031189,
      "learning_rate": 0.0001630828051111782,
      "loss": 0.3773,
      "step": 9178
    },
    {
      "epoch": 18.46881287726358,
      "grad_norm": 0.58901447057724,
      "learning_rate": 0.0001630787805614247,
      "loss": 0.3786,
      "step": 9179
    },
    {
      "epoch": 18.47082494969819,
      "grad_norm": 0.6177040934562683,
      "learning_rate": 0.00016307475601167118,
      "loss": 0.388,
      "step": 9180
    },
    {
      "epoch": 18.472837022132797,
      "grad_norm": 0.6369751691818237,
      "learning_rate": 0.00016307073146191772,
      "loss": 0.4146,
      "step": 9181
    },
    {
      "epoch": 18.474849094567404,
      "grad_norm": 0.5955101251602173,
      "learning_rate": 0.0001630667069121642,
      "loss": 0.3668,
      "step": 9182
    },
    {
      "epoch": 18.476861167002014,
      "grad_norm": 0.5890856981277466,
      "learning_rate": 0.00016306268236241072,
      "loss": 0.4079,
      "step": 9183
    },
    {
      "epoch": 18.47887323943662,
      "grad_norm": 0.560407817363739,
      "learning_rate": 0.0001630586578126572,
      "loss": 0.4221,
      "step": 9184
    },
    {
      "epoch": 18.480885311871226,
      "grad_norm": 0.5761708617210388,
      "learning_rate": 0.00016305463326290372,
      "loss": 0.4283,
      "step": 9185
    },
    {
      "epoch": 18.482897384305836,
      "grad_norm": 0.5473275184631348,
      "learning_rate": 0.00016305060871315023,
      "loss": 0.3774,
      "step": 9186
    },
    {
      "epoch": 18.484909456740443,
      "grad_norm": 0.591447651386261,
      "learning_rate": 0.00016304658416339674,
      "loss": 0.4335,
      "step": 9187
    },
    {
      "epoch": 18.48692152917505,
      "grad_norm": 0.5761905312538147,
      "learning_rate": 0.00016304255961364323,
      "loss": 0.3857,
      "step": 9188
    },
    {
      "epoch": 18.48893360160966,
      "grad_norm": 0.6037344932556152,
      "learning_rate": 0.00016303853506388974,
      "loss": 0.3874,
      "step": 9189
    },
    {
      "epoch": 18.490945674044266,
      "grad_norm": 0.598220705986023,
      "learning_rate": 0.00016303451051413623,
      "loss": 0.4073,
      "step": 9190
    },
    {
      "epoch": 18.492957746478872,
      "grad_norm": 0.5818392634391785,
      "learning_rate": 0.00016303048596438274,
      "loss": 0.3953,
      "step": 9191
    },
    {
      "epoch": 18.494969818913482,
      "grad_norm": 0.5779407024383545,
      "learning_rate": 0.00016302646141462925,
      "loss": 0.4085,
      "step": 9192
    },
    {
      "epoch": 18.49698189134809,
      "grad_norm": 0.6027477979660034,
      "learning_rate": 0.00016302243686487576,
      "loss": 0.3863,
      "step": 9193
    },
    {
      "epoch": 18.498993963782695,
      "grad_norm": 0.5774568319320679,
      "learning_rate": 0.00016301841231512225,
      "loss": 0.392,
      "step": 9194
    },
    {
      "epoch": 18.501006036217305,
      "grad_norm": 0.5753859281539917,
      "learning_rate": 0.00016301438776536876,
      "loss": 0.3752,
      "step": 9195
    },
    {
      "epoch": 18.50301810865191,
      "grad_norm": 0.5947478413581848,
      "learning_rate": 0.00016301036321561527,
      "loss": 0.3942,
      "step": 9196
    },
    {
      "epoch": 18.505030181086518,
      "grad_norm": 0.5707882642745972,
      "learning_rate": 0.00016300633866586176,
      "loss": 0.3565,
      "step": 9197
    },
    {
      "epoch": 18.507042253521128,
      "grad_norm": 0.5825402140617371,
      "learning_rate": 0.00016300231411610827,
      "loss": 0.3943,
      "step": 9198
    },
    {
      "epoch": 18.509054325955734,
      "grad_norm": 0.5888604521751404,
      "learning_rate": 0.00016299828956635475,
      "loss": 0.3947,
      "step": 9199
    },
    {
      "epoch": 18.51106639839034,
      "grad_norm": 0.6163996458053589,
      "learning_rate": 0.00016299426501660127,
      "loss": 0.3885,
      "step": 9200
    },
    {
      "epoch": 18.51307847082495,
      "grad_norm": 0.5398669838905334,
      "learning_rate": 0.00016299024046684778,
      "loss": 0.3587,
      "step": 9201
    },
    {
      "epoch": 18.515090543259557,
      "grad_norm": 0.6049273610115051,
      "learning_rate": 0.0001629862159170943,
      "loss": 0.4322,
      "step": 9202
    },
    {
      "epoch": 18.517102615694164,
      "grad_norm": 0.6167529225349426,
      "learning_rate": 0.00016298219136734078,
      "loss": 0.37,
      "step": 9203
    },
    {
      "epoch": 18.519114688128774,
      "grad_norm": 0.5944352746009827,
      "learning_rate": 0.0001629781668175873,
      "loss": 0.4143,
      "step": 9204
    },
    {
      "epoch": 18.52112676056338,
      "grad_norm": 0.5629788637161255,
      "learning_rate": 0.00016297414226783377,
      "loss": 0.3859,
      "step": 9205
    },
    {
      "epoch": 18.523138832997986,
      "grad_norm": 0.5766093730926514,
      "learning_rate": 0.0001629701177180803,
      "loss": 0.4241,
      "step": 9206
    },
    {
      "epoch": 18.525150905432596,
      "grad_norm": 0.5696446895599365,
      "learning_rate": 0.0001629660931683268,
      "loss": 0.3818,
      "step": 9207
    },
    {
      "epoch": 18.527162977867203,
      "grad_norm": 0.5662217140197754,
      "learning_rate": 0.0001629620686185733,
      "loss": 0.4151,
      "step": 9208
    },
    {
      "epoch": 18.52917505030181,
      "grad_norm": 0.5737598538398743,
      "learning_rate": 0.0001629580440688198,
      "loss": 0.375,
      "step": 9209
    },
    {
      "epoch": 18.53118712273642,
      "grad_norm": 0.5824291110038757,
      "learning_rate": 0.0001629540195190663,
      "loss": 0.4071,
      "step": 9210
    },
    {
      "epoch": 18.533199195171026,
      "grad_norm": 0.5908308625221252,
      "learning_rate": 0.00016294999496931282,
      "loss": 0.4202,
      "step": 9211
    },
    {
      "epoch": 18.535211267605632,
      "grad_norm": 0.5872223377227783,
      "learning_rate": 0.00016294597041955933,
      "loss": 0.4012,
      "step": 9212
    },
    {
      "epoch": 18.537223340040242,
      "grad_norm": 0.575498104095459,
      "learning_rate": 0.00016294194586980582,
      "loss": 0.4058,
      "step": 9213
    },
    {
      "epoch": 18.53923541247485,
      "grad_norm": 0.5904775857925415,
      "learning_rate": 0.00016293792132005233,
      "loss": 0.3771,
      "step": 9214
    },
    {
      "epoch": 18.541247484909455,
      "grad_norm": 0.5911662578582764,
      "learning_rate": 0.00016293389677029881,
      "loss": 0.4192,
      "step": 9215
    },
    {
      "epoch": 18.543259557344065,
      "grad_norm": 0.5854606032371521,
      "learning_rate": 0.00016292987222054535,
      "loss": 0.4039,
      "step": 9216
    },
    {
      "epoch": 18.54527162977867,
      "grad_norm": 0.6140941977500916,
      "learning_rate": 0.00016292584767079184,
      "loss": 0.3927,
      "step": 9217
    },
    {
      "epoch": 18.547283702213278,
      "grad_norm": 0.5558741688728333,
      "learning_rate": 0.00016292182312103835,
      "loss": 0.3671,
      "step": 9218
    },
    {
      "epoch": 18.549295774647888,
      "grad_norm": 0.6149550676345825,
      "learning_rate": 0.00016291779857128484,
      "loss": 0.3989,
      "step": 9219
    },
    {
      "epoch": 18.551307847082494,
      "grad_norm": 0.6332974433898926,
      "learning_rate": 0.00016291377402153135,
      "loss": 0.4261,
      "step": 9220
    },
    {
      "epoch": 18.5533199195171,
      "grad_norm": 0.5952218770980835,
      "learning_rate": 0.00016290974947177786,
      "loss": 0.3903,
      "step": 9221
    },
    {
      "epoch": 18.55533199195171,
      "grad_norm": 0.6207593083381653,
      "learning_rate": 0.00016290572492202437,
      "loss": 0.4126,
      "step": 9222
    },
    {
      "epoch": 18.557344064386317,
      "grad_norm": 0.6040320992469788,
      "learning_rate": 0.00016290170037227086,
      "loss": 0.3796,
      "step": 9223
    },
    {
      "epoch": 18.559356136820927,
      "grad_norm": 0.5914999842643738,
      "learning_rate": 0.00016289767582251737,
      "loss": 0.3989,
      "step": 9224
    },
    {
      "epoch": 18.561368209255534,
      "grad_norm": 0.5404723286628723,
      "learning_rate": 0.00016289365127276385,
      "loss": 0.363,
      "step": 9225
    },
    {
      "epoch": 18.56338028169014,
      "grad_norm": 0.5600054264068604,
      "learning_rate": 0.00016288962672301037,
      "loss": 0.3721,
      "step": 9226
    },
    {
      "epoch": 18.56539235412475,
      "grad_norm": 0.5762823224067688,
      "learning_rate": 0.00016288560217325688,
      "loss": 0.4297,
      "step": 9227
    },
    {
      "epoch": 18.567404426559357,
      "grad_norm": 0.557880163192749,
      "learning_rate": 0.0001628815776235034,
      "loss": 0.3671,
      "step": 9228
    },
    {
      "epoch": 18.569416498993963,
      "grad_norm": 0.6203036904335022,
      "learning_rate": 0.00016287755307374988,
      "loss": 0.4143,
      "step": 9229
    },
    {
      "epoch": 18.571428571428573,
      "grad_norm": 0.589930534362793,
      "learning_rate": 0.0001628735285239964,
      "loss": 0.3952,
      "step": 9230
    },
    {
      "epoch": 18.57344064386318,
      "grad_norm": 0.6621046662330627,
      "learning_rate": 0.0001628695039742429,
      "loss": 0.4301,
      "step": 9231
    },
    {
      "epoch": 18.575452716297786,
      "grad_norm": 0.5695210695266724,
      "learning_rate": 0.00016286547942448939,
      "loss": 0.4002,
      "step": 9232
    },
    {
      "epoch": 18.577464788732396,
      "grad_norm": 0.5924035906791687,
      "learning_rate": 0.0001628614548747359,
      "loss": 0.4225,
      "step": 9233
    },
    {
      "epoch": 18.579476861167002,
      "grad_norm": 0.6145206093788147,
      "learning_rate": 0.00016285743032498238,
      "loss": 0.4215,
      "step": 9234
    },
    {
      "epoch": 18.58148893360161,
      "grad_norm": 0.5739409923553467,
      "learning_rate": 0.0001628534057752289,
      "loss": 0.3835,
      "step": 9235
    },
    {
      "epoch": 18.58350100603622,
      "grad_norm": 0.5906938314437866,
      "learning_rate": 0.0001628493812254754,
      "loss": 0.4204,
      "step": 9236
    },
    {
      "epoch": 18.585513078470825,
      "grad_norm": 0.5943116545677185,
      "learning_rate": 0.00016284535667572192,
      "loss": 0.4077,
      "step": 9237
    },
    {
      "epoch": 18.58752515090543,
      "grad_norm": 0.5965897440910339,
      "learning_rate": 0.0001628413321259684,
      "loss": 0.4307,
      "step": 9238
    },
    {
      "epoch": 18.58953722334004,
      "grad_norm": 0.5425543189048767,
      "learning_rate": 0.00016283730757621492,
      "loss": 0.3768,
      "step": 9239
    },
    {
      "epoch": 18.591549295774648,
      "grad_norm": 0.5898336172103882,
      "learning_rate": 0.0001628332830264614,
      "loss": 0.4055,
      "step": 9240
    },
    {
      "epoch": 18.593561368209254,
      "grad_norm": 0.5908757448196411,
      "learning_rate": 0.00016282925847670794,
      "loss": 0.396,
      "step": 9241
    },
    {
      "epoch": 18.595573440643864,
      "grad_norm": 0.6450875997543335,
      "learning_rate": 0.00016282523392695443,
      "loss": 0.4416,
      "step": 9242
    },
    {
      "epoch": 18.59758551307847,
      "grad_norm": 0.6375131607055664,
      "learning_rate": 0.00016282120937720094,
      "loss": 0.3847,
      "step": 9243
    },
    {
      "epoch": 18.599597585513077,
      "grad_norm": 0.6196619272232056,
      "learning_rate": 0.00016281718482744742,
      "loss": 0.4098,
      "step": 9244
    },
    {
      "epoch": 18.601609657947687,
      "grad_norm": 0.5646458864212036,
      "learning_rate": 0.00016281316027769394,
      "loss": 0.3694,
      "step": 9245
    },
    {
      "epoch": 18.603621730382294,
      "grad_norm": 0.5666971802711487,
      "learning_rate": 0.00016280913572794045,
      "loss": 0.3904,
      "step": 9246
    },
    {
      "epoch": 18.6056338028169,
      "grad_norm": 0.5817564129829407,
      "learning_rate": 0.00016280511117818696,
      "loss": 0.4006,
      "step": 9247
    },
    {
      "epoch": 18.60764587525151,
      "grad_norm": 0.6618497371673584,
      "learning_rate": 0.00016280108662843345,
      "loss": 0.4256,
      "step": 9248
    },
    {
      "epoch": 18.609657947686117,
      "grad_norm": 0.589901328086853,
      "learning_rate": 0.00016279706207867996,
      "loss": 0.4236,
      "step": 9249
    },
    {
      "epoch": 18.611670020120723,
      "grad_norm": 0.6244890689849854,
      "learning_rate": 0.00016279303752892644,
      "loss": 0.4043,
      "step": 9250
    },
    {
      "epoch": 18.613682092555333,
      "grad_norm": 0.5590836405754089,
      "learning_rate": 0.00016278901297917298,
      "loss": 0.3757,
      "step": 9251
    },
    {
      "epoch": 18.61569416498994,
      "grad_norm": 0.5864677429199219,
      "learning_rate": 0.00016278498842941947,
      "loss": 0.3988,
      "step": 9252
    },
    {
      "epoch": 18.617706237424546,
      "grad_norm": 0.6276631355285645,
      "learning_rate": 0.00016278096387966598,
      "loss": 0.416,
      "step": 9253
    },
    {
      "epoch": 18.619718309859156,
      "grad_norm": 0.5447779893875122,
      "learning_rate": 0.00016277693932991247,
      "loss": 0.3666,
      "step": 9254
    },
    {
      "epoch": 18.621730382293762,
      "grad_norm": 0.5667353868484497,
      "learning_rate": 0.00016277291478015898,
      "loss": 0.3818,
      "step": 9255
    },
    {
      "epoch": 18.62374245472837,
      "grad_norm": 0.5786727070808411,
      "learning_rate": 0.0001627688902304055,
      "loss": 0.4014,
      "step": 9256
    },
    {
      "epoch": 18.62575452716298,
      "grad_norm": 0.5992423295974731,
      "learning_rate": 0.000162764865680652,
      "loss": 0.407,
      "step": 9257
    },
    {
      "epoch": 18.627766599597585,
      "grad_norm": 0.5998122692108154,
      "learning_rate": 0.0001627608411308985,
      "loss": 0.4024,
      "step": 9258
    },
    {
      "epoch": 18.62977867203219,
      "grad_norm": 0.584723174571991,
      "learning_rate": 0.000162756816581145,
      "loss": 0.4322,
      "step": 9259
    },
    {
      "epoch": 18.6317907444668,
      "grad_norm": 0.5955401659011841,
      "learning_rate": 0.00016275279203139148,
      "loss": 0.4148,
      "step": 9260
    },
    {
      "epoch": 18.633802816901408,
      "grad_norm": 0.545697033405304,
      "learning_rate": 0.000162748767481638,
      "loss": 0.3745,
      "step": 9261
    },
    {
      "epoch": 18.635814889336014,
      "grad_norm": 0.5946397185325623,
      "learning_rate": 0.0001627447429318845,
      "loss": 0.4049,
      "step": 9262
    },
    {
      "epoch": 18.637826961770624,
      "grad_norm": 0.5642681121826172,
      "learning_rate": 0.00016274071838213102,
      "loss": 0.3578,
      "step": 9263
    },
    {
      "epoch": 18.63983903420523,
      "grad_norm": 0.570641279220581,
      "learning_rate": 0.0001627366938323775,
      "loss": 0.3607,
      "step": 9264
    },
    {
      "epoch": 18.641851106639837,
      "grad_norm": 0.6160386204719543,
      "learning_rate": 0.00016273266928262402,
      "loss": 0.401,
      "step": 9265
    },
    {
      "epoch": 18.643863179074447,
      "grad_norm": 0.5811742544174194,
      "learning_rate": 0.00016272864473287053,
      "loss": 0.3701,
      "step": 9266
    },
    {
      "epoch": 18.645875251509054,
      "grad_norm": 0.6328752636909485,
      "learning_rate": 0.00016272462018311702,
      "loss": 0.3945,
      "step": 9267
    },
    {
      "epoch": 18.647887323943664,
      "grad_norm": 0.5763841867446899,
      "learning_rate": 0.00016272059563336353,
      "loss": 0.3871,
      "step": 9268
    },
    {
      "epoch": 18.64989939637827,
      "grad_norm": 0.578066349029541,
      "learning_rate": 0.00016271657108361,
      "loss": 0.3946,
      "step": 9269
    },
    {
      "epoch": 18.651911468812877,
      "grad_norm": 0.6400648951530457,
      "learning_rate": 0.00016271254653385653,
      "loss": 0.4398,
      "step": 9270
    },
    {
      "epoch": 18.653923541247487,
      "grad_norm": 0.5859150886535645,
      "learning_rate": 0.00016270852198410304,
      "loss": 0.4229,
      "step": 9271
    },
    {
      "epoch": 18.655935613682093,
      "grad_norm": 0.5706754326820374,
      "learning_rate": 0.00016270449743434955,
      "loss": 0.3956,
      "step": 9272
    },
    {
      "epoch": 18.6579476861167,
      "grad_norm": 0.5869488716125488,
      "learning_rate": 0.00016270047288459603,
      "loss": 0.4189,
      "step": 9273
    },
    {
      "epoch": 18.65995975855131,
      "grad_norm": 0.5414531826972961,
      "learning_rate": 0.00016269644833484255,
      "loss": 0.3758,
      "step": 9274
    },
    {
      "epoch": 18.661971830985916,
      "grad_norm": 0.5947801470756531,
      "learning_rate": 0.00016269242378508903,
      "loss": 0.4138,
      "step": 9275
    },
    {
      "epoch": 18.663983903420522,
      "grad_norm": 0.5647242069244385,
      "learning_rate": 0.00016268839923533557,
      "loss": 0.376,
      "step": 9276
    },
    {
      "epoch": 18.665995975855132,
      "grad_norm": 0.6098572611808777,
      "learning_rate": 0.00016268437468558206,
      "loss": 0.4133,
      "step": 9277
    },
    {
      "epoch": 18.66800804828974,
      "grad_norm": 0.5960507988929749,
      "learning_rate": 0.00016268035013582857,
      "loss": 0.41,
      "step": 9278
    },
    {
      "epoch": 18.670020120724345,
      "grad_norm": 0.6379257440567017,
      "learning_rate": 0.00016267632558607505,
      "loss": 0.419,
      "step": 9279
    },
    {
      "epoch": 18.672032193158955,
      "grad_norm": 0.6142597794532776,
      "learning_rate": 0.00016267230103632157,
      "loss": 0.4311,
      "step": 9280
    },
    {
      "epoch": 18.67404426559356,
      "grad_norm": 0.6004951000213623,
      "learning_rate": 0.00016266827648656808,
      "loss": 0.4167,
      "step": 9281
    },
    {
      "epoch": 18.676056338028168,
      "grad_norm": 0.5970123410224915,
      "learning_rate": 0.0001626642519368146,
      "loss": 0.4255,
      "step": 9282
    },
    {
      "epoch": 18.678068410462778,
      "grad_norm": 0.5892970561981201,
      "learning_rate": 0.00016266022738706108,
      "loss": 0.43,
      "step": 9283
    },
    {
      "epoch": 18.680080482897385,
      "grad_norm": 0.5894626379013062,
      "learning_rate": 0.0001626562028373076,
      "loss": 0.3992,
      "step": 9284
    },
    {
      "epoch": 18.68209255533199,
      "grad_norm": 0.5681537985801697,
      "learning_rate": 0.00016265217828755407,
      "loss": 0.4122,
      "step": 9285
    },
    {
      "epoch": 18.6841046277666,
      "grad_norm": 0.5858867168426514,
      "learning_rate": 0.0001626481537378006,
      "loss": 0.3829,
      "step": 9286
    },
    {
      "epoch": 18.686116700201207,
      "grad_norm": 0.5571285486221313,
      "learning_rate": 0.0001626441291880471,
      "loss": 0.3878,
      "step": 9287
    },
    {
      "epoch": 18.688128772635814,
      "grad_norm": 0.5868299007415771,
      "learning_rate": 0.0001626401046382936,
      "loss": 0.3915,
      "step": 9288
    },
    {
      "epoch": 18.690140845070424,
      "grad_norm": 0.5820125937461853,
      "learning_rate": 0.0001626360800885401,
      "loss": 0.3972,
      "step": 9289
    },
    {
      "epoch": 18.69215291750503,
      "grad_norm": 0.5851859450340271,
      "learning_rate": 0.0001626320555387866,
      "loss": 0.4087,
      "step": 9290
    },
    {
      "epoch": 18.694164989939637,
      "grad_norm": 0.5803654789924622,
      "learning_rate": 0.00016262803098903312,
      "loss": 0.396,
      "step": 9291
    },
    {
      "epoch": 18.696177062374247,
      "grad_norm": 0.582663893699646,
      "learning_rate": 0.00016262400643927963,
      "loss": 0.3804,
      "step": 9292
    },
    {
      "epoch": 18.698189134808853,
      "grad_norm": 0.5748541355133057,
      "learning_rate": 0.00016261998188952612,
      "loss": 0.3975,
      "step": 9293
    },
    {
      "epoch": 18.70020120724346,
      "grad_norm": 0.581874430179596,
      "learning_rate": 0.00016261595733977263,
      "loss": 0.3777,
      "step": 9294
    },
    {
      "epoch": 18.70221327967807,
      "grad_norm": 0.6095137596130371,
      "learning_rate": 0.00016261193279001911,
      "loss": 0.4441,
      "step": 9295
    },
    {
      "epoch": 18.704225352112676,
      "grad_norm": 0.5623226165771484,
      "learning_rate": 0.00016260790824026563,
      "loss": 0.4062,
      "step": 9296
    },
    {
      "epoch": 18.706237424547282,
      "grad_norm": 0.5591612458229065,
      "learning_rate": 0.00016260388369051214,
      "loss": 0.4088,
      "step": 9297
    },
    {
      "epoch": 18.708249496981892,
      "grad_norm": 0.598615288734436,
      "learning_rate": 0.00016259985914075865,
      "loss": 0.4274,
      "step": 9298
    },
    {
      "epoch": 18.7102615694165,
      "grad_norm": 0.5861489176750183,
      "learning_rate": 0.00016259583459100514,
      "loss": 0.3903,
      "step": 9299
    },
    {
      "epoch": 18.712273641851105,
      "grad_norm": 0.5743632912635803,
      "learning_rate": 0.00016259181004125165,
      "loss": 0.4057,
      "step": 9300
    },
    {
      "epoch": 18.714285714285715,
      "grad_norm": 0.6221715211868286,
      "learning_rate": 0.00016258778549149816,
      "loss": 0.4049,
      "step": 9301
    },
    {
      "epoch": 18.71629778672032,
      "grad_norm": 0.597846269607544,
      "learning_rate": 0.00016258376094174465,
      "loss": 0.4305,
      "step": 9302
    },
    {
      "epoch": 18.718309859154928,
      "grad_norm": 0.5786214470863342,
      "learning_rate": 0.00016257973639199116,
      "loss": 0.3581,
      "step": 9303
    },
    {
      "epoch": 18.720321931589538,
      "grad_norm": 0.570779025554657,
      "learning_rate": 0.00016257571184223764,
      "loss": 0.3989,
      "step": 9304
    },
    {
      "epoch": 18.722334004024145,
      "grad_norm": 0.6092882752418518,
      "learning_rate": 0.00016257168729248415,
      "loss": 0.4268,
      "step": 9305
    },
    {
      "epoch": 18.72434607645875,
      "grad_norm": 0.6087427735328674,
      "learning_rate": 0.00016256766274273067,
      "loss": 0.3971,
      "step": 9306
    },
    {
      "epoch": 18.72635814889336,
      "grad_norm": 0.6003498435020447,
      "learning_rate": 0.00016256363819297718,
      "loss": 0.4079,
      "step": 9307
    },
    {
      "epoch": 18.728370221327967,
      "grad_norm": 0.5889801383018494,
      "learning_rate": 0.00016255961364322366,
      "loss": 0.4,
      "step": 9308
    },
    {
      "epoch": 18.730382293762574,
      "grad_norm": 0.5747770071029663,
      "learning_rate": 0.00016255558909347018,
      "loss": 0.3838,
      "step": 9309
    },
    {
      "epoch": 18.732394366197184,
      "grad_norm": 0.6116838455200195,
      "learning_rate": 0.00016255156454371666,
      "loss": 0.4153,
      "step": 9310
    },
    {
      "epoch": 18.73440643863179,
      "grad_norm": 0.6328554749488831,
      "learning_rate": 0.0001625475399939632,
      "loss": 0.429,
      "step": 9311
    },
    {
      "epoch": 18.736418511066397,
      "grad_norm": 0.5778709053993225,
      "learning_rate": 0.00016254351544420969,
      "loss": 0.3742,
      "step": 9312
    },
    {
      "epoch": 18.738430583501007,
      "grad_norm": 0.5919867157936096,
      "learning_rate": 0.0001625394908944562,
      "loss": 0.3854,
      "step": 9313
    },
    {
      "epoch": 18.740442655935613,
      "grad_norm": 0.5918256640434265,
      "learning_rate": 0.00016253546634470268,
      "loss": 0.3969,
      "step": 9314
    },
    {
      "epoch": 18.74245472837022,
      "grad_norm": 0.5955549478530884,
      "learning_rate": 0.0001625314417949492,
      "loss": 0.3996,
      "step": 9315
    },
    {
      "epoch": 18.74446680080483,
      "grad_norm": 0.597868800163269,
      "learning_rate": 0.0001625274172451957,
      "loss": 0.4029,
      "step": 9316
    },
    {
      "epoch": 18.746478873239436,
      "grad_norm": 0.5660099387168884,
      "learning_rate": 0.00016252339269544222,
      "loss": 0.4244,
      "step": 9317
    },
    {
      "epoch": 18.748490945674043,
      "grad_norm": 0.6164073348045349,
      "learning_rate": 0.0001625193681456887,
      "loss": 0.4481,
      "step": 9318
    },
    {
      "epoch": 18.750503018108652,
      "grad_norm": 0.5959474444389343,
      "learning_rate": 0.00016251534359593522,
      "loss": 0.405,
      "step": 9319
    },
    {
      "epoch": 18.75251509054326,
      "grad_norm": 0.578220784664154,
      "learning_rate": 0.0001625113190461817,
      "loss": 0.4128,
      "step": 9320
    },
    {
      "epoch": 18.754527162977865,
      "grad_norm": 0.5894545316696167,
      "learning_rate": 0.00016250729449642824,
      "loss": 0.3832,
      "step": 9321
    },
    {
      "epoch": 18.756539235412475,
      "grad_norm": 0.5717635750770569,
      "learning_rate": 0.00016250326994667473,
      "loss": 0.3918,
      "step": 9322
    },
    {
      "epoch": 18.758551307847082,
      "grad_norm": 0.5854669809341431,
      "learning_rate": 0.00016249924539692124,
      "loss": 0.4117,
      "step": 9323
    },
    {
      "epoch": 18.760563380281692,
      "grad_norm": 0.5808769464492798,
      "learning_rate": 0.00016249522084716772,
      "loss": 0.3841,
      "step": 9324
    },
    {
      "epoch": 18.7625754527163,
      "grad_norm": 0.6015349626541138,
      "learning_rate": 0.00016249119629741424,
      "loss": 0.3747,
      "step": 9325
    },
    {
      "epoch": 18.764587525150905,
      "grad_norm": 0.6211759448051453,
      "learning_rate": 0.00016248717174766075,
      "loss": 0.4124,
      "step": 9326
    },
    {
      "epoch": 18.766599597585515,
      "grad_norm": 0.5577972531318665,
      "learning_rate": 0.00016248314719790726,
      "loss": 0.411,
      "step": 9327
    },
    {
      "epoch": 18.76861167002012,
      "grad_norm": 0.5873842239379883,
      "learning_rate": 0.00016247912264815375,
      "loss": 0.405,
      "step": 9328
    },
    {
      "epoch": 18.770623742454728,
      "grad_norm": 0.586208164691925,
      "learning_rate": 0.00016247509809840026,
      "loss": 0.4322,
      "step": 9329
    },
    {
      "epoch": 18.772635814889338,
      "grad_norm": 0.5833867788314819,
      "learning_rate": 0.00016247107354864674,
      "loss": 0.4039,
      "step": 9330
    },
    {
      "epoch": 18.774647887323944,
      "grad_norm": 0.5885566473007202,
      "learning_rate": 0.00016246704899889326,
      "loss": 0.3977,
      "step": 9331
    },
    {
      "epoch": 18.77665995975855,
      "grad_norm": 0.6046439409255981,
      "learning_rate": 0.00016246302444913977,
      "loss": 0.4437,
      "step": 9332
    },
    {
      "epoch": 18.77867203219316,
      "grad_norm": 0.5780220627784729,
      "learning_rate": 0.00016245899989938625,
      "loss": 0.423,
      "step": 9333
    },
    {
      "epoch": 18.780684104627767,
      "grad_norm": 0.6014211177825928,
      "learning_rate": 0.00016245497534963276,
      "loss": 0.407,
      "step": 9334
    },
    {
      "epoch": 18.782696177062373,
      "grad_norm": 0.6084173917770386,
      "learning_rate": 0.00016245095079987928,
      "loss": 0.3836,
      "step": 9335
    },
    {
      "epoch": 18.784708249496983,
      "grad_norm": 0.6100538969039917,
      "learning_rate": 0.0001624469262501258,
      "loss": 0.4316,
      "step": 9336
    },
    {
      "epoch": 18.78672032193159,
      "grad_norm": 0.6180610060691833,
      "learning_rate": 0.00016244290170037227,
      "loss": 0.3995,
      "step": 9337
    },
    {
      "epoch": 18.788732394366196,
      "grad_norm": 0.6219474077224731,
      "learning_rate": 0.0001624388771506188,
      "loss": 0.4412,
      "step": 9338
    },
    {
      "epoch": 18.790744466800806,
      "grad_norm": 0.5697031617164612,
      "learning_rate": 0.00016243485260086527,
      "loss": 0.3877,
      "step": 9339
    },
    {
      "epoch": 18.792756539235413,
      "grad_norm": 0.5858192443847656,
      "learning_rate": 0.00016243082805111178,
      "loss": 0.4464,
      "step": 9340
    },
    {
      "epoch": 18.79476861167002,
      "grad_norm": 0.5786712169647217,
      "learning_rate": 0.00016242680350135827,
      "loss": 0.3998,
      "step": 9341
    },
    {
      "epoch": 18.79678068410463,
      "grad_norm": 0.569456160068512,
      "learning_rate": 0.0001624227789516048,
      "loss": 0.3791,
      "step": 9342
    },
    {
      "epoch": 18.798792756539235,
      "grad_norm": 0.5917392373085022,
      "learning_rate": 0.0001624187544018513,
      "loss": 0.3978,
      "step": 9343
    },
    {
      "epoch": 18.800804828973842,
      "grad_norm": 0.6157620549201965,
      "learning_rate": 0.0001624147298520978,
      "loss": 0.4332,
      "step": 9344
    },
    {
      "epoch": 18.802816901408452,
      "grad_norm": 0.5766895413398743,
      "learning_rate": 0.0001624107053023443,
      "loss": 0.3974,
      "step": 9345
    },
    {
      "epoch": 18.80482897384306,
      "grad_norm": 0.5966703295707703,
      "learning_rate": 0.0001624066807525908,
      "loss": 0.4148,
      "step": 9346
    },
    {
      "epoch": 18.806841046277665,
      "grad_norm": 0.6158912181854248,
      "learning_rate": 0.00016240265620283732,
      "loss": 0.4093,
      "step": 9347
    },
    {
      "epoch": 18.808853118712275,
      "grad_norm": 0.6353716254234314,
      "learning_rate": 0.00016239863165308383,
      "loss": 0.4232,
      "step": 9348
    },
    {
      "epoch": 18.81086519114688,
      "grad_norm": 0.5880708694458008,
      "learning_rate": 0.0001623946071033303,
      "loss": 0.4291,
      "step": 9349
    },
    {
      "epoch": 18.812877263581488,
      "grad_norm": 0.5930196642875671,
      "learning_rate": 0.00016239058255357682,
      "loss": 0.4207,
      "step": 9350
    },
    {
      "epoch": 18.814889336016098,
      "grad_norm": 0.6362302303314209,
      "learning_rate": 0.0001623865580038233,
      "loss": 0.3669,
      "step": 9351
    },
    {
      "epoch": 18.816901408450704,
      "grad_norm": 0.5648832321166992,
      "learning_rate": 0.00016238253345406985,
      "loss": 0.4295,
      "step": 9352
    },
    {
      "epoch": 18.81891348088531,
      "grad_norm": 0.5959052443504333,
      "learning_rate": 0.00016237850890431633,
      "loss": 0.4284,
      "step": 9353
    },
    {
      "epoch": 18.82092555331992,
      "grad_norm": 0.6298961639404297,
      "learning_rate": 0.00016237448435456285,
      "loss": 0.4298,
      "step": 9354
    },
    {
      "epoch": 18.822937625754527,
      "grad_norm": 0.6184532642364502,
      "learning_rate": 0.00016237045980480933,
      "loss": 0.3871,
      "step": 9355
    },
    {
      "epoch": 18.824949698189133,
      "grad_norm": 0.6139959692955017,
      "learning_rate": 0.00016236643525505584,
      "loss": 0.4152,
      "step": 9356
    },
    {
      "epoch": 18.826961770623743,
      "grad_norm": 0.5756946206092834,
      "learning_rate": 0.00016236241070530236,
      "loss": 0.4045,
      "step": 9357
    },
    {
      "epoch": 18.82897384305835,
      "grad_norm": 0.6261759400367737,
      "learning_rate": 0.00016235838615554887,
      "loss": 0.4214,
      "step": 9358
    },
    {
      "epoch": 18.830985915492956,
      "grad_norm": 0.5924904942512512,
      "learning_rate": 0.00016235436160579535,
      "loss": 0.4265,
      "step": 9359
    },
    {
      "epoch": 18.832997987927566,
      "grad_norm": 0.5563544631004333,
      "learning_rate": 0.00016235033705604187,
      "loss": 0.3995,
      "step": 9360
    },
    {
      "epoch": 18.835010060362173,
      "grad_norm": 0.5766776204109192,
      "learning_rate": 0.00016234631250628835,
      "loss": 0.4124,
      "step": 9361
    },
    {
      "epoch": 18.83702213279678,
      "grad_norm": 0.6014429330825806,
      "learning_rate": 0.0001623422879565349,
      "loss": 0.3938,
      "step": 9362
    },
    {
      "epoch": 18.83903420523139,
      "grad_norm": 0.6146978139877319,
      "learning_rate": 0.00016233826340678138,
      "loss": 0.4141,
      "step": 9363
    },
    {
      "epoch": 18.841046277665995,
      "grad_norm": 0.6052590012550354,
      "learning_rate": 0.0001623342388570279,
      "loss": 0.4347,
      "step": 9364
    },
    {
      "epoch": 18.843058350100602,
      "grad_norm": 0.5812432765960693,
      "learning_rate": 0.00016233021430727437,
      "loss": 0.3764,
      "step": 9365
    },
    {
      "epoch": 18.845070422535212,
      "grad_norm": 0.6130304932594299,
      "learning_rate": 0.00016232618975752088,
      "loss": 0.4315,
      "step": 9366
    },
    {
      "epoch": 18.84708249496982,
      "grad_norm": 0.5937533378601074,
      "learning_rate": 0.0001623221652077674,
      "loss": 0.4432,
      "step": 9367
    },
    {
      "epoch": 18.84909456740443,
      "grad_norm": 0.5674647688865662,
      "learning_rate": 0.00016231814065801388,
      "loss": 0.4037,
      "step": 9368
    },
    {
      "epoch": 18.851106639839035,
      "grad_norm": 0.6192668676376343,
      "learning_rate": 0.0001623141161082604,
      "loss": 0.433,
      "step": 9369
    },
    {
      "epoch": 18.85311871227364,
      "grad_norm": 0.6160228252410889,
      "learning_rate": 0.0001623100915585069,
      "loss": 0.4035,
      "step": 9370
    },
    {
      "epoch": 18.85513078470825,
      "grad_norm": 0.575337290763855,
      "learning_rate": 0.0001623060670087534,
      "loss": 0.391,
      "step": 9371
    },
    {
      "epoch": 18.857142857142858,
      "grad_norm": 0.6007067561149597,
      "learning_rate": 0.0001623020424589999,
      "loss": 0.4428,
      "step": 9372
    },
    {
      "epoch": 18.859154929577464,
      "grad_norm": 0.5961697101593018,
      "learning_rate": 0.00016229801790924642,
      "loss": 0.4345,
      "step": 9373
    },
    {
      "epoch": 18.861167002012074,
      "grad_norm": 0.5582261085510254,
      "learning_rate": 0.0001622939933594929,
      "loss": 0.4147,
      "step": 9374
    },
    {
      "epoch": 18.86317907444668,
      "grad_norm": 0.632604718208313,
      "learning_rate": 0.0001622899688097394,
      "loss": 0.4113,
      "step": 9375
    },
    {
      "epoch": 18.865191146881287,
      "grad_norm": 0.5939131379127502,
      "learning_rate": 0.0001622859442599859,
      "loss": 0.3894,
      "step": 9376
    },
    {
      "epoch": 18.867203219315897,
      "grad_norm": 0.5972134470939636,
      "learning_rate": 0.00016228191971023244,
      "loss": 0.436,
      "step": 9377
    },
    {
      "epoch": 18.869215291750503,
      "grad_norm": 0.5627316832542419,
      "learning_rate": 0.00016227789516047892,
      "loss": 0.3955,
      "step": 9378
    },
    {
      "epoch": 18.87122736418511,
      "grad_norm": 0.610586404800415,
      "learning_rate": 0.00016227387061072544,
      "loss": 0.4049,
      "step": 9379
    },
    {
      "epoch": 18.87323943661972,
      "grad_norm": 0.559516966342926,
      "learning_rate": 0.00016226984606097192,
      "loss": 0.413,
      "step": 9380
    },
    {
      "epoch": 18.875251509054326,
      "grad_norm": 0.5751433372497559,
      "learning_rate": 0.00016226582151121843,
      "loss": 0.4023,
      "step": 9381
    },
    {
      "epoch": 18.877263581488933,
      "grad_norm": 0.6164553165435791,
      "learning_rate": 0.00016226179696146494,
      "loss": 0.414,
      "step": 9382
    },
    {
      "epoch": 18.879275653923543,
      "grad_norm": 0.6102567315101624,
      "learning_rate": 0.00016225777241171146,
      "loss": 0.4372,
      "step": 9383
    },
    {
      "epoch": 18.88128772635815,
      "grad_norm": 0.6001976728439331,
      "learning_rate": 0.00016225374786195794,
      "loss": 0.3982,
      "step": 9384
    },
    {
      "epoch": 18.883299798792756,
      "grad_norm": 0.6068341732025146,
      "learning_rate": 0.00016224972331220445,
      "loss": 0.4312,
      "step": 9385
    },
    {
      "epoch": 18.885311871227366,
      "grad_norm": 0.5982809066772461,
      "learning_rate": 0.00016224569876245094,
      "loss": 0.4116,
      "step": 9386
    },
    {
      "epoch": 18.887323943661972,
      "grad_norm": 0.5902340412139893,
      "learning_rate": 0.00016224167421269748,
      "loss": 0.4013,
      "step": 9387
    },
    {
      "epoch": 18.88933601609658,
      "grad_norm": 0.5589666366577148,
      "learning_rate": 0.00016223764966294396,
      "loss": 0.3965,
      "step": 9388
    },
    {
      "epoch": 18.89134808853119,
      "grad_norm": 0.5771203637123108,
      "learning_rate": 0.00016223362511319048,
      "loss": 0.4052,
      "step": 9389
    },
    {
      "epoch": 18.893360160965795,
      "grad_norm": 0.6166917681694031,
      "learning_rate": 0.00016222960056343696,
      "loss": 0.4056,
      "step": 9390
    },
    {
      "epoch": 18.8953722334004,
      "grad_norm": 0.607899010181427,
      "learning_rate": 0.00016222557601368347,
      "loss": 0.4101,
      "step": 9391
    },
    {
      "epoch": 18.89738430583501,
      "grad_norm": 0.5695868730545044,
      "learning_rate": 0.00016222155146392999,
      "loss": 0.4048,
      "step": 9392
    },
    {
      "epoch": 18.899396378269618,
      "grad_norm": 0.621821403503418,
      "learning_rate": 0.0001622175269141765,
      "loss": 0.3725,
      "step": 9393
    },
    {
      "epoch": 18.901408450704224,
      "grad_norm": 0.58479243516922,
      "learning_rate": 0.00016221350236442298,
      "loss": 0.4031,
      "step": 9394
    },
    {
      "epoch": 18.903420523138834,
      "grad_norm": 0.5986174941062927,
      "learning_rate": 0.0001622094778146695,
      "loss": 0.4033,
      "step": 9395
    },
    {
      "epoch": 18.90543259557344,
      "grad_norm": 0.5832358002662659,
      "learning_rate": 0.00016220545326491598,
      "loss": 0.3872,
      "step": 9396
    },
    {
      "epoch": 18.907444668008047,
      "grad_norm": 0.5570285320281982,
      "learning_rate": 0.00016220142871516252,
      "loss": 0.3963,
      "step": 9397
    },
    {
      "epoch": 18.909456740442657,
      "grad_norm": 0.5487640500068665,
      "learning_rate": 0.000162197404165409,
      "loss": 0.354,
      "step": 9398
    },
    {
      "epoch": 18.911468812877263,
      "grad_norm": 0.6111677885055542,
      "learning_rate": 0.00016219337961565552,
      "loss": 0.4293,
      "step": 9399
    },
    {
      "epoch": 18.91348088531187,
      "grad_norm": 0.5660973191261292,
      "learning_rate": 0.000162189355065902,
      "loss": 0.4073,
      "step": 9400
    },
    {
      "epoch": 18.91549295774648,
      "grad_norm": 0.5626188516616821,
      "learning_rate": 0.00016218533051614851,
      "loss": 0.4006,
      "step": 9401
    },
    {
      "epoch": 18.917505030181086,
      "grad_norm": 0.5921252965927124,
      "learning_rate": 0.00016218130596639503,
      "loss": 0.3907,
      "step": 9402
    },
    {
      "epoch": 18.919517102615693,
      "grad_norm": 0.588236927986145,
      "learning_rate": 0.0001621772814166415,
      "loss": 0.3893,
      "step": 9403
    },
    {
      "epoch": 18.921529175050303,
      "grad_norm": 0.6193208694458008,
      "learning_rate": 0.00016217325686688802,
      "loss": 0.4141,
      "step": 9404
    },
    {
      "epoch": 18.92354124748491,
      "grad_norm": 0.6222044825553894,
      "learning_rate": 0.00016216923231713454,
      "loss": 0.4382,
      "step": 9405
    },
    {
      "epoch": 18.925553319919516,
      "grad_norm": 0.5885708928108215,
      "learning_rate": 0.00016216520776738102,
      "loss": 0.4281,
      "step": 9406
    },
    {
      "epoch": 18.927565392354126,
      "grad_norm": 0.5778523087501526,
      "learning_rate": 0.00016216118321762753,
      "loss": 0.3943,
      "step": 9407
    },
    {
      "epoch": 18.929577464788732,
      "grad_norm": 0.5704748034477234,
      "learning_rate": 0.00016215715866787405,
      "loss": 0.3803,
      "step": 9408
    },
    {
      "epoch": 18.93158953722334,
      "grad_norm": 0.5710411667823792,
      "learning_rate": 0.00016215313411812053,
      "loss": 0.4556,
      "step": 9409
    },
    {
      "epoch": 18.93360160965795,
      "grad_norm": 0.5579212307929993,
      "learning_rate": 0.00016214910956836704,
      "loss": 0.4143,
      "step": 9410
    },
    {
      "epoch": 18.935613682092555,
      "grad_norm": 0.5721811056137085,
      "learning_rate": 0.00016214508501861353,
      "loss": 0.4304,
      "step": 9411
    },
    {
      "epoch": 18.93762575452716,
      "grad_norm": 0.5898754000663757,
      "learning_rate": 0.00016214106046886007,
      "loss": 0.4197,
      "step": 9412
    },
    {
      "epoch": 18.93963782696177,
      "grad_norm": 0.5999670028686523,
      "learning_rate": 0.00016213703591910655,
      "loss": 0.4328,
      "step": 9413
    },
    {
      "epoch": 18.941649899396378,
      "grad_norm": 0.6060871481895447,
      "learning_rate": 0.00016213301136935306,
      "loss": 0.4065,
      "step": 9414
    },
    {
      "epoch": 18.943661971830984,
      "grad_norm": 0.5771962404251099,
      "learning_rate": 0.00016212898681959955,
      "loss": 0.3812,
      "step": 9415
    },
    {
      "epoch": 18.945674044265594,
      "grad_norm": 0.5657105445861816,
      "learning_rate": 0.00016212496226984606,
      "loss": 0.3907,
      "step": 9416
    },
    {
      "epoch": 18.9476861167002,
      "grad_norm": 0.6011835336685181,
      "learning_rate": 0.00016212093772009257,
      "loss": 0.4048,
      "step": 9417
    },
    {
      "epoch": 18.949698189134807,
      "grad_norm": 0.5774875283241272,
      "learning_rate": 0.00016211691317033909,
      "loss": 0.4298,
      "step": 9418
    },
    {
      "epoch": 18.951710261569417,
      "grad_norm": 0.5771370530128479,
      "learning_rate": 0.00016211288862058557,
      "loss": 0.4223,
      "step": 9419
    },
    {
      "epoch": 18.953722334004024,
      "grad_norm": 0.6175776124000549,
      "learning_rate": 0.00016210886407083208,
      "loss": 0.4285,
      "step": 9420
    },
    {
      "epoch": 18.955734406438633,
      "grad_norm": 0.6154356002807617,
      "learning_rate": 0.00016210483952107857,
      "loss": 0.4119,
      "step": 9421
    },
    {
      "epoch": 18.95774647887324,
      "grad_norm": 0.586681604385376,
      "learning_rate": 0.0001621008149713251,
      "loss": 0.4061,
      "step": 9422
    },
    {
      "epoch": 18.959758551307846,
      "grad_norm": 0.6104661226272583,
      "learning_rate": 0.0001620967904215716,
      "loss": 0.4037,
      "step": 9423
    },
    {
      "epoch": 18.961770623742456,
      "grad_norm": 0.5953109860420227,
      "learning_rate": 0.0001620927658718181,
      "loss": 0.4303,
      "step": 9424
    },
    {
      "epoch": 18.963782696177063,
      "grad_norm": 0.6069705486297607,
      "learning_rate": 0.0001620887413220646,
      "loss": 0.4165,
      "step": 9425
    },
    {
      "epoch": 18.96579476861167,
      "grad_norm": 0.5870117545127869,
      "learning_rate": 0.0001620847167723111,
      "loss": 0.4147,
      "step": 9426
    },
    {
      "epoch": 18.96780684104628,
      "grad_norm": 0.6078792214393616,
      "learning_rate": 0.00016208069222255762,
      "loss": 0.4311,
      "step": 9427
    },
    {
      "epoch": 18.969818913480886,
      "grad_norm": 0.5724340677261353,
      "learning_rate": 0.00016207666767280413,
      "loss": 0.4127,
      "step": 9428
    },
    {
      "epoch": 18.971830985915492,
      "grad_norm": 0.6097545027732849,
      "learning_rate": 0.0001620726431230506,
      "loss": 0.4158,
      "step": 9429
    },
    {
      "epoch": 18.973843058350102,
      "grad_norm": 0.5630708932876587,
      "learning_rate": 0.00016206861857329712,
      "loss": 0.4154,
      "step": 9430
    },
    {
      "epoch": 18.97585513078471,
      "grad_norm": 0.5944795608520508,
      "learning_rate": 0.0001620645940235436,
      "loss": 0.4468,
      "step": 9431
    },
    {
      "epoch": 18.977867203219315,
      "grad_norm": 0.5688939094543457,
      "learning_rate": 0.00016206056947379015,
      "loss": 0.4509,
      "step": 9432
    },
    {
      "epoch": 18.979879275653925,
      "grad_norm": 0.5763916969299316,
      "learning_rate": 0.00016205654492403663,
      "loss": 0.4196,
      "step": 9433
    },
    {
      "epoch": 18.98189134808853,
      "grad_norm": 0.590554416179657,
      "learning_rate": 0.00016205252037428315,
      "loss": 0.4218,
      "step": 9434
    },
    {
      "epoch": 18.983903420523138,
      "grad_norm": 0.6424907445907593,
      "learning_rate": 0.00016204849582452963,
      "loss": 0.4248,
      "step": 9435
    },
    {
      "epoch": 18.985915492957748,
      "grad_norm": 0.6003082394599915,
      "learning_rate": 0.00016204447127477614,
      "loss": 0.435,
      "step": 9436
    },
    {
      "epoch": 18.987927565392354,
      "grad_norm": 0.5845565795898438,
      "learning_rate": 0.00016204044672502266,
      "loss": 0.405,
      "step": 9437
    },
    {
      "epoch": 18.98993963782696,
      "grad_norm": 0.5661982893943787,
      "learning_rate": 0.00016203642217526914,
      "loss": 0.406,
      "step": 9438
    },
    {
      "epoch": 18.99195171026157,
      "grad_norm": 0.5793968439102173,
      "learning_rate": 0.00016203239762551565,
      "loss": 0.407,
      "step": 9439
    },
    {
      "epoch": 18.993963782696177,
      "grad_norm": 0.6034826636314392,
      "learning_rate": 0.00016202837307576217,
      "loss": 0.418,
      "step": 9440
    },
    {
      "epoch": 18.995975855130784,
      "grad_norm": 0.6309398412704468,
      "learning_rate": 0.00016202434852600865,
      "loss": 0.4164,
      "step": 9441
    },
    {
      "epoch": 18.997987927565394,
      "grad_norm": 0.6097513437271118,
      "learning_rate": 0.00016202032397625516,
      "loss": 0.4264,
      "step": 9442
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.5869156718254089,
      "learning_rate": 0.00016201629942650168,
      "loss": 0.4003,
      "step": 9443
    },
    {
      "epoch": 19.0,
      "eval_loss": 0.9350093007087708,
      "eval_runtime": 49.8061,
      "eval_samples_per_second": 19.917,
      "eval_steps_per_second": 2.49,
      "step": 9443
    },
    {
      "epoch": 19.002012072434606,
      "grad_norm": 0.5351329445838928,
      "learning_rate": 0.00016201227487674816,
      "loss": 0.3466,
      "step": 9444
    },
    {
      "epoch": 19.004024144869216,
      "grad_norm": 0.5242241024971008,
      "learning_rate": 0.00016200825032699467,
      "loss": 0.3536,
      "step": 9445
    },
    {
      "epoch": 19.006036217303823,
      "grad_norm": 0.4979827105998993,
      "learning_rate": 0.00016200422577724116,
      "loss": 0.3195,
      "step": 9446
    },
    {
      "epoch": 19.00804828973843,
      "grad_norm": 0.5850406885147095,
      "learning_rate": 0.0001620002012274877,
      "loss": 0.3459,
      "step": 9447
    },
    {
      "epoch": 19.01006036217304,
      "grad_norm": 0.6560065746307373,
      "learning_rate": 0.00016199617667773418,
      "loss": 0.3765,
      "step": 9448
    },
    {
      "epoch": 19.012072434607646,
      "grad_norm": 0.6069265007972717,
      "learning_rate": 0.0001619921521279807,
      "loss": 0.321,
      "step": 9449
    },
    {
      "epoch": 19.014084507042252,
      "grad_norm": 0.6250485181808472,
      "learning_rate": 0.00016198812757822718,
      "loss": 0.3673,
      "step": 9450
    },
    {
      "epoch": 19.016096579476862,
      "grad_norm": 0.5931373238563538,
      "learning_rate": 0.0001619841030284737,
      "loss": 0.3554,
      "step": 9451
    },
    {
      "epoch": 19.01810865191147,
      "grad_norm": 0.5421473383903503,
      "learning_rate": 0.0001619800784787202,
      "loss": 0.3516,
      "step": 9452
    },
    {
      "epoch": 19.020120724346075,
      "grad_norm": 0.5339416861534119,
      "learning_rate": 0.00016197605392896672,
      "loss": 0.3298,
      "step": 9453
    },
    {
      "epoch": 19.022132796780685,
      "grad_norm": 0.595792293548584,
      "learning_rate": 0.0001619720293792132,
      "loss": 0.3868,
      "step": 9454
    },
    {
      "epoch": 19.02414486921529,
      "grad_norm": 0.540290355682373,
      "learning_rate": 0.0001619680048294597,
      "loss": 0.3382,
      "step": 9455
    },
    {
      "epoch": 19.026156941649898,
      "grad_norm": 0.5371302366256714,
      "learning_rate": 0.0001619639802797062,
      "loss": 0.3552,
      "step": 9456
    },
    {
      "epoch": 19.028169014084508,
      "grad_norm": 0.5566381216049194,
      "learning_rate": 0.00016195995572995274,
      "loss": 0.3244,
      "step": 9457
    },
    {
      "epoch": 19.030181086519114,
      "grad_norm": 0.5735707879066467,
      "learning_rate": 0.00016195593118019922,
      "loss": 0.3461,
      "step": 9458
    },
    {
      "epoch": 19.03219315895372,
      "grad_norm": 0.6537361741065979,
      "learning_rate": 0.00016195190663044573,
      "loss": 0.355,
      "step": 9459
    },
    {
      "epoch": 19.03420523138833,
      "grad_norm": 0.5720341205596924,
      "learning_rate": 0.00016194788208069222,
      "loss": 0.3458,
      "step": 9460
    },
    {
      "epoch": 19.036217303822937,
      "grad_norm": 0.5461044311523438,
      "learning_rate": 0.00016194385753093873,
      "loss": 0.3442,
      "step": 9461
    },
    {
      "epoch": 19.038229376257544,
      "grad_norm": 0.5960284471511841,
      "learning_rate": 0.00016193983298118524,
      "loss": 0.3834,
      "step": 9462
    },
    {
      "epoch": 19.040241448692154,
      "grad_norm": 0.597919225692749,
      "learning_rate": 0.00016193580843143176,
      "loss": 0.3772,
      "step": 9463
    },
    {
      "epoch": 19.04225352112676,
      "grad_norm": 0.5718139410018921,
      "learning_rate": 0.00016193178388167824,
      "loss": 0.3686,
      "step": 9464
    },
    {
      "epoch": 19.044265593561367,
      "grad_norm": 0.5692356824874878,
      "learning_rate": 0.00016192775933192475,
      "loss": 0.3617,
      "step": 9465
    },
    {
      "epoch": 19.046277665995976,
      "grad_norm": 0.5909030437469482,
      "learning_rate": 0.00016192373478217124,
      "loss": 0.3406,
      "step": 9466
    },
    {
      "epoch": 19.048289738430583,
      "grad_norm": 0.5973435044288635,
      "learning_rate": 0.00016191971023241778,
      "loss": 0.4027,
      "step": 9467
    },
    {
      "epoch": 19.050301810865193,
      "grad_norm": 0.5873505473136902,
      "learning_rate": 0.00016191568568266426,
      "loss": 0.3188,
      "step": 9468
    },
    {
      "epoch": 19.0523138832998,
      "grad_norm": 0.5987678170204163,
      "learning_rate": 0.00016191166113291078,
      "loss": 0.3492,
      "step": 9469
    },
    {
      "epoch": 19.054325955734406,
      "grad_norm": 0.576042115688324,
      "learning_rate": 0.00016190763658315726,
      "loss": 0.3561,
      "step": 9470
    },
    {
      "epoch": 19.056338028169016,
      "grad_norm": 0.5652662515640259,
      "learning_rate": 0.00016190361203340377,
      "loss": 0.3426,
      "step": 9471
    },
    {
      "epoch": 19.058350100603622,
      "grad_norm": 0.5623165965080261,
      "learning_rate": 0.00016189958748365029,
      "loss": 0.3466,
      "step": 9472
    },
    {
      "epoch": 19.06036217303823,
      "grad_norm": 0.539936363697052,
      "learning_rate": 0.00016189556293389677,
      "loss": 0.333,
      "step": 9473
    },
    {
      "epoch": 19.06237424547284,
      "grad_norm": 0.5257034301757812,
      "learning_rate": 0.00016189153838414328,
      "loss": 0.319,
      "step": 9474
    },
    {
      "epoch": 19.064386317907445,
      "grad_norm": 0.5470592379570007,
      "learning_rate": 0.00016188751383438977,
      "loss": 0.3438,
      "step": 9475
    },
    {
      "epoch": 19.06639839034205,
      "grad_norm": 0.5856961011886597,
      "learning_rate": 0.00016188348928463628,
      "loss": 0.3455,
      "step": 9476
    },
    {
      "epoch": 19.06841046277666,
      "grad_norm": 0.5795720219612122,
      "learning_rate": 0.0001618794647348828,
      "loss": 0.3482,
      "step": 9477
    },
    {
      "epoch": 19.070422535211268,
      "grad_norm": 0.6038300395011902,
      "learning_rate": 0.0001618754401851293,
      "loss": 0.3923,
      "step": 9478
    },
    {
      "epoch": 19.072434607645874,
      "grad_norm": 0.5550521612167358,
      "learning_rate": 0.0001618714156353758,
      "loss": 0.3301,
      "step": 9479
    },
    {
      "epoch": 19.074446680080484,
      "grad_norm": 0.5930954217910767,
      "learning_rate": 0.0001618673910856223,
      "loss": 0.3631,
      "step": 9480
    },
    {
      "epoch": 19.07645875251509,
      "grad_norm": 0.5602627992630005,
      "learning_rate": 0.0001618633665358688,
      "loss": 0.3165,
      "step": 9481
    },
    {
      "epoch": 19.078470824949697,
      "grad_norm": 0.5618765354156494,
      "learning_rate": 0.00016185934198611533,
      "loss": 0.3195,
      "step": 9482
    },
    {
      "epoch": 19.080482897384307,
      "grad_norm": 0.6122277975082397,
      "learning_rate": 0.0001618553174363618,
      "loss": 0.3216,
      "step": 9483
    },
    {
      "epoch": 19.082494969818914,
      "grad_norm": 0.5920249819755554,
      "learning_rate": 0.00016185129288660832,
      "loss": 0.3892,
      "step": 9484
    },
    {
      "epoch": 19.08450704225352,
      "grad_norm": 0.5571092367172241,
      "learning_rate": 0.0001618472683368548,
      "loss": 0.3472,
      "step": 9485
    },
    {
      "epoch": 19.08651911468813,
      "grad_norm": 0.5565868616104126,
      "learning_rate": 0.00016184324378710132,
      "loss": 0.3176,
      "step": 9486
    },
    {
      "epoch": 19.088531187122737,
      "grad_norm": 0.6245245337486267,
      "learning_rate": 0.00016183921923734783,
      "loss": 0.3546,
      "step": 9487
    },
    {
      "epoch": 19.090543259557343,
      "grad_norm": 0.6031544208526611,
      "learning_rate": 0.00016183519468759435,
      "loss": 0.3345,
      "step": 9488
    },
    {
      "epoch": 19.092555331991953,
      "grad_norm": 0.5673683881759644,
      "learning_rate": 0.00016183117013784083,
      "loss": 0.3402,
      "step": 9489
    },
    {
      "epoch": 19.09456740442656,
      "grad_norm": 0.5475314855575562,
      "learning_rate": 0.00016182714558808734,
      "loss": 0.3737,
      "step": 9490
    },
    {
      "epoch": 19.096579476861166,
      "grad_norm": 0.592964231967926,
      "learning_rate": 0.00016182312103833383,
      "loss": 0.3378,
      "step": 9491
    },
    {
      "epoch": 19.098591549295776,
      "grad_norm": 0.5942293405532837,
      "learning_rate": 0.00016181909648858037,
      "loss": 0.3482,
      "step": 9492
    },
    {
      "epoch": 19.100603621730382,
      "grad_norm": 0.5523989200592041,
      "learning_rate": 0.00016181507193882685,
      "loss": 0.3219,
      "step": 9493
    },
    {
      "epoch": 19.10261569416499,
      "grad_norm": 0.5655272603034973,
      "learning_rate": 0.00016181104738907336,
      "loss": 0.3436,
      "step": 9494
    },
    {
      "epoch": 19.1046277665996,
      "grad_norm": 0.5905311703681946,
      "learning_rate": 0.00016180702283931985,
      "loss": 0.3391,
      "step": 9495
    },
    {
      "epoch": 19.106639839034205,
      "grad_norm": 0.5857401490211487,
      "learning_rate": 0.00016180299828956636,
      "loss": 0.3357,
      "step": 9496
    },
    {
      "epoch": 19.10865191146881,
      "grad_norm": 0.6002636551856995,
      "learning_rate": 0.00016179897373981287,
      "loss": 0.3323,
      "step": 9497
    },
    {
      "epoch": 19.11066398390342,
      "grad_norm": 0.607311487197876,
      "learning_rate": 0.00016179494919005939,
      "loss": 0.3436,
      "step": 9498
    },
    {
      "epoch": 19.112676056338028,
      "grad_norm": 0.5881986021995544,
      "learning_rate": 0.00016179092464030587,
      "loss": 0.3406,
      "step": 9499
    },
    {
      "epoch": 19.114688128772634,
      "grad_norm": 0.5760266780853271,
      "learning_rate": 0.00016178690009055238,
      "loss": 0.3639,
      "step": 9500
    },
    {
      "epoch": 19.116700201207244,
      "grad_norm": 0.5945664644241333,
      "learning_rate": 0.00016178287554079887,
      "loss": 0.3569,
      "step": 9501
    },
    {
      "epoch": 19.11871227364185,
      "grad_norm": 0.5686091780662537,
      "learning_rate": 0.00016177885099104538,
      "loss": 0.3534,
      "step": 9502
    },
    {
      "epoch": 19.120724346076457,
      "grad_norm": 0.5660408735275269,
      "learning_rate": 0.0001617748264412919,
      "loss": 0.3623,
      "step": 9503
    },
    {
      "epoch": 19.122736418511067,
      "grad_norm": 0.5808612704277039,
      "learning_rate": 0.0001617708018915384,
      "loss": 0.3606,
      "step": 9504
    },
    {
      "epoch": 19.124748490945674,
      "grad_norm": 0.5795732736587524,
      "learning_rate": 0.0001617667773417849,
      "loss": 0.3414,
      "step": 9505
    },
    {
      "epoch": 19.12676056338028,
      "grad_norm": 0.6434433460235596,
      "learning_rate": 0.0001617627527920314,
      "loss": 0.3992,
      "step": 9506
    },
    {
      "epoch": 19.12877263581489,
      "grad_norm": 0.5984839200973511,
      "learning_rate": 0.00016175872824227791,
      "loss": 0.3402,
      "step": 9507
    },
    {
      "epoch": 19.130784708249497,
      "grad_norm": 0.5882528424263,
      "learning_rate": 0.0001617547036925244,
      "loss": 0.3305,
      "step": 9508
    },
    {
      "epoch": 19.132796780684103,
      "grad_norm": 0.5915786027908325,
      "learning_rate": 0.0001617506791427709,
      "loss": 0.3405,
      "step": 9509
    },
    {
      "epoch": 19.134808853118713,
      "grad_norm": 0.6042912006378174,
      "learning_rate": 0.0001617466545930174,
      "loss": 0.3662,
      "step": 9510
    },
    {
      "epoch": 19.13682092555332,
      "grad_norm": 0.5687742829322815,
      "learning_rate": 0.0001617426300432639,
      "loss": 0.3317,
      "step": 9511
    },
    {
      "epoch": 19.138832997987926,
      "grad_norm": 0.6060497164726257,
      "learning_rate": 0.00016173860549351042,
      "loss": 0.3403,
      "step": 9512
    },
    {
      "epoch": 19.140845070422536,
      "grad_norm": 0.5885406136512756,
      "learning_rate": 0.00016173458094375693,
      "loss": 0.3547,
      "step": 9513
    },
    {
      "epoch": 19.142857142857142,
      "grad_norm": 0.5996478796005249,
      "learning_rate": 0.00016173055639400342,
      "loss": 0.3641,
      "step": 9514
    },
    {
      "epoch": 19.14486921529175,
      "grad_norm": 0.6074408888816833,
      "learning_rate": 0.00016172653184424993,
      "loss": 0.3424,
      "step": 9515
    },
    {
      "epoch": 19.14688128772636,
      "grad_norm": 0.5968513488769531,
      "learning_rate": 0.00016172250729449642,
      "loss": 0.3593,
      "step": 9516
    },
    {
      "epoch": 19.148893360160965,
      "grad_norm": 0.5656097531318665,
      "learning_rate": 0.00016171848274474296,
      "loss": 0.3663,
      "step": 9517
    },
    {
      "epoch": 19.15090543259557,
      "grad_norm": 0.60065096616745,
      "learning_rate": 0.00016171445819498944,
      "loss": 0.3464,
      "step": 9518
    },
    {
      "epoch": 19.15291750503018,
      "grad_norm": 0.6033589243888855,
      "learning_rate": 0.00016171043364523595,
      "loss": 0.3518,
      "step": 9519
    },
    {
      "epoch": 19.154929577464788,
      "grad_norm": 0.6139466166496277,
      "learning_rate": 0.00016170640909548244,
      "loss": 0.3999,
      "step": 9520
    },
    {
      "epoch": 19.156941649899398,
      "grad_norm": 0.5993718504905701,
      "learning_rate": 0.00016170238454572895,
      "loss": 0.3455,
      "step": 9521
    },
    {
      "epoch": 19.158953722334005,
      "grad_norm": 0.6549693942070007,
      "learning_rate": 0.00016169835999597546,
      "loss": 0.3984,
      "step": 9522
    },
    {
      "epoch": 19.16096579476861,
      "grad_norm": 0.59002286195755,
      "learning_rate": 0.00016169433544622197,
      "loss": 0.3643,
      "step": 9523
    },
    {
      "epoch": 19.16297786720322,
      "grad_norm": 0.6463244557380676,
      "learning_rate": 0.00016169031089646846,
      "loss": 0.3497,
      "step": 9524
    },
    {
      "epoch": 19.164989939637827,
      "grad_norm": 0.5879363417625427,
      "learning_rate": 0.00016168628634671497,
      "loss": 0.3862,
      "step": 9525
    },
    {
      "epoch": 19.167002012072434,
      "grad_norm": 0.5813133120536804,
      "learning_rate": 0.00016168226179696146,
      "loss": 0.3432,
      "step": 9526
    },
    {
      "epoch": 19.169014084507044,
      "grad_norm": 0.5971788763999939,
      "learning_rate": 0.000161678237247208,
      "loss": 0.3671,
      "step": 9527
    },
    {
      "epoch": 19.17102615694165,
      "grad_norm": 0.5611990690231323,
      "learning_rate": 0.00016167421269745448,
      "loss": 0.3583,
      "step": 9528
    },
    {
      "epoch": 19.173038229376257,
      "grad_norm": 0.5673062801361084,
      "learning_rate": 0.000161670188147701,
      "loss": 0.335,
      "step": 9529
    },
    {
      "epoch": 19.175050301810867,
      "grad_norm": 0.6125994324684143,
      "learning_rate": 0.00016166616359794748,
      "loss": 0.349,
      "step": 9530
    },
    {
      "epoch": 19.177062374245473,
      "grad_norm": 0.5723164081573486,
      "learning_rate": 0.000161662139048194,
      "loss": 0.3521,
      "step": 9531
    },
    {
      "epoch": 19.17907444668008,
      "grad_norm": 0.5799605846405029,
      "learning_rate": 0.0001616581144984405,
      "loss": 0.3586,
      "step": 9532
    },
    {
      "epoch": 19.18108651911469,
      "grad_norm": 0.6440849304199219,
      "learning_rate": 0.00016165408994868702,
      "loss": 0.3958,
      "step": 9533
    },
    {
      "epoch": 19.183098591549296,
      "grad_norm": 0.6150636076927185,
      "learning_rate": 0.0001616500653989335,
      "loss": 0.3714,
      "step": 9534
    },
    {
      "epoch": 19.185110663983902,
      "grad_norm": 0.5989243984222412,
      "learning_rate": 0.00016164604084918,
      "loss": 0.346,
      "step": 9535
    },
    {
      "epoch": 19.187122736418512,
      "grad_norm": 0.5972611904144287,
      "learning_rate": 0.0001616420162994265,
      "loss": 0.3674,
      "step": 9536
    },
    {
      "epoch": 19.18913480885312,
      "grad_norm": 0.6057702302932739,
      "learning_rate": 0.000161637991749673,
      "loss": 0.3626,
      "step": 9537
    },
    {
      "epoch": 19.191146881287725,
      "grad_norm": 0.6168180108070374,
      "learning_rate": 0.00016163396719991952,
      "loss": 0.3834,
      "step": 9538
    },
    {
      "epoch": 19.193158953722335,
      "grad_norm": 0.6091771125793457,
      "learning_rate": 0.00016162994265016603,
      "loss": 0.3699,
      "step": 9539
    },
    {
      "epoch": 19.19517102615694,
      "grad_norm": 0.5953874588012695,
      "learning_rate": 0.00016162591810041252,
      "loss": 0.3793,
      "step": 9540
    },
    {
      "epoch": 19.197183098591548,
      "grad_norm": 0.5814846754074097,
      "learning_rate": 0.00016162189355065903,
      "loss": 0.3523,
      "step": 9541
    },
    {
      "epoch": 19.199195171026158,
      "grad_norm": 0.5930284857749939,
      "learning_rate": 0.00016161786900090554,
      "loss": 0.3613,
      "step": 9542
    },
    {
      "epoch": 19.201207243460765,
      "grad_norm": 0.5949593782424927,
      "learning_rate": 0.00016161384445115203,
      "loss": 0.3305,
      "step": 9543
    },
    {
      "epoch": 19.20321931589537,
      "grad_norm": 0.5836253762245178,
      "learning_rate": 0.00016160981990139854,
      "loss": 0.3437,
      "step": 9544
    },
    {
      "epoch": 19.20523138832998,
      "grad_norm": 0.6007815003395081,
      "learning_rate": 0.00016160579535164503,
      "loss": 0.342,
      "step": 9545
    },
    {
      "epoch": 19.207243460764587,
      "grad_norm": 0.6056126952171326,
      "learning_rate": 0.00016160177080189154,
      "loss": 0.3799,
      "step": 9546
    },
    {
      "epoch": 19.209255533199194,
      "grad_norm": 0.5674320459365845,
      "learning_rate": 0.00016159774625213805,
      "loss": 0.3471,
      "step": 9547
    },
    {
      "epoch": 19.211267605633804,
      "grad_norm": 0.5897592306137085,
      "learning_rate": 0.00016159372170238456,
      "loss": 0.3404,
      "step": 9548
    },
    {
      "epoch": 19.21327967806841,
      "grad_norm": 0.6260926723480225,
      "learning_rate": 0.00016158969715263105,
      "loss": 0.3647,
      "step": 9549
    },
    {
      "epoch": 19.215291750503017,
      "grad_norm": 0.5905731916427612,
      "learning_rate": 0.00016158567260287756,
      "loss": 0.3499,
      "step": 9550
    },
    {
      "epoch": 19.217303822937627,
      "grad_norm": 0.6104515790939331,
      "learning_rate": 0.00016158164805312405,
      "loss": 0.353,
      "step": 9551
    },
    {
      "epoch": 19.219315895372233,
      "grad_norm": 0.6388885378837585,
      "learning_rate": 0.00016157762350337059,
      "loss": 0.372,
      "step": 9552
    },
    {
      "epoch": 19.22132796780684,
      "grad_norm": 0.594662070274353,
      "learning_rate": 0.00016157359895361707,
      "loss": 0.3356,
      "step": 9553
    },
    {
      "epoch": 19.22334004024145,
      "grad_norm": 0.5881706476211548,
      "learning_rate": 0.00016156957440386358,
      "loss": 0.3955,
      "step": 9554
    },
    {
      "epoch": 19.225352112676056,
      "grad_norm": 0.5664695501327515,
      "learning_rate": 0.00016156554985411007,
      "loss": 0.384,
      "step": 9555
    },
    {
      "epoch": 19.227364185110662,
      "grad_norm": 0.5528225302696228,
      "learning_rate": 0.00016156152530435658,
      "loss": 0.3491,
      "step": 9556
    },
    {
      "epoch": 19.229376257545272,
      "grad_norm": 0.5994189977645874,
      "learning_rate": 0.0001615575007546031,
      "loss": 0.3604,
      "step": 9557
    },
    {
      "epoch": 19.23138832997988,
      "grad_norm": 0.6198027729988098,
      "learning_rate": 0.0001615534762048496,
      "loss": 0.3799,
      "step": 9558
    },
    {
      "epoch": 19.233400402414485,
      "grad_norm": 0.5948337912559509,
      "learning_rate": 0.0001615494516550961,
      "loss": 0.344,
      "step": 9559
    },
    {
      "epoch": 19.235412474849095,
      "grad_norm": 0.608069121837616,
      "learning_rate": 0.0001615454271053426,
      "loss": 0.3721,
      "step": 9560
    },
    {
      "epoch": 19.2374245472837,
      "grad_norm": 0.6226170659065247,
      "learning_rate": 0.0001615414025555891,
      "loss": 0.3784,
      "step": 9561
    },
    {
      "epoch": 19.239436619718308,
      "grad_norm": 0.6008031368255615,
      "learning_rate": 0.00016153737800583563,
      "loss": 0.3763,
      "step": 9562
    },
    {
      "epoch": 19.241448692152918,
      "grad_norm": 0.5912685394287109,
      "learning_rate": 0.0001615333534560821,
      "loss": 0.3784,
      "step": 9563
    },
    {
      "epoch": 19.243460764587525,
      "grad_norm": 0.5601087808609009,
      "learning_rate": 0.00016152932890632862,
      "loss": 0.3425,
      "step": 9564
    },
    {
      "epoch": 19.24547283702213,
      "grad_norm": 0.6138153672218323,
      "learning_rate": 0.0001615253043565751,
      "loss": 0.3429,
      "step": 9565
    },
    {
      "epoch": 19.24748490945674,
      "grad_norm": 0.6060938239097595,
      "learning_rate": 0.00016152127980682162,
      "loss": 0.3635,
      "step": 9566
    },
    {
      "epoch": 19.249496981891348,
      "grad_norm": 0.5877524614334106,
      "learning_rate": 0.00016151725525706813,
      "loss": 0.3461,
      "step": 9567
    },
    {
      "epoch": 19.251509054325957,
      "grad_norm": 0.5911391973495483,
      "learning_rate": 0.00016151323070731465,
      "loss": 0.3685,
      "step": 9568
    },
    {
      "epoch": 19.253521126760564,
      "grad_norm": 0.6093594431877136,
      "learning_rate": 0.00016150920615756113,
      "loss": 0.3543,
      "step": 9569
    },
    {
      "epoch": 19.25553319919517,
      "grad_norm": 0.6253029108047485,
      "learning_rate": 0.00016150518160780764,
      "loss": 0.3676,
      "step": 9570
    },
    {
      "epoch": 19.25754527162978,
      "grad_norm": 0.5845372676849365,
      "learning_rate": 0.00016150115705805413,
      "loss": 0.3736,
      "step": 9571
    },
    {
      "epoch": 19.259557344064387,
      "grad_norm": 0.6223100423812866,
      "learning_rate": 0.00016149713250830064,
      "loss": 0.3532,
      "step": 9572
    },
    {
      "epoch": 19.261569416498993,
      "grad_norm": 0.5903262495994568,
      "learning_rate": 0.00016149310795854715,
      "loss": 0.3649,
      "step": 9573
    },
    {
      "epoch": 19.263581488933603,
      "grad_norm": 0.6003103256225586,
      "learning_rate": 0.00016148908340879366,
      "loss": 0.3987,
      "step": 9574
    },
    {
      "epoch": 19.26559356136821,
      "grad_norm": 0.6326061487197876,
      "learning_rate": 0.00016148505885904015,
      "loss": 0.3404,
      "step": 9575
    },
    {
      "epoch": 19.267605633802816,
      "grad_norm": 0.650904655456543,
      "learning_rate": 0.00016148103430928666,
      "loss": 0.3709,
      "step": 9576
    },
    {
      "epoch": 19.269617706237426,
      "grad_norm": 0.6202402710914612,
      "learning_rate": 0.00016147700975953317,
      "loss": 0.3547,
      "step": 9577
    },
    {
      "epoch": 19.271629778672033,
      "grad_norm": 0.6037269830703735,
      "learning_rate": 0.00016147298520977966,
      "loss": 0.3548,
      "step": 9578
    },
    {
      "epoch": 19.27364185110664,
      "grad_norm": 0.6315158009529114,
      "learning_rate": 0.00016146896066002617,
      "loss": 0.3839,
      "step": 9579
    },
    {
      "epoch": 19.27565392354125,
      "grad_norm": 0.5905126333236694,
      "learning_rate": 0.00016146493611027266,
      "loss": 0.3548,
      "step": 9580
    },
    {
      "epoch": 19.277665995975855,
      "grad_norm": 0.6271910071372986,
      "learning_rate": 0.00016146091156051917,
      "loss": 0.3984,
      "step": 9581
    },
    {
      "epoch": 19.279678068410462,
      "grad_norm": 0.5811467170715332,
      "learning_rate": 0.00016145688701076568,
      "loss": 0.3388,
      "step": 9582
    },
    {
      "epoch": 19.281690140845072,
      "grad_norm": 0.6002029776573181,
      "learning_rate": 0.0001614528624610122,
      "loss": 0.3934,
      "step": 9583
    },
    {
      "epoch": 19.28370221327968,
      "grad_norm": 0.6094833016395569,
      "learning_rate": 0.00016144883791125868,
      "loss": 0.3362,
      "step": 9584
    },
    {
      "epoch": 19.285714285714285,
      "grad_norm": 0.679962694644928,
      "learning_rate": 0.0001614448133615052,
      "loss": 0.3824,
      "step": 9585
    },
    {
      "epoch": 19.287726358148895,
      "grad_norm": 0.6218423247337341,
      "learning_rate": 0.00016144078881175168,
      "loss": 0.3802,
      "step": 9586
    },
    {
      "epoch": 19.2897384305835,
      "grad_norm": 0.6241465210914612,
      "learning_rate": 0.0001614367642619982,
      "loss": 0.3752,
      "step": 9587
    },
    {
      "epoch": 19.291750503018108,
      "grad_norm": 0.6439821124076843,
      "learning_rate": 0.0001614327397122447,
      "loss": 0.3736,
      "step": 9588
    },
    {
      "epoch": 19.293762575452718,
      "grad_norm": 0.6156650185585022,
      "learning_rate": 0.0001614287151624912,
      "loss": 0.3592,
      "step": 9589
    },
    {
      "epoch": 19.295774647887324,
      "grad_norm": 0.6141968965530396,
      "learning_rate": 0.0001614246906127377,
      "loss": 0.3577,
      "step": 9590
    },
    {
      "epoch": 19.29778672032193,
      "grad_norm": 0.6460944414138794,
      "learning_rate": 0.0001614206660629842,
      "loss": 0.3358,
      "step": 9591
    },
    {
      "epoch": 19.29979879275654,
      "grad_norm": 0.6206516027450562,
      "learning_rate": 0.0001614166415132307,
      "loss": 0.3813,
      "step": 9592
    },
    {
      "epoch": 19.301810865191147,
      "grad_norm": 0.6391536593437195,
      "learning_rate": 0.00016141261696347723,
      "loss": 0.339,
      "step": 9593
    },
    {
      "epoch": 19.303822937625753,
      "grad_norm": 0.6574810743331909,
      "learning_rate": 0.00016140859241372372,
      "loss": 0.3453,
      "step": 9594
    },
    {
      "epoch": 19.305835010060363,
      "grad_norm": 0.601816713809967,
      "learning_rate": 0.00016140456786397023,
      "loss": 0.3542,
      "step": 9595
    },
    {
      "epoch": 19.30784708249497,
      "grad_norm": 0.607257068157196,
      "learning_rate": 0.00016140054331421672,
      "loss": 0.3914,
      "step": 9596
    },
    {
      "epoch": 19.309859154929576,
      "grad_norm": 0.6461842060089111,
      "learning_rate": 0.00016139651876446323,
      "loss": 0.3788,
      "step": 9597
    },
    {
      "epoch": 19.311871227364186,
      "grad_norm": 0.5735903978347778,
      "learning_rate": 0.00016139249421470974,
      "loss": 0.3275,
      "step": 9598
    },
    {
      "epoch": 19.313883299798793,
      "grad_norm": 0.5988346934318542,
      "learning_rate": 0.00016138846966495625,
      "loss": 0.3599,
      "step": 9599
    },
    {
      "epoch": 19.3158953722334,
      "grad_norm": 0.6438978910446167,
      "learning_rate": 0.00016138444511520274,
      "loss": 0.3886,
      "step": 9600
    },
    {
      "epoch": 19.31790744466801,
      "grad_norm": 0.628776490688324,
      "learning_rate": 0.00016138042056544925,
      "loss": 0.3917,
      "step": 9601
    },
    {
      "epoch": 19.319919517102615,
      "grad_norm": 0.6555601954460144,
      "learning_rate": 0.00016137639601569574,
      "loss": 0.3511,
      "step": 9602
    },
    {
      "epoch": 19.321931589537222,
      "grad_norm": 0.6540068984031677,
      "learning_rate": 0.00016137237146594227,
      "loss": 0.3599,
      "step": 9603
    },
    {
      "epoch": 19.323943661971832,
      "grad_norm": 0.5984681248664856,
      "learning_rate": 0.00016136834691618876,
      "loss": 0.3499,
      "step": 9604
    },
    {
      "epoch": 19.32595573440644,
      "grad_norm": 0.6260390877723694,
      "learning_rate": 0.00016136432236643527,
      "loss": 0.3891,
      "step": 9605
    },
    {
      "epoch": 19.327967806841045,
      "grad_norm": 0.6007453799247742,
      "learning_rate": 0.00016136029781668176,
      "loss": 0.3721,
      "step": 9606
    },
    {
      "epoch": 19.329979879275655,
      "grad_norm": 0.6079867482185364,
      "learning_rate": 0.00016135627326692827,
      "loss": 0.3647,
      "step": 9607
    },
    {
      "epoch": 19.33199195171026,
      "grad_norm": 0.5903894901275635,
      "learning_rate": 0.00016135224871717478,
      "loss": 0.3844,
      "step": 9608
    },
    {
      "epoch": 19.334004024144868,
      "grad_norm": 0.5879485011100769,
      "learning_rate": 0.0001613482241674213,
      "loss": 0.3631,
      "step": 9609
    },
    {
      "epoch": 19.336016096579478,
      "grad_norm": 0.6493830680847168,
      "learning_rate": 0.00016134419961766778,
      "loss": 0.3815,
      "step": 9610
    },
    {
      "epoch": 19.338028169014084,
      "grad_norm": 0.6187055110931396,
      "learning_rate": 0.0001613401750679143,
      "loss": 0.3985,
      "step": 9611
    },
    {
      "epoch": 19.34004024144869,
      "grad_norm": 0.5805991888046265,
      "learning_rate": 0.00016133615051816078,
      "loss": 0.3468,
      "step": 9612
    },
    {
      "epoch": 19.3420523138833,
      "grad_norm": 0.6185300350189209,
      "learning_rate": 0.0001613321259684073,
      "loss": 0.3645,
      "step": 9613
    },
    {
      "epoch": 19.344064386317907,
      "grad_norm": 0.6049790382385254,
      "learning_rate": 0.0001613281014186538,
      "loss": 0.3935,
      "step": 9614
    },
    {
      "epoch": 19.346076458752513,
      "grad_norm": 0.6153202056884766,
      "learning_rate": 0.00016132407686890029,
      "loss": 0.371,
      "step": 9615
    },
    {
      "epoch": 19.348088531187123,
      "grad_norm": 0.612626850605011,
      "learning_rate": 0.0001613200523191468,
      "loss": 0.3601,
      "step": 9616
    },
    {
      "epoch": 19.35010060362173,
      "grad_norm": 0.6112525463104248,
      "learning_rate": 0.00016131602776939328,
      "loss": 0.3832,
      "step": 9617
    },
    {
      "epoch": 19.352112676056336,
      "grad_norm": 0.6299009919166565,
      "learning_rate": 0.00016131200321963982,
      "loss": 0.3948,
      "step": 9618
    },
    {
      "epoch": 19.354124748490946,
      "grad_norm": 0.6210620999336243,
      "learning_rate": 0.0001613079786698863,
      "loss": 0.3649,
      "step": 9619
    },
    {
      "epoch": 19.356136820925553,
      "grad_norm": 0.605051577091217,
      "learning_rate": 0.00016130395412013282,
      "loss": 0.3606,
      "step": 9620
    },
    {
      "epoch": 19.358148893360163,
      "grad_norm": 0.636077344417572,
      "learning_rate": 0.0001612999295703793,
      "loss": 0.3877,
      "step": 9621
    },
    {
      "epoch": 19.36016096579477,
      "grad_norm": 0.5852496027946472,
      "learning_rate": 0.00016129590502062582,
      "loss": 0.374,
      "step": 9622
    },
    {
      "epoch": 19.362173038229376,
      "grad_norm": 0.6296610832214355,
      "learning_rate": 0.00016129188047087233,
      "loss": 0.3755,
      "step": 9623
    },
    {
      "epoch": 19.364185110663986,
      "grad_norm": 0.5856506824493408,
      "learning_rate": 0.00016128785592111884,
      "loss": 0.373,
      "step": 9624
    },
    {
      "epoch": 19.366197183098592,
      "grad_norm": 0.6243460774421692,
      "learning_rate": 0.00016128383137136533,
      "loss": 0.3627,
      "step": 9625
    },
    {
      "epoch": 19.3682092555332,
      "grad_norm": 0.6372529864311218,
      "learning_rate": 0.00016127980682161184,
      "loss": 0.4116,
      "step": 9626
    },
    {
      "epoch": 19.37022132796781,
      "grad_norm": 0.6044037342071533,
      "learning_rate": 0.00016127578227185832,
      "loss": 0.3684,
      "step": 9627
    },
    {
      "epoch": 19.372233400402415,
      "grad_norm": 0.6124458312988281,
      "learning_rate": 0.00016127175772210486,
      "loss": 0.398,
      "step": 9628
    },
    {
      "epoch": 19.37424547283702,
      "grad_norm": 0.5903940796852112,
      "learning_rate": 0.00016126773317235135,
      "loss": 0.382,
      "step": 9629
    },
    {
      "epoch": 19.37625754527163,
      "grad_norm": 0.6328954696655273,
      "learning_rate": 0.00016126370862259786,
      "loss": 0.4076,
      "step": 9630
    },
    {
      "epoch": 19.378269617706238,
      "grad_norm": 0.5835891366004944,
      "learning_rate": 0.00016125968407284435,
      "loss": 0.363,
      "step": 9631
    },
    {
      "epoch": 19.380281690140844,
      "grad_norm": 0.6041941046714783,
      "learning_rate": 0.00016125565952309086,
      "loss": 0.3639,
      "step": 9632
    },
    {
      "epoch": 19.382293762575454,
      "grad_norm": 0.6131976842880249,
      "learning_rate": 0.00016125163497333737,
      "loss": 0.3788,
      "step": 9633
    },
    {
      "epoch": 19.38430583501006,
      "grad_norm": 0.5977177619934082,
      "learning_rate": 0.00016124761042358388,
      "loss": 0.3632,
      "step": 9634
    },
    {
      "epoch": 19.386317907444667,
      "grad_norm": 0.5936530828475952,
      "learning_rate": 0.00016124358587383037,
      "loss": 0.3564,
      "step": 9635
    },
    {
      "epoch": 19.388329979879277,
      "grad_norm": 0.60202956199646,
      "learning_rate": 0.00016123956132407688,
      "loss": 0.3527,
      "step": 9636
    },
    {
      "epoch": 19.390342052313883,
      "grad_norm": 0.6230529546737671,
      "learning_rate": 0.00016123553677432336,
      "loss": 0.39,
      "step": 9637
    },
    {
      "epoch": 19.39235412474849,
      "grad_norm": 0.6025057435035706,
      "learning_rate": 0.0001612315122245699,
      "loss": 0.3681,
      "step": 9638
    },
    {
      "epoch": 19.3943661971831,
      "grad_norm": 0.6022600531578064,
      "learning_rate": 0.0001612274876748164,
      "loss": 0.3724,
      "step": 9639
    },
    {
      "epoch": 19.396378269617706,
      "grad_norm": 0.6430566310882568,
      "learning_rate": 0.0001612234631250629,
      "loss": 0.3746,
      "step": 9640
    },
    {
      "epoch": 19.398390342052313,
      "grad_norm": 0.6034055352210999,
      "learning_rate": 0.00016121943857530939,
      "loss": 0.3389,
      "step": 9641
    },
    {
      "epoch": 19.400402414486923,
      "grad_norm": 0.6169795393943787,
      "learning_rate": 0.0001612154140255559,
      "loss": 0.3848,
      "step": 9642
    },
    {
      "epoch": 19.40241448692153,
      "grad_norm": 0.5868494510650635,
      "learning_rate": 0.0001612113894758024,
      "loss": 0.3823,
      "step": 9643
    },
    {
      "epoch": 19.404426559356136,
      "grad_norm": 0.6104922890663147,
      "learning_rate": 0.0001612073649260489,
      "loss": 0.3883,
      "step": 9644
    },
    {
      "epoch": 19.406438631790746,
      "grad_norm": 0.6061251163482666,
      "learning_rate": 0.0001612033403762954,
      "loss": 0.3736,
      "step": 9645
    },
    {
      "epoch": 19.408450704225352,
      "grad_norm": 0.6232894659042358,
      "learning_rate": 0.00016119931582654192,
      "loss": 0.3979,
      "step": 9646
    },
    {
      "epoch": 19.41046277665996,
      "grad_norm": 0.6312263607978821,
      "learning_rate": 0.0001611952912767884,
      "loss": 0.3672,
      "step": 9647
    },
    {
      "epoch": 19.41247484909457,
      "grad_norm": 0.6397218704223633,
      "learning_rate": 0.00016119126672703492,
      "loss": 0.3963,
      "step": 9648
    },
    {
      "epoch": 19.414486921529175,
      "grad_norm": 0.6517423987388611,
      "learning_rate": 0.00016118724217728143,
      "loss": 0.383,
      "step": 9649
    },
    {
      "epoch": 19.41649899396378,
      "grad_norm": 0.6698516011238098,
      "learning_rate": 0.00016118321762752791,
      "loss": 0.3759,
      "step": 9650
    },
    {
      "epoch": 19.41851106639839,
      "grad_norm": 0.5979880690574646,
      "learning_rate": 0.00016117919307777443,
      "loss": 0.3799,
      "step": 9651
    },
    {
      "epoch": 19.420523138832998,
      "grad_norm": 0.6486223340034485,
      "learning_rate": 0.0001611751685280209,
      "loss": 0.3791,
      "step": 9652
    },
    {
      "epoch": 19.422535211267604,
      "grad_norm": 0.6134116053581238,
      "learning_rate": 0.00016117114397826745,
      "loss": 0.3996,
      "step": 9653
    },
    {
      "epoch": 19.424547283702214,
      "grad_norm": 0.6211183071136475,
      "learning_rate": 0.00016116711942851394,
      "loss": 0.3965,
      "step": 9654
    },
    {
      "epoch": 19.42655935613682,
      "grad_norm": 0.6386448740959167,
      "learning_rate": 0.00016116309487876045,
      "loss": 0.4145,
      "step": 9655
    },
    {
      "epoch": 19.428571428571427,
      "grad_norm": 0.5827217698097229,
      "learning_rate": 0.00016115907032900693,
      "loss": 0.3615,
      "step": 9656
    },
    {
      "epoch": 19.430583501006037,
      "grad_norm": 0.6132209300994873,
      "learning_rate": 0.00016115504577925345,
      "loss": 0.3902,
      "step": 9657
    },
    {
      "epoch": 19.432595573440643,
      "grad_norm": 0.6117795705795288,
      "learning_rate": 0.00016115102122949996,
      "loss": 0.3802,
      "step": 9658
    },
    {
      "epoch": 19.43460764587525,
      "grad_norm": 0.6296144723892212,
      "learning_rate": 0.00016114699667974647,
      "loss": 0.3842,
      "step": 9659
    },
    {
      "epoch": 19.43661971830986,
      "grad_norm": 0.6310805678367615,
      "learning_rate": 0.00016114297212999296,
      "loss": 0.3894,
      "step": 9660
    },
    {
      "epoch": 19.438631790744466,
      "grad_norm": 0.5867171287536621,
      "learning_rate": 0.00016113894758023947,
      "loss": 0.3878,
      "step": 9661
    },
    {
      "epoch": 19.440643863179073,
      "grad_norm": 0.6204647421836853,
      "learning_rate": 0.00016113492303048595,
      "loss": 0.3783,
      "step": 9662
    },
    {
      "epoch": 19.442655935613683,
      "grad_norm": 0.5984715223312378,
      "learning_rate": 0.0001611308984807325,
      "loss": 0.3874,
      "step": 9663
    },
    {
      "epoch": 19.44466800804829,
      "grad_norm": 0.5916219353675842,
      "learning_rate": 0.00016112687393097898,
      "loss": 0.3799,
      "step": 9664
    },
    {
      "epoch": 19.446680080482896,
      "grad_norm": 0.6053093075752258,
      "learning_rate": 0.0001611228493812255,
      "loss": 0.3938,
      "step": 9665
    },
    {
      "epoch": 19.448692152917506,
      "grad_norm": 0.5997858643531799,
      "learning_rate": 0.00016111882483147197,
      "loss": 0.3815,
      "step": 9666
    },
    {
      "epoch": 19.450704225352112,
      "grad_norm": 0.5676199197769165,
      "learning_rate": 0.0001611148002817185,
      "loss": 0.3404,
      "step": 9667
    },
    {
      "epoch": 19.452716297786722,
      "grad_norm": 0.6710733771324158,
      "learning_rate": 0.000161110775731965,
      "loss": 0.4196,
      "step": 9668
    },
    {
      "epoch": 19.45472837022133,
      "grad_norm": 0.6121013164520264,
      "learning_rate": 0.0001611067511822115,
      "loss": 0.3959,
      "step": 9669
    },
    {
      "epoch": 19.456740442655935,
      "grad_norm": 0.5967275500297546,
      "learning_rate": 0.000161102726632458,
      "loss": 0.3567,
      "step": 9670
    },
    {
      "epoch": 19.458752515090545,
      "grad_norm": 0.6181759238243103,
      "learning_rate": 0.0001610987020827045,
      "loss": 0.3801,
      "step": 9671
    },
    {
      "epoch": 19.46076458752515,
      "grad_norm": 0.6113443374633789,
      "learning_rate": 0.000161094677532951,
      "loss": 0.3701,
      "step": 9672
    },
    {
      "epoch": 19.462776659959758,
      "grad_norm": 0.590690553188324,
      "learning_rate": 0.00016109065298319753,
      "loss": 0.3611,
      "step": 9673
    },
    {
      "epoch": 19.464788732394368,
      "grad_norm": 0.6256887316703796,
      "learning_rate": 0.00016108662843344402,
      "loss": 0.3747,
      "step": 9674
    },
    {
      "epoch": 19.466800804828974,
      "grad_norm": 0.5880828499794006,
      "learning_rate": 0.00016108260388369053,
      "loss": 0.3992,
      "step": 9675
    },
    {
      "epoch": 19.46881287726358,
      "grad_norm": 0.6362907290458679,
      "learning_rate": 0.00016107857933393702,
      "loss": 0.4085,
      "step": 9676
    },
    {
      "epoch": 19.47082494969819,
      "grad_norm": 0.6262091398239136,
      "learning_rate": 0.00016107455478418353,
      "loss": 0.3761,
      "step": 9677
    },
    {
      "epoch": 19.472837022132797,
      "grad_norm": 0.646375834941864,
      "learning_rate": 0.00016107053023443004,
      "loss": 0.4079,
      "step": 9678
    },
    {
      "epoch": 19.474849094567404,
      "grad_norm": 0.6220842599868774,
      "learning_rate": 0.00016106650568467653,
      "loss": 0.3992,
      "step": 9679
    },
    {
      "epoch": 19.476861167002014,
      "grad_norm": 0.6260296702384949,
      "learning_rate": 0.00016106248113492304,
      "loss": 0.3814,
      "step": 9680
    },
    {
      "epoch": 19.47887323943662,
      "grad_norm": 0.6316717267036438,
      "learning_rate": 0.00016105845658516955,
      "loss": 0.3918,
      "step": 9681
    },
    {
      "epoch": 19.480885311871226,
      "grad_norm": 0.6083411574363708,
      "learning_rate": 0.00016105443203541603,
      "loss": 0.3864,
      "step": 9682
    },
    {
      "epoch": 19.482897384305836,
      "grad_norm": 0.6291136741638184,
      "learning_rate": 0.00016105040748566255,
      "loss": 0.3897,
      "step": 9683
    },
    {
      "epoch": 19.484909456740443,
      "grad_norm": 0.6104207038879395,
      "learning_rate": 0.00016104638293590906,
      "loss": 0.3779,
      "step": 9684
    },
    {
      "epoch": 19.48692152917505,
      "grad_norm": 0.6354424953460693,
      "learning_rate": 0.00016104235838615554,
      "loss": 0.4083,
      "step": 9685
    },
    {
      "epoch": 19.48893360160966,
      "grad_norm": 0.6318783164024353,
      "learning_rate": 0.00016103833383640206,
      "loss": 0.3927,
      "step": 9686
    },
    {
      "epoch": 19.490945674044266,
      "grad_norm": 0.6083723902702332,
      "learning_rate": 0.00016103430928664854,
      "loss": 0.3568,
      "step": 9687
    },
    {
      "epoch": 19.492957746478872,
      "grad_norm": 0.6195005178451538,
      "learning_rate": 0.00016103028473689508,
      "loss": 0.3772,
      "step": 9688
    },
    {
      "epoch": 19.494969818913482,
      "grad_norm": 0.6289617419242859,
      "learning_rate": 0.00016102626018714157,
      "loss": 0.4056,
      "step": 9689
    },
    {
      "epoch": 19.49698189134809,
      "grad_norm": 0.6116771101951599,
      "learning_rate": 0.00016102223563738808,
      "loss": 0.3636,
      "step": 9690
    },
    {
      "epoch": 19.498993963782695,
      "grad_norm": 0.6092427372932434,
      "learning_rate": 0.00016101821108763456,
      "loss": 0.3977,
      "step": 9691
    },
    {
      "epoch": 19.501006036217305,
      "grad_norm": 0.6483413577079773,
      "learning_rate": 0.00016101418653788108,
      "loss": 0.4308,
      "step": 9692
    },
    {
      "epoch": 19.50301810865191,
      "grad_norm": 0.6126871109008789,
      "learning_rate": 0.0001610101619881276,
      "loss": 0.3891,
      "step": 9693
    },
    {
      "epoch": 19.505030181086518,
      "grad_norm": 0.5921927094459534,
      "learning_rate": 0.0001610061374383741,
      "loss": 0.3563,
      "step": 9694
    },
    {
      "epoch": 19.507042253521128,
      "grad_norm": 0.6256179809570312,
      "learning_rate": 0.00016100211288862059,
      "loss": 0.404,
      "step": 9695
    },
    {
      "epoch": 19.509054325955734,
      "grad_norm": 0.6076879501342773,
      "learning_rate": 0.0001609980883388671,
      "loss": 0.3892,
      "step": 9696
    },
    {
      "epoch": 19.51106639839034,
      "grad_norm": 0.6122047901153564,
      "learning_rate": 0.00016099406378911358,
      "loss": 0.3842,
      "step": 9697
    },
    {
      "epoch": 19.51307847082495,
      "grad_norm": 0.6394185423851013,
      "learning_rate": 0.00016099003923936012,
      "loss": 0.3964,
      "step": 9698
    },
    {
      "epoch": 19.515090543259557,
      "grad_norm": 0.6134519577026367,
      "learning_rate": 0.0001609860146896066,
      "loss": 0.3674,
      "step": 9699
    },
    {
      "epoch": 19.517102615694164,
      "grad_norm": 0.6690266728401184,
      "learning_rate": 0.00016098199013985312,
      "loss": 0.3934,
      "step": 9700
    },
    {
      "epoch": 19.519114688128774,
      "grad_norm": 0.6355472803115845,
      "learning_rate": 0.0001609779655900996,
      "loss": 0.408,
      "step": 9701
    },
    {
      "epoch": 19.52112676056338,
      "grad_norm": 0.6355864405632019,
      "learning_rate": 0.00016097394104034612,
      "loss": 0.3948,
      "step": 9702
    },
    {
      "epoch": 19.523138832997986,
      "grad_norm": 0.6398283243179321,
      "learning_rate": 0.00016096991649059263,
      "loss": 0.4078,
      "step": 9703
    },
    {
      "epoch": 19.525150905432596,
      "grad_norm": 0.6142268776893616,
      "learning_rate": 0.00016096589194083914,
      "loss": 0.3901,
      "step": 9704
    },
    {
      "epoch": 19.527162977867203,
      "grad_norm": 0.590145468711853,
      "learning_rate": 0.00016096186739108563,
      "loss": 0.3779,
      "step": 9705
    },
    {
      "epoch": 19.52917505030181,
      "grad_norm": 0.6419333815574646,
      "learning_rate": 0.00016095784284133214,
      "loss": 0.3957,
      "step": 9706
    },
    {
      "epoch": 19.53118712273642,
      "grad_norm": 0.6099156141281128,
      "learning_rate": 0.00016095381829157862,
      "loss": 0.3947,
      "step": 9707
    },
    {
      "epoch": 19.533199195171026,
      "grad_norm": 0.6143208742141724,
      "learning_rate": 0.00016094979374182516,
      "loss": 0.3826,
      "step": 9708
    },
    {
      "epoch": 19.535211267605632,
      "grad_norm": 0.6070312261581421,
      "learning_rate": 0.00016094576919207165,
      "loss": 0.3949,
      "step": 9709
    },
    {
      "epoch": 19.537223340040242,
      "grad_norm": 0.6129298210144043,
      "learning_rate": 0.00016094174464231816,
      "loss": 0.3935,
      "step": 9710
    },
    {
      "epoch": 19.53923541247485,
      "grad_norm": 0.6792940497398376,
      "learning_rate": 0.00016093772009256465,
      "loss": 0.3963,
      "step": 9711
    },
    {
      "epoch": 19.541247484909455,
      "grad_norm": 0.5883803367614746,
      "learning_rate": 0.00016093369554281116,
      "loss": 0.3582,
      "step": 9712
    },
    {
      "epoch": 19.543259557344065,
      "grad_norm": 0.6310952305793762,
      "learning_rate": 0.00016092967099305767,
      "loss": 0.396,
      "step": 9713
    },
    {
      "epoch": 19.54527162977867,
      "grad_norm": 0.6173484921455383,
      "learning_rate": 0.00016092564644330415,
      "loss": 0.3757,
      "step": 9714
    },
    {
      "epoch": 19.547283702213278,
      "grad_norm": 0.5943913459777832,
      "learning_rate": 0.00016092162189355067,
      "loss": 0.3702,
      "step": 9715
    },
    {
      "epoch": 19.549295774647888,
      "grad_norm": 0.5882556438446045,
      "learning_rate": 0.00016091759734379718,
      "loss": 0.3968,
      "step": 9716
    },
    {
      "epoch": 19.551307847082494,
      "grad_norm": 0.586495578289032,
      "learning_rate": 0.00016091357279404366,
      "loss": 0.3953,
      "step": 9717
    },
    {
      "epoch": 19.5533199195171,
      "grad_norm": 0.5956388115882874,
      "learning_rate": 0.00016090954824429018,
      "loss": 0.3518,
      "step": 9718
    },
    {
      "epoch": 19.55533199195171,
      "grad_norm": 0.5996805429458618,
      "learning_rate": 0.0001609055236945367,
      "loss": 0.3822,
      "step": 9719
    },
    {
      "epoch": 19.557344064386317,
      "grad_norm": 0.6461006999015808,
      "learning_rate": 0.00016090149914478317,
      "loss": 0.3911,
      "step": 9720
    },
    {
      "epoch": 19.559356136820927,
      "grad_norm": 0.598129153251648,
      "learning_rate": 0.00016089747459502969,
      "loss": 0.4076,
      "step": 9721
    },
    {
      "epoch": 19.561368209255534,
      "grad_norm": 0.6127679944038391,
      "learning_rate": 0.00016089345004527617,
      "loss": 0.3782,
      "step": 9722
    },
    {
      "epoch": 19.56338028169014,
      "grad_norm": 0.6166625022888184,
      "learning_rate": 0.0001608894254955227,
      "loss": 0.3894,
      "step": 9723
    },
    {
      "epoch": 19.56539235412475,
      "grad_norm": 0.6341310739517212,
      "learning_rate": 0.0001608854009457692,
      "loss": 0.3826,
      "step": 9724
    },
    {
      "epoch": 19.567404426559357,
      "grad_norm": 0.6206219792366028,
      "learning_rate": 0.0001608813763960157,
      "loss": 0.4036,
      "step": 9725
    },
    {
      "epoch": 19.569416498993963,
      "grad_norm": 0.5939571857452393,
      "learning_rate": 0.0001608773518462622,
      "loss": 0.3943,
      "step": 9726
    },
    {
      "epoch": 19.571428571428573,
      "grad_norm": 0.6476714611053467,
      "learning_rate": 0.0001608733272965087,
      "loss": 0.4078,
      "step": 9727
    },
    {
      "epoch": 19.57344064386318,
      "grad_norm": 0.5888431668281555,
      "learning_rate": 0.00016086930274675522,
      "loss": 0.3993,
      "step": 9728
    },
    {
      "epoch": 19.575452716297786,
      "grad_norm": 0.5947572588920593,
      "learning_rate": 0.00016086527819700173,
      "loss": 0.3791,
      "step": 9729
    },
    {
      "epoch": 19.577464788732396,
      "grad_norm": 0.5747206807136536,
      "learning_rate": 0.00016086125364724821,
      "loss": 0.3808,
      "step": 9730
    },
    {
      "epoch": 19.579476861167002,
      "grad_norm": 0.6333842873573303,
      "learning_rate": 0.00016085722909749473,
      "loss": 0.3905,
      "step": 9731
    },
    {
      "epoch": 19.58148893360161,
      "grad_norm": 0.629721462726593,
      "learning_rate": 0.0001608532045477412,
      "loss": 0.3747,
      "step": 9732
    },
    {
      "epoch": 19.58350100603622,
      "grad_norm": 0.609032392501831,
      "learning_rate": 0.00016084917999798775,
      "loss": 0.377,
      "step": 9733
    },
    {
      "epoch": 19.585513078470825,
      "grad_norm": 0.642394483089447,
      "learning_rate": 0.00016084515544823424,
      "loss": 0.3895,
      "step": 9734
    },
    {
      "epoch": 19.58752515090543,
      "grad_norm": 0.6377471685409546,
      "learning_rate": 0.00016084113089848075,
      "loss": 0.4014,
      "step": 9735
    },
    {
      "epoch": 19.58953722334004,
      "grad_norm": 0.6146484613418579,
      "learning_rate": 0.00016083710634872723,
      "loss": 0.3943,
      "step": 9736
    },
    {
      "epoch": 19.591549295774648,
      "grad_norm": 0.6204086542129517,
      "learning_rate": 0.00016083308179897375,
      "loss": 0.4176,
      "step": 9737
    },
    {
      "epoch": 19.593561368209254,
      "grad_norm": 0.6181548833847046,
      "learning_rate": 0.00016082905724922026,
      "loss": 0.3643,
      "step": 9738
    },
    {
      "epoch": 19.595573440643864,
      "grad_norm": 0.6295543909072876,
      "learning_rate": 0.00016082503269946677,
      "loss": 0.3698,
      "step": 9739
    },
    {
      "epoch": 19.59758551307847,
      "grad_norm": 0.6262315511703491,
      "learning_rate": 0.00016082100814971326,
      "loss": 0.3896,
      "step": 9740
    },
    {
      "epoch": 19.599597585513077,
      "grad_norm": 0.605821967124939,
      "learning_rate": 0.00016081698359995977,
      "loss": 0.3704,
      "step": 9741
    },
    {
      "epoch": 19.601609657947687,
      "grad_norm": 0.63700270652771,
      "learning_rate": 0.00016081295905020625,
      "loss": 0.4117,
      "step": 9742
    },
    {
      "epoch": 19.603621730382294,
      "grad_norm": 0.5834521651268005,
      "learning_rate": 0.0001608089345004528,
      "loss": 0.3774,
      "step": 9743
    },
    {
      "epoch": 19.6056338028169,
      "grad_norm": 0.5863724946975708,
      "learning_rate": 0.00016080490995069928,
      "loss": 0.3835,
      "step": 9744
    },
    {
      "epoch": 19.60764587525151,
      "grad_norm": 0.608551025390625,
      "learning_rate": 0.0001608008854009458,
      "loss": 0.3703,
      "step": 9745
    },
    {
      "epoch": 19.609657947686117,
      "grad_norm": 0.611417829990387,
      "learning_rate": 0.00016079686085119227,
      "loss": 0.3937,
      "step": 9746
    },
    {
      "epoch": 19.611670020120723,
      "grad_norm": 0.6321586966514587,
      "learning_rate": 0.0001607928363014388,
      "loss": 0.4031,
      "step": 9747
    },
    {
      "epoch": 19.613682092555333,
      "grad_norm": 0.646386981010437,
      "learning_rate": 0.0001607888117516853,
      "loss": 0.3925,
      "step": 9748
    },
    {
      "epoch": 19.61569416498994,
      "grad_norm": 0.6133596301078796,
      "learning_rate": 0.00016078478720193178,
      "loss": 0.4018,
      "step": 9749
    },
    {
      "epoch": 19.617706237424546,
      "grad_norm": 0.6348004341125488,
      "learning_rate": 0.0001607807626521783,
      "loss": 0.3945,
      "step": 9750
    },
    {
      "epoch": 19.619718309859156,
      "grad_norm": 0.613166332244873,
      "learning_rate": 0.0001607767381024248,
      "loss": 0.3927,
      "step": 9751
    },
    {
      "epoch": 19.621730382293762,
      "grad_norm": 0.6327924132347107,
      "learning_rate": 0.0001607727135526713,
      "loss": 0.3863,
      "step": 9752
    },
    {
      "epoch": 19.62374245472837,
      "grad_norm": 0.6355540156364441,
      "learning_rate": 0.0001607686890029178,
      "loss": 0.413,
      "step": 9753
    },
    {
      "epoch": 19.62575452716298,
      "grad_norm": 0.5910919308662415,
      "learning_rate": 0.00016076466445316432,
      "loss": 0.385,
      "step": 9754
    },
    {
      "epoch": 19.627766599597585,
      "grad_norm": 0.6038983464241028,
      "learning_rate": 0.0001607606399034108,
      "loss": 0.3807,
      "step": 9755
    },
    {
      "epoch": 19.62977867203219,
      "grad_norm": 0.6342186331748962,
      "learning_rate": 0.00016075661535365732,
      "loss": 0.4045,
      "step": 9756
    },
    {
      "epoch": 19.6317907444668,
      "grad_norm": 0.6274504065513611,
      "learning_rate": 0.0001607525908039038,
      "loss": 0.3521,
      "step": 9757
    },
    {
      "epoch": 19.633802816901408,
      "grad_norm": 0.6377506852149963,
      "learning_rate": 0.00016074856625415034,
      "loss": 0.3884,
      "step": 9758
    },
    {
      "epoch": 19.635814889336014,
      "grad_norm": 0.6219619512557983,
      "learning_rate": 0.00016074454170439683,
      "loss": 0.3938,
      "step": 9759
    },
    {
      "epoch": 19.637826961770624,
      "grad_norm": 0.6117458939552307,
      "learning_rate": 0.00016074051715464334,
      "loss": 0.3762,
      "step": 9760
    },
    {
      "epoch": 19.63983903420523,
      "grad_norm": 0.6496665477752686,
      "learning_rate": 0.00016073649260488982,
      "loss": 0.392,
      "step": 9761
    },
    {
      "epoch": 19.641851106639837,
      "grad_norm": 0.6432372331619263,
      "learning_rate": 0.00016073246805513633,
      "loss": 0.4042,
      "step": 9762
    },
    {
      "epoch": 19.643863179074447,
      "grad_norm": 0.603333592414856,
      "learning_rate": 0.00016072844350538285,
      "loss": 0.3681,
      "step": 9763
    },
    {
      "epoch": 19.645875251509054,
      "grad_norm": 0.6115491986274719,
      "learning_rate": 0.00016072441895562936,
      "loss": 0.3948,
      "step": 9764
    },
    {
      "epoch": 19.647887323943664,
      "grad_norm": 0.6699473261833191,
      "learning_rate": 0.00016072039440587584,
      "loss": 0.3942,
      "step": 9765
    },
    {
      "epoch": 19.64989939637827,
      "grad_norm": 0.6005640625953674,
      "learning_rate": 0.00016071636985612236,
      "loss": 0.3778,
      "step": 9766
    },
    {
      "epoch": 19.651911468812877,
      "grad_norm": 0.6037852168083191,
      "learning_rate": 0.00016071234530636884,
      "loss": 0.3892,
      "step": 9767
    },
    {
      "epoch": 19.653923541247487,
      "grad_norm": 0.6170168519020081,
      "learning_rate": 0.00016070832075661538,
      "loss": 0.3864,
      "step": 9768
    },
    {
      "epoch": 19.655935613682093,
      "grad_norm": 0.6295775175094604,
      "learning_rate": 0.00016070429620686187,
      "loss": 0.4232,
      "step": 9769
    },
    {
      "epoch": 19.6579476861167,
      "grad_norm": 0.5837637782096863,
      "learning_rate": 0.00016070027165710838,
      "loss": 0.3692,
      "step": 9770
    },
    {
      "epoch": 19.65995975855131,
      "grad_norm": 0.58616042137146,
      "learning_rate": 0.00016069624710735486,
      "loss": 0.3848,
      "step": 9771
    },
    {
      "epoch": 19.661971830985916,
      "grad_norm": 0.6244552731513977,
      "learning_rate": 0.00016069222255760138,
      "loss": 0.4,
      "step": 9772
    },
    {
      "epoch": 19.663983903420522,
      "grad_norm": 0.6901564598083496,
      "learning_rate": 0.0001606881980078479,
      "loss": 0.4131,
      "step": 9773
    },
    {
      "epoch": 19.665995975855132,
      "grad_norm": 0.6156535148620605,
      "learning_rate": 0.0001606841734580944,
      "loss": 0.4102,
      "step": 9774
    },
    {
      "epoch": 19.66800804828974,
      "grad_norm": 0.6465331315994263,
      "learning_rate": 0.00016068014890834089,
      "loss": 0.3852,
      "step": 9775
    },
    {
      "epoch": 19.670020120724345,
      "grad_norm": 0.6447793245315552,
      "learning_rate": 0.0001606761243585874,
      "loss": 0.3574,
      "step": 9776
    },
    {
      "epoch": 19.672032193158955,
      "grad_norm": 0.6336570382118225,
      "learning_rate": 0.00016067209980883388,
      "loss": 0.4059,
      "step": 9777
    },
    {
      "epoch": 19.67404426559356,
      "grad_norm": 0.646923840045929,
      "learning_rate": 0.00016066807525908042,
      "loss": 0.3883,
      "step": 9778
    },
    {
      "epoch": 19.676056338028168,
      "grad_norm": 0.5791881084442139,
      "learning_rate": 0.0001606640507093269,
      "loss": 0.3915,
      "step": 9779
    },
    {
      "epoch": 19.678068410462778,
      "grad_norm": 0.6286138296127319,
      "learning_rate": 0.00016066002615957342,
      "loss": 0.3944,
      "step": 9780
    },
    {
      "epoch": 19.680080482897385,
      "grad_norm": 0.618157684803009,
      "learning_rate": 0.0001606560016098199,
      "loss": 0.3912,
      "step": 9781
    },
    {
      "epoch": 19.68209255533199,
      "grad_norm": 0.6737782955169678,
      "learning_rate": 0.00016065197706006642,
      "loss": 0.4058,
      "step": 9782
    },
    {
      "epoch": 19.6841046277666,
      "grad_norm": 0.6296402812004089,
      "learning_rate": 0.00016064795251031293,
      "loss": 0.4161,
      "step": 9783
    },
    {
      "epoch": 19.686116700201207,
      "grad_norm": 0.6295286417007446,
      "learning_rate": 0.00016064392796055941,
      "loss": 0.3556,
      "step": 9784
    },
    {
      "epoch": 19.688128772635814,
      "grad_norm": 0.6276440620422363,
      "learning_rate": 0.00016063990341080593,
      "loss": 0.4279,
      "step": 9785
    },
    {
      "epoch": 19.690140845070424,
      "grad_norm": 0.5935528874397278,
      "learning_rate": 0.00016063587886105244,
      "loss": 0.3867,
      "step": 9786
    },
    {
      "epoch": 19.69215291750503,
      "grad_norm": 0.6006163358688354,
      "learning_rate": 0.00016063185431129892,
      "loss": 0.4041,
      "step": 9787
    },
    {
      "epoch": 19.694164989939637,
      "grad_norm": 0.5982703566551208,
      "learning_rate": 0.00016062782976154544,
      "loss": 0.3927,
      "step": 9788
    },
    {
      "epoch": 19.696177062374247,
      "grad_norm": 0.6066840291023254,
      "learning_rate": 0.00016062380521179195,
      "loss": 0.4249,
      "step": 9789
    },
    {
      "epoch": 19.698189134808853,
      "grad_norm": 0.588076651096344,
      "learning_rate": 0.00016061978066203843,
      "loss": 0.3868,
      "step": 9790
    },
    {
      "epoch": 19.70020120724346,
      "grad_norm": 0.6291435956954956,
      "learning_rate": 0.00016061575611228494,
      "loss": 0.3698,
      "step": 9791
    },
    {
      "epoch": 19.70221327967807,
      "grad_norm": 0.610761284828186,
      "learning_rate": 0.00016061173156253143,
      "loss": 0.3881,
      "step": 9792
    },
    {
      "epoch": 19.704225352112676,
      "grad_norm": 0.6157933473587036,
      "learning_rate": 0.00016060770701277797,
      "loss": 0.4138,
      "step": 9793
    },
    {
      "epoch": 19.706237424547282,
      "grad_norm": 0.5844764709472656,
      "learning_rate": 0.00016060368246302445,
      "loss": 0.3735,
      "step": 9794
    },
    {
      "epoch": 19.708249496981892,
      "grad_norm": 0.6572527289390564,
      "learning_rate": 0.00016059965791327097,
      "loss": 0.4288,
      "step": 9795
    },
    {
      "epoch": 19.7102615694165,
      "grad_norm": 0.622523307800293,
      "learning_rate": 0.00016059563336351745,
      "loss": 0.389,
      "step": 9796
    },
    {
      "epoch": 19.712273641851105,
      "grad_norm": 0.6156442165374756,
      "learning_rate": 0.00016059160881376396,
      "loss": 0.3831,
      "step": 9797
    },
    {
      "epoch": 19.714285714285715,
      "grad_norm": 0.6375553011894226,
      "learning_rate": 0.00016058758426401048,
      "loss": 0.3787,
      "step": 9798
    },
    {
      "epoch": 19.71629778672032,
      "grad_norm": 0.6562865376472473,
      "learning_rate": 0.000160583559714257,
      "loss": 0.3789,
      "step": 9799
    },
    {
      "epoch": 19.718309859154928,
      "grad_norm": 0.6009577512741089,
      "learning_rate": 0.00016057953516450347,
      "loss": 0.4076,
      "step": 9800
    },
    {
      "epoch": 19.720321931589538,
      "grad_norm": 0.614372193813324,
      "learning_rate": 0.00016057551061474999,
      "loss": 0.3992,
      "step": 9801
    },
    {
      "epoch": 19.722334004024145,
      "grad_norm": 0.621286928653717,
      "learning_rate": 0.00016057148606499647,
      "loss": 0.3924,
      "step": 9802
    },
    {
      "epoch": 19.72434607645875,
      "grad_norm": 0.6172139644622803,
      "learning_rate": 0.000160567461515243,
      "loss": 0.376,
      "step": 9803
    },
    {
      "epoch": 19.72635814889336,
      "grad_norm": 0.6275620460510254,
      "learning_rate": 0.0001605634369654895,
      "loss": 0.4014,
      "step": 9804
    },
    {
      "epoch": 19.728370221327967,
      "grad_norm": 0.6177468299865723,
      "learning_rate": 0.000160559412415736,
      "loss": 0.4015,
      "step": 9805
    },
    {
      "epoch": 19.730382293762574,
      "grad_norm": 0.6194390058517456,
      "learning_rate": 0.0001605553878659825,
      "loss": 0.3991,
      "step": 9806
    },
    {
      "epoch": 19.732394366197184,
      "grad_norm": 0.6089140176773071,
      "learning_rate": 0.000160551363316229,
      "loss": 0.3974,
      "step": 9807
    },
    {
      "epoch": 19.73440643863179,
      "grad_norm": 0.5947270393371582,
      "learning_rate": 0.00016054733876647552,
      "loss": 0.383,
      "step": 9808
    },
    {
      "epoch": 19.736418511066397,
      "grad_norm": 0.5878908634185791,
      "learning_rate": 0.00016054331421672203,
      "loss": 0.383,
      "step": 9809
    },
    {
      "epoch": 19.738430583501007,
      "grad_norm": 0.6422356367111206,
      "learning_rate": 0.00016053928966696851,
      "loss": 0.4109,
      "step": 9810
    },
    {
      "epoch": 19.740442655935613,
      "grad_norm": 0.6037762761116028,
      "learning_rate": 0.00016053526511721503,
      "loss": 0.3962,
      "step": 9811
    },
    {
      "epoch": 19.74245472837022,
      "grad_norm": 0.6062354445457458,
      "learning_rate": 0.0001605312405674615,
      "loss": 0.3645,
      "step": 9812
    },
    {
      "epoch": 19.74446680080483,
      "grad_norm": 0.6312394142150879,
      "learning_rate": 0.00016052721601770802,
      "loss": 0.3808,
      "step": 9813
    },
    {
      "epoch": 19.746478873239436,
      "grad_norm": 0.6184208989143372,
      "learning_rate": 0.00016052319146795454,
      "loss": 0.3971,
      "step": 9814
    },
    {
      "epoch": 19.748490945674043,
      "grad_norm": 0.6314585208892822,
      "learning_rate": 0.00016051916691820105,
      "loss": 0.3777,
      "step": 9815
    },
    {
      "epoch": 19.750503018108652,
      "grad_norm": 0.603980541229248,
      "learning_rate": 0.00016051514236844753,
      "loss": 0.3953,
      "step": 9816
    },
    {
      "epoch": 19.75251509054326,
      "grad_norm": 0.5955771803855896,
      "learning_rate": 0.00016051111781869405,
      "loss": 0.3716,
      "step": 9817
    },
    {
      "epoch": 19.754527162977865,
      "grad_norm": 0.5926977396011353,
      "learning_rate": 0.00016050709326894056,
      "loss": 0.3808,
      "step": 9818
    },
    {
      "epoch": 19.756539235412475,
      "grad_norm": 0.6393460035324097,
      "learning_rate": 0.00016050306871918704,
      "loss": 0.3775,
      "step": 9819
    },
    {
      "epoch": 19.758551307847082,
      "grad_norm": 0.637179434299469,
      "learning_rate": 0.00016049904416943356,
      "loss": 0.3934,
      "step": 9820
    },
    {
      "epoch": 19.760563380281692,
      "grad_norm": 0.5984997153282166,
      "learning_rate": 0.00016049501961968004,
      "loss": 0.3794,
      "step": 9821
    },
    {
      "epoch": 19.7625754527163,
      "grad_norm": 0.6625850796699524,
      "learning_rate": 0.00016049099506992655,
      "loss": 0.4118,
      "step": 9822
    },
    {
      "epoch": 19.764587525150905,
      "grad_norm": 0.6383673548698425,
      "learning_rate": 0.00016048697052017306,
      "loss": 0.4013,
      "step": 9823
    },
    {
      "epoch": 19.766599597585515,
      "grad_norm": 0.6356176733970642,
      "learning_rate": 0.00016048294597041958,
      "loss": 0.4145,
      "step": 9824
    },
    {
      "epoch": 19.76861167002012,
      "grad_norm": 0.6434711813926697,
      "learning_rate": 0.00016047892142066606,
      "loss": 0.3591,
      "step": 9825
    },
    {
      "epoch": 19.770623742454728,
      "grad_norm": 0.6166480183601379,
      "learning_rate": 0.00016047489687091257,
      "loss": 0.3816,
      "step": 9826
    },
    {
      "epoch": 19.772635814889338,
      "grad_norm": 0.638789176940918,
      "learning_rate": 0.00016047087232115906,
      "loss": 0.4071,
      "step": 9827
    },
    {
      "epoch": 19.774647887323944,
      "grad_norm": 0.6252416968345642,
      "learning_rate": 0.00016046684777140557,
      "loss": 0.3853,
      "step": 9828
    },
    {
      "epoch": 19.77665995975855,
      "grad_norm": 0.5992819666862488,
      "learning_rate": 0.00016046282322165208,
      "loss": 0.3994,
      "step": 9829
    },
    {
      "epoch": 19.77867203219316,
      "grad_norm": 0.6069707870483398,
      "learning_rate": 0.0001604587986718986,
      "loss": 0.4086,
      "step": 9830
    },
    {
      "epoch": 19.780684104627767,
      "grad_norm": 0.5843513011932373,
      "learning_rate": 0.00016045477412214508,
      "loss": 0.3676,
      "step": 9831
    },
    {
      "epoch": 19.782696177062373,
      "grad_norm": 0.5899802446365356,
      "learning_rate": 0.0001604507495723916,
      "loss": 0.3553,
      "step": 9832
    },
    {
      "epoch": 19.784708249496983,
      "grad_norm": 0.63255774974823,
      "learning_rate": 0.00016044672502263808,
      "loss": 0.3794,
      "step": 9833
    },
    {
      "epoch": 19.78672032193159,
      "grad_norm": 0.6237971782684326,
      "learning_rate": 0.00016044270047288462,
      "loss": 0.3764,
      "step": 9834
    },
    {
      "epoch": 19.788732394366196,
      "grad_norm": 0.6026938557624817,
      "learning_rate": 0.0001604386759231311,
      "loss": 0.3798,
      "step": 9835
    },
    {
      "epoch": 19.790744466800806,
      "grad_norm": 0.6270806193351746,
      "learning_rate": 0.00016043465137337762,
      "loss": 0.3957,
      "step": 9836
    },
    {
      "epoch": 19.792756539235413,
      "grad_norm": 0.621950626373291,
      "learning_rate": 0.0001604306268236241,
      "loss": 0.4035,
      "step": 9837
    },
    {
      "epoch": 19.79476861167002,
      "grad_norm": 0.6320857405662537,
      "learning_rate": 0.0001604266022738706,
      "loss": 0.4051,
      "step": 9838
    },
    {
      "epoch": 19.79678068410463,
      "grad_norm": 0.6358267664909363,
      "learning_rate": 0.00016042257772411712,
      "loss": 0.3709,
      "step": 9839
    },
    {
      "epoch": 19.798792756539235,
      "grad_norm": 0.619723916053772,
      "learning_rate": 0.00016041855317436364,
      "loss": 0.378,
      "step": 9840
    },
    {
      "epoch": 19.800804828973842,
      "grad_norm": 0.623852014541626,
      "learning_rate": 0.00016041452862461012,
      "loss": 0.4126,
      "step": 9841
    },
    {
      "epoch": 19.802816901408452,
      "grad_norm": 0.604036808013916,
      "learning_rate": 0.00016041050407485663,
      "loss": 0.3857,
      "step": 9842
    },
    {
      "epoch": 19.80482897384306,
      "grad_norm": 0.5882650017738342,
      "learning_rate": 0.00016040647952510312,
      "loss": 0.3924,
      "step": 9843
    },
    {
      "epoch": 19.806841046277665,
      "grad_norm": 0.5985305905342102,
      "learning_rate": 0.00016040245497534966,
      "loss": 0.3686,
      "step": 9844
    },
    {
      "epoch": 19.808853118712275,
      "grad_norm": 0.5944402813911438,
      "learning_rate": 0.00016039843042559614,
      "loss": 0.3892,
      "step": 9845
    },
    {
      "epoch": 19.81086519114688,
      "grad_norm": 0.6342511773109436,
      "learning_rate": 0.00016039440587584266,
      "loss": 0.4075,
      "step": 9846
    },
    {
      "epoch": 19.812877263581488,
      "grad_norm": 0.6011137962341309,
      "learning_rate": 0.00016039038132608914,
      "loss": 0.3946,
      "step": 9847
    },
    {
      "epoch": 19.814889336016098,
      "grad_norm": 0.6316730380058289,
      "learning_rate": 0.00016038635677633565,
      "loss": 0.4122,
      "step": 9848
    },
    {
      "epoch": 19.816901408450704,
      "grad_norm": 0.6127215027809143,
      "learning_rate": 0.00016038233222658217,
      "loss": 0.4041,
      "step": 9849
    },
    {
      "epoch": 19.81891348088531,
      "grad_norm": 0.6071065068244934,
      "learning_rate": 0.00016037830767682868,
      "loss": 0.3816,
      "step": 9850
    },
    {
      "epoch": 19.82092555331992,
      "grad_norm": 0.6369394659996033,
      "learning_rate": 0.00016037428312707516,
      "loss": 0.4052,
      "step": 9851
    },
    {
      "epoch": 19.822937625754527,
      "grad_norm": 0.6275830864906311,
      "learning_rate": 0.00016037025857732168,
      "loss": 0.4082,
      "step": 9852
    },
    {
      "epoch": 19.824949698189133,
      "grad_norm": 0.5948636531829834,
      "learning_rate": 0.00016036623402756816,
      "loss": 0.3743,
      "step": 9853
    },
    {
      "epoch": 19.826961770623743,
      "grad_norm": 0.5826905369758606,
      "learning_rate": 0.00016036220947781467,
      "loss": 0.3788,
      "step": 9854
    },
    {
      "epoch": 19.82897384305835,
      "grad_norm": 0.6222944855690002,
      "learning_rate": 0.00016035818492806118,
      "loss": 0.4118,
      "step": 9855
    },
    {
      "epoch": 19.830985915492956,
      "grad_norm": 0.6008985042572021,
      "learning_rate": 0.00016035416037830767,
      "loss": 0.3821,
      "step": 9856
    },
    {
      "epoch": 19.832997987927566,
      "grad_norm": 0.6186251044273376,
      "learning_rate": 0.00016035013582855418,
      "loss": 0.3926,
      "step": 9857
    },
    {
      "epoch": 19.835010060362173,
      "grad_norm": 0.6183859705924988,
      "learning_rate": 0.0001603461112788007,
      "loss": 0.3824,
      "step": 9858
    },
    {
      "epoch": 19.83702213279678,
      "grad_norm": 0.6388624906539917,
      "learning_rate": 0.0001603420867290472,
      "loss": 0.3661,
      "step": 9859
    },
    {
      "epoch": 19.83903420523139,
      "grad_norm": 0.6362793445587158,
      "learning_rate": 0.0001603380621792937,
      "loss": 0.4101,
      "step": 9860
    },
    {
      "epoch": 19.841046277665995,
      "grad_norm": 0.659403383731842,
      "learning_rate": 0.0001603340376295402,
      "loss": 0.4283,
      "step": 9861
    },
    {
      "epoch": 19.843058350100602,
      "grad_norm": 0.6101717948913574,
      "learning_rate": 0.0001603300130797867,
      "loss": 0.3886,
      "step": 9862
    },
    {
      "epoch": 19.845070422535212,
      "grad_norm": 0.643545389175415,
      "learning_rate": 0.0001603259885300332,
      "loss": 0.3885,
      "step": 9863
    },
    {
      "epoch": 19.84708249496982,
      "grad_norm": 0.6189062595367432,
      "learning_rate": 0.0001603219639802797,
      "loss": 0.3853,
      "step": 9864
    },
    {
      "epoch": 19.84909456740443,
      "grad_norm": 0.6426402926445007,
      "learning_rate": 0.00016031793943052623,
      "loss": 0.3846,
      "step": 9865
    },
    {
      "epoch": 19.851106639839035,
      "grad_norm": 0.6308242082595825,
      "learning_rate": 0.0001603139148807727,
      "loss": 0.4038,
      "step": 9866
    },
    {
      "epoch": 19.85311871227364,
      "grad_norm": 0.6390024423599243,
      "learning_rate": 0.00016030989033101922,
      "loss": 0.41,
      "step": 9867
    },
    {
      "epoch": 19.85513078470825,
      "grad_norm": 0.6037446856498718,
      "learning_rate": 0.0001603058657812657,
      "loss": 0.4005,
      "step": 9868
    },
    {
      "epoch": 19.857142857142858,
      "grad_norm": 0.6576488018035889,
      "learning_rate": 0.00016030184123151225,
      "loss": 0.3884,
      "step": 9869
    },
    {
      "epoch": 19.859154929577464,
      "grad_norm": 0.6000384092330933,
      "learning_rate": 0.00016029781668175873,
      "loss": 0.3654,
      "step": 9870
    },
    {
      "epoch": 19.861167002012074,
      "grad_norm": 0.5939432382583618,
      "learning_rate": 0.00016029379213200524,
      "loss": 0.4056,
      "step": 9871
    },
    {
      "epoch": 19.86317907444668,
      "grad_norm": 0.6060811281204224,
      "learning_rate": 0.00016028976758225173,
      "loss": 0.3918,
      "step": 9872
    },
    {
      "epoch": 19.865191146881287,
      "grad_norm": 0.625694215297699,
      "learning_rate": 0.00016028574303249824,
      "loss": 0.4047,
      "step": 9873
    },
    {
      "epoch": 19.867203219315897,
      "grad_norm": 0.5955747961997986,
      "learning_rate": 0.00016028171848274475,
      "loss": 0.4,
      "step": 9874
    },
    {
      "epoch": 19.869215291750503,
      "grad_norm": 0.6018070578575134,
      "learning_rate": 0.00016027769393299127,
      "loss": 0.3856,
      "step": 9875
    },
    {
      "epoch": 19.87122736418511,
      "grad_norm": 0.637403666973114,
      "learning_rate": 0.00016027366938323775,
      "loss": 0.4007,
      "step": 9876
    },
    {
      "epoch": 19.87323943661972,
      "grad_norm": 0.6199771761894226,
      "learning_rate": 0.00016026964483348426,
      "loss": 0.399,
      "step": 9877
    },
    {
      "epoch": 19.875251509054326,
      "grad_norm": 0.6289218664169312,
      "learning_rate": 0.00016026562028373075,
      "loss": 0.3984,
      "step": 9878
    },
    {
      "epoch": 19.877263581488933,
      "grad_norm": 0.6044004559516907,
      "learning_rate": 0.0001602615957339773,
      "loss": 0.4092,
      "step": 9879
    },
    {
      "epoch": 19.879275653923543,
      "grad_norm": 0.6291543841362,
      "learning_rate": 0.00016025757118422377,
      "loss": 0.4008,
      "step": 9880
    },
    {
      "epoch": 19.88128772635815,
      "grad_norm": 0.626125693321228,
      "learning_rate": 0.00016025354663447029,
      "loss": 0.4073,
      "step": 9881
    },
    {
      "epoch": 19.883299798792756,
      "grad_norm": 0.6234421133995056,
      "learning_rate": 0.00016024952208471677,
      "loss": 0.4231,
      "step": 9882
    },
    {
      "epoch": 19.885311871227366,
      "grad_norm": 0.591363251209259,
      "learning_rate": 0.00016024549753496328,
      "loss": 0.4118,
      "step": 9883
    },
    {
      "epoch": 19.887323943661972,
      "grad_norm": 0.5695511102676392,
      "learning_rate": 0.0001602414729852098,
      "loss": 0.392,
      "step": 9884
    },
    {
      "epoch": 19.88933601609658,
      "grad_norm": 0.6116238832473755,
      "learning_rate": 0.0001602374484354563,
      "loss": 0.4074,
      "step": 9885
    },
    {
      "epoch": 19.89134808853119,
      "grad_norm": 0.6539974212646484,
      "learning_rate": 0.0001602334238857028,
      "loss": 0.3649,
      "step": 9886
    },
    {
      "epoch": 19.893360160965795,
      "grad_norm": 0.6124972105026245,
      "learning_rate": 0.0001602293993359493,
      "loss": 0.397,
      "step": 9887
    },
    {
      "epoch": 19.8953722334004,
      "grad_norm": 0.5914614796638489,
      "learning_rate": 0.0001602253747861958,
      "loss": 0.3894,
      "step": 9888
    },
    {
      "epoch": 19.89738430583501,
      "grad_norm": 0.6180393099784851,
      "learning_rate": 0.0001602213502364423,
      "loss": 0.408,
      "step": 9889
    },
    {
      "epoch": 19.899396378269618,
      "grad_norm": 0.6184770464897156,
      "learning_rate": 0.00016021732568668881,
      "loss": 0.393,
      "step": 9890
    },
    {
      "epoch": 19.901408450704224,
      "grad_norm": 0.5924871563911438,
      "learning_rate": 0.0001602133011369353,
      "loss": 0.3548,
      "step": 9891
    },
    {
      "epoch": 19.903420523138834,
      "grad_norm": 0.6121363043785095,
      "learning_rate": 0.0001602092765871818,
      "loss": 0.4232,
      "step": 9892
    },
    {
      "epoch": 19.90543259557344,
      "grad_norm": 0.6102245450019836,
      "learning_rate": 0.00016020525203742832,
      "loss": 0.4185,
      "step": 9893
    },
    {
      "epoch": 19.907444668008047,
      "grad_norm": 0.6204831600189209,
      "learning_rate": 0.00016020122748767484,
      "loss": 0.3974,
      "step": 9894
    },
    {
      "epoch": 19.909456740442657,
      "grad_norm": 0.605472981929779,
      "learning_rate": 0.00016019720293792132,
      "loss": 0.4255,
      "step": 9895
    },
    {
      "epoch": 19.911468812877263,
      "grad_norm": 0.6068896651268005,
      "learning_rate": 0.00016019317838816783,
      "loss": 0.4171,
      "step": 9896
    },
    {
      "epoch": 19.91348088531187,
      "grad_norm": 0.608593761920929,
      "learning_rate": 0.00016018915383841432,
      "loss": 0.383,
      "step": 9897
    },
    {
      "epoch": 19.91549295774648,
      "grad_norm": 0.6044619083404541,
      "learning_rate": 0.00016018512928866083,
      "loss": 0.387,
      "step": 9898
    },
    {
      "epoch": 19.917505030181086,
      "grad_norm": 0.5982322096824646,
      "learning_rate": 0.00016018110473890734,
      "loss": 0.3891,
      "step": 9899
    },
    {
      "epoch": 19.919517102615693,
      "grad_norm": 0.6608003973960876,
      "learning_rate": 0.00016017708018915386,
      "loss": 0.4187,
      "step": 9900
    },
    {
      "epoch": 19.921529175050303,
      "grad_norm": 0.6368961930274963,
      "learning_rate": 0.00016017305563940034,
      "loss": 0.409,
      "step": 9901
    },
    {
      "epoch": 19.92354124748491,
      "grad_norm": 0.6313184499740601,
      "learning_rate": 0.00016016903108964685,
      "loss": 0.4367,
      "step": 9902
    },
    {
      "epoch": 19.925553319919516,
      "grad_norm": 0.6539599299430847,
      "learning_rate": 0.00016016500653989334,
      "loss": 0.4099,
      "step": 9903
    },
    {
      "epoch": 19.927565392354126,
      "grad_norm": 0.5821061134338379,
      "learning_rate": 0.00016016098199013988,
      "loss": 0.3913,
      "step": 9904
    },
    {
      "epoch": 19.929577464788732,
      "grad_norm": 0.6183323860168457,
      "learning_rate": 0.00016015695744038636,
      "loss": 0.3777,
      "step": 9905
    },
    {
      "epoch": 19.93158953722334,
      "grad_norm": 0.5890645980834961,
      "learning_rate": 0.00016015293289063287,
      "loss": 0.4055,
      "step": 9906
    },
    {
      "epoch": 19.93360160965795,
      "grad_norm": 0.6035996079444885,
      "learning_rate": 0.00016014890834087936,
      "loss": 0.3932,
      "step": 9907
    },
    {
      "epoch": 19.935613682092555,
      "grad_norm": 0.6448595523834229,
      "learning_rate": 0.00016014488379112587,
      "loss": 0.3984,
      "step": 9908
    },
    {
      "epoch": 19.93762575452716,
      "grad_norm": 0.6094782948493958,
      "learning_rate": 0.00016014085924137238,
      "loss": 0.4192,
      "step": 9909
    },
    {
      "epoch": 19.93963782696177,
      "grad_norm": 0.5874561071395874,
      "learning_rate": 0.0001601368346916189,
      "loss": 0.4057,
      "step": 9910
    },
    {
      "epoch": 19.941649899396378,
      "grad_norm": 0.6173020005226135,
      "learning_rate": 0.00016013281014186538,
      "loss": 0.4004,
      "step": 9911
    },
    {
      "epoch": 19.943661971830984,
      "grad_norm": 0.6334295272827148,
      "learning_rate": 0.0001601287855921119,
      "loss": 0.4074,
      "step": 9912
    },
    {
      "epoch": 19.945674044265594,
      "grad_norm": 0.6126084327697754,
      "learning_rate": 0.00016012476104235838,
      "loss": 0.3807,
      "step": 9913
    },
    {
      "epoch": 19.9476861167002,
      "grad_norm": 0.5680297017097473,
      "learning_rate": 0.00016012073649260492,
      "loss": 0.3845,
      "step": 9914
    },
    {
      "epoch": 19.949698189134807,
      "grad_norm": 0.5867983102798462,
      "learning_rate": 0.0001601167119428514,
      "loss": 0.3751,
      "step": 9915
    },
    {
      "epoch": 19.951710261569417,
      "grad_norm": 0.6295360922813416,
      "learning_rate": 0.00016011268739309791,
      "loss": 0.3919,
      "step": 9916
    },
    {
      "epoch": 19.953722334004024,
      "grad_norm": 0.5935953259468079,
      "learning_rate": 0.0001601086628433444,
      "loss": 0.3741,
      "step": 9917
    },
    {
      "epoch": 19.955734406438633,
      "grad_norm": 0.6059830188751221,
      "learning_rate": 0.0001601046382935909,
      "loss": 0.3906,
      "step": 9918
    },
    {
      "epoch": 19.95774647887324,
      "grad_norm": 0.6146640777587891,
      "learning_rate": 0.00016010061374383742,
      "loss": 0.4233,
      "step": 9919
    },
    {
      "epoch": 19.959758551307846,
      "grad_norm": 0.5981711745262146,
      "learning_rate": 0.00016009658919408394,
      "loss": 0.4112,
      "step": 9920
    },
    {
      "epoch": 19.961770623742456,
      "grad_norm": 0.6068710088729858,
      "learning_rate": 0.00016009256464433042,
      "loss": 0.3795,
      "step": 9921
    },
    {
      "epoch": 19.963782696177063,
      "grad_norm": 0.6344112753868103,
      "learning_rate": 0.00016008854009457693,
      "loss": 0.4094,
      "step": 9922
    },
    {
      "epoch": 19.96579476861167,
      "grad_norm": 0.5920925140380859,
      "learning_rate": 0.00016008451554482342,
      "loss": 0.3985,
      "step": 9923
    },
    {
      "epoch": 19.96780684104628,
      "grad_norm": 0.6258984804153442,
      "learning_rate": 0.00016008049099506993,
      "loss": 0.43,
      "step": 9924
    },
    {
      "epoch": 19.969818913480886,
      "grad_norm": 0.6666433811187744,
      "learning_rate": 0.00016007646644531644,
      "loss": 0.4345,
      "step": 9925
    },
    {
      "epoch": 19.971830985915492,
      "grad_norm": 0.6182475090026855,
      "learning_rate": 0.00016007244189556293,
      "loss": 0.3847,
      "step": 9926
    },
    {
      "epoch": 19.973843058350102,
      "grad_norm": 0.5855392217636108,
      "learning_rate": 0.00016006841734580944,
      "loss": 0.3867,
      "step": 9927
    },
    {
      "epoch": 19.97585513078471,
      "grad_norm": 0.6162840723991394,
      "learning_rate": 0.00016006439279605595,
      "loss": 0.3602,
      "step": 9928
    },
    {
      "epoch": 19.977867203219315,
      "grad_norm": 0.5945459604263306,
      "learning_rate": 0.00016006036824630247,
      "loss": 0.3936,
      "step": 9929
    },
    {
      "epoch": 19.979879275653925,
      "grad_norm": 0.6082882881164551,
      "learning_rate": 0.00016005634369654895,
      "loss": 0.4105,
      "step": 9930
    },
    {
      "epoch": 19.98189134808853,
      "grad_norm": 0.6109197735786438,
      "learning_rate": 0.00016005231914679546,
      "loss": 0.4182,
      "step": 9931
    },
    {
      "epoch": 19.983903420523138,
      "grad_norm": 0.6861446499824524,
      "learning_rate": 0.00016004829459704195,
      "loss": 0.3751,
      "step": 9932
    },
    {
      "epoch": 19.985915492957748,
      "grad_norm": 0.6206985712051392,
      "learning_rate": 0.00016004427004728846,
      "loss": 0.3812,
      "step": 9933
    },
    {
      "epoch": 19.987927565392354,
      "grad_norm": 0.6149314641952515,
      "learning_rate": 0.00016004024549753497,
      "loss": 0.3984,
      "step": 9934
    },
    {
      "epoch": 19.98993963782696,
      "grad_norm": 0.5906439423561096,
      "learning_rate": 0.00016003622094778148,
      "loss": 0.4072,
      "step": 9935
    },
    {
      "epoch": 19.99195171026157,
      "grad_norm": 0.6377672553062439,
      "learning_rate": 0.00016003219639802797,
      "loss": 0.4334,
      "step": 9936
    },
    {
      "epoch": 19.993963782696177,
      "grad_norm": 0.6311076283454895,
      "learning_rate": 0.00016002817184827448,
      "loss": 0.4171,
      "step": 9937
    },
    {
      "epoch": 19.995975855130784,
      "grad_norm": 0.6171601414680481,
      "learning_rate": 0.00016002414729852097,
      "loss": 0.3927,
      "step": 9938
    },
    {
      "epoch": 19.997987927565394,
      "grad_norm": 0.6006421446800232,
      "learning_rate": 0.0001600201227487675,
      "loss": 0.4108,
      "step": 9939
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.586426317691803,
      "learning_rate": 0.000160016098199014,
      "loss": 0.3716,
      "step": 9940
    },
    {
      "epoch": 20.0,
      "eval_loss": 0.961184024810791,
      "eval_runtime": 49.8225,
      "eval_samples_per_second": 19.911,
      "eval_steps_per_second": 2.489,
      "step": 9940
    },
    {
      "epoch": 20.002012072434606,
      "grad_norm": 0.5467376112937927,
      "learning_rate": 0.0001600120736492605,
      "loss": 0.3243,
      "step": 9941
    },
    {
      "epoch": 20.004024144869216,
      "grad_norm": 0.5253647565841675,
      "learning_rate": 0.000160008049099507,
      "loss": 0.3231,
      "step": 9942
    },
    {
      "epoch": 20.006036217303823,
      "grad_norm": 0.5335626006126404,
      "learning_rate": 0.0001600040245497535,
      "loss": 0.3208,
      "step": 9943
    },
    {
      "epoch": 20.00804828973843,
      "grad_norm": 0.599514365196228,
      "learning_rate": 0.00016,
      "loss": 0.3307,
      "step": 9944
    },
    {
      "epoch": 20.01006036217304,
      "grad_norm": 0.6171444654464722,
      "learning_rate": 0.00015999597545024653,
      "loss": 0.3337,
      "step": 9945
    },
    {
      "epoch": 20.012072434607646,
      "grad_norm": 0.6762419939041138,
      "learning_rate": 0.000159991950900493,
      "loss": 0.3596,
      "step": 9946
    },
    {
      "epoch": 20.014084507042252,
      "grad_norm": 0.6389375925064087,
      "learning_rate": 0.00015998792635073952,
      "loss": 0.3236,
      "step": 9947
    },
    {
      "epoch": 20.016096579476862,
      "grad_norm": 0.572647213935852,
      "learning_rate": 0.000159983901800986,
      "loss": 0.3238,
      "step": 9948
    },
    {
      "epoch": 20.01810865191147,
      "grad_norm": 0.6073293089866638,
      "learning_rate": 0.00015997987725123255,
      "loss": 0.3338,
      "step": 9949
    },
    {
      "epoch": 20.020120724346075,
      "grad_norm": 0.6036871075630188,
      "learning_rate": 0.00015997585270147903,
      "loss": 0.3578,
      "step": 9950
    },
    {
      "epoch": 20.022132796780685,
      "grad_norm": 0.5637190937995911,
      "learning_rate": 0.00015997182815172554,
      "loss": 0.344,
      "step": 9951
    },
    {
      "epoch": 20.02414486921529,
      "grad_norm": 0.5597584843635559,
      "learning_rate": 0.00015996780360197203,
      "loss": 0.3322,
      "step": 9952
    },
    {
      "epoch": 20.026156941649898,
      "grad_norm": 0.5909454822540283,
      "learning_rate": 0.00015996377905221854,
      "loss": 0.3204,
      "step": 9953
    },
    {
      "epoch": 20.028169014084508,
      "grad_norm": 0.608071506023407,
      "learning_rate": 0.00015995975450246505,
      "loss": 0.3614,
      "step": 9954
    },
    {
      "epoch": 20.030181086519114,
      "grad_norm": 0.6061410307884216,
      "learning_rate": 0.00015995572995271157,
      "loss": 0.3276,
      "step": 9955
    },
    {
      "epoch": 20.03219315895372,
      "grad_norm": 0.6614086031913757,
      "learning_rate": 0.00015995170540295805,
      "loss": 0.3016,
      "step": 9956
    },
    {
      "epoch": 20.03420523138833,
      "grad_norm": 0.5854612588882446,
      "learning_rate": 0.00015994768085320456,
      "loss": 0.3481,
      "step": 9957
    },
    {
      "epoch": 20.036217303822937,
      "grad_norm": 0.5912983417510986,
      "learning_rate": 0.00015994365630345105,
      "loss": 0.3214,
      "step": 9958
    },
    {
      "epoch": 20.038229376257544,
      "grad_norm": 0.5793889760971069,
      "learning_rate": 0.00015993963175369756,
      "loss": 0.3188,
      "step": 9959
    },
    {
      "epoch": 20.040241448692154,
      "grad_norm": 0.5736851692199707,
      "learning_rate": 0.00015993560720394407,
      "loss": 0.3174,
      "step": 9960
    },
    {
      "epoch": 20.04225352112676,
      "grad_norm": 0.5822368860244751,
      "learning_rate": 0.00015993158265419056,
      "loss": 0.3182,
      "step": 9961
    },
    {
      "epoch": 20.044265593561367,
      "grad_norm": 0.5661136507987976,
      "learning_rate": 0.00015992755810443707,
      "loss": 0.3661,
      "step": 9962
    },
    {
      "epoch": 20.046277665995976,
      "grad_norm": 0.5764719843864441,
      "learning_rate": 0.00015992353355468356,
      "loss": 0.3057,
      "step": 9963
    },
    {
      "epoch": 20.048289738430583,
      "grad_norm": 0.6073579788208008,
      "learning_rate": 0.0001599195090049301,
      "loss": 0.3443,
      "step": 9964
    },
    {
      "epoch": 20.050301810865193,
      "grad_norm": 0.6767641305923462,
      "learning_rate": 0.00015991548445517658,
      "loss": 0.3345,
      "step": 9965
    },
    {
      "epoch": 20.0523138832998,
      "grad_norm": 0.6315709352493286,
      "learning_rate": 0.0001599114599054231,
      "loss": 0.3632,
      "step": 9966
    },
    {
      "epoch": 20.054325955734406,
      "grad_norm": 0.5915910005569458,
      "learning_rate": 0.00015990743535566958,
      "loss": 0.3354,
      "step": 9967
    },
    {
      "epoch": 20.056338028169016,
      "grad_norm": 0.5880117416381836,
      "learning_rate": 0.0001599034108059161,
      "loss": 0.3388,
      "step": 9968
    },
    {
      "epoch": 20.058350100603622,
      "grad_norm": 0.6072680950164795,
      "learning_rate": 0.0001598993862561626,
      "loss": 0.3263,
      "step": 9969
    },
    {
      "epoch": 20.06036217303823,
      "grad_norm": 0.6013838648796082,
      "learning_rate": 0.00015989536170640911,
      "loss": 0.3526,
      "step": 9970
    },
    {
      "epoch": 20.06237424547284,
      "grad_norm": 0.5986178517341614,
      "learning_rate": 0.0001598913371566556,
      "loss": 0.3416,
      "step": 9971
    },
    {
      "epoch": 20.064386317907445,
      "grad_norm": 0.6046113967895508,
      "learning_rate": 0.0001598873126069021,
      "loss": 0.3684,
      "step": 9972
    },
    {
      "epoch": 20.06639839034205,
      "grad_norm": 0.591972827911377,
      "learning_rate": 0.0001598832880571486,
      "loss": 0.321,
      "step": 9973
    },
    {
      "epoch": 20.06841046277666,
      "grad_norm": 0.6100971102714539,
      "learning_rate": 0.00015987926350739514,
      "loss": 0.3237,
      "step": 9974
    },
    {
      "epoch": 20.070422535211268,
      "grad_norm": 0.5645297765731812,
      "learning_rate": 0.00015987523895764162,
      "loss": 0.3117,
      "step": 9975
    },
    {
      "epoch": 20.072434607645874,
      "grad_norm": 0.6059005260467529,
      "learning_rate": 0.00015987121440788813,
      "loss": 0.3523,
      "step": 9976
    },
    {
      "epoch": 20.074446680080484,
      "grad_norm": 0.6010304689407349,
      "learning_rate": 0.00015986718985813462,
      "loss": 0.3173,
      "step": 9977
    },
    {
      "epoch": 20.07645875251509,
      "grad_norm": 0.6319211721420288,
      "learning_rate": 0.00015986316530838113,
      "loss": 0.349,
      "step": 9978
    },
    {
      "epoch": 20.078470824949697,
      "grad_norm": 0.6245617270469666,
      "learning_rate": 0.00015985914075862764,
      "loss": 0.344,
      "step": 9979
    },
    {
      "epoch": 20.080482897384307,
      "grad_norm": 0.6236413717269897,
      "learning_rate": 0.00015985511620887415,
      "loss": 0.3466,
      "step": 9980
    },
    {
      "epoch": 20.082494969818914,
      "grad_norm": 0.6090903878211975,
      "learning_rate": 0.00015985109165912064,
      "loss": 0.3356,
      "step": 9981
    },
    {
      "epoch": 20.08450704225352,
      "grad_norm": 0.611362099647522,
      "learning_rate": 0.00015984706710936715,
      "loss": 0.3572,
      "step": 9982
    },
    {
      "epoch": 20.08651911468813,
      "grad_norm": 0.5885270237922668,
      "learning_rate": 0.00015984304255961364,
      "loss": 0.3223,
      "step": 9983
    },
    {
      "epoch": 20.088531187122737,
      "grad_norm": 0.6531147956848145,
      "learning_rate": 0.00015983901800986018,
      "loss": 0.359,
      "step": 9984
    },
    {
      "epoch": 20.090543259557343,
      "grad_norm": 0.6372700929641724,
      "learning_rate": 0.00015983499346010666,
      "loss": 0.3369,
      "step": 9985
    },
    {
      "epoch": 20.092555331991953,
      "grad_norm": 0.5988366603851318,
      "learning_rate": 0.00015983096891035317,
      "loss": 0.321,
      "step": 9986
    },
    {
      "epoch": 20.09456740442656,
      "grad_norm": 0.5913654565811157,
      "learning_rate": 0.00015982694436059966,
      "loss": 0.3277,
      "step": 9987
    },
    {
      "epoch": 20.096579476861166,
      "grad_norm": 0.6017153859138489,
      "learning_rate": 0.00015982291981084617,
      "loss": 0.3402,
      "step": 9988
    },
    {
      "epoch": 20.098591549295776,
      "grad_norm": 0.6043826341629028,
      "learning_rate": 0.00015981889526109268,
      "loss": 0.3564,
      "step": 9989
    },
    {
      "epoch": 20.100603621730382,
      "grad_norm": 0.5997012257575989,
      "learning_rate": 0.00015981487071133917,
      "loss": 0.3764,
      "step": 9990
    },
    {
      "epoch": 20.10261569416499,
      "grad_norm": 0.5888702273368835,
      "learning_rate": 0.00015981084616158568,
      "loss": 0.3234,
      "step": 9991
    },
    {
      "epoch": 20.1046277665996,
      "grad_norm": 0.6012991070747375,
      "learning_rate": 0.0001598068216118322,
      "loss": 0.3754,
      "step": 9992
    },
    {
      "epoch": 20.106639839034205,
      "grad_norm": 0.5995358228683472,
      "learning_rate": 0.00015980279706207868,
      "loss": 0.3266,
      "step": 9993
    },
    {
      "epoch": 20.10865191146881,
      "grad_norm": 0.59566730260849,
      "learning_rate": 0.0001597987725123252,
      "loss": 0.3145,
      "step": 9994
    },
    {
      "epoch": 20.11066398390342,
      "grad_norm": 0.669060468673706,
      "learning_rate": 0.0001597947479625717,
      "loss": 0.3563,
      "step": 9995
    },
    {
      "epoch": 20.112676056338028,
      "grad_norm": 0.6183756589889526,
      "learning_rate": 0.0001597907234128182,
      "loss": 0.3236,
      "step": 9996
    },
    {
      "epoch": 20.114688128772634,
      "grad_norm": 0.6063785552978516,
      "learning_rate": 0.0001597866988630647,
      "loss": 0.3522,
      "step": 9997
    },
    {
      "epoch": 20.116700201207244,
      "grad_norm": 0.6007139682769775,
      "learning_rate": 0.00015978267431331118,
      "loss": 0.3223,
      "step": 9998
    },
    {
      "epoch": 20.11871227364185,
      "grad_norm": 0.6032861471176147,
      "learning_rate": 0.00015977864976355772,
      "loss": 0.3573,
      "step": 9999
    },
    {
      "epoch": 20.120724346076457,
      "grad_norm": 0.6131985187530518,
      "learning_rate": 0.0001597746252138042,
      "loss": 0.3277,
      "step": 10000
    },
    {
      "epoch": 20.122736418511067,
      "grad_norm": 0.6171265840530396,
      "learning_rate": 0.00015977060066405072,
      "loss": 0.3332,
      "step": 10001
    },
    {
      "epoch": 20.124748490945674,
      "grad_norm": 0.6134573817253113,
      "learning_rate": 0.0001597665761142972,
      "loss": 0.3476,
      "step": 10002
    },
    {
      "epoch": 20.12676056338028,
      "grad_norm": 0.611739456653595,
      "learning_rate": 0.00015976255156454372,
      "loss": 0.3673,
      "step": 10003
    },
    {
      "epoch": 20.12877263581489,
      "grad_norm": 0.658367395401001,
      "learning_rate": 0.00015975852701479023,
      "loss": 0.352,
      "step": 10004
    },
    {
      "epoch": 20.130784708249497,
      "grad_norm": 0.6575143933296204,
      "learning_rate": 0.00015975450246503674,
      "loss": 0.3466,
      "step": 10005
    },
    {
      "epoch": 20.132796780684103,
      "grad_norm": 0.6742546558380127,
      "learning_rate": 0.00015975047791528323,
      "loss": 0.3363,
      "step": 10006
    },
    {
      "epoch": 20.134808853118713,
      "grad_norm": 0.6015757918357849,
      "learning_rate": 0.00015974645336552974,
      "loss": 0.3295,
      "step": 10007
    },
    {
      "epoch": 20.13682092555332,
      "grad_norm": 0.5900160074234009,
      "learning_rate": 0.00015974242881577623,
      "loss": 0.332,
      "step": 10008
    },
    {
      "epoch": 20.138832997987926,
      "grad_norm": 0.5497996211051941,
      "learning_rate": 0.00015973840426602277,
      "loss": 0.3128,
      "step": 10009
    },
    {
      "epoch": 20.140845070422536,
      "grad_norm": 0.5992598533630371,
      "learning_rate": 0.00015973437971626925,
      "loss": 0.3277,
      "step": 10010
    },
    {
      "epoch": 20.142857142857142,
      "grad_norm": 0.6805192828178406,
      "learning_rate": 0.00015973035516651576,
      "loss": 0.3212,
      "step": 10011
    },
    {
      "epoch": 20.14486921529175,
      "grad_norm": 0.6432026624679565,
      "learning_rate": 0.00015972633061676225,
      "loss": 0.3236,
      "step": 10012
    },
    {
      "epoch": 20.14688128772636,
      "grad_norm": 0.7011834383010864,
      "learning_rate": 0.00015972230606700876,
      "loss": 0.3571,
      "step": 10013
    },
    {
      "epoch": 20.148893360160965,
      "grad_norm": 0.617337703704834,
      "learning_rate": 0.00015971828151725527,
      "loss": 0.3324,
      "step": 10014
    },
    {
      "epoch": 20.15090543259557,
      "grad_norm": 0.6251673102378845,
      "learning_rate": 0.00015971425696750178,
      "loss": 0.3366,
      "step": 10015
    },
    {
      "epoch": 20.15291750503018,
      "grad_norm": 0.6041585206985474,
      "learning_rate": 0.00015971023241774827,
      "loss": 0.3246,
      "step": 10016
    },
    {
      "epoch": 20.154929577464788,
      "grad_norm": 0.5903050303459167,
      "learning_rate": 0.00015970620786799478,
      "loss": 0.3112,
      "step": 10017
    },
    {
      "epoch": 20.156941649899398,
      "grad_norm": 0.5923275947570801,
      "learning_rate": 0.00015970218331824127,
      "loss": 0.3393,
      "step": 10018
    },
    {
      "epoch": 20.158953722334005,
      "grad_norm": 0.6078774929046631,
      "learning_rate": 0.0001596981587684878,
      "loss": 0.3341,
      "step": 10019
    },
    {
      "epoch": 20.16096579476861,
      "grad_norm": 0.6079392433166504,
      "learning_rate": 0.0001596941342187343,
      "loss": 0.3653,
      "step": 10020
    },
    {
      "epoch": 20.16297786720322,
      "grad_norm": 0.5951425433158875,
      "learning_rate": 0.0001596901096689808,
      "loss": 0.342,
      "step": 10021
    },
    {
      "epoch": 20.164989939637827,
      "grad_norm": 0.624107837677002,
      "learning_rate": 0.0001596860851192273,
      "loss": 0.3541,
      "step": 10022
    },
    {
      "epoch": 20.167002012072434,
      "grad_norm": 0.6164961457252502,
      "learning_rate": 0.0001596820605694738,
      "loss": 0.3686,
      "step": 10023
    },
    {
      "epoch": 20.169014084507044,
      "grad_norm": 0.6399559378623962,
      "learning_rate": 0.0001596780360197203,
      "loss": 0.3467,
      "step": 10024
    },
    {
      "epoch": 20.17102615694165,
      "grad_norm": 0.6110792756080627,
      "learning_rate": 0.0001596740114699668,
      "loss": 0.3394,
      "step": 10025
    },
    {
      "epoch": 20.173038229376257,
      "grad_norm": 0.602228045463562,
      "learning_rate": 0.0001596699869202133,
      "loss": 0.3167,
      "step": 10026
    },
    {
      "epoch": 20.175050301810867,
      "grad_norm": 0.6217666864395142,
      "learning_rate": 0.00015966596237045982,
      "loss": 0.347,
      "step": 10027
    },
    {
      "epoch": 20.177062374245473,
      "grad_norm": 0.647208034992218,
      "learning_rate": 0.0001596619378207063,
      "loss": 0.3507,
      "step": 10028
    },
    {
      "epoch": 20.17907444668008,
      "grad_norm": 0.6218597888946533,
      "learning_rate": 0.00015965791327095282,
      "loss": 0.3506,
      "step": 10029
    },
    {
      "epoch": 20.18108651911469,
      "grad_norm": 0.5944368243217468,
      "learning_rate": 0.00015965388872119933,
      "loss": 0.333,
      "step": 10030
    },
    {
      "epoch": 20.183098591549296,
      "grad_norm": 0.6068530082702637,
      "learning_rate": 0.00015964986417144582,
      "loss": 0.3388,
      "step": 10031
    },
    {
      "epoch": 20.185110663983902,
      "grad_norm": 0.635140061378479,
      "learning_rate": 0.00015964583962169233,
      "loss": 0.3539,
      "step": 10032
    },
    {
      "epoch": 20.187122736418512,
      "grad_norm": 0.6231377124786377,
      "learning_rate": 0.00015964181507193881,
      "loss": 0.3177,
      "step": 10033
    },
    {
      "epoch": 20.18913480885312,
      "grad_norm": 0.6104341745376587,
      "learning_rate": 0.00015963779052218535,
      "loss": 0.3178,
      "step": 10034
    },
    {
      "epoch": 20.191146881287725,
      "grad_norm": 0.6140987277030945,
      "learning_rate": 0.00015963376597243184,
      "loss": 0.3222,
      "step": 10035
    },
    {
      "epoch": 20.193158953722335,
      "grad_norm": 0.6303417086601257,
      "learning_rate": 0.00015962974142267835,
      "loss": 0.3404,
      "step": 10036
    },
    {
      "epoch": 20.19517102615694,
      "grad_norm": 0.6744572520256042,
      "learning_rate": 0.00015962571687292484,
      "loss": 0.3488,
      "step": 10037
    },
    {
      "epoch": 20.197183098591548,
      "grad_norm": 0.5959526896476746,
      "learning_rate": 0.00015962169232317135,
      "loss": 0.3393,
      "step": 10038
    },
    {
      "epoch": 20.199195171026158,
      "grad_norm": 0.5913528203964233,
      "learning_rate": 0.00015961766777341786,
      "loss": 0.3337,
      "step": 10039
    },
    {
      "epoch": 20.201207243460765,
      "grad_norm": 0.6284990310668945,
      "learning_rate": 0.00015961364322366437,
      "loss": 0.3611,
      "step": 10040
    },
    {
      "epoch": 20.20321931589537,
      "grad_norm": 0.6215790510177612,
      "learning_rate": 0.00015960961867391086,
      "loss": 0.3258,
      "step": 10041
    },
    {
      "epoch": 20.20523138832998,
      "grad_norm": 0.6232202649116516,
      "learning_rate": 0.00015960559412415737,
      "loss": 0.363,
      "step": 10042
    },
    {
      "epoch": 20.207243460764587,
      "grad_norm": 0.6488211154937744,
      "learning_rate": 0.00015960156957440386,
      "loss": 0.3482,
      "step": 10043
    },
    {
      "epoch": 20.209255533199194,
      "grad_norm": 0.6237021088600159,
      "learning_rate": 0.0001595975450246504,
      "loss": 0.3629,
      "step": 10044
    },
    {
      "epoch": 20.211267605633804,
      "grad_norm": 0.6129029989242554,
      "learning_rate": 0.00015959352047489688,
      "loss": 0.3484,
      "step": 10045
    },
    {
      "epoch": 20.21327967806841,
      "grad_norm": 0.6269071102142334,
      "learning_rate": 0.0001595894959251434,
      "loss": 0.339,
      "step": 10046
    },
    {
      "epoch": 20.215291750503017,
      "grad_norm": 0.6325550079345703,
      "learning_rate": 0.00015958547137538988,
      "loss": 0.3282,
      "step": 10047
    },
    {
      "epoch": 20.217303822937627,
      "grad_norm": 0.6514326930046082,
      "learning_rate": 0.0001595814468256364,
      "loss": 0.3609,
      "step": 10048
    },
    {
      "epoch": 20.219315895372233,
      "grad_norm": 0.6779909729957581,
      "learning_rate": 0.0001595774222758829,
      "loss": 0.36,
      "step": 10049
    },
    {
      "epoch": 20.22132796780684,
      "grad_norm": 0.6999015808105469,
      "learning_rate": 0.00015957339772612941,
      "loss": 0.3641,
      "step": 10050
    },
    {
      "epoch": 20.22334004024145,
      "grad_norm": 0.6609132289886475,
      "learning_rate": 0.0001595693731763759,
      "loss": 0.3515,
      "step": 10051
    },
    {
      "epoch": 20.225352112676056,
      "grad_norm": 0.6866008639335632,
      "learning_rate": 0.0001595653486266224,
      "loss": 0.3682,
      "step": 10052
    },
    {
      "epoch": 20.227364185110662,
      "grad_norm": 0.6390888094902039,
      "learning_rate": 0.0001595613240768689,
      "loss": 0.3902,
      "step": 10053
    },
    {
      "epoch": 20.229376257545272,
      "grad_norm": 0.6510134339332581,
      "learning_rate": 0.00015955729952711544,
      "loss": 0.3667,
      "step": 10054
    },
    {
      "epoch": 20.23138832997988,
      "grad_norm": 0.5988859534263611,
      "learning_rate": 0.00015955327497736192,
      "loss": 0.3485,
      "step": 10055
    },
    {
      "epoch": 20.233400402414485,
      "grad_norm": 0.6635339856147766,
      "learning_rate": 0.00015954925042760843,
      "loss": 0.3118,
      "step": 10056
    },
    {
      "epoch": 20.235412474849095,
      "grad_norm": 0.6462656855583191,
      "learning_rate": 0.00015954522587785492,
      "loss": 0.3383,
      "step": 10057
    },
    {
      "epoch": 20.2374245472837,
      "grad_norm": 0.6052905917167664,
      "learning_rate": 0.00015954120132810143,
      "loss": 0.3431,
      "step": 10058
    },
    {
      "epoch": 20.239436619718308,
      "grad_norm": 0.6337644457817078,
      "learning_rate": 0.00015953717677834794,
      "loss": 0.3581,
      "step": 10059
    },
    {
      "epoch": 20.241448692152918,
      "grad_norm": 0.6471460461616516,
      "learning_rate": 0.00015953315222859443,
      "loss": 0.3243,
      "step": 10060
    },
    {
      "epoch": 20.243460764587525,
      "grad_norm": 0.6731941103935242,
      "learning_rate": 0.00015952912767884094,
      "loss": 0.3575,
      "step": 10061
    },
    {
      "epoch": 20.24547283702213,
      "grad_norm": 0.6316708922386169,
      "learning_rate": 0.00015952510312908745,
      "loss": 0.367,
      "step": 10062
    },
    {
      "epoch": 20.24748490945674,
      "grad_norm": 0.6464518904685974,
      "learning_rate": 0.00015952107857933394,
      "loss": 0.3394,
      "step": 10063
    },
    {
      "epoch": 20.249496981891348,
      "grad_norm": 0.6098348498344421,
      "learning_rate": 0.00015951705402958045,
      "loss": 0.3665,
      "step": 10064
    },
    {
      "epoch": 20.251509054325957,
      "grad_norm": 0.5990302562713623,
      "learning_rate": 0.00015951302947982696,
      "loss": 0.3177,
      "step": 10065
    },
    {
      "epoch": 20.253521126760564,
      "grad_norm": 0.6056652069091797,
      "learning_rate": 0.00015950900493007345,
      "loss": 0.3698,
      "step": 10066
    },
    {
      "epoch": 20.25553319919517,
      "grad_norm": 0.5875251293182373,
      "learning_rate": 0.00015950498038031996,
      "loss": 0.3216,
      "step": 10067
    },
    {
      "epoch": 20.25754527162978,
      "grad_norm": 0.6047282218933105,
      "learning_rate": 0.00015950095583056644,
      "loss": 0.3507,
      "step": 10068
    },
    {
      "epoch": 20.259557344064387,
      "grad_norm": 0.6356461048126221,
      "learning_rate": 0.00015949693128081296,
      "loss": 0.3596,
      "step": 10069
    },
    {
      "epoch": 20.261569416498993,
      "grad_norm": 0.6544845700263977,
      "learning_rate": 0.00015949290673105947,
      "loss": 0.3603,
      "step": 10070
    },
    {
      "epoch": 20.263581488933603,
      "grad_norm": 0.6284120082855225,
      "learning_rate": 0.00015948888218130598,
      "loss": 0.3441,
      "step": 10071
    },
    {
      "epoch": 20.26559356136821,
      "grad_norm": 0.6207298040390015,
      "learning_rate": 0.00015948485763155247,
      "loss": 0.3704,
      "step": 10072
    },
    {
      "epoch": 20.267605633802816,
      "grad_norm": 0.6052454113960266,
      "learning_rate": 0.00015948083308179898,
      "loss": 0.3392,
      "step": 10073
    },
    {
      "epoch": 20.269617706237426,
      "grad_norm": 0.6343391537666321,
      "learning_rate": 0.00015947680853204546,
      "loss": 0.3443,
      "step": 10074
    },
    {
      "epoch": 20.271629778672033,
      "grad_norm": 0.6428400278091431,
      "learning_rate": 0.000159472783982292,
      "loss": 0.3479,
      "step": 10075
    },
    {
      "epoch": 20.27364185110664,
      "grad_norm": 0.6048412919044495,
      "learning_rate": 0.0001594687594325385,
      "loss": 0.3487,
      "step": 10076
    },
    {
      "epoch": 20.27565392354125,
      "grad_norm": 0.6119108200073242,
      "learning_rate": 0.000159464734882785,
      "loss": 0.3513,
      "step": 10077
    },
    {
      "epoch": 20.277665995975855,
      "grad_norm": 0.6409320831298828,
      "learning_rate": 0.00015946071033303148,
      "loss": 0.3732,
      "step": 10078
    },
    {
      "epoch": 20.279678068410462,
      "grad_norm": 0.6059525012969971,
      "learning_rate": 0.000159456685783278,
      "loss": 0.3254,
      "step": 10079
    },
    {
      "epoch": 20.281690140845072,
      "grad_norm": 0.6294896006584167,
      "learning_rate": 0.0001594526612335245,
      "loss": 0.324,
      "step": 10080
    },
    {
      "epoch": 20.28370221327968,
      "grad_norm": 0.6396111845970154,
      "learning_rate": 0.00015944863668377102,
      "loss": 0.3657,
      "step": 10081
    },
    {
      "epoch": 20.285714285714285,
      "grad_norm": 0.6474282145500183,
      "learning_rate": 0.0001594446121340175,
      "loss": 0.3643,
      "step": 10082
    },
    {
      "epoch": 20.287726358148895,
      "grad_norm": 0.6393147706985474,
      "learning_rate": 0.00015944058758426402,
      "loss": 0.3472,
      "step": 10083
    },
    {
      "epoch": 20.2897384305835,
      "grad_norm": 0.6232065558433533,
      "learning_rate": 0.0001594365630345105,
      "loss": 0.3526,
      "step": 10084
    },
    {
      "epoch": 20.291750503018108,
      "grad_norm": 0.6147559881210327,
      "learning_rate": 0.00015943253848475704,
      "loss": 0.368,
      "step": 10085
    },
    {
      "epoch": 20.293762575452718,
      "grad_norm": 0.6651532649993896,
      "learning_rate": 0.00015942851393500353,
      "loss": 0.3861,
      "step": 10086
    },
    {
      "epoch": 20.295774647887324,
      "grad_norm": 0.6165881752967834,
      "learning_rate": 0.00015942448938525004,
      "loss": 0.3743,
      "step": 10087
    },
    {
      "epoch": 20.29778672032193,
      "grad_norm": 0.6059823632240295,
      "learning_rate": 0.00015942046483549653,
      "loss": 0.3539,
      "step": 10088
    },
    {
      "epoch": 20.29979879275654,
      "grad_norm": 0.632892906665802,
      "learning_rate": 0.00015941644028574304,
      "loss": 0.3572,
      "step": 10089
    },
    {
      "epoch": 20.301810865191147,
      "grad_norm": 0.6165842413902283,
      "learning_rate": 0.00015941241573598955,
      "loss": 0.342,
      "step": 10090
    },
    {
      "epoch": 20.303822937625753,
      "grad_norm": 0.6202801465988159,
      "learning_rate": 0.00015940839118623606,
      "loss": 0.3316,
      "step": 10091
    },
    {
      "epoch": 20.305835010060363,
      "grad_norm": 0.7090452313423157,
      "learning_rate": 0.00015940436663648255,
      "loss": 0.387,
      "step": 10092
    },
    {
      "epoch": 20.30784708249497,
      "grad_norm": 0.6366012096405029,
      "learning_rate": 0.00015940034208672906,
      "loss": 0.3653,
      "step": 10093
    },
    {
      "epoch": 20.309859154929576,
      "grad_norm": 0.6540359854698181,
      "learning_rate": 0.00015939631753697554,
      "loss": 0.3506,
      "step": 10094
    },
    {
      "epoch": 20.311871227364186,
      "grad_norm": 0.6192591190338135,
      "learning_rate": 0.00015939229298722206,
      "loss": 0.3776,
      "step": 10095
    },
    {
      "epoch": 20.313883299798793,
      "grad_norm": 0.6358162760734558,
      "learning_rate": 0.00015938826843746857,
      "loss": 0.349,
      "step": 10096
    },
    {
      "epoch": 20.3158953722334,
      "grad_norm": 0.6535856127738953,
      "learning_rate": 0.00015938424388771508,
      "loss": 0.3681,
      "step": 10097
    },
    {
      "epoch": 20.31790744466801,
      "grad_norm": 0.6263103485107422,
      "learning_rate": 0.00015938021933796157,
      "loss": 0.3548,
      "step": 10098
    },
    {
      "epoch": 20.319919517102615,
      "grad_norm": 0.5878938436508179,
      "learning_rate": 0.00015937619478820808,
      "loss": 0.3343,
      "step": 10099
    },
    {
      "epoch": 20.321931589537222,
      "grad_norm": 0.6323274970054626,
      "learning_rate": 0.0001593721702384546,
      "loss": 0.3575,
      "step": 10100
    },
    {
      "epoch": 20.323943661971832,
      "grad_norm": 0.6220384240150452,
      "learning_rate": 0.00015936814568870108,
      "loss": 0.3432,
      "step": 10101
    },
    {
      "epoch": 20.32595573440644,
      "grad_norm": 0.6415174603462219,
      "learning_rate": 0.0001593641211389476,
      "loss": 0.3356,
      "step": 10102
    },
    {
      "epoch": 20.327967806841045,
      "grad_norm": 0.6515518426895142,
      "learning_rate": 0.00015936009658919407,
      "loss": 0.3629,
      "step": 10103
    },
    {
      "epoch": 20.329979879275655,
      "grad_norm": 0.6171150207519531,
      "learning_rate": 0.00015935607203944059,
      "loss": 0.3642,
      "step": 10104
    },
    {
      "epoch": 20.33199195171026,
      "grad_norm": 0.640308141708374,
      "learning_rate": 0.0001593520474896871,
      "loss": 0.3545,
      "step": 10105
    },
    {
      "epoch": 20.334004024144868,
      "grad_norm": 0.6550650596618652,
      "learning_rate": 0.0001593480229399336,
      "loss": 0.3534,
      "step": 10106
    },
    {
      "epoch": 20.336016096579478,
      "grad_norm": 0.6150626540184021,
      "learning_rate": 0.0001593439983901801,
      "loss": 0.3591,
      "step": 10107
    },
    {
      "epoch": 20.338028169014084,
      "grad_norm": 0.6428616642951965,
      "learning_rate": 0.0001593399738404266,
      "loss": 0.3698,
      "step": 10108
    },
    {
      "epoch": 20.34004024144869,
      "grad_norm": 0.6515308618545532,
      "learning_rate": 0.0001593359492906731,
      "loss": 0.36,
      "step": 10109
    },
    {
      "epoch": 20.3420523138833,
      "grad_norm": 0.6332904100418091,
      "learning_rate": 0.00015933192474091963,
      "loss": 0.3658,
      "step": 10110
    },
    {
      "epoch": 20.344064386317907,
      "grad_norm": 0.6028360724449158,
      "learning_rate": 0.00015932790019116612,
      "loss": 0.3516,
      "step": 10111
    },
    {
      "epoch": 20.346076458752513,
      "grad_norm": 0.6107322573661804,
      "learning_rate": 0.00015932387564141263,
      "loss": 0.3426,
      "step": 10112
    },
    {
      "epoch": 20.348088531187123,
      "grad_norm": 0.6254901885986328,
      "learning_rate": 0.00015931985109165911,
      "loss": 0.3553,
      "step": 10113
    },
    {
      "epoch": 20.35010060362173,
      "grad_norm": 0.6767026782035828,
      "learning_rate": 0.00015931582654190563,
      "loss": 0.3588,
      "step": 10114
    },
    {
      "epoch": 20.352112676056336,
      "grad_norm": 0.6275861263275146,
      "learning_rate": 0.00015931180199215214,
      "loss": 0.3621,
      "step": 10115
    },
    {
      "epoch": 20.354124748490946,
      "grad_norm": 0.6267346739768982,
      "learning_rate": 0.00015930777744239865,
      "loss": 0.368,
      "step": 10116
    },
    {
      "epoch": 20.356136820925553,
      "grad_norm": 0.6100643873214722,
      "learning_rate": 0.00015930375289264514,
      "loss": 0.3431,
      "step": 10117
    },
    {
      "epoch": 20.358148893360163,
      "grad_norm": 0.627616286277771,
      "learning_rate": 0.00015929972834289165,
      "loss": 0.3813,
      "step": 10118
    },
    {
      "epoch": 20.36016096579477,
      "grad_norm": 0.6512315273284912,
      "learning_rate": 0.00015929570379313813,
      "loss": 0.3486,
      "step": 10119
    },
    {
      "epoch": 20.362173038229376,
      "grad_norm": 0.6169552803039551,
      "learning_rate": 0.00015929167924338467,
      "loss": 0.3281,
      "step": 10120
    },
    {
      "epoch": 20.364185110663986,
      "grad_norm": 0.6654161810874939,
      "learning_rate": 0.00015928765469363116,
      "loss": 0.3706,
      "step": 10121
    },
    {
      "epoch": 20.366197183098592,
      "grad_norm": 0.628133237361908,
      "learning_rate": 0.00015928363014387767,
      "loss": 0.357,
      "step": 10122
    },
    {
      "epoch": 20.3682092555332,
      "grad_norm": 0.6269707083702087,
      "learning_rate": 0.00015927960559412415,
      "loss": 0.3594,
      "step": 10123
    },
    {
      "epoch": 20.37022132796781,
      "grad_norm": 0.6775343418121338,
      "learning_rate": 0.00015927558104437067,
      "loss": 0.4017,
      "step": 10124
    },
    {
      "epoch": 20.372233400402415,
      "grad_norm": 0.6625306010246277,
      "learning_rate": 0.00015927155649461718,
      "loss": 0.3482,
      "step": 10125
    },
    {
      "epoch": 20.37424547283702,
      "grad_norm": 0.6199356913566589,
      "learning_rate": 0.0001592675319448637,
      "loss": 0.3643,
      "step": 10126
    },
    {
      "epoch": 20.37625754527163,
      "grad_norm": 0.6434483528137207,
      "learning_rate": 0.00015926350739511018,
      "loss": 0.3781,
      "step": 10127
    },
    {
      "epoch": 20.378269617706238,
      "grad_norm": 0.6316325664520264,
      "learning_rate": 0.0001592594828453567,
      "loss": 0.3632,
      "step": 10128
    },
    {
      "epoch": 20.380281690140844,
      "grad_norm": 0.6379221081733704,
      "learning_rate": 0.00015925545829560317,
      "loss": 0.373,
      "step": 10129
    },
    {
      "epoch": 20.382293762575454,
      "grad_norm": 0.6429516673088074,
      "learning_rate": 0.00015925143374584969,
      "loss": 0.378,
      "step": 10130
    },
    {
      "epoch": 20.38430583501006,
      "grad_norm": 0.6218671202659607,
      "learning_rate": 0.0001592474091960962,
      "loss": 0.3597,
      "step": 10131
    },
    {
      "epoch": 20.386317907444667,
      "grad_norm": 0.6226216554641724,
      "learning_rate": 0.00015924338464634268,
      "loss": 0.3405,
      "step": 10132
    },
    {
      "epoch": 20.388329979879277,
      "grad_norm": 0.6371235847473145,
      "learning_rate": 0.0001592393600965892,
      "loss": 0.3762,
      "step": 10133
    },
    {
      "epoch": 20.390342052313883,
      "grad_norm": 0.6107571721076965,
      "learning_rate": 0.0001592353355468357,
      "loss": 0.3499,
      "step": 10134
    },
    {
      "epoch": 20.39235412474849,
      "grad_norm": 0.6173198223114014,
      "learning_rate": 0.00015923131099708222,
      "loss": 0.3314,
      "step": 10135
    },
    {
      "epoch": 20.3943661971831,
      "grad_norm": 0.6369442939758301,
      "learning_rate": 0.0001592272864473287,
      "loss": 0.3408,
      "step": 10136
    },
    {
      "epoch": 20.396378269617706,
      "grad_norm": 0.688690185546875,
      "learning_rate": 0.00015922326189757522,
      "loss": 0.3994,
      "step": 10137
    },
    {
      "epoch": 20.398390342052313,
      "grad_norm": 0.6500935554504395,
      "learning_rate": 0.0001592192373478217,
      "loss": 0.3686,
      "step": 10138
    },
    {
      "epoch": 20.400402414486923,
      "grad_norm": 0.6092783212661743,
      "learning_rate": 0.00015921521279806821,
      "loss": 0.3519,
      "step": 10139
    },
    {
      "epoch": 20.40241448692153,
      "grad_norm": 0.6157505512237549,
      "learning_rate": 0.00015921118824831473,
      "loss": 0.3365,
      "step": 10140
    },
    {
      "epoch": 20.404426559356136,
      "grad_norm": 0.6188225150108337,
      "learning_rate": 0.00015920716369856124,
      "loss": 0.3608,
      "step": 10141
    },
    {
      "epoch": 20.406438631790746,
      "grad_norm": 0.6245047450065613,
      "learning_rate": 0.00015920313914880772,
      "loss": 0.3584,
      "step": 10142
    },
    {
      "epoch": 20.408450704225352,
      "grad_norm": 0.6600744128227234,
      "learning_rate": 0.00015919911459905424,
      "loss": 0.3941,
      "step": 10143
    },
    {
      "epoch": 20.41046277665996,
      "grad_norm": 0.6405308246612549,
      "learning_rate": 0.00015919509004930072,
      "loss": 0.3446,
      "step": 10144
    },
    {
      "epoch": 20.41247484909457,
      "grad_norm": 0.6406362652778625,
      "learning_rate": 0.00015919106549954726,
      "loss": 0.353,
      "step": 10145
    },
    {
      "epoch": 20.414486921529175,
      "grad_norm": 0.6514295935630798,
      "learning_rate": 0.00015918704094979375,
      "loss": 0.3541,
      "step": 10146
    },
    {
      "epoch": 20.41649899396378,
      "grad_norm": 0.6554050445556641,
      "learning_rate": 0.00015918301640004026,
      "loss": 0.3475,
      "step": 10147
    },
    {
      "epoch": 20.41851106639839,
      "grad_norm": 0.6713681817054749,
      "learning_rate": 0.00015917899185028674,
      "loss": 0.3691,
      "step": 10148
    },
    {
      "epoch": 20.420523138832998,
      "grad_norm": 0.6624171733856201,
      "learning_rate": 0.00015917496730053326,
      "loss": 0.3456,
      "step": 10149
    },
    {
      "epoch": 20.422535211267604,
      "grad_norm": 0.6378286480903625,
      "learning_rate": 0.00015917094275077977,
      "loss": 0.384,
      "step": 10150
    },
    {
      "epoch": 20.424547283702214,
      "grad_norm": 0.6238173246383667,
      "learning_rate": 0.00015916691820102628,
      "loss": 0.3395,
      "step": 10151
    },
    {
      "epoch": 20.42655935613682,
      "grad_norm": 0.6212871074676514,
      "learning_rate": 0.00015916289365127277,
      "loss": 0.3667,
      "step": 10152
    },
    {
      "epoch": 20.428571428571427,
      "grad_norm": 0.6050427556037903,
      "learning_rate": 0.00015915886910151928,
      "loss": 0.3479,
      "step": 10153
    },
    {
      "epoch": 20.430583501006037,
      "grad_norm": 0.6138110756874084,
      "learning_rate": 0.00015915484455176576,
      "loss": 0.3438,
      "step": 10154
    },
    {
      "epoch": 20.432595573440643,
      "grad_norm": 0.6294659972190857,
      "learning_rate": 0.0001591508200020123,
      "loss": 0.3495,
      "step": 10155
    },
    {
      "epoch": 20.43460764587525,
      "grad_norm": 0.6483877897262573,
      "learning_rate": 0.0001591467954522588,
      "loss": 0.3768,
      "step": 10156
    },
    {
      "epoch": 20.43661971830986,
      "grad_norm": 0.6648746728897095,
      "learning_rate": 0.0001591427709025053,
      "loss": 0.38,
      "step": 10157
    },
    {
      "epoch": 20.438631790744466,
      "grad_norm": 0.6255798935890198,
      "learning_rate": 0.00015913874635275178,
      "loss": 0.3766,
      "step": 10158
    },
    {
      "epoch": 20.440643863179073,
      "grad_norm": 0.6107755303382874,
      "learning_rate": 0.0001591347218029983,
      "loss": 0.346,
      "step": 10159
    },
    {
      "epoch": 20.442655935613683,
      "grad_norm": 0.6538180708885193,
      "learning_rate": 0.0001591306972532448,
      "loss": 0.3683,
      "step": 10160
    },
    {
      "epoch": 20.44466800804829,
      "grad_norm": 0.6502869725227356,
      "learning_rate": 0.00015912667270349132,
      "loss": 0.3859,
      "step": 10161
    },
    {
      "epoch": 20.446680080482896,
      "grad_norm": 0.6205381751060486,
      "learning_rate": 0.0001591226481537378,
      "loss": 0.3595,
      "step": 10162
    },
    {
      "epoch": 20.448692152917506,
      "grad_norm": 0.6537185907363892,
      "learning_rate": 0.00015911862360398432,
      "loss": 0.3702,
      "step": 10163
    },
    {
      "epoch": 20.450704225352112,
      "grad_norm": 0.6351974010467529,
      "learning_rate": 0.0001591145990542308,
      "loss": 0.3463,
      "step": 10164
    },
    {
      "epoch": 20.452716297786722,
      "grad_norm": 0.6360763311386108,
      "learning_rate": 0.00015911057450447732,
      "loss": 0.3488,
      "step": 10165
    },
    {
      "epoch": 20.45472837022133,
      "grad_norm": 0.6451500058174133,
      "learning_rate": 0.00015910654995472383,
      "loss": 0.365,
      "step": 10166
    },
    {
      "epoch": 20.456740442655935,
      "grad_norm": 0.6398126482963562,
      "learning_rate": 0.0001591025254049703,
      "loss": 0.3733,
      "step": 10167
    },
    {
      "epoch": 20.458752515090545,
      "grad_norm": 0.647463858127594,
      "learning_rate": 0.00015909850085521683,
      "loss": 0.4006,
      "step": 10168
    },
    {
      "epoch": 20.46076458752515,
      "grad_norm": 0.6496067047119141,
      "learning_rate": 0.00015909447630546334,
      "loss": 0.3688,
      "step": 10169
    },
    {
      "epoch": 20.462776659959758,
      "grad_norm": 0.6776132583618164,
      "learning_rate": 0.00015909045175570985,
      "loss": 0.3914,
      "step": 10170
    },
    {
      "epoch": 20.464788732394368,
      "grad_norm": 0.6719145774841309,
      "learning_rate": 0.00015908642720595633,
      "loss": 0.352,
      "step": 10171
    },
    {
      "epoch": 20.466800804828974,
      "grad_norm": 0.6532291769981384,
      "learning_rate": 0.00015908240265620285,
      "loss": 0.3623,
      "step": 10172
    },
    {
      "epoch": 20.46881287726358,
      "grad_norm": 0.6443752646446228,
      "learning_rate": 0.00015907837810644933,
      "loss": 0.3185,
      "step": 10173
    },
    {
      "epoch": 20.47082494969819,
      "grad_norm": 0.6324925422668457,
      "learning_rate": 0.00015907435355669584,
      "loss": 0.36,
      "step": 10174
    },
    {
      "epoch": 20.472837022132797,
      "grad_norm": 0.6186694502830505,
      "learning_rate": 0.00015907032900694236,
      "loss": 0.3642,
      "step": 10175
    },
    {
      "epoch": 20.474849094567404,
      "grad_norm": 0.6635547876358032,
      "learning_rate": 0.00015906630445718887,
      "loss": 0.3836,
      "step": 10176
    },
    {
      "epoch": 20.476861167002014,
      "grad_norm": 0.6731573343276978,
      "learning_rate": 0.00015906227990743535,
      "loss": 0.3505,
      "step": 10177
    },
    {
      "epoch": 20.47887323943662,
      "grad_norm": 0.6313450336456299,
      "learning_rate": 0.00015905825535768187,
      "loss": 0.3767,
      "step": 10178
    },
    {
      "epoch": 20.480885311871226,
      "grad_norm": 0.6570813655853271,
      "learning_rate": 0.00015905423080792835,
      "loss": 0.3884,
      "step": 10179
    },
    {
      "epoch": 20.482897384305836,
      "grad_norm": 0.6510561108589172,
      "learning_rate": 0.0001590502062581749,
      "loss": 0.3677,
      "step": 10180
    },
    {
      "epoch": 20.484909456740443,
      "grad_norm": 0.6020180583000183,
      "learning_rate": 0.00015904618170842138,
      "loss": 0.3542,
      "step": 10181
    },
    {
      "epoch": 20.48692152917505,
      "grad_norm": 0.6411917209625244,
      "learning_rate": 0.0001590421571586679,
      "loss": 0.3478,
      "step": 10182
    },
    {
      "epoch": 20.48893360160966,
      "grad_norm": 0.6095837950706482,
      "learning_rate": 0.00015903813260891437,
      "loss": 0.3271,
      "step": 10183
    },
    {
      "epoch": 20.490945674044266,
      "grad_norm": 0.6607338786125183,
      "learning_rate": 0.00015903410805916089,
      "loss": 0.3838,
      "step": 10184
    },
    {
      "epoch": 20.492957746478872,
      "grad_norm": 0.725406289100647,
      "learning_rate": 0.0001590300835094074,
      "loss": 0.3462,
      "step": 10185
    },
    {
      "epoch": 20.494969818913482,
      "grad_norm": 0.7015853524208069,
      "learning_rate": 0.0001590260589596539,
      "loss": 0.3498,
      "step": 10186
    },
    {
      "epoch": 20.49698189134809,
      "grad_norm": 0.682050883769989,
      "learning_rate": 0.0001590220344099004,
      "loss": 0.3826,
      "step": 10187
    },
    {
      "epoch": 20.498993963782695,
      "grad_norm": 0.6207718849182129,
      "learning_rate": 0.0001590180098601469,
      "loss": 0.3622,
      "step": 10188
    },
    {
      "epoch": 20.501006036217305,
      "grad_norm": 0.6230242848396301,
      "learning_rate": 0.0001590139853103934,
      "loss": 0.3804,
      "step": 10189
    },
    {
      "epoch": 20.50301810865191,
      "grad_norm": 0.6556674838066101,
      "learning_rate": 0.00015900996076063993,
      "loss": 0.3638,
      "step": 10190
    },
    {
      "epoch": 20.505030181086518,
      "grad_norm": 0.6230106949806213,
      "learning_rate": 0.00015900593621088642,
      "loss": 0.359,
      "step": 10191
    },
    {
      "epoch": 20.507042253521128,
      "grad_norm": 0.6292091608047485,
      "learning_rate": 0.00015900191166113293,
      "loss": 0.3261,
      "step": 10192
    },
    {
      "epoch": 20.509054325955734,
      "grad_norm": 0.6433991193771362,
      "learning_rate": 0.00015899788711137941,
      "loss": 0.4009,
      "step": 10193
    },
    {
      "epoch": 20.51106639839034,
      "grad_norm": 0.6557523012161255,
      "learning_rate": 0.00015899386256162593,
      "loss": 0.3703,
      "step": 10194
    },
    {
      "epoch": 20.51307847082495,
      "grad_norm": 0.6453779339790344,
      "learning_rate": 0.00015898983801187244,
      "loss": 0.3941,
      "step": 10195
    },
    {
      "epoch": 20.515090543259557,
      "grad_norm": 0.6198368072509766,
      "learning_rate": 0.00015898581346211895,
      "loss": 0.3462,
      "step": 10196
    },
    {
      "epoch": 20.517102615694164,
      "grad_norm": 0.6478729844093323,
      "learning_rate": 0.00015898178891236544,
      "loss": 0.3955,
      "step": 10197
    },
    {
      "epoch": 20.519114688128774,
      "grad_norm": 0.6399478912353516,
      "learning_rate": 0.00015897776436261195,
      "loss": 0.3822,
      "step": 10198
    },
    {
      "epoch": 20.52112676056338,
      "grad_norm": 0.6475479006767273,
      "learning_rate": 0.00015897373981285843,
      "loss": 0.3658,
      "step": 10199
    },
    {
      "epoch": 20.523138832997986,
      "grad_norm": 0.6900739669799805,
      "learning_rate": 0.00015896971526310495,
      "loss": 0.3978,
      "step": 10200
    },
    {
      "epoch": 20.525150905432596,
      "grad_norm": 0.6204274296760559,
      "learning_rate": 0.00015896569071335146,
      "loss": 0.372,
      "step": 10201
    },
    {
      "epoch": 20.527162977867203,
      "grad_norm": 0.6617024540901184,
      "learning_rate": 0.00015896166616359794,
      "loss": 0.3576,
      "step": 10202
    },
    {
      "epoch": 20.52917505030181,
      "grad_norm": 0.6379483938217163,
      "learning_rate": 0.00015895764161384445,
      "loss": 0.3668,
      "step": 10203
    },
    {
      "epoch": 20.53118712273642,
      "grad_norm": 0.6342847347259521,
      "learning_rate": 0.00015895361706409097,
      "loss": 0.3435,
      "step": 10204
    },
    {
      "epoch": 20.533199195171026,
      "grad_norm": 0.6394708752632141,
      "learning_rate": 0.00015894959251433748,
      "loss": 0.3672,
      "step": 10205
    },
    {
      "epoch": 20.535211267605632,
      "grad_norm": 0.6612034440040588,
      "learning_rate": 0.00015894556796458396,
      "loss": 0.3491,
      "step": 10206
    },
    {
      "epoch": 20.537223340040242,
      "grad_norm": 0.6282998323440552,
      "learning_rate": 0.00015894154341483048,
      "loss": 0.3622,
      "step": 10207
    },
    {
      "epoch": 20.53923541247485,
      "grad_norm": 0.6574758887290955,
      "learning_rate": 0.00015893751886507696,
      "loss": 0.377,
      "step": 10208
    },
    {
      "epoch": 20.541247484909455,
      "grad_norm": 0.6170530915260315,
      "learning_rate": 0.00015893349431532347,
      "loss": 0.3889,
      "step": 10209
    },
    {
      "epoch": 20.543259557344065,
      "grad_norm": 0.6463958621025085,
      "learning_rate": 0.00015892946976556999,
      "loss": 0.3812,
      "step": 10210
    },
    {
      "epoch": 20.54527162977867,
      "grad_norm": 0.6148120164871216,
      "learning_rate": 0.0001589254452158165,
      "loss": 0.3816,
      "step": 10211
    },
    {
      "epoch": 20.547283702213278,
      "grad_norm": 0.6204009652137756,
      "learning_rate": 0.00015892142066606298,
      "loss": 0.3733,
      "step": 10212
    },
    {
      "epoch": 20.549295774647888,
      "grad_norm": 0.6309577822685242,
      "learning_rate": 0.0001589173961163095,
      "loss": 0.3493,
      "step": 10213
    },
    {
      "epoch": 20.551307847082494,
      "grad_norm": 0.5875040292739868,
      "learning_rate": 0.00015891337156655598,
      "loss": 0.3489,
      "step": 10214
    },
    {
      "epoch": 20.5533199195171,
      "grad_norm": 0.621636152267456,
      "learning_rate": 0.00015890934701680252,
      "loss": 0.3377,
      "step": 10215
    },
    {
      "epoch": 20.55533199195171,
      "grad_norm": 0.6626805663108826,
      "learning_rate": 0.000158905322467049,
      "loss": 0.3626,
      "step": 10216
    },
    {
      "epoch": 20.557344064386317,
      "grad_norm": 0.6322381496429443,
      "learning_rate": 0.00015890129791729552,
      "loss": 0.3846,
      "step": 10217
    },
    {
      "epoch": 20.559356136820927,
      "grad_norm": 0.644977867603302,
      "learning_rate": 0.000158897273367542,
      "loss": 0.3782,
      "step": 10218
    },
    {
      "epoch": 20.561368209255534,
      "grad_norm": 0.6093277335166931,
      "learning_rate": 0.00015889324881778851,
      "loss": 0.3341,
      "step": 10219
    },
    {
      "epoch": 20.56338028169014,
      "grad_norm": 0.6195976734161377,
      "learning_rate": 0.00015888922426803503,
      "loss": 0.3908,
      "step": 10220
    },
    {
      "epoch": 20.56539235412475,
      "grad_norm": 0.6336710453033447,
      "learning_rate": 0.00015888519971828154,
      "loss": 0.3798,
      "step": 10221
    },
    {
      "epoch": 20.567404426559357,
      "grad_norm": 0.6057290434837341,
      "learning_rate": 0.00015888117516852802,
      "loss": 0.3574,
      "step": 10222
    },
    {
      "epoch": 20.569416498993963,
      "grad_norm": 0.6559727787971497,
      "learning_rate": 0.00015887715061877454,
      "loss": 0.3817,
      "step": 10223
    },
    {
      "epoch": 20.571428571428573,
      "grad_norm": 0.6355257630348206,
      "learning_rate": 0.00015887312606902102,
      "loss": 0.3889,
      "step": 10224
    },
    {
      "epoch": 20.57344064386318,
      "grad_norm": 0.6517664194107056,
      "learning_rate": 0.00015886910151926756,
      "loss": 0.3911,
      "step": 10225
    },
    {
      "epoch": 20.575452716297786,
      "grad_norm": 0.616327166557312,
      "learning_rate": 0.00015886507696951405,
      "loss": 0.3653,
      "step": 10226
    },
    {
      "epoch": 20.577464788732396,
      "grad_norm": 0.7080254554748535,
      "learning_rate": 0.00015886105241976056,
      "loss": 0.4131,
      "step": 10227
    },
    {
      "epoch": 20.579476861167002,
      "grad_norm": 0.6618353128433228,
      "learning_rate": 0.00015885702787000704,
      "loss": 0.368,
      "step": 10228
    },
    {
      "epoch": 20.58148893360161,
      "grad_norm": 0.6879287362098694,
      "learning_rate": 0.00015885300332025356,
      "loss": 0.3755,
      "step": 10229
    },
    {
      "epoch": 20.58350100603622,
      "grad_norm": 0.6327314972877502,
      "learning_rate": 0.00015884897877050007,
      "loss": 0.3795,
      "step": 10230
    },
    {
      "epoch": 20.585513078470825,
      "grad_norm": 0.6196925640106201,
      "learning_rate": 0.00015884495422074658,
      "loss": 0.3985,
      "step": 10231
    },
    {
      "epoch": 20.58752515090543,
      "grad_norm": 0.6447443962097168,
      "learning_rate": 0.00015884092967099306,
      "loss": 0.3836,
      "step": 10232
    },
    {
      "epoch": 20.58953722334004,
      "grad_norm": 0.6706633567810059,
      "learning_rate": 0.00015883690512123958,
      "loss": 0.3654,
      "step": 10233
    },
    {
      "epoch": 20.591549295774648,
      "grad_norm": 0.6914812922477722,
      "learning_rate": 0.00015883288057148606,
      "loss": 0.3449,
      "step": 10234
    },
    {
      "epoch": 20.593561368209254,
      "grad_norm": 0.637000322341919,
      "learning_rate": 0.00015882885602173257,
      "loss": 0.3444,
      "step": 10235
    },
    {
      "epoch": 20.595573440643864,
      "grad_norm": 0.6851493716239929,
      "learning_rate": 0.0001588248314719791,
      "loss": 0.3515,
      "step": 10236
    },
    {
      "epoch": 20.59758551307847,
      "grad_norm": 0.6252692341804504,
      "learning_rate": 0.00015882080692222557,
      "loss": 0.3683,
      "step": 10237
    },
    {
      "epoch": 20.599597585513077,
      "grad_norm": 0.6433092951774597,
      "learning_rate": 0.00015881678237247208,
      "loss": 0.3692,
      "step": 10238
    },
    {
      "epoch": 20.601609657947687,
      "grad_norm": 0.6497558355331421,
      "learning_rate": 0.0001588127578227186,
      "loss": 0.3936,
      "step": 10239
    },
    {
      "epoch": 20.603621730382294,
      "grad_norm": 0.666873037815094,
      "learning_rate": 0.0001588087332729651,
      "loss": 0.349,
      "step": 10240
    },
    {
      "epoch": 20.6056338028169,
      "grad_norm": 0.6610987186431885,
      "learning_rate": 0.0001588047087232116,
      "loss": 0.3989,
      "step": 10241
    },
    {
      "epoch": 20.60764587525151,
      "grad_norm": 0.6463634371757507,
      "learning_rate": 0.0001588006841734581,
      "loss": 0.3619,
      "step": 10242
    },
    {
      "epoch": 20.609657947686117,
      "grad_norm": 0.6355234384536743,
      "learning_rate": 0.0001587966596237046,
      "loss": 0.3471,
      "step": 10243
    },
    {
      "epoch": 20.611670020120723,
      "grad_norm": 0.6536128520965576,
      "learning_rate": 0.0001587926350739511,
      "loss": 0.3824,
      "step": 10244
    },
    {
      "epoch": 20.613682092555333,
      "grad_norm": 0.6421621441841125,
      "learning_rate": 0.00015878861052419762,
      "loss": 0.3816,
      "step": 10245
    },
    {
      "epoch": 20.61569416498994,
      "grad_norm": 0.6295020580291748,
      "learning_rate": 0.00015878458597444413,
      "loss": 0.3753,
      "step": 10246
    },
    {
      "epoch": 20.617706237424546,
      "grad_norm": 0.6428349614143372,
      "learning_rate": 0.0001587805614246906,
      "loss": 0.3756,
      "step": 10247
    },
    {
      "epoch": 20.619718309859156,
      "grad_norm": 0.6573374271392822,
      "learning_rate": 0.00015877653687493712,
      "loss": 0.3708,
      "step": 10248
    },
    {
      "epoch": 20.621730382293762,
      "grad_norm": 0.6537702083587646,
      "learning_rate": 0.0001587725123251836,
      "loss": 0.3681,
      "step": 10249
    },
    {
      "epoch": 20.62374245472837,
      "grad_norm": 0.6242749094963074,
      "learning_rate": 0.00015876848777543015,
      "loss": 0.3491,
      "step": 10250
    },
    {
      "epoch": 20.62575452716298,
      "grad_norm": 0.6441621780395508,
      "learning_rate": 0.00015876446322567663,
      "loss": 0.3865,
      "step": 10251
    },
    {
      "epoch": 20.627766599597585,
      "grad_norm": 0.6589761972427368,
      "learning_rate": 0.00015876043867592315,
      "loss": 0.3908,
      "step": 10252
    },
    {
      "epoch": 20.62977867203219,
      "grad_norm": 0.6314073204994202,
      "learning_rate": 0.00015875641412616963,
      "loss": 0.3912,
      "step": 10253
    },
    {
      "epoch": 20.6317907444668,
      "grad_norm": 0.6292763352394104,
      "learning_rate": 0.00015875238957641614,
      "loss": 0.3792,
      "step": 10254
    },
    {
      "epoch": 20.633802816901408,
      "grad_norm": 0.6606830358505249,
      "learning_rate": 0.00015874836502666266,
      "loss": 0.3824,
      "step": 10255
    },
    {
      "epoch": 20.635814889336014,
      "grad_norm": 0.6485365033149719,
      "learning_rate": 0.00015874434047690917,
      "loss": 0.3665,
      "step": 10256
    },
    {
      "epoch": 20.637826961770624,
      "grad_norm": 0.6679793000221252,
      "learning_rate": 0.00015874031592715565,
      "loss": 0.3901,
      "step": 10257
    },
    {
      "epoch": 20.63983903420523,
      "grad_norm": 0.6420570611953735,
      "learning_rate": 0.00015873629137740217,
      "loss": 0.3814,
      "step": 10258
    },
    {
      "epoch": 20.641851106639837,
      "grad_norm": 0.6357447504997253,
      "learning_rate": 0.00015873226682764865,
      "loss": 0.3694,
      "step": 10259
    },
    {
      "epoch": 20.643863179074447,
      "grad_norm": 0.6332382559776306,
      "learning_rate": 0.0001587282422778952,
      "loss": 0.363,
      "step": 10260
    },
    {
      "epoch": 20.645875251509054,
      "grad_norm": 0.6548154354095459,
      "learning_rate": 0.00015872421772814168,
      "loss": 0.3975,
      "step": 10261
    },
    {
      "epoch": 20.647887323943664,
      "grad_norm": 0.670627236366272,
      "learning_rate": 0.0001587201931783882,
      "loss": 0.374,
      "step": 10262
    },
    {
      "epoch": 20.64989939637827,
      "grad_norm": 0.6612160801887512,
      "learning_rate": 0.00015871616862863467,
      "loss": 0.419,
      "step": 10263
    },
    {
      "epoch": 20.651911468812877,
      "grad_norm": 0.6581363677978516,
      "learning_rate": 0.00015871214407888118,
      "loss": 0.3702,
      "step": 10264
    },
    {
      "epoch": 20.653923541247487,
      "grad_norm": 0.5941540598869324,
      "learning_rate": 0.0001587081195291277,
      "loss": 0.3553,
      "step": 10265
    },
    {
      "epoch": 20.655935613682093,
      "grad_norm": 0.6591137647628784,
      "learning_rate": 0.0001587040949793742,
      "loss": 0.3967,
      "step": 10266
    },
    {
      "epoch": 20.6579476861167,
      "grad_norm": 0.644774854183197,
      "learning_rate": 0.0001587000704296207,
      "loss": 0.3764,
      "step": 10267
    },
    {
      "epoch": 20.65995975855131,
      "grad_norm": 0.662278950214386,
      "learning_rate": 0.0001586960458798672,
      "loss": 0.3897,
      "step": 10268
    },
    {
      "epoch": 20.661971830985916,
      "grad_norm": 0.6609356999397278,
      "learning_rate": 0.0001586920213301137,
      "loss": 0.4058,
      "step": 10269
    },
    {
      "epoch": 20.663983903420522,
      "grad_norm": 0.6219537854194641,
      "learning_rate": 0.0001586879967803602,
      "loss": 0.3641,
      "step": 10270
    },
    {
      "epoch": 20.665995975855132,
      "grad_norm": 0.6370337009429932,
      "learning_rate": 0.00015868397223060672,
      "loss": 0.3733,
      "step": 10271
    },
    {
      "epoch": 20.66800804828974,
      "grad_norm": 0.6514348387718201,
      "learning_rate": 0.0001586799476808532,
      "loss": 0.4042,
      "step": 10272
    },
    {
      "epoch": 20.670020120724345,
      "grad_norm": 0.6419581770896912,
      "learning_rate": 0.0001586759231310997,
      "loss": 0.3684,
      "step": 10273
    },
    {
      "epoch": 20.672032193158955,
      "grad_norm": 0.6667201519012451,
      "learning_rate": 0.00015867189858134623,
      "loss": 0.3601,
      "step": 10274
    },
    {
      "epoch": 20.67404426559356,
      "grad_norm": 0.6836665272712708,
      "learning_rate": 0.00015866787403159274,
      "loss": 0.359,
      "step": 10275
    },
    {
      "epoch": 20.676056338028168,
      "grad_norm": 0.6350740790367126,
      "learning_rate": 0.00015866384948183922,
      "loss": 0.4069,
      "step": 10276
    },
    {
      "epoch": 20.678068410462778,
      "grad_norm": 0.6348785758018494,
      "learning_rate": 0.00015865982493208574,
      "loss": 0.3832,
      "step": 10277
    },
    {
      "epoch": 20.680080482897385,
      "grad_norm": 0.6001418828964233,
      "learning_rate": 0.00015865580038233222,
      "loss": 0.3449,
      "step": 10278
    },
    {
      "epoch": 20.68209255533199,
      "grad_norm": 0.643522322177887,
      "learning_rate": 0.00015865177583257873,
      "loss": 0.361,
      "step": 10279
    },
    {
      "epoch": 20.6841046277666,
      "grad_norm": 0.6187612414360046,
      "learning_rate": 0.00015864775128282524,
      "loss": 0.3802,
      "step": 10280
    },
    {
      "epoch": 20.686116700201207,
      "grad_norm": 0.6202197670936584,
      "learning_rate": 0.00015864372673307176,
      "loss": 0.3539,
      "step": 10281
    },
    {
      "epoch": 20.688128772635814,
      "grad_norm": 0.6463501453399658,
      "learning_rate": 0.00015863970218331824,
      "loss": 0.394,
      "step": 10282
    },
    {
      "epoch": 20.690140845070424,
      "grad_norm": 0.6287118196487427,
      "learning_rate": 0.00015863567763356475,
      "loss": 0.3399,
      "step": 10283
    },
    {
      "epoch": 20.69215291750503,
      "grad_norm": 0.6678581237792969,
      "learning_rate": 0.00015863165308381124,
      "loss": 0.3734,
      "step": 10284
    },
    {
      "epoch": 20.694164989939637,
      "grad_norm": 0.6614579558372498,
      "learning_rate": 0.00015862762853405778,
      "loss": 0.378,
      "step": 10285
    },
    {
      "epoch": 20.696177062374247,
      "grad_norm": 0.6312954425811768,
      "learning_rate": 0.00015862360398430426,
      "loss": 0.3671,
      "step": 10286
    },
    {
      "epoch": 20.698189134808853,
      "grad_norm": 0.6291627287864685,
      "learning_rate": 0.00015861957943455078,
      "loss": 0.348,
      "step": 10287
    },
    {
      "epoch": 20.70020120724346,
      "grad_norm": 0.6309223771095276,
      "learning_rate": 0.00015861555488479726,
      "loss": 0.353,
      "step": 10288
    },
    {
      "epoch": 20.70221327967807,
      "grad_norm": 0.6534626483917236,
      "learning_rate": 0.00015861153033504377,
      "loss": 0.3801,
      "step": 10289
    },
    {
      "epoch": 20.704225352112676,
      "grad_norm": 0.624001145362854,
      "learning_rate": 0.00015860750578529029,
      "loss": 0.3314,
      "step": 10290
    },
    {
      "epoch": 20.706237424547282,
      "grad_norm": 0.6649353504180908,
      "learning_rate": 0.0001586034812355368,
      "loss": 0.4293,
      "step": 10291
    },
    {
      "epoch": 20.708249496981892,
      "grad_norm": 0.6562881469726562,
      "learning_rate": 0.00015859945668578328,
      "loss": 0.4034,
      "step": 10292
    },
    {
      "epoch": 20.7102615694165,
      "grad_norm": 0.6157868504524231,
      "learning_rate": 0.0001585954321360298,
      "loss": 0.3686,
      "step": 10293
    },
    {
      "epoch": 20.712273641851105,
      "grad_norm": 0.6246106624603271,
      "learning_rate": 0.00015859140758627628,
      "loss": 0.3939,
      "step": 10294
    },
    {
      "epoch": 20.714285714285715,
      "grad_norm": 0.6397445201873779,
      "learning_rate": 0.00015858738303652282,
      "loss": 0.3873,
      "step": 10295
    },
    {
      "epoch": 20.71629778672032,
      "grad_norm": 0.6836451292037964,
      "learning_rate": 0.0001585833584867693,
      "loss": 0.3585,
      "step": 10296
    },
    {
      "epoch": 20.718309859154928,
      "grad_norm": 0.6781359910964966,
      "learning_rate": 0.00015857933393701582,
      "loss": 0.3926,
      "step": 10297
    },
    {
      "epoch": 20.720321931589538,
      "grad_norm": 0.6392835974693298,
      "learning_rate": 0.0001585753093872623,
      "loss": 0.3881,
      "step": 10298
    },
    {
      "epoch": 20.722334004024145,
      "grad_norm": 0.6419488191604614,
      "learning_rate": 0.00015857128483750881,
      "loss": 0.3907,
      "step": 10299
    },
    {
      "epoch": 20.72434607645875,
      "grad_norm": 0.6377437710762024,
      "learning_rate": 0.00015856726028775533,
      "loss": 0.3955,
      "step": 10300
    },
    {
      "epoch": 20.72635814889336,
      "grad_norm": 0.6409062743186951,
      "learning_rate": 0.0001585632357380018,
      "loss": 0.3816,
      "step": 10301
    },
    {
      "epoch": 20.728370221327967,
      "grad_norm": 0.6769718527793884,
      "learning_rate": 0.00015855921118824832,
      "loss": 0.3995,
      "step": 10302
    },
    {
      "epoch": 20.730382293762574,
      "grad_norm": 0.6404618620872498,
      "learning_rate": 0.00015855518663849484,
      "loss": 0.3957,
      "step": 10303
    },
    {
      "epoch": 20.732394366197184,
      "grad_norm": 0.6571518778800964,
      "learning_rate": 0.00015855116208874132,
      "loss": 0.4036,
      "step": 10304
    },
    {
      "epoch": 20.73440643863179,
      "grad_norm": 0.5980136394500732,
      "learning_rate": 0.00015854713753898783,
      "loss": 0.3684,
      "step": 10305
    },
    {
      "epoch": 20.736418511066397,
      "grad_norm": 0.643828809261322,
      "learning_rate": 0.00015854311298923435,
      "loss": 0.3669,
      "step": 10306
    },
    {
      "epoch": 20.738430583501007,
      "grad_norm": 0.6496272087097168,
      "learning_rate": 0.00015853908843948083,
      "loss": 0.4135,
      "step": 10307
    },
    {
      "epoch": 20.740442655935613,
      "grad_norm": 0.6370492577552795,
      "learning_rate": 0.00015853506388972734,
      "loss": 0.3704,
      "step": 10308
    },
    {
      "epoch": 20.74245472837022,
      "grad_norm": 0.6327297687530518,
      "learning_rate": 0.00015853103933997383,
      "loss": 0.3578,
      "step": 10309
    },
    {
      "epoch": 20.74446680080483,
      "grad_norm": 0.6235727667808533,
      "learning_rate": 0.00015852701479022034,
      "loss": 0.3733,
      "step": 10310
    },
    {
      "epoch": 20.746478873239436,
      "grad_norm": 0.6466639041900635,
      "learning_rate": 0.00015852299024046685,
      "loss": 0.3953,
      "step": 10311
    },
    {
      "epoch": 20.748490945674043,
      "grad_norm": 0.6391831636428833,
      "learning_rate": 0.00015851896569071336,
      "loss": 0.3723,
      "step": 10312
    },
    {
      "epoch": 20.750503018108652,
      "grad_norm": 0.6727954745292664,
      "learning_rate": 0.00015851494114095985,
      "loss": 0.3746,
      "step": 10313
    },
    {
      "epoch": 20.75251509054326,
      "grad_norm": 0.656318724155426,
      "learning_rate": 0.00015851091659120636,
      "loss": 0.3799,
      "step": 10314
    },
    {
      "epoch": 20.754527162977865,
      "grad_norm": 0.6080541014671326,
      "learning_rate": 0.00015850689204145285,
      "loss": 0.3533,
      "step": 10315
    },
    {
      "epoch": 20.756539235412475,
      "grad_norm": 0.6271794438362122,
      "learning_rate": 0.0001585028674916994,
      "loss": 0.4075,
      "step": 10316
    },
    {
      "epoch": 20.758551307847082,
      "grad_norm": 0.6329393982887268,
      "learning_rate": 0.00015849884294194587,
      "loss": 0.339,
      "step": 10317
    },
    {
      "epoch": 20.760563380281692,
      "grad_norm": 0.6338741183280945,
      "learning_rate": 0.00015849481839219238,
      "loss": 0.3864,
      "step": 10318
    },
    {
      "epoch": 20.7625754527163,
      "grad_norm": 0.6509870886802673,
      "learning_rate": 0.00015849079384243887,
      "loss": 0.3949,
      "step": 10319
    },
    {
      "epoch": 20.764587525150905,
      "grad_norm": 0.6240851879119873,
      "learning_rate": 0.00015848676929268538,
      "loss": 0.3969,
      "step": 10320
    },
    {
      "epoch": 20.766599597585515,
      "grad_norm": 0.5966699719429016,
      "learning_rate": 0.0001584827447429319,
      "loss": 0.37,
      "step": 10321
    },
    {
      "epoch": 20.76861167002012,
      "grad_norm": 0.6346555352210999,
      "learning_rate": 0.0001584787201931784,
      "loss": 0.3676,
      "step": 10322
    },
    {
      "epoch": 20.770623742454728,
      "grad_norm": 0.616939902305603,
      "learning_rate": 0.0001584746956434249,
      "loss": 0.3613,
      "step": 10323
    },
    {
      "epoch": 20.772635814889338,
      "grad_norm": 0.6442338228225708,
      "learning_rate": 0.0001584706710936714,
      "loss": 0.3667,
      "step": 10324
    },
    {
      "epoch": 20.774647887323944,
      "grad_norm": 0.6050323843955994,
      "learning_rate": 0.0001584666465439179,
      "loss": 0.3778,
      "step": 10325
    },
    {
      "epoch": 20.77665995975855,
      "grad_norm": 0.6672195792198181,
      "learning_rate": 0.00015846262199416443,
      "loss": 0.3803,
      "step": 10326
    },
    {
      "epoch": 20.77867203219316,
      "grad_norm": 0.6759498119354248,
      "learning_rate": 0.0001584585974444109,
      "loss": 0.4021,
      "step": 10327
    },
    {
      "epoch": 20.780684104627767,
      "grad_norm": 0.6619129776954651,
      "learning_rate": 0.00015845457289465742,
      "loss": 0.3799,
      "step": 10328
    },
    {
      "epoch": 20.782696177062373,
      "grad_norm": 0.6614825129508972,
      "learning_rate": 0.0001584505483449039,
      "loss": 0.3673,
      "step": 10329
    },
    {
      "epoch": 20.784708249496983,
      "grad_norm": 0.6579188704490662,
      "learning_rate": 0.00015844652379515042,
      "loss": 0.352,
      "step": 10330
    },
    {
      "epoch": 20.78672032193159,
      "grad_norm": 0.6590856909751892,
      "learning_rate": 0.00015844249924539693,
      "loss": 0.4012,
      "step": 10331
    },
    {
      "epoch": 20.788732394366196,
      "grad_norm": 0.6773701310157776,
      "learning_rate": 0.00015843847469564345,
      "loss": 0.3749,
      "step": 10332
    },
    {
      "epoch": 20.790744466800806,
      "grad_norm": 0.6585274934768677,
      "learning_rate": 0.00015843445014588993,
      "loss": 0.3747,
      "step": 10333
    },
    {
      "epoch": 20.792756539235413,
      "grad_norm": 0.6429858207702637,
      "learning_rate": 0.00015843042559613644,
      "loss": 0.3793,
      "step": 10334
    },
    {
      "epoch": 20.79476861167002,
      "grad_norm": 0.6828256249427795,
      "learning_rate": 0.00015842640104638293,
      "loss": 0.3592,
      "step": 10335
    },
    {
      "epoch": 20.79678068410463,
      "grad_norm": 0.6728583574295044,
      "learning_rate": 0.00015842237649662944,
      "loss": 0.3686,
      "step": 10336
    },
    {
      "epoch": 20.798792756539235,
      "grad_norm": 0.689751923084259,
      "learning_rate": 0.00015841835194687595,
      "loss": 0.4002,
      "step": 10337
    },
    {
      "epoch": 20.800804828973842,
      "grad_norm": 0.692089855670929,
      "learning_rate": 0.00015841432739712247,
      "loss": 0.382,
      "step": 10338
    },
    {
      "epoch": 20.802816901408452,
      "grad_norm": 0.6422674655914307,
      "learning_rate": 0.00015841030284736895,
      "loss": 0.4076,
      "step": 10339
    },
    {
      "epoch": 20.80482897384306,
      "grad_norm": 0.6313271522521973,
      "learning_rate": 0.00015840627829761546,
      "loss": 0.3922,
      "step": 10340
    },
    {
      "epoch": 20.806841046277665,
      "grad_norm": 0.6243056058883667,
      "learning_rate": 0.00015840225374786198,
      "loss": 0.3925,
      "step": 10341
    },
    {
      "epoch": 20.808853118712275,
      "grad_norm": 0.6303446292877197,
      "learning_rate": 0.00015839822919810846,
      "loss": 0.3521,
      "step": 10342
    },
    {
      "epoch": 20.81086519114688,
      "grad_norm": 0.6471031904220581,
      "learning_rate": 0.00015839420464835497,
      "loss": 0.3896,
      "step": 10343
    },
    {
      "epoch": 20.812877263581488,
      "grad_norm": 0.6567786335945129,
      "learning_rate": 0.00015839018009860146,
      "loss": 0.4012,
      "step": 10344
    },
    {
      "epoch": 20.814889336016098,
      "grad_norm": 0.6793673634529114,
      "learning_rate": 0.00015838615554884797,
      "loss": 0.417,
      "step": 10345
    },
    {
      "epoch": 20.816901408450704,
      "grad_norm": 0.6818448901176453,
      "learning_rate": 0.00015838213099909448,
      "loss": 0.3903,
      "step": 10346
    },
    {
      "epoch": 20.81891348088531,
      "grad_norm": 0.6499441862106323,
      "learning_rate": 0.000158378106449341,
      "loss": 0.3743,
      "step": 10347
    },
    {
      "epoch": 20.82092555331992,
      "grad_norm": 0.6387280821800232,
      "learning_rate": 0.00015837408189958748,
      "loss": 0.3811,
      "step": 10348
    },
    {
      "epoch": 20.822937625754527,
      "grad_norm": 0.6140574216842651,
      "learning_rate": 0.000158370057349834,
      "loss": 0.3714,
      "step": 10349
    },
    {
      "epoch": 20.824949698189133,
      "grad_norm": 0.6482887864112854,
      "learning_rate": 0.00015836603280008048,
      "loss": 0.3901,
      "step": 10350
    },
    {
      "epoch": 20.826961770623743,
      "grad_norm": 0.615289568901062,
      "learning_rate": 0.00015836200825032702,
      "loss": 0.374,
      "step": 10351
    },
    {
      "epoch": 20.82897384305835,
      "grad_norm": 0.6615856885910034,
      "learning_rate": 0.0001583579837005735,
      "loss": 0.3722,
      "step": 10352
    },
    {
      "epoch": 20.830985915492956,
      "grad_norm": 0.6348168253898621,
      "learning_rate": 0.00015835395915082,
      "loss": 0.3849,
      "step": 10353
    },
    {
      "epoch": 20.832997987927566,
      "grad_norm": 0.6395126581192017,
      "learning_rate": 0.0001583499346010665,
      "loss": 0.3847,
      "step": 10354
    },
    {
      "epoch": 20.835010060362173,
      "grad_norm": 0.648194432258606,
      "learning_rate": 0.000158345910051313,
      "loss": 0.3752,
      "step": 10355
    },
    {
      "epoch": 20.83702213279678,
      "grad_norm": 0.6557753086090088,
      "learning_rate": 0.00015834188550155952,
      "loss": 0.3735,
      "step": 10356
    },
    {
      "epoch": 20.83903420523139,
      "grad_norm": 0.6262752413749695,
      "learning_rate": 0.00015833786095180604,
      "loss": 0.3864,
      "step": 10357
    },
    {
      "epoch": 20.841046277665995,
      "grad_norm": 0.6529794335365295,
      "learning_rate": 0.00015833383640205252,
      "loss": 0.3711,
      "step": 10358
    },
    {
      "epoch": 20.843058350100602,
      "grad_norm": 0.6428966522216797,
      "learning_rate": 0.00015832981185229903,
      "loss": 0.3707,
      "step": 10359
    },
    {
      "epoch": 20.845070422535212,
      "grad_norm": 0.6629635095596313,
      "learning_rate": 0.00015832578730254552,
      "loss": 0.3932,
      "step": 10360
    },
    {
      "epoch": 20.84708249496982,
      "grad_norm": 0.6353763341903687,
      "learning_rate": 0.00015832176275279206,
      "loss": 0.3545,
      "step": 10361
    },
    {
      "epoch": 20.84909456740443,
      "grad_norm": 0.6608523726463318,
      "learning_rate": 0.00015831773820303854,
      "loss": 0.3947,
      "step": 10362
    },
    {
      "epoch": 20.851106639839035,
      "grad_norm": 0.6309412717819214,
      "learning_rate": 0.00015831371365328505,
      "loss": 0.383,
      "step": 10363
    },
    {
      "epoch": 20.85311871227364,
      "grad_norm": 0.6362184286117554,
      "learning_rate": 0.00015830968910353154,
      "loss": 0.3821,
      "step": 10364
    },
    {
      "epoch": 20.85513078470825,
      "grad_norm": 0.6385937929153442,
      "learning_rate": 0.00015830566455377805,
      "loss": 0.3778,
      "step": 10365
    },
    {
      "epoch": 20.857142857142858,
      "grad_norm": 0.6402448415756226,
      "learning_rate": 0.00015830164000402456,
      "loss": 0.388,
      "step": 10366
    },
    {
      "epoch": 20.859154929577464,
      "grad_norm": 0.6049596071243286,
      "learning_rate": 0.00015829761545427108,
      "loss": 0.4043,
      "step": 10367
    },
    {
      "epoch": 20.861167002012074,
      "grad_norm": 0.5990254878997803,
      "learning_rate": 0.00015829359090451756,
      "loss": 0.3673,
      "step": 10368
    },
    {
      "epoch": 20.86317907444668,
      "grad_norm": 0.6466720700263977,
      "learning_rate": 0.00015828956635476407,
      "loss": 0.3597,
      "step": 10369
    },
    {
      "epoch": 20.865191146881287,
      "grad_norm": 0.6264626979827881,
      "learning_rate": 0.00015828554180501056,
      "loss": 0.406,
      "step": 10370
    },
    {
      "epoch": 20.867203219315897,
      "grad_norm": 0.6070539951324463,
      "learning_rate": 0.00015828151725525707,
      "loss": 0.3844,
      "step": 10371
    },
    {
      "epoch": 20.869215291750503,
      "grad_norm": 0.6230968832969666,
      "learning_rate": 0.00015827749270550358,
      "loss": 0.3818,
      "step": 10372
    },
    {
      "epoch": 20.87122736418511,
      "grad_norm": 0.6667260527610779,
      "learning_rate": 0.0001582734681557501,
      "loss": 0.411,
      "step": 10373
    },
    {
      "epoch": 20.87323943661972,
      "grad_norm": 0.6519621014595032,
      "learning_rate": 0.00015826944360599658,
      "loss": 0.3811,
      "step": 10374
    },
    {
      "epoch": 20.875251509054326,
      "grad_norm": 0.6383554339408875,
      "learning_rate": 0.0001582654190562431,
      "loss": 0.3834,
      "step": 10375
    },
    {
      "epoch": 20.877263581488933,
      "grad_norm": 0.6427074074745178,
      "learning_rate": 0.0001582613945064896,
      "loss": 0.3703,
      "step": 10376
    },
    {
      "epoch": 20.879275653923543,
      "grad_norm": 0.6526944041252136,
      "learning_rate": 0.0001582573699567361,
      "loss": 0.3964,
      "step": 10377
    },
    {
      "epoch": 20.88128772635815,
      "grad_norm": 0.6653400659561157,
      "learning_rate": 0.0001582533454069826,
      "loss": 0.3833,
      "step": 10378
    },
    {
      "epoch": 20.883299798792756,
      "grad_norm": 0.612633228302002,
      "learning_rate": 0.0001582493208572291,
      "loss": 0.3649,
      "step": 10379
    },
    {
      "epoch": 20.885311871227366,
      "grad_norm": 0.6762634515762329,
      "learning_rate": 0.0001582452963074756,
      "loss": 0.4015,
      "step": 10380
    },
    {
      "epoch": 20.887323943661972,
      "grad_norm": 0.6574019193649292,
      "learning_rate": 0.0001582412717577221,
      "loss": 0.4124,
      "step": 10381
    },
    {
      "epoch": 20.88933601609658,
      "grad_norm": 0.6484007835388184,
      "learning_rate": 0.00015823724720796862,
      "loss": 0.3792,
      "step": 10382
    },
    {
      "epoch": 20.89134808853119,
      "grad_norm": 0.6551673412322998,
      "learning_rate": 0.0001582332226582151,
      "loss": 0.3673,
      "step": 10383
    },
    {
      "epoch": 20.893360160965795,
      "grad_norm": 0.6347448229789734,
      "learning_rate": 0.00015822919810846162,
      "loss": 0.4076,
      "step": 10384
    },
    {
      "epoch": 20.8953722334004,
      "grad_norm": 0.6582870483398438,
      "learning_rate": 0.0001582251735587081,
      "loss": 0.3997,
      "step": 10385
    },
    {
      "epoch": 20.89738430583501,
      "grad_norm": 0.6504316926002502,
      "learning_rate": 0.00015822114900895465,
      "loss": 0.4049,
      "step": 10386
    },
    {
      "epoch": 20.899396378269618,
      "grad_norm": 0.6184023022651672,
      "learning_rate": 0.00015821712445920113,
      "loss": 0.4053,
      "step": 10387
    },
    {
      "epoch": 20.901408450704224,
      "grad_norm": 0.6450591683387756,
      "learning_rate": 0.00015821309990944764,
      "loss": 0.3831,
      "step": 10388
    },
    {
      "epoch": 20.903420523138834,
      "grad_norm": 0.6711007952690125,
      "learning_rate": 0.00015820907535969413,
      "loss": 0.4012,
      "step": 10389
    },
    {
      "epoch": 20.90543259557344,
      "grad_norm": 0.6184110045433044,
      "learning_rate": 0.00015820505080994064,
      "loss": 0.36,
      "step": 10390
    },
    {
      "epoch": 20.907444668008047,
      "grad_norm": 0.6507449150085449,
      "learning_rate": 0.00015820102626018715,
      "loss": 0.3866,
      "step": 10391
    },
    {
      "epoch": 20.909456740442657,
      "grad_norm": 0.6898977756500244,
      "learning_rate": 0.00015819700171043366,
      "loss": 0.4077,
      "step": 10392
    },
    {
      "epoch": 20.911468812877263,
      "grad_norm": 0.6584407091140747,
      "learning_rate": 0.00015819297716068015,
      "loss": 0.4171,
      "step": 10393
    },
    {
      "epoch": 20.91348088531187,
      "grad_norm": 0.6152172684669495,
      "learning_rate": 0.00015818895261092666,
      "loss": 0.3715,
      "step": 10394
    },
    {
      "epoch": 20.91549295774648,
      "grad_norm": 0.5796513557434082,
      "learning_rate": 0.00015818492806117315,
      "loss": 0.3666,
      "step": 10395
    },
    {
      "epoch": 20.917505030181086,
      "grad_norm": 0.6363822221755981,
      "learning_rate": 0.00015818090351141969,
      "loss": 0.3634,
      "step": 10396
    },
    {
      "epoch": 20.919517102615693,
      "grad_norm": 0.6480606198310852,
      "learning_rate": 0.00015817687896166617,
      "loss": 0.3913,
      "step": 10397
    },
    {
      "epoch": 20.921529175050303,
      "grad_norm": 0.6530114412307739,
      "learning_rate": 0.00015817285441191268,
      "loss": 0.3701,
      "step": 10398
    },
    {
      "epoch": 20.92354124748491,
      "grad_norm": 0.6490141153335571,
      "learning_rate": 0.00015816882986215917,
      "loss": 0.3959,
      "step": 10399
    },
    {
      "epoch": 20.925553319919516,
      "grad_norm": 0.6580320596694946,
      "learning_rate": 0.00015816480531240568,
      "loss": 0.3955,
      "step": 10400
    },
    {
      "epoch": 20.927565392354126,
      "grad_norm": 0.6261548399925232,
      "learning_rate": 0.0001581607807626522,
      "loss": 0.3703,
      "step": 10401
    },
    {
      "epoch": 20.929577464788732,
      "grad_norm": 0.6741929650306702,
      "learning_rate": 0.0001581567562128987,
      "loss": 0.4079,
      "step": 10402
    },
    {
      "epoch": 20.93158953722334,
      "grad_norm": 0.6201490759849548,
      "learning_rate": 0.0001581527316631452,
      "loss": 0.3676,
      "step": 10403
    },
    {
      "epoch": 20.93360160965795,
      "grad_norm": 0.6043313145637512,
      "learning_rate": 0.0001581487071133917,
      "loss": 0.3755,
      "step": 10404
    },
    {
      "epoch": 20.935613682092555,
      "grad_norm": 0.7023007869720459,
      "learning_rate": 0.0001581446825636382,
      "loss": 0.4262,
      "step": 10405
    },
    {
      "epoch": 20.93762575452716,
      "grad_norm": 0.6566973924636841,
      "learning_rate": 0.0001581406580138847,
      "loss": 0.3738,
      "step": 10406
    },
    {
      "epoch": 20.93963782696177,
      "grad_norm": 0.6807201504707336,
      "learning_rate": 0.0001581366334641312,
      "loss": 0.3944,
      "step": 10407
    },
    {
      "epoch": 20.941649899396378,
      "grad_norm": 0.6554458737373352,
      "learning_rate": 0.00015813260891437772,
      "loss": 0.3562,
      "step": 10408
    },
    {
      "epoch": 20.943661971830984,
      "grad_norm": 0.6304301023483276,
      "learning_rate": 0.0001581285843646242,
      "loss": 0.3866,
      "step": 10409
    },
    {
      "epoch": 20.945674044265594,
      "grad_norm": 0.667836606502533,
      "learning_rate": 0.00015812455981487072,
      "loss": 0.3923,
      "step": 10410
    },
    {
      "epoch": 20.9476861167002,
      "grad_norm": 0.5981630682945251,
      "learning_rate": 0.00015812053526511723,
      "loss": 0.3714,
      "step": 10411
    },
    {
      "epoch": 20.949698189134807,
      "grad_norm": 0.6309501528739929,
      "learning_rate": 0.00015811651071536372,
      "loss": 0.373,
      "step": 10412
    },
    {
      "epoch": 20.951710261569417,
      "grad_norm": 0.6723869442939758,
      "learning_rate": 0.00015811248616561023,
      "loss": 0.3872,
      "step": 10413
    },
    {
      "epoch": 20.953722334004024,
      "grad_norm": 0.6348802447319031,
      "learning_rate": 0.00015810846161585672,
      "loss": 0.4251,
      "step": 10414
    },
    {
      "epoch": 20.955734406438633,
      "grad_norm": 0.654401421546936,
      "learning_rate": 0.00015810443706610323,
      "loss": 0.3963,
      "step": 10415
    },
    {
      "epoch": 20.95774647887324,
      "grad_norm": 0.6539290547370911,
      "learning_rate": 0.00015810041251634974,
      "loss": 0.3844,
      "step": 10416
    },
    {
      "epoch": 20.959758551307846,
      "grad_norm": 0.675552487373352,
      "learning_rate": 0.00015809638796659625,
      "loss": 0.4076,
      "step": 10417
    },
    {
      "epoch": 20.961770623742456,
      "grad_norm": 0.6261074542999268,
      "learning_rate": 0.00015809236341684274,
      "loss": 0.3861,
      "step": 10418
    },
    {
      "epoch": 20.963782696177063,
      "grad_norm": 0.651609480381012,
      "learning_rate": 0.00015808833886708925,
      "loss": 0.4092,
      "step": 10419
    },
    {
      "epoch": 20.96579476861167,
      "grad_norm": 0.6345506310462952,
      "learning_rate": 0.00015808431431733574,
      "loss": 0.3674,
      "step": 10420
    },
    {
      "epoch": 20.96780684104628,
      "grad_norm": 0.6667433977127075,
      "learning_rate": 0.00015808028976758227,
      "loss": 0.388,
      "step": 10421
    },
    {
      "epoch": 20.969818913480886,
      "grad_norm": 0.6233676075935364,
      "learning_rate": 0.00015807626521782876,
      "loss": 0.374,
      "step": 10422
    },
    {
      "epoch": 20.971830985915492,
      "grad_norm": 0.6399162411689758,
      "learning_rate": 0.00015807224066807527,
      "loss": 0.4053,
      "step": 10423
    },
    {
      "epoch": 20.973843058350102,
      "grad_norm": 0.6373052597045898,
      "learning_rate": 0.00015806821611832176,
      "loss": 0.3908,
      "step": 10424
    },
    {
      "epoch": 20.97585513078471,
      "grad_norm": 0.6812615990638733,
      "learning_rate": 0.00015806419156856827,
      "loss": 0.3897,
      "step": 10425
    },
    {
      "epoch": 20.977867203219315,
      "grad_norm": 0.6484848856925964,
      "learning_rate": 0.00015806016701881478,
      "loss": 0.3949,
      "step": 10426
    },
    {
      "epoch": 20.979879275653925,
      "grad_norm": 0.6345784664154053,
      "learning_rate": 0.0001580561424690613,
      "loss": 0.3794,
      "step": 10427
    },
    {
      "epoch": 20.98189134808853,
      "grad_norm": 0.6110357046127319,
      "learning_rate": 0.00015805211791930778,
      "loss": 0.3868,
      "step": 10428
    },
    {
      "epoch": 20.983903420523138,
      "grad_norm": 0.6534637808799744,
      "learning_rate": 0.0001580480933695543,
      "loss": 0.3848,
      "step": 10429
    },
    {
      "epoch": 20.985915492957748,
      "grad_norm": 0.6479259729385376,
      "learning_rate": 0.00015804406881980078,
      "loss": 0.3824,
      "step": 10430
    },
    {
      "epoch": 20.987927565392354,
      "grad_norm": 0.6488195061683655,
      "learning_rate": 0.00015804004427004732,
      "loss": 0.3543,
      "step": 10431
    },
    {
      "epoch": 20.98993963782696,
      "grad_norm": 0.6396759152412415,
      "learning_rate": 0.0001580360197202938,
      "loss": 0.378,
      "step": 10432
    },
    {
      "epoch": 20.99195171026157,
      "grad_norm": 0.6604446172714233,
      "learning_rate": 0.0001580319951705403,
      "loss": 0.387,
      "step": 10433
    },
    {
      "epoch": 20.993963782696177,
      "grad_norm": 0.6264545321464539,
      "learning_rate": 0.0001580279706207868,
      "loss": 0.3872,
      "step": 10434
    },
    {
      "epoch": 20.995975855130784,
      "grad_norm": 0.6719357371330261,
      "learning_rate": 0.0001580239460710333,
      "loss": 0.3979,
      "step": 10435
    },
    {
      "epoch": 20.997987927565394,
      "grad_norm": 0.6632851958274841,
      "learning_rate": 0.00015801992152127982,
      "loss": 0.3961,
      "step": 10436
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.6577103137969971,
      "learning_rate": 0.00015801589697152633,
      "loss": 0.4154,
      "step": 10437
    },
    {
      "epoch": 21.0,
      "eval_loss": 0.9879242777824402,
      "eval_runtime": 49.8081,
      "eval_samples_per_second": 19.916,
      "eval_steps_per_second": 2.49,
      "step": 10437
    },
    {
      "epoch": 21.002012072434606,
      "grad_norm": 0.5787360072135925,
      "learning_rate": 0.00015801187242177282,
      "loss": 0.3221,
      "step": 10438
    },
    {
      "epoch": 21.004024144869216,
      "grad_norm": 0.5472237467765808,
      "learning_rate": 0.00015800784787201933,
      "loss": 0.3074,
      "step": 10439
    },
    {
      "epoch": 21.006036217303823,
      "grad_norm": 0.5908329486846924,
      "learning_rate": 0.00015800382332226582,
      "loss": 0.3252,
      "step": 10440
    },
    {
      "epoch": 21.00804828973843,
      "grad_norm": 0.5791902542114258,
      "learning_rate": 0.00015799979877251233,
      "loss": 0.3078,
      "step": 10441
    },
    {
      "epoch": 21.01006036217304,
      "grad_norm": 0.6301017999649048,
      "learning_rate": 0.00015799577422275884,
      "loss": 0.3358,
      "step": 10442
    },
    {
      "epoch": 21.012072434607646,
      "grad_norm": 0.65146404504776,
      "learning_rate": 0.00015799174967300535,
      "loss": 0.3004,
      "step": 10443
    },
    {
      "epoch": 21.014084507042252,
      "grad_norm": 0.6595565676689148,
      "learning_rate": 0.00015798772512325184,
      "loss": 0.3374,
      "step": 10444
    },
    {
      "epoch": 21.016096579476862,
      "grad_norm": 0.6769835948944092,
      "learning_rate": 0.00015798370057349835,
      "loss": 0.3383,
      "step": 10445
    },
    {
      "epoch": 21.01810865191147,
      "grad_norm": 0.6097155809402466,
      "learning_rate": 0.00015797967602374486,
      "loss": 0.3066,
      "step": 10446
    },
    {
      "epoch": 21.020120724346075,
      "grad_norm": 0.5973064303398132,
      "learning_rate": 0.00015797565147399135,
      "loss": 0.332,
      "step": 10447
    },
    {
      "epoch": 21.022132796780685,
      "grad_norm": 0.5927070379257202,
      "learning_rate": 0.00015797162692423786,
      "loss": 0.3248,
      "step": 10448
    },
    {
      "epoch": 21.02414486921529,
      "grad_norm": 0.5717093348503113,
      "learning_rate": 0.00015796760237448435,
      "loss": 0.309,
      "step": 10449
    },
    {
      "epoch": 21.026156941649898,
      "grad_norm": 0.6067897081375122,
      "learning_rate": 0.00015796357782473086,
      "loss": 0.334,
      "step": 10450
    },
    {
      "epoch": 21.028169014084508,
      "grad_norm": 0.6388777494430542,
      "learning_rate": 0.00015795955327497737,
      "loss": 0.3457,
      "step": 10451
    },
    {
      "epoch": 21.030181086519114,
      "grad_norm": 0.6155003309249878,
      "learning_rate": 0.00015795552872522388,
      "loss": 0.3181,
      "step": 10452
    },
    {
      "epoch": 21.03219315895372,
      "grad_norm": 0.5873265862464905,
      "learning_rate": 0.00015795150417547037,
      "loss": 0.2952,
      "step": 10453
    },
    {
      "epoch": 21.03420523138833,
      "grad_norm": 0.6339027285575867,
      "learning_rate": 0.00015794747962571688,
      "loss": 0.3189,
      "step": 10454
    },
    {
      "epoch": 21.036217303822937,
      "grad_norm": 0.6190780401229858,
      "learning_rate": 0.00015794345507596336,
      "loss": 0.2858,
      "step": 10455
    },
    {
      "epoch": 21.038229376257544,
      "grad_norm": 0.6298551559448242,
      "learning_rate": 0.0001579394305262099,
      "loss": 0.3179,
      "step": 10456
    },
    {
      "epoch": 21.040241448692154,
      "grad_norm": 0.6214793920516968,
      "learning_rate": 0.0001579354059764564,
      "loss": 0.3313,
      "step": 10457
    },
    {
      "epoch": 21.04225352112676,
      "grad_norm": 0.6273308396339417,
      "learning_rate": 0.0001579313814267029,
      "loss": 0.3086,
      "step": 10458
    },
    {
      "epoch": 21.044265593561367,
      "grad_norm": 0.6218445897102356,
      "learning_rate": 0.0001579273568769494,
      "loss": 0.3037,
      "step": 10459
    },
    {
      "epoch": 21.046277665995976,
      "grad_norm": 0.6041991114616394,
      "learning_rate": 0.0001579233323271959,
      "loss": 0.3184,
      "step": 10460
    },
    {
      "epoch": 21.048289738430583,
      "grad_norm": 0.6467341780662537,
      "learning_rate": 0.0001579193077774424,
      "loss": 0.306,
      "step": 10461
    },
    {
      "epoch": 21.050301810865193,
      "grad_norm": 0.6289189457893372,
      "learning_rate": 0.00015791528322768892,
      "loss": 0.3051,
      "step": 10462
    },
    {
      "epoch": 21.0523138832998,
      "grad_norm": 0.6269478797912598,
      "learning_rate": 0.0001579112586779354,
      "loss": 0.3146,
      "step": 10463
    },
    {
      "epoch": 21.054325955734406,
      "grad_norm": 0.6103598475456238,
      "learning_rate": 0.00015790723412818192,
      "loss": 0.3037,
      "step": 10464
    },
    {
      "epoch": 21.056338028169016,
      "grad_norm": 0.6598883867263794,
      "learning_rate": 0.0001579032095784284,
      "loss": 0.3518,
      "step": 10465
    },
    {
      "epoch": 21.058350100603622,
      "grad_norm": 0.6394757628440857,
      "learning_rate": 0.00015789918502867495,
      "loss": 0.3072,
      "step": 10466
    },
    {
      "epoch": 21.06036217303823,
      "grad_norm": 0.63572758436203,
      "learning_rate": 0.00015789516047892143,
      "loss": 0.2867,
      "step": 10467
    },
    {
      "epoch": 21.06237424547284,
      "grad_norm": 0.6156637668609619,
      "learning_rate": 0.00015789113592916794,
      "loss": 0.3271,
      "step": 10468
    },
    {
      "epoch": 21.064386317907445,
      "grad_norm": 0.6186680197715759,
      "learning_rate": 0.00015788711137941443,
      "loss": 0.3304,
      "step": 10469
    },
    {
      "epoch": 21.06639839034205,
      "grad_norm": 0.6055941581726074,
      "learning_rate": 0.00015788308682966094,
      "loss": 0.3158,
      "step": 10470
    },
    {
      "epoch": 21.06841046277666,
      "grad_norm": 0.6249728798866272,
      "learning_rate": 0.00015787906227990745,
      "loss": 0.334,
      "step": 10471
    },
    {
      "epoch": 21.070422535211268,
      "grad_norm": 0.6791443228721619,
      "learning_rate": 0.00015787503773015396,
      "loss": 0.3269,
      "step": 10472
    },
    {
      "epoch": 21.072434607645874,
      "grad_norm": 0.6338722109794617,
      "learning_rate": 0.00015787101318040045,
      "loss": 0.3052,
      "step": 10473
    },
    {
      "epoch": 21.074446680080484,
      "grad_norm": 0.6157063245773315,
      "learning_rate": 0.00015786698863064696,
      "loss": 0.3307,
      "step": 10474
    },
    {
      "epoch": 21.07645875251509,
      "grad_norm": 0.5866705179214478,
      "learning_rate": 0.00015786296408089345,
      "loss": 0.3125,
      "step": 10475
    },
    {
      "epoch": 21.078470824949697,
      "grad_norm": 0.608578085899353,
      "learning_rate": 0.00015785893953113996,
      "loss": 0.3319,
      "step": 10476
    },
    {
      "epoch": 21.080482897384307,
      "grad_norm": 0.620746374130249,
      "learning_rate": 0.00015785491498138647,
      "loss": 0.3034,
      "step": 10477
    },
    {
      "epoch": 21.082494969818914,
      "grad_norm": 0.7057660222053528,
      "learning_rate": 0.00015785089043163296,
      "loss": 0.3118,
      "step": 10478
    },
    {
      "epoch": 21.08450704225352,
      "grad_norm": 0.6430541276931763,
      "learning_rate": 0.00015784686588187947,
      "loss": 0.3203,
      "step": 10479
    },
    {
      "epoch": 21.08651911468813,
      "grad_norm": 0.6279340982437134,
      "learning_rate": 0.00015784284133212598,
      "loss": 0.2923,
      "step": 10480
    },
    {
      "epoch": 21.088531187122737,
      "grad_norm": 0.5978948473930359,
      "learning_rate": 0.0001578388167823725,
      "loss": 0.31,
      "step": 10481
    },
    {
      "epoch": 21.090543259557343,
      "grad_norm": 0.6007302403450012,
      "learning_rate": 0.00015783479223261898,
      "loss": 0.3229,
      "step": 10482
    },
    {
      "epoch": 21.092555331991953,
      "grad_norm": 0.613315224647522,
      "learning_rate": 0.0001578307676828655,
      "loss": 0.3092,
      "step": 10483
    },
    {
      "epoch": 21.09456740442656,
      "grad_norm": 0.6139834523200989,
      "learning_rate": 0.00015782674313311198,
      "loss": 0.3053,
      "step": 10484
    },
    {
      "epoch": 21.096579476861166,
      "grad_norm": 0.6155381202697754,
      "learning_rate": 0.0001578227185833585,
      "loss": 0.32,
      "step": 10485
    },
    {
      "epoch": 21.098591549295776,
      "grad_norm": 0.5908092856407166,
      "learning_rate": 0.000157818694033605,
      "loss": 0.2985,
      "step": 10486
    },
    {
      "epoch": 21.100603621730382,
      "grad_norm": 0.6731280088424683,
      "learning_rate": 0.0001578146694838515,
      "loss": 0.3034,
      "step": 10487
    },
    {
      "epoch": 21.10261569416499,
      "grad_norm": 0.6477617621421814,
      "learning_rate": 0.000157810644934098,
      "loss": 0.317,
      "step": 10488
    },
    {
      "epoch": 21.1046277665996,
      "grad_norm": 0.653077244758606,
      "learning_rate": 0.0001578066203843445,
      "loss": 0.3301,
      "step": 10489
    },
    {
      "epoch": 21.106639839034205,
      "grad_norm": 0.630093514919281,
      "learning_rate": 0.000157802595834591,
      "loss": 0.3222,
      "step": 10490
    },
    {
      "epoch": 21.10865191146881,
      "grad_norm": 0.6184636950492859,
      "learning_rate": 0.00015779857128483753,
      "loss": 0.3159,
      "step": 10491
    },
    {
      "epoch": 21.11066398390342,
      "grad_norm": 0.6189894080162048,
      "learning_rate": 0.00015779454673508402,
      "loss": 0.3429,
      "step": 10492
    },
    {
      "epoch": 21.112676056338028,
      "grad_norm": 0.6871920228004456,
      "learning_rate": 0.00015779052218533053,
      "loss": 0.3266,
      "step": 10493
    },
    {
      "epoch": 21.114688128772634,
      "grad_norm": 0.6481470465660095,
      "learning_rate": 0.00015778649763557702,
      "loss": 0.3325,
      "step": 10494
    },
    {
      "epoch": 21.116700201207244,
      "grad_norm": 0.6127797365188599,
      "learning_rate": 0.00015778247308582353,
      "loss": 0.3266,
      "step": 10495
    },
    {
      "epoch": 21.11871227364185,
      "grad_norm": 0.6200624704360962,
      "learning_rate": 0.00015777844853607004,
      "loss": 0.3008,
      "step": 10496
    },
    {
      "epoch": 21.120724346076457,
      "grad_norm": 0.6353716850280762,
      "learning_rate": 0.00015777442398631655,
      "loss": 0.3284,
      "step": 10497
    },
    {
      "epoch": 21.122736418511067,
      "grad_norm": 0.6300300359725952,
      "learning_rate": 0.00015777039943656304,
      "loss": 0.3115,
      "step": 10498
    },
    {
      "epoch": 21.124748490945674,
      "grad_norm": 0.6525741219520569,
      "learning_rate": 0.00015776637488680955,
      "loss": 0.3526,
      "step": 10499
    },
    {
      "epoch": 21.12676056338028,
      "grad_norm": 0.6051322221755981,
      "learning_rate": 0.00015776235033705604,
      "loss": 0.3001,
      "step": 10500
    },
    {
      "epoch": 21.12877263581489,
      "grad_norm": 0.6829505562782288,
      "learning_rate": 0.00015775832578730257,
      "loss": 0.3239,
      "step": 10501
    },
    {
      "epoch": 21.130784708249497,
      "grad_norm": 0.5992572903633118,
      "learning_rate": 0.00015775430123754906,
      "loss": 0.3078,
      "step": 10502
    },
    {
      "epoch": 21.132796780684103,
      "grad_norm": 0.6588045358657837,
      "learning_rate": 0.00015775027668779557,
      "loss": 0.3123,
      "step": 10503
    },
    {
      "epoch": 21.134808853118713,
      "grad_norm": 0.6208041906356812,
      "learning_rate": 0.00015774625213804206,
      "loss": 0.3226,
      "step": 10504
    },
    {
      "epoch": 21.13682092555332,
      "grad_norm": 0.5955015420913696,
      "learning_rate": 0.00015774222758828857,
      "loss": 0.2978,
      "step": 10505
    },
    {
      "epoch": 21.138832997987926,
      "grad_norm": 0.6625122427940369,
      "learning_rate": 0.00015773820303853508,
      "loss": 0.337,
      "step": 10506
    },
    {
      "epoch": 21.140845070422536,
      "grad_norm": 0.6271966695785522,
      "learning_rate": 0.0001577341784887816,
      "loss": 0.3316,
      "step": 10507
    },
    {
      "epoch": 21.142857142857142,
      "grad_norm": 0.6900834441184998,
      "learning_rate": 0.00015773015393902808,
      "loss": 0.3542,
      "step": 10508
    },
    {
      "epoch": 21.14486921529175,
      "grad_norm": 0.6603121757507324,
      "learning_rate": 0.0001577261293892746,
      "loss": 0.3468,
      "step": 10509
    },
    {
      "epoch": 21.14688128772636,
      "grad_norm": 0.6624199748039246,
      "learning_rate": 0.00015772210483952108,
      "loss": 0.3544,
      "step": 10510
    },
    {
      "epoch": 21.148893360160965,
      "grad_norm": 0.6470438241958618,
      "learning_rate": 0.0001577180802897676,
      "loss": 0.3151,
      "step": 10511
    },
    {
      "epoch": 21.15090543259557,
      "grad_norm": 0.6327998042106628,
      "learning_rate": 0.0001577140557400141,
      "loss": 0.3249,
      "step": 10512
    },
    {
      "epoch": 21.15291750503018,
      "grad_norm": 0.6374551653862,
      "learning_rate": 0.00015771003119026059,
      "loss": 0.3486,
      "step": 10513
    },
    {
      "epoch": 21.154929577464788,
      "grad_norm": 0.6775150895118713,
      "learning_rate": 0.0001577060066405071,
      "loss": 0.3588,
      "step": 10514
    },
    {
      "epoch": 21.156941649899398,
      "grad_norm": 0.6294099688529968,
      "learning_rate": 0.0001577019820907536,
      "loss": 0.3189,
      "step": 10515
    },
    {
      "epoch": 21.158953722334005,
      "grad_norm": 0.676668107509613,
      "learning_rate": 0.00015769795754100012,
      "loss": 0.3396,
      "step": 10516
    },
    {
      "epoch": 21.16096579476861,
      "grad_norm": 0.6121885180473328,
      "learning_rate": 0.0001576939329912466,
      "loss": 0.3531,
      "step": 10517
    },
    {
      "epoch": 21.16297786720322,
      "grad_norm": 0.6392073035240173,
      "learning_rate": 0.00015768990844149312,
      "loss": 0.3339,
      "step": 10518
    },
    {
      "epoch": 21.164989939637827,
      "grad_norm": 0.6389955878257751,
      "learning_rate": 0.0001576858838917396,
      "loss": 0.3383,
      "step": 10519
    },
    {
      "epoch": 21.167002012072434,
      "grad_norm": 0.617040753364563,
      "learning_rate": 0.00015768185934198612,
      "loss": 0.3213,
      "step": 10520
    },
    {
      "epoch": 21.169014084507044,
      "grad_norm": 0.6324586868286133,
      "learning_rate": 0.00015767783479223263,
      "loss": 0.3313,
      "step": 10521
    },
    {
      "epoch": 21.17102615694165,
      "grad_norm": 0.650944709777832,
      "learning_rate": 0.00015767381024247914,
      "loss": 0.3322,
      "step": 10522
    },
    {
      "epoch": 21.173038229376257,
      "grad_norm": 0.6155761480331421,
      "learning_rate": 0.00015766978569272563,
      "loss": 0.305,
      "step": 10523
    },
    {
      "epoch": 21.175050301810867,
      "grad_norm": 0.6876364350318909,
      "learning_rate": 0.00015766576114297214,
      "loss": 0.3473,
      "step": 10524
    },
    {
      "epoch": 21.177062374245473,
      "grad_norm": 0.6437659859657288,
      "learning_rate": 0.00015766173659321862,
      "loss": 0.3236,
      "step": 10525
    },
    {
      "epoch": 21.17907444668008,
      "grad_norm": 0.6317092776298523,
      "learning_rate": 0.00015765771204346516,
      "loss": 0.3156,
      "step": 10526
    },
    {
      "epoch": 21.18108651911469,
      "grad_norm": 0.6518024206161499,
      "learning_rate": 0.00015765368749371165,
      "loss": 0.3479,
      "step": 10527
    },
    {
      "epoch": 21.183098591549296,
      "grad_norm": 0.6470601558685303,
      "learning_rate": 0.00015764966294395816,
      "loss": 0.3418,
      "step": 10528
    },
    {
      "epoch": 21.185110663983902,
      "grad_norm": 0.6315720081329346,
      "learning_rate": 0.00015764563839420465,
      "loss": 0.3238,
      "step": 10529
    },
    {
      "epoch": 21.187122736418512,
      "grad_norm": 0.661467969417572,
      "learning_rate": 0.00015764161384445116,
      "loss": 0.3545,
      "step": 10530
    },
    {
      "epoch": 21.18913480885312,
      "grad_norm": 0.6491302847862244,
      "learning_rate": 0.00015763758929469767,
      "loss": 0.3557,
      "step": 10531
    },
    {
      "epoch": 21.191146881287725,
      "grad_norm": 0.6586462259292603,
      "learning_rate": 0.00015763356474494418,
      "loss": 0.3271,
      "step": 10532
    },
    {
      "epoch": 21.193158953722335,
      "grad_norm": 0.6518386602401733,
      "learning_rate": 0.00015762954019519067,
      "loss": 0.3231,
      "step": 10533
    },
    {
      "epoch": 21.19517102615694,
      "grad_norm": 0.6216747760772705,
      "learning_rate": 0.00015762551564543718,
      "loss": 0.3059,
      "step": 10534
    },
    {
      "epoch": 21.197183098591548,
      "grad_norm": 0.649355411529541,
      "learning_rate": 0.00015762149109568366,
      "loss": 0.3406,
      "step": 10535
    },
    {
      "epoch": 21.199195171026158,
      "grad_norm": 0.6214601993560791,
      "learning_rate": 0.0001576174665459302,
      "loss": 0.3345,
      "step": 10536
    },
    {
      "epoch": 21.201207243460765,
      "grad_norm": 0.6770251989364624,
      "learning_rate": 0.0001576134419961767,
      "loss": 0.3738,
      "step": 10537
    },
    {
      "epoch": 21.20321931589537,
      "grad_norm": 0.6516475081443787,
      "learning_rate": 0.0001576094174464232,
      "loss": 0.3275,
      "step": 10538
    },
    {
      "epoch": 21.20523138832998,
      "grad_norm": 0.6456672549247742,
      "learning_rate": 0.00015760539289666969,
      "loss": 0.3349,
      "step": 10539
    },
    {
      "epoch": 21.207243460764587,
      "grad_norm": 0.6675655841827393,
      "learning_rate": 0.0001576013683469162,
      "loss": 0.3538,
      "step": 10540
    },
    {
      "epoch": 21.209255533199194,
      "grad_norm": 0.6772715449333191,
      "learning_rate": 0.0001575973437971627,
      "loss": 0.337,
      "step": 10541
    },
    {
      "epoch": 21.211267605633804,
      "grad_norm": 0.6540195345878601,
      "learning_rate": 0.00015759331924740922,
      "loss": 0.3338,
      "step": 10542
    },
    {
      "epoch": 21.21327967806841,
      "grad_norm": 0.6540589928627014,
      "learning_rate": 0.0001575892946976557,
      "loss": 0.3326,
      "step": 10543
    },
    {
      "epoch": 21.215291750503017,
      "grad_norm": 0.6505635380744934,
      "learning_rate": 0.00015758527014790222,
      "loss": 0.3189,
      "step": 10544
    },
    {
      "epoch": 21.217303822937627,
      "grad_norm": 0.6595509052276611,
      "learning_rate": 0.0001575812455981487,
      "loss": 0.3425,
      "step": 10545
    },
    {
      "epoch": 21.219315895372233,
      "grad_norm": 0.6305087804794312,
      "learning_rate": 0.00015757722104839522,
      "loss": 0.3193,
      "step": 10546
    },
    {
      "epoch": 21.22132796780684,
      "grad_norm": 0.6944378018379211,
      "learning_rate": 0.00015757319649864173,
      "loss": 0.3257,
      "step": 10547
    },
    {
      "epoch": 21.22334004024145,
      "grad_norm": 0.6580755114555359,
      "learning_rate": 0.00015756917194888822,
      "loss": 0.3289,
      "step": 10548
    },
    {
      "epoch": 21.225352112676056,
      "grad_norm": 0.6955858469009399,
      "learning_rate": 0.00015756514739913473,
      "loss": 0.326,
      "step": 10549
    },
    {
      "epoch": 21.227364185110662,
      "grad_norm": 0.6613259315490723,
      "learning_rate": 0.00015756112284938124,
      "loss": 0.3408,
      "step": 10550
    },
    {
      "epoch": 21.229376257545272,
      "grad_norm": 0.631305456161499,
      "learning_rate": 0.00015755709829962772,
      "loss": 0.3036,
      "step": 10551
    },
    {
      "epoch": 21.23138832997988,
      "grad_norm": 0.6240108013153076,
      "learning_rate": 0.00015755307374987424,
      "loss": 0.3264,
      "step": 10552
    },
    {
      "epoch": 21.233400402414485,
      "grad_norm": 0.6424181461334229,
      "learning_rate": 0.00015754904920012075,
      "loss": 0.3282,
      "step": 10553
    },
    {
      "epoch": 21.235412474849095,
      "grad_norm": 0.629095196723938,
      "learning_rate": 0.00015754502465036723,
      "loss": 0.3228,
      "step": 10554
    },
    {
      "epoch": 21.2374245472837,
      "grad_norm": 0.6731550693511963,
      "learning_rate": 0.00015754100010061375,
      "loss": 0.3392,
      "step": 10555
    },
    {
      "epoch": 21.239436619718308,
      "grad_norm": 0.6705611944198608,
      "learning_rate": 0.00015753697555086023,
      "loss": 0.352,
      "step": 10556
    },
    {
      "epoch": 21.241448692152918,
      "grad_norm": 0.6779994964599609,
      "learning_rate": 0.00015753295100110677,
      "loss": 0.3444,
      "step": 10557
    },
    {
      "epoch": 21.243460764587525,
      "grad_norm": 0.6425980925559998,
      "learning_rate": 0.00015752892645135326,
      "loss": 0.3428,
      "step": 10558
    },
    {
      "epoch": 21.24547283702213,
      "grad_norm": 0.6709171533584595,
      "learning_rate": 0.00015752490190159977,
      "loss": 0.3574,
      "step": 10559
    },
    {
      "epoch": 21.24748490945674,
      "grad_norm": 0.6607544422149658,
      "learning_rate": 0.00015752087735184625,
      "loss": 0.3272,
      "step": 10560
    },
    {
      "epoch": 21.249496981891348,
      "grad_norm": 0.6805381774902344,
      "learning_rate": 0.00015751685280209277,
      "loss": 0.362,
      "step": 10561
    },
    {
      "epoch": 21.251509054325957,
      "grad_norm": 0.6650251746177673,
      "learning_rate": 0.00015751282825233928,
      "loss": 0.3552,
      "step": 10562
    },
    {
      "epoch": 21.253521126760564,
      "grad_norm": 0.6551395058631897,
      "learning_rate": 0.0001575088037025858,
      "loss": 0.325,
      "step": 10563
    },
    {
      "epoch": 21.25553319919517,
      "grad_norm": 0.6985772848129272,
      "learning_rate": 0.00015750477915283227,
      "loss": 0.3335,
      "step": 10564
    },
    {
      "epoch": 21.25754527162978,
      "grad_norm": 0.6783944368362427,
      "learning_rate": 0.0001575007546030788,
      "loss": 0.3717,
      "step": 10565
    },
    {
      "epoch": 21.259557344064387,
      "grad_norm": 0.6577588319778442,
      "learning_rate": 0.00015749673005332527,
      "loss": 0.318,
      "step": 10566
    },
    {
      "epoch": 21.261569416498993,
      "grad_norm": 0.6821459531784058,
      "learning_rate": 0.0001574927055035718,
      "loss": 0.3356,
      "step": 10567
    },
    {
      "epoch": 21.263581488933603,
      "grad_norm": 0.6587826609611511,
      "learning_rate": 0.0001574886809538183,
      "loss": 0.3409,
      "step": 10568
    },
    {
      "epoch": 21.26559356136821,
      "grad_norm": 0.7017121315002441,
      "learning_rate": 0.0001574846564040648,
      "loss": 0.3663,
      "step": 10569
    },
    {
      "epoch": 21.267605633802816,
      "grad_norm": 0.6730988025665283,
      "learning_rate": 0.0001574806318543113,
      "loss": 0.3054,
      "step": 10570
    },
    {
      "epoch": 21.269617706237426,
      "grad_norm": 0.6256486773490906,
      "learning_rate": 0.0001574766073045578,
      "loss": 0.3371,
      "step": 10571
    },
    {
      "epoch": 21.271629778672033,
      "grad_norm": 0.6300408244132996,
      "learning_rate": 0.00015747258275480432,
      "loss": 0.3345,
      "step": 10572
    },
    {
      "epoch": 21.27364185110664,
      "grad_norm": 0.6633731126785278,
      "learning_rate": 0.00015746855820505083,
      "loss": 0.3207,
      "step": 10573
    },
    {
      "epoch": 21.27565392354125,
      "grad_norm": 0.6420387029647827,
      "learning_rate": 0.00015746453365529732,
      "loss": 0.3199,
      "step": 10574
    },
    {
      "epoch": 21.277665995975855,
      "grad_norm": 0.6704420447349548,
      "learning_rate": 0.00015746050910554383,
      "loss": 0.3488,
      "step": 10575
    },
    {
      "epoch": 21.279678068410462,
      "grad_norm": 0.6530492901802063,
      "learning_rate": 0.0001574564845557903,
      "loss": 0.3533,
      "step": 10576
    },
    {
      "epoch": 21.281690140845072,
      "grad_norm": 0.6688225865364075,
      "learning_rate": 0.00015745246000603685,
      "loss": 0.3413,
      "step": 10577
    },
    {
      "epoch": 21.28370221327968,
      "grad_norm": 0.7170483469963074,
      "learning_rate": 0.00015744843545628334,
      "loss": 0.3371,
      "step": 10578
    },
    {
      "epoch": 21.285714285714285,
      "grad_norm": 0.6584850549697876,
      "learning_rate": 0.00015744441090652985,
      "loss": 0.3463,
      "step": 10579
    },
    {
      "epoch": 21.287726358148895,
      "grad_norm": 0.6721137762069702,
      "learning_rate": 0.00015744038635677633,
      "loss": 0.3433,
      "step": 10580
    },
    {
      "epoch": 21.2897384305835,
      "grad_norm": 0.7109542489051819,
      "learning_rate": 0.00015743636180702285,
      "loss": 0.3579,
      "step": 10581
    },
    {
      "epoch": 21.291750503018108,
      "grad_norm": 0.6484711170196533,
      "learning_rate": 0.00015743233725726936,
      "loss": 0.3286,
      "step": 10582
    },
    {
      "epoch": 21.293762575452718,
      "grad_norm": 0.6692442893981934,
      "learning_rate": 0.00015742831270751584,
      "loss": 0.3476,
      "step": 10583
    },
    {
      "epoch": 21.295774647887324,
      "grad_norm": 0.6702432036399841,
      "learning_rate": 0.00015742428815776236,
      "loss": 0.3432,
      "step": 10584
    },
    {
      "epoch": 21.29778672032193,
      "grad_norm": 0.6711817979812622,
      "learning_rate": 0.00015742026360800887,
      "loss": 0.3564,
      "step": 10585
    },
    {
      "epoch": 21.29979879275654,
      "grad_norm": 0.6603679060935974,
      "learning_rate": 0.00015741623905825535,
      "loss": 0.3117,
      "step": 10586
    },
    {
      "epoch": 21.301810865191147,
      "grad_norm": 0.6511550545692444,
      "learning_rate": 0.00015741221450850187,
      "loss": 0.3886,
      "step": 10587
    },
    {
      "epoch": 21.303822937625753,
      "grad_norm": 0.6545180082321167,
      "learning_rate": 0.00015740818995874838,
      "loss": 0.3357,
      "step": 10588
    },
    {
      "epoch": 21.305835010060363,
      "grad_norm": 0.6649202108383179,
      "learning_rate": 0.00015740416540899486,
      "loss": 0.3415,
      "step": 10589
    },
    {
      "epoch": 21.30784708249497,
      "grad_norm": 0.5823249220848083,
      "learning_rate": 0.00015740014085924138,
      "loss": 0.3108,
      "step": 10590
    },
    {
      "epoch": 21.309859154929576,
      "grad_norm": 0.6713405251502991,
      "learning_rate": 0.00015739611630948786,
      "loss": 0.3455,
      "step": 10591
    },
    {
      "epoch": 21.311871227364186,
      "grad_norm": 0.6517661213874817,
      "learning_rate": 0.0001573920917597344,
      "loss": 0.3362,
      "step": 10592
    },
    {
      "epoch": 21.313883299798793,
      "grad_norm": 0.652946949005127,
      "learning_rate": 0.00015738806720998089,
      "loss": 0.3429,
      "step": 10593
    },
    {
      "epoch": 21.3158953722334,
      "grad_norm": 0.6843669414520264,
      "learning_rate": 0.0001573840426602274,
      "loss": 0.3638,
      "step": 10594
    },
    {
      "epoch": 21.31790744466801,
      "grad_norm": 0.6554597020149231,
      "learning_rate": 0.00015738001811047388,
      "loss": 0.339,
      "step": 10595
    },
    {
      "epoch": 21.319919517102615,
      "grad_norm": 0.727908730506897,
      "learning_rate": 0.0001573759935607204,
      "loss": 0.3473,
      "step": 10596
    },
    {
      "epoch": 21.321931589537222,
      "grad_norm": 0.6580264568328857,
      "learning_rate": 0.0001573719690109669,
      "loss": 0.3467,
      "step": 10597
    },
    {
      "epoch": 21.323943661971832,
      "grad_norm": 0.6342746019363403,
      "learning_rate": 0.00015736794446121342,
      "loss": 0.3568,
      "step": 10598
    },
    {
      "epoch": 21.32595573440644,
      "grad_norm": 0.6619905829429626,
      "learning_rate": 0.0001573639199114599,
      "loss": 0.3251,
      "step": 10599
    },
    {
      "epoch": 21.327967806841045,
      "grad_norm": 0.6665283441543579,
      "learning_rate": 0.00015735989536170642,
      "loss": 0.3533,
      "step": 10600
    },
    {
      "epoch": 21.329979879275655,
      "grad_norm": 0.6281741261482239,
      "learning_rate": 0.0001573558708119529,
      "loss": 0.3258,
      "step": 10601
    },
    {
      "epoch": 21.33199195171026,
      "grad_norm": 0.6545296907424927,
      "learning_rate": 0.00015735184626219944,
      "loss": 0.3228,
      "step": 10602
    },
    {
      "epoch": 21.334004024144868,
      "grad_norm": 0.6741050481796265,
      "learning_rate": 0.00015734782171244593,
      "loss": 0.3483,
      "step": 10603
    },
    {
      "epoch": 21.336016096579478,
      "grad_norm": 0.6872515678405762,
      "learning_rate": 0.00015734379716269244,
      "loss": 0.3203,
      "step": 10604
    },
    {
      "epoch": 21.338028169014084,
      "grad_norm": 0.675523579120636,
      "learning_rate": 0.00015733977261293892,
      "loss": 0.357,
      "step": 10605
    },
    {
      "epoch": 21.34004024144869,
      "grad_norm": 0.6720902323722839,
      "learning_rate": 0.00015733574806318544,
      "loss": 0.3564,
      "step": 10606
    },
    {
      "epoch": 21.3420523138833,
      "grad_norm": 0.6386394500732422,
      "learning_rate": 0.00015733172351343195,
      "loss": 0.3216,
      "step": 10607
    },
    {
      "epoch": 21.344064386317907,
      "grad_norm": 0.6689925193786621,
      "learning_rate": 0.00015732769896367846,
      "loss": 0.3679,
      "step": 10608
    },
    {
      "epoch": 21.346076458752513,
      "grad_norm": 0.6533534526824951,
      "learning_rate": 0.00015732367441392495,
      "loss": 0.3283,
      "step": 10609
    },
    {
      "epoch": 21.348088531187123,
      "grad_norm": 0.712520956993103,
      "learning_rate": 0.00015731964986417146,
      "loss": 0.341,
      "step": 10610
    },
    {
      "epoch": 21.35010060362173,
      "grad_norm": 0.6751779913902283,
      "learning_rate": 0.00015731562531441794,
      "loss": 0.3333,
      "step": 10611
    },
    {
      "epoch": 21.352112676056336,
      "grad_norm": 0.6324125528335571,
      "learning_rate": 0.00015731160076466448,
      "loss": 0.3527,
      "step": 10612
    },
    {
      "epoch": 21.354124748490946,
      "grad_norm": 0.6535431146621704,
      "learning_rate": 0.00015730757621491097,
      "loss": 0.3669,
      "step": 10613
    },
    {
      "epoch": 21.356136820925553,
      "grad_norm": 0.6374139189720154,
      "learning_rate": 0.00015730355166515748,
      "loss": 0.3129,
      "step": 10614
    },
    {
      "epoch": 21.358148893360163,
      "grad_norm": 0.6822538375854492,
      "learning_rate": 0.00015729952711540396,
      "loss": 0.3749,
      "step": 10615
    },
    {
      "epoch": 21.36016096579477,
      "grad_norm": 0.6837489604949951,
      "learning_rate": 0.00015729550256565048,
      "loss": 0.3449,
      "step": 10616
    },
    {
      "epoch": 21.362173038229376,
      "grad_norm": 0.6473528742790222,
      "learning_rate": 0.000157291478015897,
      "loss": 0.3479,
      "step": 10617
    },
    {
      "epoch": 21.364185110663986,
      "grad_norm": 0.6407689452171326,
      "learning_rate": 0.00015728745346614347,
      "loss": 0.3382,
      "step": 10618
    },
    {
      "epoch": 21.366197183098592,
      "grad_norm": 0.6652231812477112,
      "learning_rate": 0.00015728342891638999,
      "loss": 0.3494,
      "step": 10619
    },
    {
      "epoch": 21.3682092555332,
      "grad_norm": 0.6944474577903748,
      "learning_rate": 0.00015727940436663647,
      "loss": 0.3559,
      "step": 10620
    },
    {
      "epoch": 21.37022132796781,
      "grad_norm": 0.6512615084648132,
      "learning_rate": 0.00015727537981688298,
      "loss": 0.343,
      "step": 10621
    },
    {
      "epoch": 21.372233400402415,
      "grad_norm": 0.6283923387527466,
      "learning_rate": 0.0001572713552671295,
      "loss": 0.3381,
      "step": 10622
    },
    {
      "epoch": 21.37424547283702,
      "grad_norm": 0.6797710657119751,
      "learning_rate": 0.000157267330717376,
      "loss": 0.358,
      "step": 10623
    },
    {
      "epoch": 21.37625754527163,
      "grad_norm": 0.6873477101325989,
      "learning_rate": 0.0001572633061676225,
      "loss": 0.3383,
      "step": 10624
    },
    {
      "epoch": 21.378269617706238,
      "grad_norm": 0.6792781352996826,
      "learning_rate": 0.000157259281617869,
      "loss": 0.3528,
      "step": 10625
    },
    {
      "epoch": 21.380281690140844,
      "grad_norm": 0.6760749816894531,
      "learning_rate": 0.0001572552570681155,
      "loss": 0.3085,
      "step": 10626
    },
    {
      "epoch": 21.382293762575454,
      "grad_norm": 0.6898311376571655,
      "learning_rate": 0.00015725123251836203,
      "loss": 0.3315,
      "step": 10627
    },
    {
      "epoch": 21.38430583501006,
      "grad_norm": 0.6436765789985657,
      "learning_rate": 0.00015724720796860851,
      "loss": 0.3244,
      "step": 10628
    },
    {
      "epoch": 21.386317907444667,
      "grad_norm": 0.675089955329895,
      "learning_rate": 0.00015724318341885503,
      "loss": 0.3518,
      "step": 10629
    },
    {
      "epoch": 21.388329979879277,
      "grad_norm": 0.6861754655838013,
      "learning_rate": 0.0001572391588691015,
      "loss": 0.3583,
      "step": 10630
    },
    {
      "epoch": 21.390342052313883,
      "grad_norm": 0.7068761587142944,
      "learning_rate": 0.00015723513431934802,
      "loss": 0.3763,
      "step": 10631
    },
    {
      "epoch": 21.39235412474849,
      "grad_norm": 0.6675922274589539,
      "learning_rate": 0.00015723110976959454,
      "loss": 0.38,
      "step": 10632
    },
    {
      "epoch": 21.3943661971831,
      "grad_norm": 0.667874813079834,
      "learning_rate": 0.00015722708521984105,
      "loss": 0.3663,
      "step": 10633
    },
    {
      "epoch": 21.396378269617706,
      "grad_norm": 0.6838729977607727,
      "learning_rate": 0.00015722306067008753,
      "loss": 0.3204,
      "step": 10634
    },
    {
      "epoch": 21.398390342052313,
      "grad_norm": 0.6711393594741821,
      "learning_rate": 0.00015721903612033405,
      "loss": 0.3531,
      "step": 10635
    },
    {
      "epoch": 21.400402414486923,
      "grad_norm": 0.6953645944595337,
      "learning_rate": 0.00015721501157058053,
      "loss": 0.3396,
      "step": 10636
    },
    {
      "epoch": 21.40241448692153,
      "grad_norm": 0.6760970950126648,
      "learning_rate": 0.00015721098702082707,
      "loss": 0.354,
      "step": 10637
    },
    {
      "epoch": 21.404426559356136,
      "grad_norm": 0.7152876853942871,
      "learning_rate": 0.00015720696247107356,
      "loss": 0.3702,
      "step": 10638
    },
    {
      "epoch": 21.406438631790746,
      "grad_norm": 0.6378909945487976,
      "learning_rate": 0.00015720293792132007,
      "loss": 0.3279,
      "step": 10639
    },
    {
      "epoch": 21.408450704225352,
      "grad_norm": 0.7203442454338074,
      "learning_rate": 0.00015719891337156655,
      "loss": 0.3146,
      "step": 10640
    },
    {
      "epoch": 21.41046277665996,
      "grad_norm": 0.6986491084098816,
      "learning_rate": 0.00015719488882181307,
      "loss": 0.3949,
      "step": 10641
    },
    {
      "epoch": 21.41247484909457,
      "grad_norm": 0.662339448928833,
      "learning_rate": 0.00015719086427205958,
      "loss": 0.3409,
      "step": 10642
    },
    {
      "epoch": 21.414486921529175,
      "grad_norm": 0.6283078193664551,
      "learning_rate": 0.0001571868397223061,
      "loss": 0.3455,
      "step": 10643
    },
    {
      "epoch": 21.41649899396378,
      "grad_norm": 0.6868119835853577,
      "learning_rate": 0.00015718281517255257,
      "loss": 0.3593,
      "step": 10644
    },
    {
      "epoch": 21.41851106639839,
      "grad_norm": 0.6832136511802673,
      "learning_rate": 0.0001571787906227991,
      "loss": 0.3783,
      "step": 10645
    },
    {
      "epoch": 21.420523138832998,
      "grad_norm": 0.6479544043540955,
      "learning_rate": 0.00015717476607304557,
      "loss": 0.3502,
      "step": 10646
    },
    {
      "epoch": 21.422535211267604,
      "grad_norm": 0.6892265677452087,
      "learning_rate": 0.00015717074152329208,
      "loss": 0.3427,
      "step": 10647
    },
    {
      "epoch": 21.424547283702214,
      "grad_norm": 0.6992242336273193,
      "learning_rate": 0.0001571667169735386,
      "loss": 0.373,
      "step": 10648
    },
    {
      "epoch": 21.42655935613682,
      "grad_norm": 0.7008114457130432,
      "learning_rate": 0.0001571626924237851,
      "loss": 0.3424,
      "step": 10649
    },
    {
      "epoch": 21.428571428571427,
      "grad_norm": 0.6729255318641663,
      "learning_rate": 0.0001571586678740316,
      "loss": 0.3414,
      "step": 10650
    },
    {
      "epoch": 21.430583501006037,
      "grad_norm": 0.6970345377922058,
      "learning_rate": 0.0001571546433242781,
      "loss": 0.3336,
      "step": 10651
    },
    {
      "epoch": 21.432595573440643,
      "grad_norm": 0.6536749005317688,
      "learning_rate": 0.00015715061877452462,
      "loss": 0.3639,
      "step": 10652
    },
    {
      "epoch": 21.43460764587525,
      "grad_norm": 0.6741597652435303,
      "learning_rate": 0.0001571465942247711,
      "loss": 0.3292,
      "step": 10653
    },
    {
      "epoch": 21.43661971830986,
      "grad_norm": 0.6322081089019775,
      "learning_rate": 0.00015714256967501762,
      "loss": 0.3459,
      "step": 10654
    },
    {
      "epoch": 21.438631790744466,
      "grad_norm": 0.6863152980804443,
      "learning_rate": 0.0001571385451252641,
      "loss": 0.3572,
      "step": 10655
    },
    {
      "epoch": 21.440643863179073,
      "grad_norm": 0.6776425242424011,
      "learning_rate": 0.0001571345205755106,
      "loss": 0.3615,
      "step": 10656
    },
    {
      "epoch": 21.442655935613683,
      "grad_norm": 0.6556669473648071,
      "learning_rate": 0.00015713049602575713,
      "loss": 0.3434,
      "step": 10657
    },
    {
      "epoch": 21.44466800804829,
      "grad_norm": 0.6870164275169373,
      "learning_rate": 0.00015712647147600364,
      "loss": 0.3528,
      "step": 10658
    },
    {
      "epoch": 21.446680080482896,
      "grad_norm": 0.7236956357955933,
      "learning_rate": 0.00015712244692625012,
      "loss": 0.3608,
      "step": 10659
    },
    {
      "epoch": 21.448692152917506,
      "grad_norm": 0.6378389596939087,
      "learning_rate": 0.00015711842237649663,
      "loss": 0.3478,
      "step": 10660
    },
    {
      "epoch": 21.450704225352112,
      "grad_norm": 0.6960129141807556,
      "learning_rate": 0.00015711439782674312,
      "loss": 0.3584,
      "step": 10661
    },
    {
      "epoch": 21.452716297786722,
      "grad_norm": 0.7130511999130249,
      "learning_rate": 0.00015711037327698966,
      "loss": 0.3569,
      "step": 10662
    },
    {
      "epoch": 21.45472837022133,
      "grad_norm": 0.640358030796051,
      "learning_rate": 0.00015710634872723614,
      "loss": 0.3678,
      "step": 10663
    },
    {
      "epoch": 21.456740442655935,
      "grad_norm": 0.645683765411377,
      "learning_rate": 0.00015710232417748266,
      "loss": 0.3671,
      "step": 10664
    },
    {
      "epoch": 21.458752515090545,
      "grad_norm": 0.6532497406005859,
      "learning_rate": 0.00015709829962772914,
      "loss": 0.3173,
      "step": 10665
    },
    {
      "epoch": 21.46076458752515,
      "grad_norm": 0.6679643988609314,
      "learning_rate": 0.00015709427507797565,
      "loss": 0.3567,
      "step": 10666
    },
    {
      "epoch": 21.462776659959758,
      "grad_norm": 0.6687288880348206,
      "learning_rate": 0.00015709025052822217,
      "loss": 0.359,
      "step": 10667
    },
    {
      "epoch": 21.464788732394368,
      "grad_norm": 0.6615095138549805,
      "learning_rate": 0.00015708622597846868,
      "loss": 0.3472,
      "step": 10668
    },
    {
      "epoch": 21.466800804828974,
      "grad_norm": 0.6637873649597168,
      "learning_rate": 0.00015708220142871516,
      "loss": 0.3502,
      "step": 10669
    },
    {
      "epoch": 21.46881287726358,
      "grad_norm": 0.6407297849655151,
      "learning_rate": 0.00015707817687896168,
      "loss": 0.3553,
      "step": 10670
    },
    {
      "epoch": 21.47082494969819,
      "grad_norm": 0.6957241892814636,
      "learning_rate": 0.00015707415232920816,
      "loss": 0.3801,
      "step": 10671
    },
    {
      "epoch": 21.472837022132797,
      "grad_norm": 0.6770107746124268,
      "learning_rate": 0.0001570701277794547,
      "loss": 0.3676,
      "step": 10672
    },
    {
      "epoch": 21.474849094567404,
      "grad_norm": 0.6630766987800598,
      "learning_rate": 0.00015706610322970119,
      "loss": 0.3326,
      "step": 10673
    },
    {
      "epoch": 21.476861167002014,
      "grad_norm": 0.6831535696983337,
      "learning_rate": 0.0001570620786799477,
      "loss": 0.3894,
      "step": 10674
    },
    {
      "epoch": 21.47887323943662,
      "grad_norm": 0.6626226902008057,
      "learning_rate": 0.00015705805413019418,
      "loss": 0.3706,
      "step": 10675
    },
    {
      "epoch": 21.480885311871226,
      "grad_norm": 0.6270767450332642,
      "learning_rate": 0.0001570540295804407,
      "loss": 0.3381,
      "step": 10676
    },
    {
      "epoch": 21.482897384305836,
      "grad_norm": 0.6678076386451721,
      "learning_rate": 0.0001570500050306872,
      "loss": 0.3343,
      "step": 10677
    },
    {
      "epoch": 21.484909456740443,
      "grad_norm": 0.6540224552154541,
      "learning_rate": 0.00015704598048093372,
      "loss": 0.3545,
      "step": 10678
    },
    {
      "epoch": 21.48692152917505,
      "grad_norm": 0.6590915322303772,
      "learning_rate": 0.0001570419559311802,
      "loss": 0.3489,
      "step": 10679
    },
    {
      "epoch": 21.48893360160966,
      "grad_norm": 0.7097570300102234,
      "learning_rate": 0.00015703793138142672,
      "loss": 0.3785,
      "step": 10680
    },
    {
      "epoch": 21.490945674044266,
      "grad_norm": 0.6886107325553894,
      "learning_rate": 0.0001570339068316732,
      "loss": 0.3702,
      "step": 10681
    },
    {
      "epoch": 21.492957746478872,
      "grad_norm": 0.6843357086181641,
      "learning_rate": 0.00015702988228191971,
      "loss": 0.3737,
      "step": 10682
    },
    {
      "epoch": 21.494969818913482,
      "grad_norm": 0.6792058348655701,
      "learning_rate": 0.00015702585773216623,
      "loss": 0.3809,
      "step": 10683
    },
    {
      "epoch": 21.49698189134809,
      "grad_norm": 0.6458714604377747,
      "learning_rate": 0.00015702183318241274,
      "loss": 0.3506,
      "step": 10684
    },
    {
      "epoch": 21.498993963782695,
      "grad_norm": 0.6290184259414673,
      "learning_rate": 0.00015701780863265922,
      "loss": 0.3602,
      "step": 10685
    },
    {
      "epoch": 21.501006036217305,
      "grad_norm": 0.6435966491699219,
      "learning_rate": 0.00015701378408290574,
      "loss": 0.3689,
      "step": 10686
    },
    {
      "epoch": 21.50301810865191,
      "grad_norm": 0.6652573347091675,
      "learning_rate": 0.00015700975953315225,
      "loss": 0.3616,
      "step": 10687
    },
    {
      "epoch": 21.505030181086518,
      "grad_norm": 0.6683009266853333,
      "learning_rate": 0.00015700573498339873,
      "loss": 0.3522,
      "step": 10688
    },
    {
      "epoch": 21.507042253521128,
      "grad_norm": 0.6865572333335876,
      "learning_rate": 0.00015700171043364524,
      "loss": 0.355,
      "step": 10689
    },
    {
      "epoch": 21.509054325955734,
      "grad_norm": 0.6669847369194031,
      "learning_rate": 0.00015699768588389173,
      "loss": 0.3423,
      "step": 10690
    },
    {
      "epoch": 21.51106639839034,
      "grad_norm": 0.6979323625564575,
      "learning_rate": 0.00015699366133413824,
      "loss": 0.3713,
      "step": 10691
    },
    {
      "epoch": 21.51307847082495,
      "grad_norm": 0.6854338049888611,
      "learning_rate": 0.00015698963678438475,
      "loss": 0.3562,
      "step": 10692
    },
    {
      "epoch": 21.515090543259557,
      "grad_norm": 0.6724552512168884,
      "learning_rate": 0.00015698561223463127,
      "loss": 0.3741,
      "step": 10693
    },
    {
      "epoch": 21.517102615694164,
      "grad_norm": 0.7077024579048157,
      "learning_rate": 0.00015698158768487775,
      "loss": 0.3353,
      "step": 10694
    },
    {
      "epoch": 21.519114688128774,
      "grad_norm": 0.6556420922279358,
      "learning_rate": 0.00015697756313512426,
      "loss": 0.3229,
      "step": 10695
    },
    {
      "epoch": 21.52112676056338,
      "grad_norm": 0.6936045289039612,
      "learning_rate": 0.00015697353858537075,
      "loss": 0.3763,
      "step": 10696
    },
    {
      "epoch": 21.523138832997986,
      "grad_norm": 0.6679341793060303,
      "learning_rate": 0.0001569695140356173,
      "loss": 0.3632,
      "step": 10697
    },
    {
      "epoch": 21.525150905432596,
      "grad_norm": 0.6329892873764038,
      "learning_rate": 0.00015696548948586377,
      "loss": 0.3569,
      "step": 10698
    },
    {
      "epoch": 21.527162977867203,
      "grad_norm": 0.7081050276756287,
      "learning_rate": 0.00015696146493611029,
      "loss": 0.382,
      "step": 10699
    },
    {
      "epoch": 21.52917505030181,
      "grad_norm": 0.6670721173286438,
      "learning_rate": 0.00015695744038635677,
      "loss": 0.3383,
      "step": 10700
    },
    {
      "epoch": 21.53118712273642,
      "grad_norm": 0.6719807386398315,
      "learning_rate": 0.00015695341583660328,
      "loss": 0.3482,
      "step": 10701
    },
    {
      "epoch": 21.533199195171026,
      "grad_norm": 0.7008523344993591,
      "learning_rate": 0.0001569493912868498,
      "loss": 0.3465,
      "step": 10702
    },
    {
      "epoch": 21.535211267605632,
      "grad_norm": 0.6662424802780151,
      "learning_rate": 0.0001569453667370963,
      "loss": 0.3519,
      "step": 10703
    },
    {
      "epoch": 21.537223340040242,
      "grad_norm": 0.7017675042152405,
      "learning_rate": 0.0001569413421873428,
      "loss": 0.3324,
      "step": 10704
    },
    {
      "epoch": 21.53923541247485,
      "grad_norm": 0.6501271724700928,
      "learning_rate": 0.0001569373176375893,
      "loss": 0.3335,
      "step": 10705
    },
    {
      "epoch": 21.541247484909455,
      "grad_norm": 0.6874621510505676,
      "learning_rate": 0.0001569332930878358,
      "loss": 0.3623,
      "step": 10706
    },
    {
      "epoch": 21.543259557344065,
      "grad_norm": 0.6906274557113647,
      "learning_rate": 0.00015692926853808233,
      "loss": 0.3462,
      "step": 10707
    },
    {
      "epoch": 21.54527162977867,
      "grad_norm": 0.663257896900177,
      "learning_rate": 0.00015692524398832881,
      "loss": 0.3461,
      "step": 10708
    },
    {
      "epoch": 21.547283702213278,
      "grad_norm": 0.6505257487297058,
      "learning_rate": 0.00015692121943857533,
      "loss": 0.3432,
      "step": 10709
    },
    {
      "epoch": 21.549295774647888,
      "grad_norm": 0.7046090960502625,
      "learning_rate": 0.0001569171948888218,
      "loss": 0.3558,
      "step": 10710
    },
    {
      "epoch": 21.551307847082494,
      "grad_norm": 0.7507506608963013,
      "learning_rate": 0.00015691317033906832,
      "loss": 0.3532,
      "step": 10711
    },
    {
      "epoch": 21.5533199195171,
      "grad_norm": 0.6353752613067627,
      "learning_rate": 0.00015690914578931484,
      "loss": 0.3212,
      "step": 10712
    },
    {
      "epoch": 21.55533199195171,
      "grad_norm": 0.631951630115509,
      "learning_rate": 0.00015690512123956135,
      "loss": 0.3192,
      "step": 10713
    },
    {
      "epoch": 21.557344064386317,
      "grad_norm": 0.647981584072113,
      "learning_rate": 0.00015690109668980783,
      "loss": 0.348,
      "step": 10714
    },
    {
      "epoch": 21.559356136820927,
      "grad_norm": 0.6310216784477234,
      "learning_rate": 0.00015689707214005435,
      "loss": 0.3716,
      "step": 10715
    },
    {
      "epoch": 21.561368209255534,
      "grad_norm": 0.6960569620132446,
      "learning_rate": 0.00015689304759030083,
      "loss": 0.3285,
      "step": 10716
    },
    {
      "epoch": 21.56338028169014,
      "grad_norm": 0.6500499248504639,
      "learning_rate": 0.00015688902304054734,
      "loss": 0.3716,
      "step": 10717
    },
    {
      "epoch": 21.56539235412475,
      "grad_norm": 0.6571601033210754,
      "learning_rate": 0.00015688499849079386,
      "loss": 0.3541,
      "step": 10718
    },
    {
      "epoch": 21.567404426559357,
      "grad_norm": 0.6196706891059875,
      "learning_rate": 0.00015688097394104037,
      "loss": 0.3191,
      "step": 10719
    },
    {
      "epoch": 21.569416498993963,
      "grad_norm": 0.6418238282203674,
      "learning_rate": 0.00015687694939128685,
      "loss": 0.3483,
      "step": 10720
    },
    {
      "epoch": 21.571428571428573,
      "grad_norm": 0.673262894153595,
      "learning_rate": 0.00015687292484153336,
      "loss": 0.3298,
      "step": 10721
    },
    {
      "epoch": 21.57344064386318,
      "grad_norm": 0.6794801950454712,
      "learning_rate": 0.00015686890029177988,
      "loss": 0.3554,
      "step": 10722
    },
    {
      "epoch": 21.575452716297786,
      "grad_norm": 0.6741250157356262,
      "learning_rate": 0.00015686487574202636,
      "loss": 0.381,
      "step": 10723
    },
    {
      "epoch": 21.577464788732396,
      "grad_norm": 0.6725693345069885,
      "learning_rate": 0.00015686085119227287,
      "loss": 0.3513,
      "step": 10724
    },
    {
      "epoch": 21.579476861167002,
      "grad_norm": 0.6505621075630188,
      "learning_rate": 0.00015685682664251936,
      "loss": 0.3585,
      "step": 10725
    },
    {
      "epoch": 21.58148893360161,
      "grad_norm": 0.6682150959968567,
      "learning_rate": 0.00015685280209276587,
      "loss": 0.3876,
      "step": 10726
    },
    {
      "epoch": 21.58350100603622,
      "grad_norm": 0.6567766070365906,
      "learning_rate": 0.00015684877754301238,
      "loss": 0.3735,
      "step": 10727
    },
    {
      "epoch": 21.585513078470825,
      "grad_norm": 0.6503377556800842,
      "learning_rate": 0.0001568447529932589,
      "loss": 0.3555,
      "step": 10728
    },
    {
      "epoch": 21.58752515090543,
      "grad_norm": 0.6734160780906677,
      "learning_rate": 0.00015684072844350538,
      "loss": 0.3632,
      "step": 10729
    },
    {
      "epoch": 21.58953722334004,
      "grad_norm": 0.7344443798065186,
      "learning_rate": 0.0001568367038937519,
      "loss": 0.3914,
      "step": 10730
    },
    {
      "epoch": 21.591549295774648,
      "grad_norm": 0.6424832344055176,
      "learning_rate": 0.00015683267934399838,
      "loss": 0.3517,
      "step": 10731
    },
    {
      "epoch": 21.593561368209254,
      "grad_norm": 0.6460546255111694,
      "learning_rate": 0.00015682865479424492,
      "loss": 0.371,
      "step": 10732
    },
    {
      "epoch": 21.595573440643864,
      "grad_norm": 0.6744744181632996,
      "learning_rate": 0.0001568246302444914,
      "loss": 0.3699,
      "step": 10733
    },
    {
      "epoch": 21.59758551307847,
      "grad_norm": 0.6064800024032593,
      "learning_rate": 0.00015682060569473792,
      "loss": 0.3333,
      "step": 10734
    },
    {
      "epoch": 21.599597585513077,
      "grad_norm": 0.6715245842933655,
      "learning_rate": 0.0001568165811449844,
      "loss": 0.3632,
      "step": 10735
    },
    {
      "epoch": 21.601609657947687,
      "grad_norm": 0.6344109177589417,
      "learning_rate": 0.0001568125565952309,
      "loss": 0.333,
      "step": 10736
    },
    {
      "epoch": 21.603621730382294,
      "grad_norm": 0.6607821583747864,
      "learning_rate": 0.00015680853204547742,
      "loss": 0.3582,
      "step": 10737
    },
    {
      "epoch": 21.6056338028169,
      "grad_norm": 0.701316237449646,
      "learning_rate": 0.00015680450749572394,
      "loss": 0.3523,
      "step": 10738
    },
    {
      "epoch": 21.60764587525151,
      "grad_norm": 0.6490917205810547,
      "learning_rate": 0.00015680048294597042,
      "loss": 0.3616,
      "step": 10739
    },
    {
      "epoch": 21.609657947686117,
      "grad_norm": 0.6839737296104431,
      "learning_rate": 0.00015679645839621693,
      "loss": 0.3728,
      "step": 10740
    },
    {
      "epoch": 21.611670020120723,
      "grad_norm": 0.688994824886322,
      "learning_rate": 0.00015679243384646342,
      "loss": 0.3775,
      "step": 10741
    },
    {
      "epoch": 21.613682092555333,
      "grad_norm": 0.6504197120666504,
      "learning_rate": 0.00015678840929670996,
      "loss": 0.3485,
      "step": 10742
    },
    {
      "epoch": 21.61569416498994,
      "grad_norm": 0.6927348375320435,
      "learning_rate": 0.00015678438474695644,
      "loss": 0.3565,
      "step": 10743
    },
    {
      "epoch": 21.617706237424546,
      "grad_norm": 0.6490197777748108,
      "learning_rate": 0.00015678036019720296,
      "loss": 0.385,
      "step": 10744
    },
    {
      "epoch": 21.619718309859156,
      "grad_norm": 0.68657386302948,
      "learning_rate": 0.00015677633564744944,
      "loss": 0.3848,
      "step": 10745
    },
    {
      "epoch": 21.621730382293762,
      "grad_norm": 0.6587663888931274,
      "learning_rate": 0.00015677231109769595,
      "loss": 0.3757,
      "step": 10746
    },
    {
      "epoch": 21.62374245472837,
      "grad_norm": 0.637367308139801,
      "learning_rate": 0.00015676828654794247,
      "loss": 0.3423,
      "step": 10747
    },
    {
      "epoch": 21.62575452716298,
      "grad_norm": 0.6901160478591919,
      "learning_rate": 0.00015676426199818898,
      "loss": 0.3692,
      "step": 10748
    },
    {
      "epoch": 21.627766599597585,
      "grad_norm": 0.6945027709007263,
      "learning_rate": 0.00015676023744843546,
      "loss": 0.3696,
      "step": 10749
    },
    {
      "epoch": 21.62977867203219,
      "grad_norm": 0.6575519442558289,
      "learning_rate": 0.00015675621289868198,
      "loss": 0.3622,
      "step": 10750
    },
    {
      "epoch": 21.6317907444668,
      "grad_norm": 0.644169270992279,
      "learning_rate": 0.00015675218834892846,
      "loss": 0.3747,
      "step": 10751
    },
    {
      "epoch": 21.633802816901408,
      "grad_norm": 0.644533097743988,
      "learning_rate": 0.00015674816379917497,
      "loss": 0.3629,
      "step": 10752
    },
    {
      "epoch": 21.635814889336014,
      "grad_norm": 0.6409751772880554,
      "learning_rate": 0.00015674413924942148,
      "loss": 0.3744,
      "step": 10753
    },
    {
      "epoch": 21.637826961770624,
      "grad_norm": 0.6533217430114746,
      "learning_rate": 0.000156740114699668,
      "loss": 0.3539,
      "step": 10754
    },
    {
      "epoch": 21.63983903420523,
      "grad_norm": 0.6523537039756775,
      "learning_rate": 0.00015673609014991448,
      "loss": 0.3532,
      "step": 10755
    },
    {
      "epoch": 21.641851106639837,
      "grad_norm": 0.6677791476249695,
      "learning_rate": 0.000156732065600161,
      "loss": 0.3553,
      "step": 10756
    },
    {
      "epoch": 21.643863179074447,
      "grad_norm": 0.7272111177444458,
      "learning_rate": 0.0001567280410504075,
      "loss": 0.3689,
      "step": 10757
    },
    {
      "epoch": 21.645875251509054,
      "grad_norm": 0.7100117206573486,
      "learning_rate": 0.000156724016500654,
      "loss": 0.3471,
      "step": 10758
    },
    {
      "epoch": 21.647887323943664,
      "grad_norm": 0.7122907042503357,
      "learning_rate": 0.0001567199919509005,
      "loss": 0.3876,
      "step": 10759
    },
    {
      "epoch": 21.64989939637827,
      "grad_norm": 0.693077027797699,
      "learning_rate": 0.000156715967401147,
      "loss": 0.3554,
      "step": 10760
    },
    {
      "epoch": 21.651911468812877,
      "grad_norm": 0.6595898866653442,
      "learning_rate": 0.0001567119428513935,
      "loss": 0.3411,
      "step": 10761
    },
    {
      "epoch": 21.653923541247487,
      "grad_norm": 0.6719328165054321,
      "learning_rate": 0.00015670791830164,
      "loss": 0.3496,
      "step": 10762
    },
    {
      "epoch": 21.655935613682093,
      "grad_norm": 0.6847321391105652,
      "learning_rate": 0.00015670389375188653,
      "loss": 0.3806,
      "step": 10763
    },
    {
      "epoch": 21.6579476861167,
      "grad_norm": 0.6353549957275391,
      "learning_rate": 0.000156699869202133,
      "loss": 0.3601,
      "step": 10764
    },
    {
      "epoch": 21.65995975855131,
      "grad_norm": 0.6722428202629089,
      "learning_rate": 0.00015669584465237952,
      "loss": 0.3602,
      "step": 10765
    },
    {
      "epoch": 21.661971830985916,
      "grad_norm": 0.673509955406189,
      "learning_rate": 0.000156691820102626,
      "loss": 0.3813,
      "step": 10766
    },
    {
      "epoch": 21.663983903420522,
      "grad_norm": 0.734536349773407,
      "learning_rate": 0.00015668779555287255,
      "loss": 0.3477,
      "step": 10767
    },
    {
      "epoch": 21.665995975855132,
      "grad_norm": 0.6969720125198364,
      "learning_rate": 0.00015668377100311903,
      "loss": 0.3756,
      "step": 10768
    },
    {
      "epoch": 21.66800804828974,
      "grad_norm": 0.6861918568611145,
      "learning_rate": 0.00015667974645336554,
      "loss": 0.3339,
      "step": 10769
    },
    {
      "epoch": 21.670020120724345,
      "grad_norm": 0.6888151168823242,
      "learning_rate": 0.00015667572190361203,
      "loss": 0.3754,
      "step": 10770
    },
    {
      "epoch": 21.672032193158955,
      "grad_norm": 0.635809600353241,
      "learning_rate": 0.00015667169735385854,
      "loss": 0.3482,
      "step": 10771
    },
    {
      "epoch": 21.67404426559356,
      "grad_norm": 0.684702455997467,
      "learning_rate": 0.00015666767280410505,
      "loss": 0.3473,
      "step": 10772
    },
    {
      "epoch": 21.676056338028168,
      "grad_norm": 0.6164427399635315,
      "learning_rate": 0.00015666364825435157,
      "loss": 0.3327,
      "step": 10773
    },
    {
      "epoch": 21.678068410462778,
      "grad_norm": 0.6705545783042908,
      "learning_rate": 0.00015665962370459805,
      "loss": 0.3751,
      "step": 10774
    },
    {
      "epoch": 21.680080482897385,
      "grad_norm": 0.6676380634307861,
      "learning_rate": 0.00015665559915484456,
      "loss": 0.3582,
      "step": 10775
    },
    {
      "epoch": 21.68209255533199,
      "grad_norm": 0.6816802620887756,
      "learning_rate": 0.00015665157460509105,
      "loss": 0.3642,
      "step": 10776
    },
    {
      "epoch": 21.6841046277666,
      "grad_norm": 0.6602128744125366,
      "learning_rate": 0.0001566475500553376,
      "loss": 0.356,
      "step": 10777
    },
    {
      "epoch": 21.686116700201207,
      "grad_norm": 0.6792754530906677,
      "learning_rate": 0.00015664352550558407,
      "loss": 0.3787,
      "step": 10778
    },
    {
      "epoch": 21.688128772635814,
      "grad_norm": 0.6659166812896729,
      "learning_rate": 0.00015663950095583059,
      "loss": 0.393,
      "step": 10779
    },
    {
      "epoch": 21.690140845070424,
      "grad_norm": 0.6847771406173706,
      "learning_rate": 0.00015663547640607707,
      "loss": 0.3483,
      "step": 10780
    },
    {
      "epoch": 21.69215291750503,
      "grad_norm": 0.6640375256538391,
      "learning_rate": 0.00015663145185632358,
      "loss": 0.3635,
      "step": 10781
    },
    {
      "epoch": 21.694164989939637,
      "grad_norm": 0.6625841856002808,
      "learning_rate": 0.0001566274273065701,
      "loss": 0.359,
      "step": 10782
    },
    {
      "epoch": 21.696177062374247,
      "grad_norm": 0.657971978187561,
      "learning_rate": 0.0001566234027568166,
      "loss": 0.3868,
      "step": 10783
    },
    {
      "epoch": 21.698189134808853,
      "grad_norm": 0.6477604508399963,
      "learning_rate": 0.0001566193782070631,
      "loss": 0.3556,
      "step": 10784
    },
    {
      "epoch": 21.70020120724346,
      "grad_norm": 0.6374373435974121,
      "learning_rate": 0.0001566153536573096,
      "loss": 0.3604,
      "step": 10785
    },
    {
      "epoch": 21.70221327967807,
      "grad_norm": 0.689434826374054,
      "learning_rate": 0.0001566113291075561,
      "loss": 0.3795,
      "step": 10786
    },
    {
      "epoch": 21.704225352112676,
      "grad_norm": 0.6775944828987122,
      "learning_rate": 0.0001566073045578026,
      "loss": 0.3571,
      "step": 10787
    },
    {
      "epoch": 21.706237424547282,
      "grad_norm": 0.638541042804718,
      "learning_rate": 0.00015660328000804911,
      "loss": 0.3542,
      "step": 10788
    },
    {
      "epoch": 21.708249496981892,
      "grad_norm": 0.63676917552948,
      "learning_rate": 0.0001565992554582956,
      "loss": 0.3507,
      "step": 10789
    },
    {
      "epoch": 21.7102615694165,
      "grad_norm": 0.6712546348571777,
      "learning_rate": 0.0001565952309085421,
      "loss": 0.3305,
      "step": 10790
    },
    {
      "epoch": 21.712273641851105,
      "grad_norm": 0.6875172257423401,
      "learning_rate": 0.00015659120635878862,
      "loss": 0.3722,
      "step": 10791
    },
    {
      "epoch": 21.714285714285715,
      "grad_norm": 0.669201135635376,
      "learning_rate": 0.00015658718180903514,
      "loss": 0.3549,
      "step": 10792
    },
    {
      "epoch": 21.71629778672032,
      "grad_norm": 0.646683931350708,
      "learning_rate": 0.00015658315725928162,
      "loss": 0.361,
      "step": 10793
    },
    {
      "epoch": 21.718309859154928,
      "grad_norm": 0.7383713722229004,
      "learning_rate": 0.00015657913270952813,
      "loss": 0.357,
      "step": 10794
    },
    {
      "epoch": 21.720321931589538,
      "grad_norm": 0.664685070514679,
      "learning_rate": 0.00015657510815977462,
      "loss": 0.3816,
      "step": 10795
    },
    {
      "epoch": 21.722334004024145,
      "grad_norm": 0.6235716342926025,
      "learning_rate": 0.00015657108361002113,
      "loss": 0.3569,
      "step": 10796
    },
    {
      "epoch": 21.72434607645875,
      "grad_norm": 0.6750633120536804,
      "learning_rate": 0.00015656705906026762,
      "loss": 0.3434,
      "step": 10797
    },
    {
      "epoch": 21.72635814889336,
      "grad_norm": 0.6990526914596558,
      "learning_rate": 0.00015656303451051416,
      "loss": 0.3491,
      "step": 10798
    },
    {
      "epoch": 21.728370221327967,
      "grad_norm": 0.6599615812301636,
      "learning_rate": 0.00015655900996076064,
      "loss": 0.3489,
      "step": 10799
    },
    {
      "epoch": 21.730382293762574,
      "grad_norm": 0.6651110053062439,
      "learning_rate": 0.00015655498541100715,
      "loss": 0.3646,
      "step": 10800
    },
    {
      "epoch": 21.732394366197184,
      "grad_norm": 0.6726279854774475,
      "learning_rate": 0.00015655096086125364,
      "loss": 0.3703,
      "step": 10801
    },
    {
      "epoch": 21.73440643863179,
      "grad_norm": 0.6769788265228271,
      "learning_rate": 0.00015654693631150015,
      "loss": 0.3811,
      "step": 10802
    },
    {
      "epoch": 21.736418511066397,
      "grad_norm": 0.6522752046585083,
      "learning_rate": 0.00015654291176174666,
      "loss": 0.3698,
      "step": 10803
    },
    {
      "epoch": 21.738430583501007,
      "grad_norm": 0.6451250314712524,
      "learning_rate": 0.00015653888721199317,
      "loss": 0.3789,
      "step": 10804
    },
    {
      "epoch": 21.740442655935613,
      "grad_norm": 0.7070510983467102,
      "learning_rate": 0.00015653486266223966,
      "loss": 0.3727,
      "step": 10805
    },
    {
      "epoch": 21.74245472837022,
      "grad_norm": 0.694299042224884,
      "learning_rate": 0.00015653083811248617,
      "loss": 0.3685,
      "step": 10806
    },
    {
      "epoch": 21.74446680080483,
      "grad_norm": 0.6481801867485046,
      "learning_rate": 0.00015652681356273266,
      "loss": 0.3751,
      "step": 10807
    },
    {
      "epoch": 21.746478873239436,
      "grad_norm": 0.6525167226791382,
      "learning_rate": 0.0001565227890129792,
      "loss": 0.37,
      "step": 10808
    },
    {
      "epoch": 21.748490945674043,
      "grad_norm": 0.6607117056846619,
      "learning_rate": 0.00015651876446322568,
      "loss": 0.3856,
      "step": 10809
    },
    {
      "epoch": 21.750503018108652,
      "grad_norm": 0.6480705738067627,
      "learning_rate": 0.0001565147399134722,
      "loss": 0.3831,
      "step": 10810
    },
    {
      "epoch": 21.75251509054326,
      "grad_norm": 0.7107028365135193,
      "learning_rate": 0.00015651071536371868,
      "loss": 0.3829,
      "step": 10811
    },
    {
      "epoch": 21.754527162977865,
      "grad_norm": 0.663465142250061,
      "learning_rate": 0.0001565066908139652,
      "loss": 0.3825,
      "step": 10812
    },
    {
      "epoch": 21.756539235412475,
      "grad_norm": 0.6701573133468628,
      "learning_rate": 0.0001565026662642117,
      "loss": 0.3684,
      "step": 10813
    },
    {
      "epoch": 21.758551307847082,
      "grad_norm": 0.711696207523346,
      "learning_rate": 0.00015649864171445821,
      "loss": 0.3563,
      "step": 10814
    },
    {
      "epoch": 21.760563380281692,
      "grad_norm": 0.6753939390182495,
      "learning_rate": 0.0001564946171647047,
      "loss": 0.3559,
      "step": 10815
    },
    {
      "epoch": 21.7625754527163,
      "grad_norm": 0.6831269264221191,
      "learning_rate": 0.0001564905926149512,
      "loss": 0.3737,
      "step": 10816
    },
    {
      "epoch": 21.764587525150905,
      "grad_norm": 0.671218991279602,
      "learning_rate": 0.0001564865680651977,
      "loss": 0.3645,
      "step": 10817
    },
    {
      "epoch": 21.766599597585515,
      "grad_norm": 0.6736605763435364,
      "learning_rate": 0.00015648254351544424,
      "loss": 0.3811,
      "step": 10818
    },
    {
      "epoch": 21.76861167002012,
      "grad_norm": 0.6772424578666687,
      "learning_rate": 0.00015647851896569072,
      "loss": 0.3646,
      "step": 10819
    },
    {
      "epoch": 21.770623742454728,
      "grad_norm": 0.660652756690979,
      "learning_rate": 0.00015647449441593723,
      "loss": 0.3426,
      "step": 10820
    },
    {
      "epoch": 21.772635814889338,
      "grad_norm": 0.675797700881958,
      "learning_rate": 0.00015647046986618372,
      "loss": 0.3542,
      "step": 10821
    },
    {
      "epoch": 21.774647887323944,
      "grad_norm": 0.6430540084838867,
      "learning_rate": 0.00015646644531643023,
      "loss": 0.3789,
      "step": 10822
    },
    {
      "epoch": 21.77665995975855,
      "grad_norm": 0.6136296391487122,
      "learning_rate": 0.00015646242076667674,
      "loss": 0.3371,
      "step": 10823
    },
    {
      "epoch": 21.77867203219316,
      "grad_norm": 0.6695518493652344,
      "learning_rate": 0.00015645839621692323,
      "loss": 0.3527,
      "step": 10824
    },
    {
      "epoch": 21.780684104627767,
      "grad_norm": 0.6511359810829163,
      "learning_rate": 0.00015645437166716974,
      "loss": 0.353,
      "step": 10825
    },
    {
      "epoch": 21.782696177062373,
      "grad_norm": 0.6494337320327759,
      "learning_rate": 0.00015645034711741625,
      "loss": 0.3747,
      "step": 10826
    },
    {
      "epoch": 21.784708249496983,
      "grad_norm": 0.6123586893081665,
      "learning_rate": 0.00015644632256766274,
      "loss": 0.3476,
      "step": 10827
    },
    {
      "epoch": 21.78672032193159,
      "grad_norm": 0.6951351761817932,
      "learning_rate": 0.00015644229801790925,
      "loss": 0.3744,
      "step": 10828
    },
    {
      "epoch": 21.788732394366196,
      "grad_norm": 0.6777956485748291,
      "learning_rate": 0.00015643827346815576,
      "loss": 0.3906,
      "step": 10829
    },
    {
      "epoch": 21.790744466800806,
      "grad_norm": 0.6585319638252258,
      "learning_rate": 0.00015643424891840225,
      "loss": 0.3761,
      "step": 10830
    },
    {
      "epoch": 21.792756539235413,
      "grad_norm": 0.6369640231132507,
      "learning_rate": 0.00015643022436864876,
      "loss": 0.3729,
      "step": 10831
    },
    {
      "epoch": 21.79476861167002,
      "grad_norm": 0.6952780485153198,
      "learning_rate": 0.00015642619981889525,
      "loss": 0.3756,
      "step": 10832
    },
    {
      "epoch": 21.79678068410463,
      "grad_norm": 0.6822340488433838,
      "learning_rate": 0.00015642217526914178,
      "loss": 0.3621,
      "step": 10833
    },
    {
      "epoch": 21.798792756539235,
      "grad_norm": 0.7099955081939697,
      "learning_rate": 0.00015641815071938827,
      "loss": 0.3815,
      "step": 10834
    },
    {
      "epoch": 21.800804828973842,
      "grad_norm": 0.7035388946533203,
      "learning_rate": 0.00015641412616963478,
      "loss": 0.4113,
      "step": 10835
    },
    {
      "epoch": 21.802816901408452,
      "grad_norm": 0.6800743937492371,
      "learning_rate": 0.00015641010161988127,
      "loss": 0.3494,
      "step": 10836
    },
    {
      "epoch": 21.80482897384306,
      "grad_norm": 0.7042088508605957,
      "learning_rate": 0.00015640607707012778,
      "loss": 0.3924,
      "step": 10837
    },
    {
      "epoch": 21.806841046277665,
      "grad_norm": 0.6758474707603455,
      "learning_rate": 0.0001564020525203743,
      "loss": 0.3347,
      "step": 10838
    },
    {
      "epoch": 21.808853118712275,
      "grad_norm": 0.6489529013633728,
      "learning_rate": 0.0001563980279706208,
      "loss": 0.3623,
      "step": 10839
    },
    {
      "epoch": 21.81086519114688,
      "grad_norm": 0.6421984434127808,
      "learning_rate": 0.0001563940034208673,
      "loss": 0.3877,
      "step": 10840
    },
    {
      "epoch": 21.812877263581488,
      "grad_norm": 0.6711175441741943,
      "learning_rate": 0.0001563899788711138,
      "loss": 0.3679,
      "step": 10841
    },
    {
      "epoch": 21.814889336016098,
      "grad_norm": 0.6293083429336548,
      "learning_rate": 0.00015638595432136029,
      "loss": 0.3347,
      "step": 10842
    },
    {
      "epoch": 21.816901408450704,
      "grad_norm": 0.6465327739715576,
      "learning_rate": 0.00015638192977160683,
      "loss": 0.366,
      "step": 10843
    },
    {
      "epoch": 21.81891348088531,
      "grad_norm": 0.6114402413368225,
      "learning_rate": 0.0001563779052218533,
      "loss": 0.3301,
      "step": 10844
    },
    {
      "epoch": 21.82092555331992,
      "grad_norm": 0.7118566036224365,
      "learning_rate": 0.00015637388067209982,
      "loss": 0.3882,
      "step": 10845
    },
    {
      "epoch": 21.822937625754527,
      "grad_norm": 0.6714816093444824,
      "learning_rate": 0.0001563698561223463,
      "loss": 0.3675,
      "step": 10846
    },
    {
      "epoch": 21.824949698189133,
      "grad_norm": 0.674540102481842,
      "learning_rate": 0.00015636583157259282,
      "loss": 0.3644,
      "step": 10847
    },
    {
      "epoch": 21.826961770623743,
      "grad_norm": 0.6272337436676025,
      "learning_rate": 0.00015636180702283933,
      "loss": 0.3415,
      "step": 10848
    },
    {
      "epoch": 21.82897384305835,
      "grad_norm": 0.625245988368988,
      "learning_rate": 0.00015635778247308584,
      "loss": 0.3776,
      "step": 10849
    },
    {
      "epoch": 21.830985915492956,
      "grad_norm": 0.6967969536781311,
      "learning_rate": 0.00015635375792333233,
      "loss": 0.379,
      "step": 10850
    },
    {
      "epoch": 21.832997987927566,
      "grad_norm": 0.6939188838005066,
      "learning_rate": 0.00015634973337357884,
      "loss": 0.3728,
      "step": 10851
    },
    {
      "epoch": 21.835010060362173,
      "grad_norm": 0.6566330790519714,
      "learning_rate": 0.00015634570882382533,
      "loss": 0.3755,
      "step": 10852
    },
    {
      "epoch": 21.83702213279678,
      "grad_norm": 0.6742680072784424,
      "learning_rate": 0.00015634168427407187,
      "loss": 0.3854,
      "step": 10853
    },
    {
      "epoch": 21.83903420523139,
      "grad_norm": 0.6616512537002563,
      "learning_rate": 0.00015633765972431835,
      "loss": 0.3529,
      "step": 10854
    },
    {
      "epoch": 21.841046277665995,
      "grad_norm": 0.6565465331077576,
      "learning_rate": 0.00015633363517456486,
      "loss": 0.3452,
      "step": 10855
    },
    {
      "epoch": 21.843058350100602,
      "grad_norm": 0.714235782623291,
      "learning_rate": 0.00015632961062481135,
      "loss": 0.3772,
      "step": 10856
    },
    {
      "epoch": 21.845070422535212,
      "grad_norm": 0.681877851486206,
      "learning_rate": 0.00015632558607505786,
      "loss": 0.3674,
      "step": 10857
    },
    {
      "epoch": 21.84708249496982,
      "grad_norm": 0.6763404011726379,
      "learning_rate": 0.00015632156152530437,
      "loss": 0.3914,
      "step": 10858
    },
    {
      "epoch": 21.84909456740443,
      "grad_norm": 0.6794112324714661,
      "learning_rate": 0.00015631753697555086,
      "loss": 0.3828,
      "step": 10859
    },
    {
      "epoch": 21.851106639839035,
      "grad_norm": 0.6771079897880554,
      "learning_rate": 0.00015631351242579737,
      "loss": 0.3599,
      "step": 10860
    },
    {
      "epoch": 21.85311871227364,
      "grad_norm": 0.6217923164367676,
      "learning_rate": 0.00015630948787604388,
      "loss": 0.3432,
      "step": 10861
    },
    {
      "epoch": 21.85513078470825,
      "grad_norm": 0.6615872383117676,
      "learning_rate": 0.00015630546332629037,
      "loss": 0.3488,
      "step": 10862
    },
    {
      "epoch": 21.857142857142858,
      "grad_norm": 0.6660870909690857,
      "learning_rate": 0.00015630143877653688,
      "loss": 0.3854,
      "step": 10863
    },
    {
      "epoch": 21.859154929577464,
      "grad_norm": 0.6595094799995422,
      "learning_rate": 0.0001562974142267834,
      "loss": 0.3736,
      "step": 10864
    },
    {
      "epoch": 21.861167002012074,
      "grad_norm": 0.7096589207649231,
      "learning_rate": 0.00015629338967702988,
      "loss": 0.3738,
      "step": 10865
    },
    {
      "epoch": 21.86317907444668,
      "grad_norm": 0.7210185527801514,
      "learning_rate": 0.0001562893651272764,
      "loss": 0.3603,
      "step": 10866
    },
    {
      "epoch": 21.865191146881287,
      "grad_norm": 0.6804751753807068,
      "learning_rate": 0.00015628534057752287,
      "loss": 0.3566,
      "step": 10867
    },
    {
      "epoch": 21.867203219315897,
      "grad_norm": 0.6541242599487305,
      "learning_rate": 0.00015628131602776941,
      "loss": 0.371,
      "step": 10868
    },
    {
      "epoch": 21.869215291750503,
      "grad_norm": 0.6898393630981445,
      "learning_rate": 0.0001562772914780159,
      "loss": 0.3669,
      "step": 10869
    },
    {
      "epoch": 21.87122736418511,
      "grad_norm": 0.6382724046707153,
      "learning_rate": 0.0001562732669282624,
      "loss": 0.3764,
      "step": 10870
    },
    {
      "epoch": 21.87323943661972,
      "grad_norm": 0.6387780904769897,
      "learning_rate": 0.0001562692423785089,
      "loss": 0.3776,
      "step": 10871
    },
    {
      "epoch": 21.875251509054326,
      "grad_norm": 0.6574562191963196,
      "learning_rate": 0.0001562652178287554,
      "loss": 0.3632,
      "step": 10872
    },
    {
      "epoch": 21.877263581488933,
      "grad_norm": 0.6525169014930725,
      "learning_rate": 0.00015626119327900192,
      "loss": 0.3603,
      "step": 10873
    },
    {
      "epoch": 21.879275653923543,
      "grad_norm": 0.6587381362915039,
      "learning_rate": 0.00015625716872924843,
      "loss": 0.3735,
      "step": 10874
    },
    {
      "epoch": 21.88128772635815,
      "grad_norm": 0.6824111342430115,
      "learning_rate": 0.00015625314417949492,
      "loss": 0.3895,
      "step": 10875
    },
    {
      "epoch": 21.883299798792756,
      "grad_norm": 0.6955623626708984,
      "learning_rate": 0.00015624911962974143,
      "loss": 0.3987,
      "step": 10876
    },
    {
      "epoch": 21.885311871227366,
      "grad_norm": 0.6508009433746338,
      "learning_rate": 0.00015624509507998792,
      "loss": 0.3658,
      "step": 10877
    },
    {
      "epoch": 21.887323943661972,
      "grad_norm": 0.673592746257782,
      "learning_rate": 0.00015624107053023445,
      "loss": 0.3628,
      "step": 10878
    },
    {
      "epoch": 21.88933601609658,
      "grad_norm": 0.6471003293991089,
      "learning_rate": 0.00015623704598048094,
      "loss": 0.3785,
      "step": 10879
    },
    {
      "epoch": 21.89134808853119,
      "grad_norm": 0.6855356693267822,
      "learning_rate": 0.00015623302143072745,
      "loss": 0.3602,
      "step": 10880
    },
    {
      "epoch": 21.893360160965795,
      "grad_norm": 0.6630058884620667,
      "learning_rate": 0.00015622899688097394,
      "loss": 0.3732,
      "step": 10881
    },
    {
      "epoch": 21.8953722334004,
      "grad_norm": 0.654000461101532,
      "learning_rate": 0.00015622497233122045,
      "loss": 0.3408,
      "step": 10882
    },
    {
      "epoch": 21.89738430583501,
      "grad_norm": 0.6499654650688171,
      "learning_rate": 0.00015622094778146696,
      "loss": 0.4002,
      "step": 10883
    },
    {
      "epoch": 21.899396378269618,
      "grad_norm": 0.6589152812957764,
      "learning_rate": 0.00015621692323171347,
      "loss": 0.359,
      "step": 10884
    },
    {
      "epoch": 21.901408450704224,
      "grad_norm": 0.6442914605140686,
      "learning_rate": 0.00015621289868195996,
      "loss": 0.3901,
      "step": 10885
    },
    {
      "epoch": 21.903420523138834,
      "grad_norm": 0.6743274927139282,
      "learning_rate": 0.00015620887413220647,
      "loss": 0.4049,
      "step": 10886
    },
    {
      "epoch": 21.90543259557344,
      "grad_norm": 0.6590371131896973,
      "learning_rate": 0.00015620484958245296,
      "loss": 0.3677,
      "step": 10887
    },
    {
      "epoch": 21.907444668008047,
      "grad_norm": 0.6309916973114014,
      "learning_rate": 0.0001562008250326995,
      "loss": 0.3746,
      "step": 10888
    },
    {
      "epoch": 21.909456740442657,
      "grad_norm": 0.6654453873634338,
      "learning_rate": 0.00015619680048294598,
      "loss": 0.3682,
      "step": 10889
    },
    {
      "epoch": 21.911468812877263,
      "grad_norm": 0.6592744588851929,
      "learning_rate": 0.0001561927759331925,
      "loss": 0.3859,
      "step": 10890
    },
    {
      "epoch": 21.91348088531187,
      "grad_norm": 0.6488556265830994,
      "learning_rate": 0.00015618875138343898,
      "loss": 0.368,
      "step": 10891
    },
    {
      "epoch": 21.91549295774648,
      "grad_norm": 0.7105445861816406,
      "learning_rate": 0.0001561847268336855,
      "loss": 0.3773,
      "step": 10892
    },
    {
      "epoch": 21.917505030181086,
      "grad_norm": 0.6779152750968933,
      "learning_rate": 0.000156180702283932,
      "loss": 0.3604,
      "step": 10893
    },
    {
      "epoch": 21.919517102615693,
      "grad_norm": 0.6638110280036926,
      "learning_rate": 0.0001561766777341785,
      "loss": 0.3527,
      "step": 10894
    },
    {
      "epoch": 21.921529175050303,
      "grad_norm": 0.7211516499519348,
      "learning_rate": 0.000156172653184425,
      "loss": 0.3552,
      "step": 10895
    },
    {
      "epoch": 21.92354124748491,
      "grad_norm": 0.6347687244415283,
      "learning_rate": 0.0001561686286346715,
      "loss": 0.3565,
      "step": 10896
    },
    {
      "epoch": 21.925553319919516,
      "grad_norm": 0.6156103014945984,
      "learning_rate": 0.000156164604084918,
      "loss": 0.3634,
      "step": 10897
    },
    {
      "epoch": 21.927565392354126,
      "grad_norm": 0.6760454177856445,
      "learning_rate": 0.0001561605795351645,
      "loss": 0.3631,
      "step": 10898
    },
    {
      "epoch": 21.929577464788732,
      "grad_norm": 0.6205160021781921,
      "learning_rate": 0.00015615655498541102,
      "loss": 0.3474,
      "step": 10899
    },
    {
      "epoch": 21.93158953722334,
      "grad_norm": 0.6678590178489685,
      "learning_rate": 0.0001561525304356575,
      "loss": 0.3618,
      "step": 10900
    },
    {
      "epoch": 21.93360160965795,
      "grad_norm": 0.6597821712493896,
      "learning_rate": 0.00015614850588590402,
      "loss": 0.3733,
      "step": 10901
    },
    {
      "epoch": 21.935613682092555,
      "grad_norm": 0.6822740435600281,
      "learning_rate": 0.0001561444813361505,
      "loss": 0.3581,
      "step": 10902
    },
    {
      "epoch": 21.93762575452716,
      "grad_norm": 0.695582926273346,
      "learning_rate": 0.00015614045678639704,
      "loss": 0.3918,
      "step": 10903
    },
    {
      "epoch": 21.93963782696177,
      "grad_norm": 0.6708576679229736,
      "learning_rate": 0.00015613643223664353,
      "loss": 0.3636,
      "step": 10904
    },
    {
      "epoch": 21.941649899396378,
      "grad_norm": 0.7008962035179138,
      "learning_rate": 0.00015613240768689004,
      "loss": 0.4061,
      "step": 10905
    },
    {
      "epoch": 21.943661971830984,
      "grad_norm": 0.680488646030426,
      "learning_rate": 0.00015612838313713653,
      "loss": 0.3791,
      "step": 10906
    },
    {
      "epoch": 21.945674044265594,
      "grad_norm": 0.6524640917778015,
      "learning_rate": 0.00015612435858738304,
      "loss": 0.3879,
      "step": 10907
    },
    {
      "epoch": 21.9476861167002,
      "grad_norm": 0.6517143249511719,
      "learning_rate": 0.00015612033403762955,
      "loss": 0.3495,
      "step": 10908
    },
    {
      "epoch": 21.949698189134807,
      "grad_norm": 0.6466261148452759,
      "learning_rate": 0.00015611630948787606,
      "loss": 0.3918,
      "step": 10909
    },
    {
      "epoch": 21.951710261569417,
      "grad_norm": 0.6636508107185364,
      "learning_rate": 0.00015611228493812255,
      "loss": 0.3772,
      "step": 10910
    },
    {
      "epoch": 21.953722334004024,
      "grad_norm": 0.6827412843704224,
      "learning_rate": 0.00015610826038836906,
      "loss": 0.3587,
      "step": 10911
    },
    {
      "epoch": 21.955734406438633,
      "grad_norm": 0.652417778968811,
      "learning_rate": 0.00015610423583861554,
      "loss": 0.3661,
      "step": 10912
    },
    {
      "epoch": 21.95774647887324,
      "grad_norm": 0.6862840056419373,
      "learning_rate": 0.00015610021128886208,
      "loss": 0.3772,
      "step": 10913
    },
    {
      "epoch": 21.959758551307846,
      "grad_norm": 0.6658855080604553,
      "learning_rate": 0.00015609618673910857,
      "loss": 0.3556,
      "step": 10914
    },
    {
      "epoch": 21.961770623742456,
      "grad_norm": 0.7036548256874084,
      "learning_rate": 0.00015609216218935508,
      "loss": 0.3998,
      "step": 10915
    },
    {
      "epoch": 21.963782696177063,
      "grad_norm": 0.6810007691383362,
      "learning_rate": 0.00015608813763960157,
      "loss": 0.3618,
      "step": 10916
    },
    {
      "epoch": 21.96579476861167,
      "grad_norm": 0.6552274227142334,
      "learning_rate": 0.00015608411308984808,
      "loss": 0.3961,
      "step": 10917
    },
    {
      "epoch": 21.96780684104628,
      "grad_norm": 0.6805003881454468,
      "learning_rate": 0.0001560800885400946,
      "loss": 0.3895,
      "step": 10918
    },
    {
      "epoch": 21.969818913480886,
      "grad_norm": 0.6543649435043335,
      "learning_rate": 0.0001560760639903411,
      "loss": 0.3808,
      "step": 10919
    },
    {
      "epoch": 21.971830985915492,
      "grad_norm": 0.7113915681838989,
      "learning_rate": 0.0001560720394405876,
      "loss": 0.3944,
      "step": 10920
    },
    {
      "epoch": 21.973843058350102,
      "grad_norm": 0.6575527191162109,
      "learning_rate": 0.0001560680148908341,
      "loss": 0.3643,
      "step": 10921
    },
    {
      "epoch": 21.97585513078471,
      "grad_norm": 0.6528624296188354,
      "learning_rate": 0.00015606399034108059,
      "loss": 0.3635,
      "step": 10922
    },
    {
      "epoch": 21.977867203219315,
      "grad_norm": 0.6846704483032227,
      "learning_rate": 0.00015605996579132713,
      "loss": 0.39,
      "step": 10923
    },
    {
      "epoch": 21.979879275653925,
      "grad_norm": 0.7039168477058411,
      "learning_rate": 0.0001560559412415736,
      "loss": 0.3966,
      "step": 10924
    },
    {
      "epoch": 21.98189134808853,
      "grad_norm": 0.7099714279174805,
      "learning_rate": 0.00015605191669182012,
      "loss": 0.3669,
      "step": 10925
    },
    {
      "epoch": 21.983903420523138,
      "grad_norm": 0.6950516104698181,
      "learning_rate": 0.0001560478921420666,
      "loss": 0.3679,
      "step": 10926
    },
    {
      "epoch": 21.985915492957748,
      "grad_norm": 0.6744078993797302,
      "learning_rate": 0.00015604386759231312,
      "loss": 0.3876,
      "step": 10927
    },
    {
      "epoch": 21.987927565392354,
      "grad_norm": 0.6823057532310486,
      "learning_rate": 0.00015603984304255963,
      "loss": 0.3781,
      "step": 10928
    },
    {
      "epoch": 21.98993963782696,
      "grad_norm": 0.6650856733322144,
      "learning_rate": 0.00015603581849280612,
      "loss": 0.3369,
      "step": 10929
    },
    {
      "epoch": 21.99195171026157,
      "grad_norm": 0.6567440032958984,
      "learning_rate": 0.00015603179394305263,
      "loss": 0.3973,
      "step": 10930
    },
    {
      "epoch": 21.993963782696177,
      "grad_norm": 0.6563414931297302,
      "learning_rate": 0.00015602776939329914,
      "loss": 0.3493,
      "step": 10931
    },
    {
      "epoch": 21.995975855130784,
      "grad_norm": 0.670242428779602,
      "learning_rate": 0.00015602374484354563,
      "loss": 0.3786,
      "step": 10932
    },
    {
      "epoch": 21.997987927565394,
      "grad_norm": 0.6505462527275085,
      "learning_rate": 0.00015601972029379214,
      "loss": 0.3626,
      "step": 10933
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.7013369202613831,
      "learning_rate": 0.00015601569574403865,
      "loss": 0.3885,
      "step": 10934
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.0299737453460693,
      "eval_runtime": 49.8212,
      "eval_samples_per_second": 19.911,
      "eval_steps_per_second": 2.489,
      "step": 10934
    },
    {
      "epoch": 22.002012072434606,
      "grad_norm": 0.5643452405929565,
      "learning_rate": 0.00015601167119428514,
      "loss": 0.2897,
      "step": 10935
    },
    {
      "epoch": 22.004024144869216,
      "grad_norm": 0.5480560660362244,
      "learning_rate": 0.00015600764664453165,
      "loss": 0.2717,
      "step": 10936
    },
    {
      "epoch": 22.006036217303823,
      "grad_norm": 0.607078492641449,
      "learning_rate": 0.00015600362209477813,
      "loss": 0.3007,
      "step": 10937
    },
    {
      "epoch": 22.00804828973843,
      "grad_norm": 0.5916115641593933,
      "learning_rate": 0.00015599959754502467,
      "loss": 0.2948,
      "step": 10938
    },
    {
      "epoch": 22.01006036217304,
      "grad_norm": 0.6651315689086914,
      "learning_rate": 0.00015599557299527116,
      "loss": 0.3136,
      "step": 10939
    },
    {
      "epoch": 22.012072434607646,
      "grad_norm": 0.6966511607170105,
      "learning_rate": 0.00015599154844551767,
      "loss": 0.3243,
      "step": 10940
    },
    {
      "epoch": 22.014084507042252,
      "grad_norm": 0.6856515407562256,
      "learning_rate": 0.00015598752389576416,
      "loss": 0.3259,
      "step": 10941
    },
    {
      "epoch": 22.016096579476862,
      "grad_norm": 0.6790157556533813,
      "learning_rate": 0.00015598349934601067,
      "loss": 0.3267,
      "step": 10942
    },
    {
      "epoch": 22.01810865191147,
      "grad_norm": 0.6426831483840942,
      "learning_rate": 0.00015597947479625718,
      "loss": 0.3144,
      "step": 10943
    },
    {
      "epoch": 22.020120724346075,
      "grad_norm": 0.5591939091682434,
      "learning_rate": 0.0001559754502465037,
      "loss": 0.2863,
      "step": 10944
    },
    {
      "epoch": 22.022132796780685,
      "grad_norm": 0.6164931654930115,
      "learning_rate": 0.00015597142569675018,
      "loss": 0.3143,
      "step": 10945
    },
    {
      "epoch": 22.02414486921529,
      "grad_norm": 0.6219781041145325,
      "learning_rate": 0.0001559674011469967,
      "loss": 0.3134,
      "step": 10946
    },
    {
      "epoch": 22.026156941649898,
      "grad_norm": 0.6409515738487244,
      "learning_rate": 0.00015596337659724317,
      "loss": 0.328,
      "step": 10947
    },
    {
      "epoch": 22.028169014084508,
      "grad_norm": 0.6430283784866333,
      "learning_rate": 0.00015595935204748971,
      "loss": 0.3184,
      "step": 10948
    },
    {
      "epoch": 22.030181086519114,
      "grad_norm": 0.6315707564353943,
      "learning_rate": 0.0001559553274977362,
      "loss": 0.2965,
      "step": 10949
    },
    {
      "epoch": 22.03219315895372,
      "grad_norm": 0.6600205302238464,
      "learning_rate": 0.0001559513029479827,
      "loss": 0.3162,
      "step": 10950
    },
    {
      "epoch": 22.03420523138833,
      "grad_norm": 0.6394075155258179,
      "learning_rate": 0.0001559472783982292,
      "loss": 0.2975,
      "step": 10951
    },
    {
      "epoch": 22.036217303822937,
      "grad_norm": 0.6470682621002197,
      "learning_rate": 0.0001559432538484757,
      "loss": 0.322,
      "step": 10952
    },
    {
      "epoch": 22.038229376257544,
      "grad_norm": 0.5908578634262085,
      "learning_rate": 0.00015593922929872222,
      "loss": 0.2944,
      "step": 10953
    },
    {
      "epoch": 22.040241448692154,
      "grad_norm": 0.6388992667198181,
      "learning_rate": 0.00015593520474896873,
      "loss": 0.3106,
      "step": 10954
    },
    {
      "epoch": 22.04225352112676,
      "grad_norm": 0.6288800835609436,
      "learning_rate": 0.00015593118019921522,
      "loss": 0.3038,
      "step": 10955
    },
    {
      "epoch": 22.044265593561367,
      "grad_norm": 0.6267682313919067,
      "learning_rate": 0.00015592715564946173,
      "loss": 0.2859,
      "step": 10956
    },
    {
      "epoch": 22.046277665995976,
      "grad_norm": 0.6375907063484192,
      "learning_rate": 0.00015592313109970822,
      "loss": 0.2863,
      "step": 10957
    },
    {
      "epoch": 22.048289738430583,
      "grad_norm": 0.6685160994529724,
      "learning_rate": 0.00015591910654995475,
      "loss": 0.3053,
      "step": 10958
    },
    {
      "epoch": 22.050301810865193,
      "grad_norm": 0.6800223588943481,
      "learning_rate": 0.00015591508200020124,
      "loss": 0.3243,
      "step": 10959
    },
    {
      "epoch": 22.0523138832998,
      "grad_norm": 0.701605498790741,
      "learning_rate": 0.00015591105745044775,
      "loss": 0.3371,
      "step": 10960
    },
    {
      "epoch": 22.054325955734406,
      "grad_norm": 0.6075093150138855,
      "learning_rate": 0.00015590703290069424,
      "loss": 0.2905,
      "step": 10961
    },
    {
      "epoch": 22.056338028169016,
      "grad_norm": 0.5930758714675903,
      "learning_rate": 0.00015590300835094075,
      "loss": 0.2989,
      "step": 10962
    },
    {
      "epoch": 22.058350100603622,
      "grad_norm": 0.619470477104187,
      "learning_rate": 0.00015589898380118726,
      "loss": 0.3081,
      "step": 10963
    },
    {
      "epoch": 22.06036217303823,
      "grad_norm": 0.6565262079238892,
      "learning_rate": 0.00015589495925143375,
      "loss": 0.3066,
      "step": 10964
    },
    {
      "epoch": 22.06237424547284,
      "grad_norm": 0.6507402658462524,
      "learning_rate": 0.00015589093470168026,
      "loss": 0.3127,
      "step": 10965
    },
    {
      "epoch": 22.064386317907445,
      "grad_norm": 0.647459089756012,
      "learning_rate": 0.00015588691015192674,
      "loss": 0.3308,
      "step": 10966
    },
    {
      "epoch": 22.06639839034205,
      "grad_norm": 0.6569696068763733,
      "learning_rate": 0.00015588288560217326,
      "loss": 0.3112,
      "step": 10967
    },
    {
      "epoch": 22.06841046277666,
      "grad_norm": 0.7073765993118286,
      "learning_rate": 0.00015587886105241977,
      "loss": 0.3216,
      "step": 10968
    },
    {
      "epoch": 22.070422535211268,
      "grad_norm": 0.6492503881454468,
      "learning_rate": 0.00015587483650266628,
      "loss": 0.3002,
      "step": 10969
    },
    {
      "epoch": 22.072434607645874,
      "grad_norm": 0.6728681921958923,
      "learning_rate": 0.00015587081195291277,
      "loss": 0.296,
      "step": 10970
    },
    {
      "epoch": 22.074446680080484,
      "grad_norm": 0.6512775421142578,
      "learning_rate": 0.00015586678740315928,
      "loss": 0.3014,
      "step": 10971
    },
    {
      "epoch": 22.07645875251509,
      "grad_norm": 0.6489719152450562,
      "learning_rate": 0.00015586276285340576,
      "loss": 0.2961,
      "step": 10972
    },
    {
      "epoch": 22.078470824949697,
      "grad_norm": 0.6973267793655396,
      "learning_rate": 0.0001558587383036523,
      "loss": 0.3143,
      "step": 10973
    },
    {
      "epoch": 22.080482897384307,
      "grad_norm": 0.6529573798179626,
      "learning_rate": 0.0001558547137538988,
      "loss": 0.3287,
      "step": 10974
    },
    {
      "epoch": 22.082494969818914,
      "grad_norm": 0.5769098997116089,
      "learning_rate": 0.0001558506892041453,
      "loss": 0.2882,
      "step": 10975
    },
    {
      "epoch": 22.08450704225352,
      "grad_norm": 0.6283275485038757,
      "learning_rate": 0.00015584666465439178,
      "loss": 0.3283,
      "step": 10976
    },
    {
      "epoch": 22.08651911468813,
      "grad_norm": 0.6372730731964111,
      "learning_rate": 0.0001558426401046383,
      "loss": 0.3079,
      "step": 10977
    },
    {
      "epoch": 22.088531187122737,
      "grad_norm": 0.6691743731498718,
      "learning_rate": 0.0001558386155548848,
      "loss": 0.33,
      "step": 10978
    },
    {
      "epoch": 22.090543259557343,
      "grad_norm": 0.6630158424377441,
      "learning_rate": 0.00015583459100513132,
      "loss": 0.3157,
      "step": 10979
    },
    {
      "epoch": 22.092555331991953,
      "grad_norm": 0.7313514947891235,
      "learning_rate": 0.0001558305664553778,
      "loss": 0.3151,
      "step": 10980
    },
    {
      "epoch": 22.09456740442656,
      "grad_norm": 0.6804216504096985,
      "learning_rate": 0.00015582654190562432,
      "loss": 0.3069,
      "step": 10981
    },
    {
      "epoch": 22.096579476861166,
      "grad_norm": 0.6366332769393921,
      "learning_rate": 0.0001558225173558708,
      "loss": 0.3182,
      "step": 10982
    },
    {
      "epoch": 22.098591549295776,
      "grad_norm": 0.6389797925949097,
      "learning_rate": 0.00015581849280611734,
      "loss": 0.293,
      "step": 10983
    },
    {
      "epoch": 22.100603621730382,
      "grad_norm": 0.6371832489967346,
      "learning_rate": 0.00015581446825636383,
      "loss": 0.3043,
      "step": 10984
    },
    {
      "epoch": 22.10261569416499,
      "grad_norm": 0.6221136450767517,
      "learning_rate": 0.00015581044370661034,
      "loss": 0.3116,
      "step": 10985
    },
    {
      "epoch": 22.1046277665996,
      "grad_norm": 0.676074743270874,
      "learning_rate": 0.00015580641915685683,
      "loss": 0.311,
      "step": 10986
    },
    {
      "epoch": 22.106639839034205,
      "grad_norm": 0.6454647779464722,
      "learning_rate": 0.00015580239460710334,
      "loss": 0.2985,
      "step": 10987
    },
    {
      "epoch": 22.10865191146881,
      "grad_norm": 0.7188751101493835,
      "learning_rate": 0.00015579837005734985,
      "loss": 0.3253,
      "step": 10988
    },
    {
      "epoch": 22.11066398390342,
      "grad_norm": 0.6323065757751465,
      "learning_rate": 0.00015579434550759636,
      "loss": 0.2857,
      "step": 10989
    },
    {
      "epoch": 22.112676056338028,
      "grad_norm": 0.6539466977119446,
      "learning_rate": 0.00015579032095784285,
      "loss": 0.34,
      "step": 10990
    },
    {
      "epoch": 22.114688128772634,
      "grad_norm": 0.7060065865516663,
      "learning_rate": 0.00015578629640808936,
      "loss": 0.2916,
      "step": 10991
    },
    {
      "epoch": 22.116700201207244,
      "grad_norm": 0.7102216482162476,
      "learning_rate": 0.00015578227185833584,
      "loss": 0.2852,
      "step": 10992
    },
    {
      "epoch": 22.11871227364185,
      "grad_norm": 0.6829509735107422,
      "learning_rate": 0.00015577824730858236,
      "loss": 0.319,
      "step": 10993
    },
    {
      "epoch": 22.120724346076457,
      "grad_norm": 0.6916980743408203,
      "learning_rate": 0.00015577422275882887,
      "loss": 0.2954,
      "step": 10994
    },
    {
      "epoch": 22.122736418511067,
      "grad_norm": 0.6436514854431152,
      "learning_rate": 0.00015577019820907538,
      "loss": 0.3254,
      "step": 10995
    },
    {
      "epoch": 22.124748490945674,
      "grad_norm": 0.6330827474594116,
      "learning_rate": 0.00015576617365932187,
      "loss": 0.3126,
      "step": 10996
    },
    {
      "epoch": 22.12676056338028,
      "grad_norm": 0.6186783313751221,
      "learning_rate": 0.00015576214910956838,
      "loss": 0.3165,
      "step": 10997
    },
    {
      "epoch": 22.12877263581489,
      "grad_norm": 0.6834126114845276,
      "learning_rate": 0.0001557581245598149,
      "loss": 0.3021,
      "step": 10998
    },
    {
      "epoch": 22.130784708249497,
      "grad_norm": 0.6982282996177673,
      "learning_rate": 0.00015575410001006138,
      "loss": 0.332,
      "step": 10999
    },
    {
      "epoch": 22.132796780684103,
      "grad_norm": 0.6830006837844849,
      "learning_rate": 0.0001557500754603079,
      "loss": 0.3443,
      "step": 11000
    },
    {
      "epoch": 22.134808853118713,
      "grad_norm": 0.643343985080719,
      "learning_rate": 0.00015574605091055437,
      "loss": 0.286,
      "step": 11001
    },
    {
      "epoch": 22.13682092555332,
      "grad_norm": 0.6640651226043701,
      "learning_rate": 0.00015574202636080089,
      "loss": 0.3059,
      "step": 11002
    },
    {
      "epoch": 22.138832997987926,
      "grad_norm": 0.6526811122894287,
      "learning_rate": 0.0001557380018110474,
      "loss": 0.3117,
      "step": 11003
    },
    {
      "epoch": 22.140845070422536,
      "grad_norm": 0.6550019979476929,
      "learning_rate": 0.0001557339772612939,
      "loss": 0.2951,
      "step": 11004
    },
    {
      "epoch": 22.142857142857142,
      "grad_norm": 0.7108397483825684,
      "learning_rate": 0.0001557299527115404,
      "loss": 0.3252,
      "step": 11005
    },
    {
      "epoch": 22.14486921529175,
      "grad_norm": 0.6942976117134094,
      "learning_rate": 0.0001557259281617869,
      "loss": 0.3186,
      "step": 11006
    },
    {
      "epoch": 22.14688128772636,
      "grad_norm": 0.6792997717857361,
      "learning_rate": 0.0001557219036120334,
      "loss": 0.3233,
      "step": 11007
    },
    {
      "epoch": 22.148893360160965,
      "grad_norm": 0.6539202332496643,
      "learning_rate": 0.00015571787906227993,
      "loss": 0.3156,
      "step": 11008
    },
    {
      "epoch": 22.15090543259557,
      "grad_norm": 0.6264208555221558,
      "learning_rate": 0.00015571385451252642,
      "loss": 0.3003,
      "step": 11009
    },
    {
      "epoch": 22.15291750503018,
      "grad_norm": 0.6927842497825623,
      "learning_rate": 0.00015570982996277293,
      "loss": 0.3137,
      "step": 11010
    },
    {
      "epoch": 22.154929577464788,
      "grad_norm": 0.6934435963630676,
      "learning_rate": 0.00015570580541301941,
      "loss": 0.3233,
      "step": 11011
    },
    {
      "epoch": 22.156941649899398,
      "grad_norm": 0.6428242921829224,
      "learning_rate": 0.00015570178086326593,
      "loss": 0.2955,
      "step": 11012
    },
    {
      "epoch": 22.158953722334005,
      "grad_norm": 0.6425101161003113,
      "learning_rate": 0.00015569775631351244,
      "loss": 0.2947,
      "step": 11013
    },
    {
      "epoch": 22.16096579476861,
      "grad_norm": 0.6818150281906128,
      "learning_rate": 0.00015569373176375895,
      "loss": 0.3211,
      "step": 11014
    },
    {
      "epoch": 22.16297786720322,
      "grad_norm": 0.6699053049087524,
      "learning_rate": 0.00015568970721400544,
      "loss": 0.3281,
      "step": 11015
    },
    {
      "epoch": 22.164989939637827,
      "grad_norm": 0.6529034972190857,
      "learning_rate": 0.00015568568266425195,
      "loss": 0.3169,
      "step": 11016
    },
    {
      "epoch": 22.167002012072434,
      "grad_norm": 0.6446589827537537,
      "learning_rate": 0.00015568165811449843,
      "loss": 0.3052,
      "step": 11017
    },
    {
      "epoch": 22.169014084507044,
      "grad_norm": 0.6479366421699524,
      "learning_rate": 0.00015567763356474497,
      "loss": 0.3149,
      "step": 11018
    },
    {
      "epoch": 22.17102615694165,
      "grad_norm": 0.6432819366455078,
      "learning_rate": 0.00015567360901499146,
      "loss": 0.293,
      "step": 11019
    },
    {
      "epoch": 22.173038229376257,
      "grad_norm": 0.68628990650177,
      "learning_rate": 0.00015566958446523797,
      "loss": 0.3112,
      "step": 11020
    },
    {
      "epoch": 22.175050301810867,
      "grad_norm": 0.6741863489151001,
      "learning_rate": 0.00015566555991548445,
      "loss": 0.331,
      "step": 11021
    },
    {
      "epoch": 22.177062374245473,
      "grad_norm": 0.638654887676239,
      "learning_rate": 0.00015566153536573097,
      "loss": 0.3024,
      "step": 11022
    },
    {
      "epoch": 22.17907444668008,
      "grad_norm": 0.6729767322540283,
      "learning_rate": 0.00015565751081597748,
      "loss": 0.3339,
      "step": 11023
    },
    {
      "epoch": 22.18108651911469,
      "grad_norm": 0.7169809937477112,
      "learning_rate": 0.000155653486266224,
      "loss": 0.3436,
      "step": 11024
    },
    {
      "epoch": 22.183098591549296,
      "grad_norm": 0.6545036435127258,
      "learning_rate": 0.00015564946171647048,
      "loss": 0.3203,
      "step": 11025
    },
    {
      "epoch": 22.185110663983902,
      "grad_norm": 0.7032274603843689,
      "learning_rate": 0.000155645437166717,
      "loss": 0.342,
      "step": 11026
    },
    {
      "epoch": 22.187122736418512,
      "grad_norm": 0.646873950958252,
      "learning_rate": 0.00015564141261696347,
      "loss": 0.32,
      "step": 11027
    },
    {
      "epoch": 22.18913480885312,
      "grad_norm": 0.6884880065917969,
      "learning_rate": 0.00015563738806720999,
      "loss": 0.2934,
      "step": 11028
    },
    {
      "epoch": 22.191146881287725,
      "grad_norm": 0.7169767022132874,
      "learning_rate": 0.0001556333635174565,
      "loss": 0.3441,
      "step": 11029
    },
    {
      "epoch": 22.193158953722335,
      "grad_norm": 0.6296151876449585,
      "learning_rate": 0.000155629338967703,
      "loss": 0.2983,
      "step": 11030
    },
    {
      "epoch": 22.19517102615694,
      "grad_norm": 0.677729606628418,
      "learning_rate": 0.0001556253144179495,
      "loss": 0.3068,
      "step": 11031
    },
    {
      "epoch": 22.197183098591548,
      "grad_norm": 0.6509113311767578,
      "learning_rate": 0.000155621289868196,
      "loss": 0.3174,
      "step": 11032
    },
    {
      "epoch": 22.199195171026158,
      "grad_norm": 0.6881243586540222,
      "learning_rate": 0.00015561726531844252,
      "loss": 0.3272,
      "step": 11033
    },
    {
      "epoch": 22.201207243460765,
      "grad_norm": 0.6880252957344055,
      "learning_rate": 0.000155613240768689,
      "loss": 0.3239,
      "step": 11034
    },
    {
      "epoch": 22.20321931589537,
      "grad_norm": 0.6344034671783447,
      "learning_rate": 0.00015560921621893552,
      "loss": 0.319,
      "step": 11035
    },
    {
      "epoch": 22.20523138832998,
      "grad_norm": 0.6698383092880249,
      "learning_rate": 0.000155605191669182,
      "loss": 0.3184,
      "step": 11036
    },
    {
      "epoch": 22.207243460764587,
      "grad_norm": 0.6850709319114685,
      "learning_rate": 0.00015560116711942851,
      "loss": 0.3408,
      "step": 11037
    },
    {
      "epoch": 22.209255533199194,
      "grad_norm": 0.6935133337974548,
      "learning_rate": 0.00015559714256967503,
      "loss": 0.318,
      "step": 11038
    },
    {
      "epoch": 22.211267605633804,
      "grad_norm": 0.6642810702323914,
      "learning_rate": 0.00015559311801992154,
      "loss": 0.3265,
      "step": 11039
    },
    {
      "epoch": 22.21327967806841,
      "grad_norm": 0.6504292488098145,
      "learning_rate": 0.00015558909347016802,
      "loss": 0.3289,
      "step": 11040
    },
    {
      "epoch": 22.215291750503017,
      "grad_norm": 0.7261788249015808,
      "learning_rate": 0.00015558506892041454,
      "loss": 0.3407,
      "step": 11041
    },
    {
      "epoch": 22.217303822937627,
      "grad_norm": 0.6607028841972351,
      "learning_rate": 0.00015558104437066102,
      "loss": 0.3214,
      "step": 11042
    },
    {
      "epoch": 22.219315895372233,
      "grad_norm": 0.6802619695663452,
      "learning_rate": 0.00015557701982090753,
      "loss": 0.3325,
      "step": 11043
    },
    {
      "epoch": 22.22132796780684,
      "grad_norm": 0.7261828184127808,
      "learning_rate": 0.00015557299527115405,
      "loss": 0.33,
      "step": 11044
    },
    {
      "epoch": 22.22334004024145,
      "grad_norm": 0.7037702798843384,
      "learning_rate": 0.00015556897072140056,
      "loss": 0.3279,
      "step": 11045
    },
    {
      "epoch": 22.225352112676056,
      "grad_norm": 0.6300756335258484,
      "learning_rate": 0.00015556494617164704,
      "loss": 0.2934,
      "step": 11046
    },
    {
      "epoch": 22.227364185110662,
      "grad_norm": 0.6634018421173096,
      "learning_rate": 0.00015556092162189356,
      "loss": 0.3466,
      "step": 11047
    },
    {
      "epoch": 22.229376257545272,
      "grad_norm": 0.6877304911613464,
      "learning_rate": 0.00015555689707214004,
      "loss": 0.32,
      "step": 11048
    },
    {
      "epoch": 22.23138832997988,
      "grad_norm": 0.7017810940742493,
      "learning_rate": 0.00015555287252238658,
      "loss": 0.3219,
      "step": 11049
    },
    {
      "epoch": 22.233400402414485,
      "grad_norm": 0.6795785427093506,
      "learning_rate": 0.00015554884797263307,
      "loss": 0.3194,
      "step": 11050
    },
    {
      "epoch": 22.235412474849095,
      "grad_norm": 0.6818222999572754,
      "learning_rate": 0.00015554482342287958,
      "loss": 0.325,
      "step": 11051
    },
    {
      "epoch": 22.2374245472837,
      "grad_norm": 0.6485788226127625,
      "learning_rate": 0.00015554079887312606,
      "loss": 0.3092,
      "step": 11052
    },
    {
      "epoch": 22.239436619718308,
      "grad_norm": 0.7058456540107727,
      "learning_rate": 0.00015553677432337257,
      "loss": 0.3429,
      "step": 11053
    },
    {
      "epoch": 22.241448692152918,
      "grad_norm": 0.6960891485214233,
      "learning_rate": 0.0001555327497736191,
      "loss": 0.3534,
      "step": 11054
    },
    {
      "epoch": 22.243460764587525,
      "grad_norm": 0.6775084137916565,
      "learning_rate": 0.0001555287252238656,
      "loss": 0.2856,
      "step": 11055
    },
    {
      "epoch": 22.24547283702213,
      "grad_norm": 0.7198756337165833,
      "learning_rate": 0.00015552470067411208,
      "loss": 0.3194,
      "step": 11056
    },
    {
      "epoch": 22.24748490945674,
      "grad_norm": 0.7052046060562134,
      "learning_rate": 0.0001555206761243586,
      "loss": 0.33,
      "step": 11057
    },
    {
      "epoch": 22.249496981891348,
      "grad_norm": 0.7252794504165649,
      "learning_rate": 0.00015551665157460508,
      "loss": 0.348,
      "step": 11058
    },
    {
      "epoch": 22.251509054325957,
      "grad_norm": 0.713284969329834,
      "learning_rate": 0.00015551262702485162,
      "loss": 0.3383,
      "step": 11059
    },
    {
      "epoch": 22.253521126760564,
      "grad_norm": 0.6842610836029053,
      "learning_rate": 0.0001555086024750981,
      "loss": 0.3096,
      "step": 11060
    },
    {
      "epoch": 22.25553319919517,
      "grad_norm": 0.6570459604263306,
      "learning_rate": 0.00015550457792534462,
      "loss": 0.3174,
      "step": 11061
    },
    {
      "epoch": 22.25754527162978,
      "grad_norm": 0.674972653388977,
      "learning_rate": 0.0001555005533755911,
      "loss": 0.3112,
      "step": 11062
    },
    {
      "epoch": 22.259557344064387,
      "grad_norm": 0.6468398571014404,
      "learning_rate": 0.00015549652882583762,
      "loss": 0.3079,
      "step": 11063
    },
    {
      "epoch": 22.261569416498993,
      "grad_norm": 0.6933494210243225,
      "learning_rate": 0.00015549250427608413,
      "loss": 0.3129,
      "step": 11064
    },
    {
      "epoch": 22.263581488933603,
      "grad_norm": 0.7089566588401794,
      "learning_rate": 0.00015548847972633064,
      "loss": 0.3102,
      "step": 11065
    },
    {
      "epoch": 22.26559356136821,
      "grad_norm": 0.6994619369506836,
      "learning_rate": 0.00015548445517657713,
      "loss": 0.3412,
      "step": 11066
    },
    {
      "epoch": 22.267605633802816,
      "grad_norm": 0.7232766151428223,
      "learning_rate": 0.00015548043062682364,
      "loss": 0.3427,
      "step": 11067
    },
    {
      "epoch": 22.269617706237426,
      "grad_norm": 0.7212249636650085,
      "learning_rate": 0.00015547640607707012,
      "loss": 0.3411,
      "step": 11068
    },
    {
      "epoch": 22.271629778672033,
      "grad_norm": 0.6957848072052002,
      "learning_rate": 0.00015547238152731663,
      "loss": 0.329,
      "step": 11069
    },
    {
      "epoch": 22.27364185110664,
      "grad_norm": 0.6749024391174316,
      "learning_rate": 0.00015546835697756315,
      "loss": 0.3388,
      "step": 11070
    },
    {
      "epoch": 22.27565392354125,
      "grad_norm": 0.6693788766860962,
      "learning_rate": 0.00015546433242780963,
      "loss": 0.3242,
      "step": 11071
    },
    {
      "epoch": 22.277665995975855,
      "grad_norm": 0.6922094821929932,
      "learning_rate": 0.00015546030787805614,
      "loss": 0.3221,
      "step": 11072
    },
    {
      "epoch": 22.279678068410462,
      "grad_norm": 0.7045173645019531,
      "learning_rate": 0.00015545628332830266,
      "loss": 0.3101,
      "step": 11073
    },
    {
      "epoch": 22.281690140845072,
      "grad_norm": 0.7125994563102722,
      "learning_rate": 0.00015545225877854917,
      "loss": 0.3406,
      "step": 11074
    },
    {
      "epoch": 22.28370221327968,
      "grad_norm": 0.6968267560005188,
      "learning_rate": 0.00015544823422879565,
      "loss": 0.3143,
      "step": 11075
    },
    {
      "epoch": 22.285714285714285,
      "grad_norm": 0.6835523247718811,
      "learning_rate": 0.00015544420967904217,
      "loss": 0.3099,
      "step": 11076
    },
    {
      "epoch": 22.287726358148895,
      "grad_norm": 0.6725513935089111,
      "learning_rate": 0.00015544018512928865,
      "loss": 0.3218,
      "step": 11077
    },
    {
      "epoch": 22.2897384305835,
      "grad_norm": 0.6529664993286133,
      "learning_rate": 0.00015543616057953516,
      "loss": 0.3071,
      "step": 11078
    },
    {
      "epoch": 22.291750503018108,
      "grad_norm": 0.7052516341209412,
      "learning_rate": 0.00015543213602978168,
      "loss": 0.3375,
      "step": 11079
    },
    {
      "epoch": 22.293762575452718,
      "grad_norm": 0.6685795783996582,
      "learning_rate": 0.0001554281114800282,
      "loss": 0.3262,
      "step": 11080
    },
    {
      "epoch": 22.295774647887324,
      "grad_norm": 0.679581344127655,
      "learning_rate": 0.00015542408693027467,
      "loss": 0.319,
      "step": 11081
    },
    {
      "epoch": 22.29778672032193,
      "grad_norm": 0.6666128039360046,
      "learning_rate": 0.00015542006238052119,
      "loss": 0.3394,
      "step": 11082
    },
    {
      "epoch": 22.29979879275654,
      "grad_norm": 0.7032183408737183,
      "learning_rate": 0.00015541603783076767,
      "loss": 0.3394,
      "step": 11083
    },
    {
      "epoch": 22.301810865191147,
      "grad_norm": 0.6777007579803467,
      "learning_rate": 0.0001554120132810142,
      "loss": 0.355,
      "step": 11084
    },
    {
      "epoch": 22.303822937625753,
      "grad_norm": 0.6974380612373352,
      "learning_rate": 0.0001554079887312607,
      "loss": 0.3456,
      "step": 11085
    },
    {
      "epoch": 22.305835010060363,
      "grad_norm": 0.6803793907165527,
      "learning_rate": 0.0001554039641815072,
      "loss": 0.3127,
      "step": 11086
    },
    {
      "epoch": 22.30784708249497,
      "grad_norm": 0.7019612789154053,
      "learning_rate": 0.0001553999396317537,
      "loss": 0.3154,
      "step": 11087
    },
    {
      "epoch": 22.309859154929576,
      "grad_norm": 0.6723631024360657,
      "learning_rate": 0.0001553959150820002,
      "loss": 0.3401,
      "step": 11088
    },
    {
      "epoch": 22.311871227364186,
      "grad_norm": 0.6912757754325867,
      "learning_rate": 0.00015539189053224672,
      "loss": 0.3432,
      "step": 11089
    },
    {
      "epoch": 22.313883299798793,
      "grad_norm": 0.6609879732131958,
      "learning_rate": 0.00015538786598249323,
      "loss": 0.3255,
      "step": 11090
    },
    {
      "epoch": 22.3158953722334,
      "grad_norm": 0.7147169709205627,
      "learning_rate": 0.00015538384143273971,
      "loss": 0.317,
      "step": 11091
    },
    {
      "epoch": 22.31790744466801,
      "grad_norm": 0.6451283097267151,
      "learning_rate": 0.00015537981688298623,
      "loss": 0.3173,
      "step": 11092
    },
    {
      "epoch": 22.319919517102615,
      "grad_norm": 0.6637938618659973,
      "learning_rate": 0.0001553757923332327,
      "loss": 0.3297,
      "step": 11093
    },
    {
      "epoch": 22.321931589537222,
      "grad_norm": 0.6669439673423767,
      "learning_rate": 0.00015537176778347925,
      "loss": 0.3277,
      "step": 11094
    },
    {
      "epoch": 22.323943661971832,
      "grad_norm": 0.6681232452392578,
      "learning_rate": 0.00015536774323372574,
      "loss": 0.3406,
      "step": 11095
    },
    {
      "epoch": 22.32595573440644,
      "grad_norm": 0.6453863978385925,
      "learning_rate": 0.00015536371868397225,
      "loss": 0.3115,
      "step": 11096
    },
    {
      "epoch": 22.327967806841045,
      "grad_norm": 0.6724323630332947,
      "learning_rate": 0.00015535969413421873,
      "loss": 0.3587,
      "step": 11097
    },
    {
      "epoch": 22.329979879275655,
      "grad_norm": 0.6820876002311707,
      "learning_rate": 0.00015535566958446525,
      "loss": 0.3227,
      "step": 11098
    },
    {
      "epoch": 22.33199195171026,
      "grad_norm": 0.6649315357208252,
      "learning_rate": 0.00015535164503471176,
      "loss": 0.3209,
      "step": 11099
    },
    {
      "epoch": 22.334004024144868,
      "grad_norm": 0.719136655330658,
      "learning_rate": 0.00015534762048495827,
      "loss": 0.3473,
      "step": 11100
    },
    {
      "epoch": 22.336016096579478,
      "grad_norm": 0.6737385392189026,
      "learning_rate": 0.00015534359593520475,
      "loss": 0.3253,
      "step": 11101
    },
    {
      "epoch": 22.338028169014084,
      "grad_norm": 0.6728150248527527,
      "learning_rate": 0.00015533957138545127,
      "loss": 0.332,
      "step": 11102
    },
    {
      "epoch": 22.34004024144869,
      "grad_norm": 0.7230532169342041,
      "learning_rate": 0.00015533554683569775,
      "loss": 0.3625,
      "step": 11103
    },
    {
      "epoch": 22.3420523138833,
      "grad_norm": 0.6828221678733826,
      "learning_rate": 0.00015533152228594426,
      "loss": 0.3135,
      "step": 11104
    },
    {
      "epoch": 22.344064386317907,
      "grad_norm": 0.674761176109314,
      "learning_rate": 0.00015532749773619078,
      "loss": 0.3194,
      "step": 11105
    },
    {
      "epoch": 22.346076458752513,
      "grad_norm": 0.6789294481277466,
      "learning_rate": 0.00015532347318643726,
      "loss": 0.3518,
      "step": 11106
    },
    {
      "epoch": 22.348088531187123,
      "grad_norm": 0.6840299963951111,
      "learning_rate": 0.00015531944863668377,
      "loss": 0.343,
      "step": 11107
    },
    {
      "epoch": 22.35010060362173,
      "grad_norm": 0.676525354385376,
      "learning_rate": 0.00015531542408693026,
      "loss": 0.3331,
      "step": 11108
    },
    {
      "epoch": 22.352112676056336,
      "grad_norm": 0.6641796231269836,
      "learning_rate": 0.0001553113995371768,
      "loss": 0.3347,
      "step": 11109
    },
    {
      "epoch": 22.354124748490946,
      "grad_norm": 0.6689796447753906,
      "learning_rate": 0.00015530737498742328,
      "loss": 0.305,
      "step": 11110
    },
    {
      "epoch": 22.356136820925553,
      "grad_norm": 0.7367277145385742,
      "learning_rate": 0.0001553033504376698,
      "loss": 0.3438,
      "step": 11111
    },
    {
      "epoch": 22.358148893360163,
      "grad_norm": 0.7092080116271973,
      "learning_rate": 0.00015529932588791628,
      "loss": 0.3275,
      "step": 11112
    },
    {
      "epoch": 22.36016096579477,
      "grad_norm": 0.7456314563751221,
      "learning_rate": 0.0001552953013381628,
      "loss": 0.3577,
      "step": 11113
    },
    {
      "epoch": 22.362173038229376,
      "grad_norm": 0.665414035320282,
      "learning_rate": 0.0001552912767884093,
      "loss": 0.3206,
      "step": 11114
    },
    {
      "epoch": 22.364185110663986,
      "grad_norm": 0.6682604551315308,
      "learning_rate": 0.00015528725223865582,
      "loss": 0.3439,
      "step": 11115
    },
    {
      "epoch": 22.366197183098592,
      "grad_norm": 0.6763049364089966,
      "learning_rate": 0.0001552832276889023,
      "loss": 0.3554,
      "step": 11116
    },
    {
      "epoch": 22.3682092555332,
      "grad_norm": 0.6573470830917358,
      "learning_rate": 0.00015527920313914881,
      "loss": 0.3036,
      "step": 11117
    },
    {
      "epoch": 22.37022132796781,
      "grad_norm": 0.6634835600852966,
      "learning_rate": 0.0001552751785893953,
      "loss": 0.3287,
      "step": 11118
    },
    {
      "epoch": 22.372233400402415,
      "grad_norm": 0.6616905331611633,
      "learning_rate": 0.00015527115403964184,
      "loss": 0.3181,
      "step": 11119
    },
    {
      "epoch": 22.37424547283702,
      "grad_norm": 0.6662206649780273,
      "learning_rate": 0.00015526712948988832,
      "loss": 0.336,
      "step": 11120
    },
    {
      "epoch": 22.37625754527163,
      "grad_norm": 0.697422444820404,
      "learning_rate": 0.00015526310494013484,
      "loss": 0.3437,
      "step": 11121
    },
    {
      "epoch": 22.378269617706238,
      "grad_norm": 0.7055177688598633,
      "learning_rate": 0.00015525908039038132,
      "loss": 0.3389,
      "step": 11122
    },
    {
      "epoch": 22.380281690140844,
      "grad_norm": 0.6670823693275452,
      "learning_rate": 0.00015525505584062783,
      "loss": 0.3134,
      "step": 11123
    },
    {
      "epoch": 22.382293762575454,
      "grad_norm": 0.6537105441093445,
      "learning_rate": 0.00015525103129087435,
      "loss": 0.3162,
      "step": 11124
    },
    {
      "epoch": 22.38430583501006,
      "grad_norm": 0.6674312353134155,
      "learning_rate": 0.00015524700674112086,
      "loss": 0.3176,
      "step": 11125
    },
    {
      "epoch": 22.386317907444667,
      "grad_norm": 0.6743479371070862,
      "learning_rate": 0.00015524298219136734,
      "loss": 0.3215,
      "step": 11126
    },
    {
      "epoch": 22.388329979879277,
      "grad_norm": 0.6931819915771484,
      "learning_rate": 0.00015523895764161386,
      "loss": 0.3573,
      "step": 11127
    },
    {
      "epoch": 22.390342052313883,
      "grad_norm": 0.6833623647689819,
      "learning_rate": 0.00015523493309186034,
      "loss": 0.3364,
      "step": 11128
    },
    {
      "epoch": 22.39235412474849,
      "grad_norm": 0.6587010622024536,
      "learning_rate": 0.00015523090854210688,
      "loss": 0.3353,
      "step": 11129
    },
    {
      "epoch": 22.3943661971831,
      "grad_norm": 0.7125647664070129,
      "learning_rate": 0.00015522688399235337,
      "loss": 0.3268,
      "step": 11130
    },
    {
      "epoch": 22.396378269617706,
      "grad_norm": 0.6640869379043579,
      "learning_rate": 0.00015522285944259988,
      "loss": 0.341,
      "step": 11131
    },
    {
      "epoch": 22.398390342052313,
      "grad_norm": 0.6955104470252991,
      "learning_rate": 0.00015521883489284636,
      "loss": 0.3231,
      "step": 11132
    },
    {
      "epoch": 22.400402414486923,
      "grad_norm": 0.717790961265564,
      "learning_rate": 0.00015521481034309287,
      "loss": 0.3539,
      "step": 11133
    },
    {
      "epoch": 22.40241448692153,
      "grad_norm": 0.650684118270874,
      "learning_rate": 0.0001552107857933394,
      "loss": 0.3268,
      "step": 11134
    },
    {
      "epoch": 22.404426559356136,
      "grad_norm": 0.6896481513977051,
      "learning_rate": 0.00015520676124358587,
      "loss": 0.3302,
      "step": 11135
    },
    {
      "epoch": 22.406438631790746,
      "grad_norm": 0.6816508769989014,
      "learning_rate": 0.00015520273669383238,
      "loss": 0.3377,
      "step": 11136
    },
    {
      "epoch": 22.408450704225352,
      "grad_norm": 0.6469762325286865,
      "learning_rate": 0.0001551987121440789,
      "loss": 0.3282,
      "step": 11137
    },
    {
      "epoch": 22.41046277665996,
      "grad_norm": 0.7080362439155579,
      "learning_rate": 0.00015519468759432538,
      "loss": 0.3299,
      "step": 11138
    },
    {
      "epoch": 22.41247484909457,
      "grad_norm": 0.6676327586174011,
      "learning_rate": 0.0001551906630445719,
      "loss": 0.3013,
      "step": 11139
    },
    {
      "epoch": 22.414486921529175,
      "grad_norm": 0.6962764859199524,
      "learning_rate": 0.0001551866384948184,
      "loss": 0.3536,
      "step": 11140
    },
    {
      "epoch": 22.41649899396378,
      "grad_norm": 0.657577633857727,
      "learning_rate": 0.0001551826139450649,
      "loss": 0.3307,
      "step": 11141
    },
    {
      "epoch": 22.41851106639839,
      "grad_norm": 0.6988195180892944,
      "learning_rate": 0.0001551785893953114,
      "loss": 0.3411,
      "step": 11142
    },
    {
      "epoch": 22.420523138832998,
      "grad_norm": 0.6883015036582947,
      "learning_rate": 0.0001551745648455579,
      "loss": 0.3337,
      "step": 11143
    },
    {
      "epoch": 22.422535211267604,
      "grad_norm": 0.7032395601272583,
      "learning_rate": 0.00015517054029580443,
      "loss": 0.3404,
      "step": 11144
    },
    {
      "epoch": 22.424547283702214,
      "grad_norm": 0.6985458731651306,
      "learning_rate": 0.0001551665157460509,
      "loss": 0.3434,
      "step": 11145
    },
    {
      "epoch": 22.42655935613682,
      "grad_norm": 0.6777335405349731,
      "learning_rate": 0.00015516249119629742,
      "loss": 0.3399,
      "step": 11146
    },
    {
      "epoch": 22.428571428571427,
      "grad_norm": 0.6786230802536011,
      "learning_rate": 0.0001551584666465439,
      "loss": 0.3358,
      "step": 11147
    },
    {
      "epoch": 22.430583501006037,
      "grad_norm": 0.6975058913230896,
      "learning_rate": 0.00015515444209679042,
      "loss": 0.3131,
      "step": 11148
    },
    {
      "epoch": 22.432595573440643,
      "grad_norm": 0.7466111183166504,
      "learning_rate": 0.00015515041754703693,
      "loss": 0.3722,
      "step": 11149
    },
    {
      "epoch": 22.43460764587525,
      "grad_norm": 0.6971842050552368,
      "learning_rate": 0.00015514639299728345,
      "loss": 0.3497,
      "step": 11150
    },
    {
      "epoch": 22.43661971830986,
      "grad_norm": 0.6820858716964722,
      "learning_rate": 0.00015514236844752993,
      "loss": 0.342,
      "step": 11151
    },
    {
      "epoch": 22.438631790744466,
      "grad_norm": 0.7643799185752869,
      "learning_rate": 0.00015513834389777644,
      "loss": 0.3375,
      "step": 11152
    },
    {
      "epoch": 22.440643863179073,
      "grad_norm": 0.7012653350830078,
      "learning_rate": 0.00015513431934802293,
      "loss": 0.3139,
      "step": 11153
    },
    {
      "epoch": 22.442655935613683,
      "grad_norm": 0.6768770217895508,
      "learning_rate": 0.00015513029479826947,
      "loss": 0.3507,
      "step": 11154
    },
    {
      "epoch": 22.44466800804829,
      "grad_norm": 0.7024855613708496,
      "learning_rate": 0.00015512627024851595,
      "loss": 0.3266,
      "step": 11155
    },
    {
      "epoch": 22.446680080482896,
      "grad_norm": 0.6800860166549683,
      "learning_rate": 0.00015512224569876247,
      "loss": 0.3395,
      "step": 11156
    },
    {
      "epoch": 22.448692152917506,
      "grad_norm": 0.7216638326644897,
      "learning_rate": 0.00015511822114900895,
      "loss": 0.3147,
      "step": 11157
    },
    {
      "epoch": 22.450704225352112,
      "grad_norm": 0.7060620188713074,
      "learning_rate": 0.00015511419659925546,
      "loss": 0.3602,
      "step": 11158
    },
    {
      "epoch": 22.452716297786722,
      "grad_norm": 0.6629921793937683,
      "learning_rate": 0.00015511017204950198,
      "loss": 0.3101,
      "step": 11159
    },
    {
      "epoch": 22.45472837022133,
      "grad_norm": 0.671349048614502,
      "learning_rate": 0.0001551061474997485,
      "loss": 0.3425,
      "step": 11160
    },
    {
      "epoch": 22.456740442655935,
      "grad_norm": 0.6384326219558716,
      "learning_rate": 0.00015510212294999497,
      "loss": 0.3153,
      "step": 11161
    },
    {
      "epoch": 22.458752515090545,
      "grad_norm": 0.6571741700172424,
      "learning_rate": 0.00015509809840024148,
      "loss": 0.3278,
      "step": 11162
    },
    {
      "epoch": 22.46076458752515,
      "grad_norm": 0.7022557258605957,
      "learning_rate": 0.00015509407385048797,
      "loss": 0.3416,
      "step": 11163
    },
    {
      "epoch": 22.462776659959758,
      "grad_norm": 0.6909983158111572,
      "learning_rate": 0.0001550900493007345,
      "loss": 0.3478,
      "step": 11164
    },
    {
      "epoch": 22.464788732394368,
      "grad_norm": 0.7377685904502869,
      "learning_rate": 0.000155086024750981,
      "loss": 0.3455,
      "step": 11165
    },
    {
      "epoch": 22.466800804828974,
      "grad_norm": 0.6679583787918091,
      "learning_rate": 0.0001550820002012275,
      "loss": 0.3342,
      "step": 11166
    },
    {
      "epoch": 22.46881287726358,
      "grad_norm": 0.7202783226966858,
      "learning_rate": 0.000155077975651474,
      "loss": 0.344,
      "step": 11167
    },
    {
      "epoch": 22.47082494969819,
      "grad_norm": 0.6957376599311829,
      "learning_rate": 0.0001550739511017205,
      "loss": 0.3549,
      "step": 11168
    },
    {
      "epoch": 22.472837022132797,
      "grad_norm": 0.6578693389892578,
      "learning_rate": 0.00015506992655196702,
      "loss": 0.3529,
      "step": 11169
    },
    {
      "epoch": 22.474849094567404,
      "grad_norm": 0.6952345371246338,
      "learning_rate": 0.0001550659020022135,
      "loss": 0.342,
      "step": 11170
    },
    {
      "epoch": 22.476861167002014,
      "grad_norm": 0.7072376608848572,
      "learning_rate": 0.00015506187745246001,
      "loss": 0.3381,
      "step": 11171
    },
    {
      "epoch": 22.47887323943662,
      "grad_norm": 0.7143544554710388,
      "learning_rate": 0.00015505785290270653,
      "loss": 0.3541,
      "step": 11172
    },
    {
      "epoch": 22.480885311871226,
      "grad_norm": 0.7314943075180054,
      "learning_rate": 0.000155053828352953,
      "loss": 0.3603,
      "step": 11173
    },
    {
      "epoch": 22.482897384305836,
      "grad_norm": 0.6876611113548279,
      "learning_rate": 0.00015504980380319952,
      "loss": 0.3711,
      "step": 11174
    },
    {
      "epoch": 22.484909456740443,
      "grad_norm": 0.6623082756996155,
      "learning_rate": 0.00015504577925344604,
      "loss": 0.3409,
      "step": 11175
    },
    {
      "epoch": 22.48692152917505,
      "grad_norm": 0.6680355072021484,
      "learning_rate": 0.00015504175470369252,
      "loss": 0.3197,
      "step": 11176
    },
    {
      "epoch": 22.48893360160966,
      "grad_norm": 0.675877571105957,
      "learning_rate": 0.00015503773015393903,
      "loss": 0.3654,
      "step": 11177
    },
    {
      "epoch": 22.490945674044266,
      "grad_norm": 0.677800178527832,
      "learning_rate": 0.00015503370560418552,
      "loss": 0.3248,
      "step": 11178
    },
    {
      "epoch": 22.492957746478872,
      "grad_norm": 0.7191309928894043,
      "learning_rate": 0.00015502968105443206,
      "loss": 0.3272,
      "step": 11179
    },
    {
      "epoch": 22.494969818913482,
      "grad_norm": 0.6654548645019531,
      "learning_rate": 0.00015502565650467854,
      "loss": 0.313,
      "step": 11180
    },
    {
      "epoch": 22.49698189134809,
      "grad_norm": 0.7143481969833374,
      "learning_rate": 0.00015502163195492505,
      "loss": 0.328,
      "step": 11181
    },
    {
      "epoch": 22.498993963782695,
      "grad_norm": 0.6836955547332764,
      "learning_rate": 0.00015501760740517154,
      "loss": 0.3357,
      "step": 11182
    },
    {
      "epoch": 22.501006036217305,
      "grad_norm": 0.7090968489646912,
      "learning_rate": 0.00015501358285541805,
      "loss": 0.3544,
      "step": 11183
    },
    {
      "epoch": 22.50301810865191,
      "grad_norm": 0.6993557810783386,
      "learning_rate": 0.00015500955830566456,
      "loss": 0.3575,
      "step": 11184
    },
    {
      "epoch": 22.505030181086518,
      "grad_norm": 0.6753990054130554,
      "learning_rate": 0.00015500553375591108,
      "loss": 0.3573,
      "step": 11185
    },
    {
      "epoch": 22.507042253521128,
      "grad_norm": 0.6957228183746338,
      "learning_rate": 0.00015500150920615756,
      "loss": 0.3452,
      "step": 11186
    },
    {
      "epoch": 22.509054325955734,
      "grad_norm": 0.6441245079040527,
      "learning_rate": 0.00015499748465640407,
      "loss": 0.3184,
      "step": 11187
    },
    {
      "epoch": 22.51106639839034,
      "grad_norm": 0.6947740316390991,
      "learning_rate": 0.00015499346010665056,
      "loss": 0.3322,
      "step": 11188
    },
    {
      "epoch": 22.51307847082495,
      "grad_norm": 0.7058066129684448,
      "learning_rate": 0.0001549894355568971,
      "loss": 0.3705,
      "step": 11189
    },
    {
      "epoch": 22.515090543259557,
      "grad_norm": 0.7137231230735779,
      "learning_rate": 0.00015498541100714358,
      "loss": 0.3615,
      "step": 11190
    },
    {
      "epoch": 22.517102615694164,
      "grad_norm": 0.6835466623306274,
      "learning_rate": 0.0001549813864573901,
      "loss": 0.3259,
      "step": 11191
    },
    {
      "epoch": 22.519114688128774,
      "grad_norm": 0.6785365343093872,
      "learning_rate": 0.00015497736190763658,
      "loss": 0.3383,
      "step": 11192
    },
    {
      "epoch": 22.52112676056338,
      "grad_norm": 0.7072097659111023,
      "learning_rate": 0.0001549733373578831,
      "loss": 0.3791,
      "step": 11193
    },
    {
      "epoch": 22.523138832997986,
      "grad_norm": 0.676743745803833,
      "learning_rate": 0.0001549693128081296,
      "loss": 0.3338,
      "step": 11194
    },
    {
      "epoch": 22.525150905432596,
      "grad_norm": 0.6956642866134644,
      "learning_rate": 0.00015496528825837612,
      "loss": 0.3411,
      "step": 11195
    },
    {
      "epoch": 22.527162977867203,
      "grad_norm": 0.7169531583786011,
      "learning_rate": 0.0001549612637086226,
      "loss": 0.3324,
      "step": 11196
    },
    {
      "epoch": 22.52917505030181,
      "grad_norm": 0.7403793931007385,
      "learning_rate": 0.00015495723915886911,
      "loss": 0.3648,
      "step": 11197
    },
    {
      "epoch": 22.53118712273642,
      "grad_norm": 0.6816030740737915,
      "learning_rate": 0.0001549532146091156,
      "loss": 0.3462,
      "step": 11198
    },
    {
      "epoch": 22.533199195171026,
      "grad_norm": 0.7015647292137146,
      "learning_rate": 0.00015494919005936214,
      "loss": 0.3373,
      "step": 11199
    },
    {
      "epoch": 22.535211267605632,
      "grad_norm": 0.7082462310791016,
      "learning_rate": 0.00015494516550960862,
      "loss": 0.3168,
      "step": 11200
    },
    {
      "epoch": 22.537223340040242,
      "grad_norm": 0.6646217703819275,
      "learning_rate": 0.00015494114095985514,
      "loss": 0.3631,
      "step": 11201
    },
    {
      "epoch": 22.53923541247485,
      "grad_norm": 0.7368695139884949,
      "learning_rate": 0.00015493711641010162,
      "loss": 0.3936,
      "step": 11202
    },
    {
      "epoch": 22.541247484909455,
      "grad_norm": 0.6619226336479187,
      "learning_rate": 0.00015493309186034813,
      "loss": 0.3508,
      "step": 11203
    },
    {
      "epoch": 22.543259557344065,
      "grad_norm": 0.7230501770973206,
      "learning_rate": 0.00015492906731059465,
      "loss": 0.3561,
      "step": 11204
    },
    {
      "epoch": 22.54527162977867,
      "grad_norm": 0.6764487624168396,
      "learning_rate": 0.00015492504276084113,
      "loss": 0.331,
      "step": 11205
    },
    {
      "epoch": 22.547283702213278,
      "grad_norm": 0.6696088910102844,
      "learning_rate": 0.00015492101821108764,
      "loss": 0.3518,
      "step": 11206
    },
    {
      "epoch": 22.549295774647888,
      "grad_norm": 0.7208285927772522,
      "learning_rate": 0.00015491699366133416,
      "loss": 0.3439,
      "step": 11207
    },
    {
      "epoch": 22.551307847082494,
      "grad_norm": 0.6763591766357422,
      "learning_rate": 0.00015491296911158064,
      "loss": 0.3155,
      "step": 11208
    },
    {
      "epoch": 22.5533199195171,
      "grad_norm": 0.6903674006462097,
      "learning_rate": 0.00015490894456182715,
      "loss": 0.3456,
      "step": 11209
    },
    {
      "epoch": 22.55533199195171,
      "grad_norm": 0.6980395317077637,
      "learning_rate": 0.00015490492001207366,
      "loss": 0.3323,
      "step": 11210
    },
    {
      "epoch": 22.557344064386317,
      "grad_norm": 0.6987695097923279,
      "learning_rate": 0.00015490089546232015,
      "loss": 0.3597,
      "step": 11211
    },
    {
      "epoch": 22.559356136820927,
      "grad_norm": 0.6845657825469971,
      "learning_rate": 0.00015489687091256666,
      "loss": 0.343,
      "step": 11212
    },
    {
      "epoch": 22.561368209255534,
      "grad_norm": 0.7065497040748596,
      "learning_rate": 0.00015489284636281315,
      "loss": 0.3674,
      "step": 11213
    },
    {
      "epoch": 22.56338028169014,
      "grad_norm": 0.6885982751846313,
      "learning_rate": 0.0001548888218130597,
      "loss": 0.3243,
      "step": 11214
    },
    {
      "epoch": 22.56539235412475,
      "grad_norm": 0.7029686570167542,
      "learning_rate": 0.00015488479726330617,
      "loss": 0.3433,
      "step": 11215
    },
    {
      "epoch": 22.567404426559357,
      "grad_norm": 0.6522937417030334,
      "learning_rate": 0.00015488077271355268,
      "loss": 0.3202,
      "step": 11216
    },
    {
      "epoch": 22.569416498993963,
      "grad_norm": 0.6905965209007263,
      "learning_rate": 0.00015487674816379917,
      "loss": 0.3542,
      "step": 11217
    },
    {
      "epoch": 22.571428571428573,
      "grad_norm": 0.6936699748039246,
      "learning_rate": 0.00015487272361404568,
      "loss": 0.3629,
      "step": 11218
    },
    {
      "epoch": 22.57344064386318,
      "grad_norm": 0.686420202255249,
      "learning_rate": 0.0001548686990642922,
      "loss": 0.3289,
      "step": 11219
    },
    {
      "epoch": 22.575452716297786,
      "grad_norm": 0.7031345367431641,
      "learning_rate": 0.0001548646745145387,
      "loss": 0.3601,
      "step": 11220
    },
    {
      "epoch": 22.577464788732396,
      "grad_norm": 0.676455557346344,
      "learning_rate": 0.0001548606499647852,
      "loss": 0.3335,
      "step": 11221
    },
    {
      "epoch": 22.579476861167002,
      "grad_norm": 0.6791977882385254,
      "learning_rate": 0.0001548566254150317,
      "loss": 0.3477,
      "step": 11222
    },
    {
      "epoch": 22.58148893360161,
      "grad_norm": 0.6818270087242126,
      "learning_rate": 0.0001548526008652782,
      "loss": 0.3437,
      "step": 11223
    },
    {
      "epoch": 22.58350100603622,
      "grad_norm": 0.7122452259063721,
      "learning_rate": 0.00015484857631552473,
      "loss": 0.3775,
      "step": 11224
    },
    {
      "epoch": 22.585513078470825,
      "grad_norm": 0.7132319808006287,
      "learning_rate": 0.0001548445517657712,
      "loss": 0.3794,
      "step": 11225
    },
    {
      "epoch": 22.58752515090543,
      "grad_norm": 0.7368051409721375,
      "learning_rate": 0.00015484052721601772,
      "loss": 0.3431,
      "step": 11226
    },
    {
      "epoch": 22.58953722334004,
      "grad_norm": 0.6735227704048157,
      "learning_rate": 0.0001548365026662642,
      "loss": 0.3437,
      "step": 11227
    },
    {
      "epoch": 22.591549295774648,
      "grad_norm": 0.7101548314094543,
      "learning_rate": 0.00015483247811651072,
      "loss": 0.3319,
      "step": 11228
    },
    {
      "epoch": 22.593561368209254,
      "grad_norm": 0.6891064643859863,
      "learning_rate": 0.00015482845356675723,
      "loss": 0.3445,
      "step": 11229
    },
    {
      "epoch": 22.595573440643864,
      "grad_norm": 0.6873366832733154,
      "learning_rate": 0.00015482442901700375,
      "loss": 0.3394,
      "step": 11230
    },
    {
      "epoch": 22.59758551307847,
      "grad_norm": 0.6833487153053284,
      "learning_rate": 0.00015482040446725023,
      "loss": 0.3546,
      "step": 11231
    },
    {
      "epoch": 22.599597585513077,
      "grad_norm": 0.7667089104652405,
      "learning_rate": 0.00015481637991749674,
      "loss": 0.3751,
      "step": 11232
    },
    {
      "epoch": 22.601609657947687,
      "grad_norm": 0.6893424391746521,
      "learning_rate": 0.00015481235536774323,
      "loss": 0.3407,
      "step": 11233
    },
    {
      "epoch": 22.603621730382294,
      "grad_norm": 0.671194314956665,
      "learning_rate": 0.00015480833081798977,
      "loss": 0.337,
      "step": 11234
    },
    {
      "epoch": 22.6056338028169,
      "grad_norm": 0.7172518968582153,
      "learning_rate": 0.00015480430626823625,
      "loss": 0.3601,
      "step": 11235
    },
    {
      "epoch": 22.60764587525151,
      "grad_norm": 0.705696702003479,
      "learning_rate": 0.00015480028171848277,
      "loss": 0.3358,
      "step": 11236
    },
    {
      "epoch": 22.609657947686117,
      "grad_norm": 0.736041247844696,
      "learning_rate": 0.00015479625716872925,
      "loss": 0.3237,
      "step": 11237
    },
    {
      "epoch": 22.611670020120723,
      "grad_norm": 0.7555522322654724,
      "learning_rate": 0.00015479223261897576,
      "loss": 0.3419,
      "step": 11238
    },
    {
      "epoch": 22.613682092555333,
      "grad_norm": 0.7033807635307312,
      "learning_rate": 0.00015478820806922228,
      "loss": 0.3295,
      "step": 11239
    },
    {
      "epoch": 22.61569416498994,
      "grad_norm": 0.6636097431182861,
      "learning_rate": 0.00015478418351946876,
      "loss": 0.305,
      "step": 11240
    },
    {
      "epoch": 22.617706237424546,
      "grad_norm": 0.7051092982292175,
      "learning_rate": 0.00015478015896971527,
      "loss": 0.3334,
      "step": 11241
    },
    {
      "epoch": 22.619718309859156,
      "grad_norm": 0.7317057847976685,
      "learning_rate": 0.00015477613441996178,
      "loss": 0.3449,
      "step": 11242
    },
    {
      "epoch": 22.621730382293762,
      "grad_norm": 0.7230234146118164,
      "learning_rate": 0.00015477210987020827,
      "loss": 0.3785,
      "step": 11243
    },
    {
      "epoch": 22.62374245472837,
      "grad_norm": 0.6623758673667908,
      "learning_rate": 0.00015476808532045478,
      "loss": 0.3379,
      "step": 11244
    },
    {
      "epoch": 22.62575452716298,
      "grad_norm": 0.6707388758659363,
      "learning_rate": 0.0001547640607707013,
      "loss": 0.3303,
      "step": 11245
    },
    {
      "epoch": 22.627766599597585,
      "grad_norm": 0.7339880466461182,
      "learning_rate": 0.00015476003622094778,
      "loss": 0.3626,
      "step": 11246
    },
    {
      "epoch": 22.62977867203219,
      "grad_norm": 0.6884732246398926,
      "learning_rate": 0.0001547560116711943,
      "loss": 0.3305,
      "step": 11247
    },
    {
      "epoch": 22.6317907444668,
      "grad_norm": 0.6876757144927979,
      "learning_rate": 0.00015475198712144078,
      "loss": 0.3311,
      "step": 11248
    },
    {
      "epoch": 22.633802816901408,
      "grad_norm": 0.7667964100837708,
      "learning_rate": 0.00015474796257168732,
      "loss": 0.3614,
      "step": 11249
    },
    {
      "epoch": 22.635814889336014,
      "grad_norm": 0.6929194331169128,
      "learning_rate": 0.0001547439380219338,
      "loss": 0.3383,
      "step": 11250
    },
    {
      "epoch": 22.637826961770624,
      "grad_norm": 0.7021170258522034,
      "learning_rate": 0.0001547399134721803,
      "loss": 0.3446,
      "step": 11251
    },
    {
      "epoch": 22.63983903420523,
      "grad_norm": 0.6679450273513794,
      "learning_rate": 0.0001547358889224268,
      "loss": 0.3513,
      "step": 11252
    },
    {
      "epoch": 22.641851106639837,
      "grad_norm": 0.674105167388916,
      "learning_rate": 0.0001547318643726733,
      "loss": 0.3525,
      "step": 11253
    },
    {
      "epoch": 22.643863179074447,
      "grad_norm": 0.7380355596542358,
      "learning_rate": 0.00015472783982291982,
      "loss": 0.3532,
      "step": 11254
    },
    {
      "epoch": 22.645875251509054,
      "grad_norm": 0.6889665722846985,
      "learning_rate": 0.00015472381527316634,
      "loss": 0.3546,
      "step": 11255
    },
    {
      "epoch": 22.647887323943664,
      "grad_norm": 0.6848376393318176,
      "learning_rate": 0.00015471979072341282,
      "loss": 0.3651,
      "step": 11256
    },
    {
      "epoch": 22.64989939637827,
      "grad_norm": 0.6875730752944946,
      "learning_rate": 0.00015471576617365933,
      "loss": 0.3229,
      "step": 11257
    },
    {
      "epoch": 22.651911468812877,
      "grad_norm": 0.682773232460022,
      "learning_rate": 0.00015471174162390582,
      "loss": 0.3465,
      "step": 11258
    },
    {
      "epoch": 22.653923541247487,
      "grad_norm": 0.6821134686470032,
      "learning_rate": 0.00015470771707415236,
      "loss": 0.3442,
      "step": 11259
    },
    {
      "epoch": 22.655935613682093,
      "grad_norm": 0.7169476747512817,
      "learning_rate": 0.00015470369252439884,
      "loss": 0.3686,
      "step": 11260
    },
    {
      "epoch": 22.6579476861167,
      "grad_norm": 0.7321252226829529,
      "learning_rate": 0.00015469966797464535,
      "loss": 0.3495,
      "step": 11261
    },
    {
      "epoch": 22.65995975855131,
      "grad_norm": 0.713188111782074,
      "learning_rate": 0.00015469564342489184,
      "loss": 0.3453,
      "step": 11262
    },
    {
      "epoch": 22.661971830985916,
      "grad_norm": 0.7077134251594543,
      "learning_rate": 0.00015469161887513835,
      "loss": 0.3587,
      "step": 11263
    },
    {
      "epoch": 22.663983903420522,
      "grad_norm": 0.6882242560386658,
      "learning_rate": 0.00015468759432538486,
      "loss": 0.3324,
      "step": 11264
    },
    {
      "epoch": 22.665995975855132,
      "grad_norm": 0.6498900055885315,
      "learning_rate": 0.00015468356977563138,
      "loss": 0.3172,
      "step": 11265
    },
    {
      "epoch": 22.66800804828974,
      "grad_norm": 0.6993517875671387,
      "learning_rate": 0.00015467954522587786,
      "loss": 0.3326,
      "step": 11266
    },
    {
      "epoch": 22.670020120724345,
      "grad_norm": 0.7091032862663269,
      "learning_rate": 0.00015467552067612437,
      "loss": 0.3521,
      "step": 11267
    },
    {
      "epoch": 22.672032193158955,
      "grad_norm": 0.6830829977989197,
      "learning_rate": 0.00015467149612637086,
      "loss": 0.3696,
      "step": 11268
    },
    {
      "epoch": 22.67404426559356,
      "grad_norm": 0.6515423655509949,
      "learning_rate": 0.0001546674715766174,
      "loss": 0.3278,
      "step": 11269
    },
    {
      "epoch": 22.676056338028168,
      "grad_norm": 0.6972302198410034,
      "learning_rate": 0.00015466344702686388,
      "loss": 0.3302,
      "step": 11270
    },
    {
      "epoch": 22.678068410462778,
      "grad_norm": 0.7240737080574036,
      "learning_rate": 0.0001546594224771104,
      "loss": 0.3643,
      "step": 11271
    },
    {
      "epoch": 22.680080482897385,
      "grad_norm": 0.6928339004516602,
      "learning_rate": 0.00015465539792735688,
      "loss": 0.3459,
      "step": 11272
    },
    {
      "epoch": 22.68209255533199,
      "grad_norm": 0.6702710390090942,
      "learning_rate": 0.0001546513733776034,
      "loss": 0.3388,
      "step": 11273
    },
    {
      "epoch": 22.6841046277666,
      "grad_norm": 0.7463957667350769,
      "learning_rate": 0.0001546473488278499,
      "loss": 0.3681,
      "step": 11274
    },
    {
      "epoch": 22.686116700201207,
      "grad_norm": 0.7011595368385315,
      "learning_rate": 0.0001546433242780964,
      "loss": 0.3391,
      "step": 11275
    },
    {
      "epoch": 22.688128772635814,
      "grad_norm": 0.6861675381660461,
      "learning_rate": 0.0001546392997283429,
      "loss": 0.3548,
      "step": 11276
    },
    {
      "epoch": 22.690140845070424,
      "grad_norm": 0.6745193004608154,
      "learning_rate": 0.0001546352751785894,
      "loss": 0.3164,
      "step": 11277
    },
    {
      "epoch": 22.69215291750503,
      "grad_norm": 0.6905263066291809,
      "learning_rate": 0.0001546312506288359,
      "loss": 0.3493,
      "step": 11278
    },
    {
      "epoch": 22.694164989939637,
      "grad_norm": 0.6980108618736267,
      "learning_rate": 0.0001546272260790824,
      "loss": 0.3485,
      "step": 11279
    },
    {
      "epoch": 22.696177062374247,
      "grad_norm": 0.6691039204597473,
      "learning_rate": 0.00015462320152932892,
      "loss": 0.3562,
      "step": 11280
    },
    {
      "epoch": 22.698189134808853,
      "grad_norm": 0.7267390489578247,
      "learning_rate": 0.0001546191769795754,
      "loss": 0.3377,
      "step": 11281
    },
    {
      "epoch": 22.70020120724346,
      "grad_norm": 0.6999214887619019,
      "learning_rate": 0.00015461515242982192,
      "loss": 0.3441,
      "step": 11282
    },
    {
      "epoch": 22.70221327967807,
      "grad_norm": 0.6737052202224731,
      "learning_rate": 0.0001546111278800684,
      "loss": 0.3273,
      "step": 11283
    },
    {
      "epoch": 22.704225352112676,
      "grad_norm": 0.6809380054473877,
      "learning_rate": 0.00015460710333031492,
      "loss": 0.3212,
      "step": 11284
    },
    {
      "epoch": 22.706237424547282,
      "grad_norm": 0.6924184560775757,
      "learning_rate": 0.00015460307878056143,
      "loss": 0.3418,
      "step": 11285
    },
    {
      "epoch": 22.708249496981892,
      "grad_norm": 0.7198722958564758,
      "learning_rate": 0.00015459905423080794,
      "loss": 0.3632,
      "step": 11286
    },
    {
      "epoch": 22.7102615694165,
      "grad_norm": 0.6837191581726074,
      "learning_rate": 0.00015459502968105443,
      "loss": 0.3647,
      "step": 11287
    },
    {
      "epoch": 22.712273641851105,
      "grad_norm": 0.7115609049797058,
      "learning_rate": 0.00015459100513130094,
      "loss": 0.3427,
      "step": 11288
    },
    {
      "epoch": 22.714285714285715,
      "grad_norm": 0.7411859035491943,
      "learning_rate": 0.00015458698058154743,
      "loss": 0.3856,
      "step": 11289
    },
    {
      "epoch": 22.71629778672032,
      "grad_norm": 0.6946622133255005,
      "learning_rate": 0.00015458295603179396,
      "loss": 0.338,
      "step": 11290
    },
    {
      "epoch": 22.718309859154928,
      "grad_norm": 0.7271268963813782,
      "learning_rate": 0.00015457893148204045,
      "loss": 0.3641,
      "step": 11291
    },
    {
      "epoch": 22.720321931589538,
      "grad_norm": 0.681938111782074,
      "learning_rate": 0.00015457490693228696,
      "loss": 0.3086,
      "step": 11292
    },
    {
      "epoch": 22.722334004024145,
      "grad_norm": 0.6933960914611816,
      "learning_rate": 0.00015457088238253345,
      "loss": 0.3597,
      "step": 11293
    },
    {
      "epoch": 22.72434607645875,
      "grad_norm": 0.6998251080513,
      "learning_rate": 0.00015456685783277996,
      "loss": 0.3576,
      "step": 11294
    },
    {
      "epoch": 22.72635814889336,
      "grad_norm": 0.6979880928993225,
      "learning_rate": 0.00015456283328302647,
      "loss": 0.3494,
      "step": 11295
    },
    {
      "epoch": 22.728370221327967,
      "grad_norm": 0.7391951084136963,
      "learning_rate": 0.00015455880873327298,
      "loss": 0.381,
      "step": 11296
    },
    {
      "epoch": 22.730382293762574,
      "grad_norm": 0.7054518461227417,
      "learning_rate": 0.00015455478418351947,
      "loss": 0.3249,
      "step": 11297
    },
    {
      "epoch": 22.732394366197184,
      "grad_norm": 0.6704959869384766,
      "learning_rate": 0.00015455075963376598,
      "loss": 0.3431,
      "step": 11298
    },
    {
      "epoch": 22.73440643863179,
      "grad_norm": 0.734988272190094,
      "learning_rate": 0.00015454673508401247,
      "loss": 0.3351,
      "step": 11299
    },
    {
      "epoch": 22.736418511066397,
      "grad_norm": 0.7116913795471191,
      "learning_rate": 0.000154542710534259,
      "loss": 0.3672,
      "step": 11300
    },
    {
      "epoch": 22.738430583501007,
      "grad_norm": 0.7014293074607849,
      "learning_rate": 0.0001545386859845055,
      "loss": 0.3574,
      "step": 11301
    },
    {
      "epoch": 22.740442655935613,
      "grad_norm": 0.698110818862915,
      "learning_rate": 0.000154534661434752,
      "loss": 0.3646,
      "step": 11302
    },
    {
      "epoch": 22.74245472837022,
      "grad_norm": 0.6913431882858276,
      "learning_rate": 0.0001545306368849985,
      "loss": 0.3331,
      "step": 11303
    },
    {
      "epoch": 22.74446680080483,
      "grad_norm": 0.6999531388282776,
      "learning_rate": 0.000154526612335245,
      "loss": 0.3797,
      "step": 11304
    },
    {
      "epoch": 22.746478873239436,
      "grad_norm": 0.6837363243103027,
      "learning_rate": 0.0001545225877854915,
      "loss": 0.3676,
      "step": 11305
    },
    {
      "epoch": 22.748490945674043,
      "grad_norm": 0.6773632764816284,
      "learning_rate": 0.00015451856323573802,
      "loss": 0.3543,
      "step": 11306
    },
    {
      "epoch": 22.750503018108652,
      "grad_norm": 0.7006139159202576,
      "learning_rate": 0.0001545145386859845,
      "loss": 0.3618,
      "step": 11307
    },
    {
      "epoch": 22.75251509054326,
      "grad_norm": 0.7042308449745178,
      "learning_rate": 0.00015451051413623102,
      "loss": 0.344,
      "step": 11308
    },
    {
      "epoch": 22.754527162977865,
      "grad_norm": 0.6866311430931091,
      "learning_rate": 0.0001545064895864775,
      "loss": 0.3503,
      "step": 11309
    },
    {
      "epoch": 22.756539235412475,
      "grad_norm": 0.6904681324958801,
      "learning_rate": 0.00015450246503672402,
      "loss": 0.3712,
      "step": 11310
    },
    {
      "epoch": 22.758551307847082,
      "grad_norm": 0.7047892212867737,
      "learning_rate": 0.00015449844048697053,
      "loss": 0.37,
      "step": 11311
    },
    {
      "epoch": 22.760563380281692,
      "grad_norm": 0.6938291788101196,
      "learning_rate": 0.00015449441593721702,
      "loss": 0.3593,
      "step": 11312
    },
    {
      "epoch": 22.7625754527163,
      "grad_norm": 0.7169384360313416,
      "learning_rate": 0.00015449039138746353,
      "loss": 0.3338,
      "step": 11313
    },
    {
      "epoch": 22.764587525150905,
      "grad_norm": 0.7086873054504395,
      "learning_rate": 0.00015448636683771004,
      "loss": 0.3661,
      "step": 11314
    },
    {
      "epoch": 22.766599597585515,
      "grad_norm": 0.8695895671844482,
      "learning_rate": 0.00015448234228795655,
      "loss": 0.3969,
      "step": 11315
    },
    {
      "epoch": 22.76861167002012,
      "grad_norm": 0.7691351771354675,
      "learning_rate": 0.00015447831773820304,
      "loss": 0.3509,
      "step": 11316
    },
    {
      "epoch": 22.770623742454728,
      "grad_norm": 0.6986674070358276,
      "learning_rate": 0.00015447429318844955,
      "loss": 0.3606,
      "step": 11317
    },
    {
      "epoch": 22.772635814889338,
      "grad_norm": 0.6778181195259094,
      "learning_rate": 0.00015447026863869604,
      "loss": 0.331,
      "step": 11318
    },
    {
      "epoch": 22.774647887323944,
      "grad_norm": 0.7067921757698059,
      "learning_rate": 0.00015446624408894255,
      "loss": 0.3494,
      "step": 11319
    },
    {
      "epoch": 22.77665995975855,
      "grad_norm": 0.7728439569473267,
      "learning_rate": 0.00015446221953918906,
      "loss": 0.3584,
      "step": 11320
    },
    {
      "epoch": 22.77867203219316,
      "grad_norm": 0.7079077363014221,
      "learning_rate": 0.00015445819498943557,
      "loss": 0.3688,
      "step": 11321
    },
    {
      "epoch": 22.780684104627767,
      "grad_norm": 0.7145649790763855,
      "learning_rate": 0.00015445417043968206,
      "loss": 0.3695,
      "step": 11322
    },
    {
      "epoch": 22.782696177062373,
      "grad_norm": 0.6916996240615845,
      "learning_rate": 0.00015445014588992857,
      "loss": 0.362,
      "step": 11323
    },
    {
      "epoch": 22.784708249496983,
      "grad_norm": 0.6428875923156738,
      "learning_rate": 0.00015444612134017505,
      "loss": 0.3517,
      "step": 11324
    },
    {
      "epoch": 22.78672032193159,
      "grad_norm": 0.6690248250961304,
      "learning_rate": 0.0001544420967904216,
      "loss": 0.3409,
      "step": 11325
    },
    {
      "epoch": 22.788732394366196,
      "grad_norm": 0.7338866591453552,
      "learning_rate": 0.00015443807224066808,
      "loss": 0.3962,
      "step": 11326
    },
    {
      "epoch": 22.790744466800806,
      "grad_norm": 0.7128099203109741,
      "learning_rate": 0.0001544340476909146,
      "loss": 0.3711,
      "step": 11327
    },
    {
      "epoch": 22.792756539235413,
      "grad_norm": 0.7020745277404785,
      "learning_rate": 0.00015443002314116108,
      "loss": 0.3452,
      "step": 11328
    },
    {
      "epoch": 22.79476861167002,
      "grad_norm": 0.6759015321731567,
      "learning_rate": 0.0001544259985914076,
      "loss": 0.345,
      "step": 11329
    },
    {
      "epoch": 22.79678068410463,
      "grad_norm": 0.7573893666267395,
      "learning_rate": 0.0001544219740416541,
      "loss": 0.3634,
      "step": 11330
    },
    {
      "epoch": 22.798792756539235,
      "grad_norm": 0.7077041864395142,
      "learning_rate": 0.0001544179494919006,
      "loss": 0.3777,
      "step": 11331
    },
    {
      "epoch": 22.800804828973842,
      "grad_norm": 0.7011071443557739,
      "learning_rate": 0.0001544139249421471,
      "loss": 0.37,
      "step": 11332
    },
    {
      "epoch": 22.802816901408452,
      "grad_norm": 0.6663325428962708,
      "learning_rate": 0.0001544099003923936,
      "loss": 0.3625,
      "step": 11333
    },
    {
      "epoch": 22.80482897384306,
      "grad_norm": 0.6948114037513733,
      "learning_rate": 0.0001544058758426401,
      "loss": 0.3451,
      "step": 11334
    },
    {
      "epoch": 22.806841046277665,
      "grad_norm": 0.6728286147117615,
      "learning_rate": 0.00015440185129288663,
      "loss": 0.3426,
      "step": 11335
    },
    {
      "epoch": 22.808853118712275,
      "grad_norm": 0.6574895977973938,
      "learning_rate": 0.00015439782674313312,
      "loss": 0.3309,
      "step": 11336
    },
    {
      "epoch": 22.81086519114688,
      "grad_norm": 0.7249425053596497,
      "learning_rate": 0.00015439380219337963,
      "loss": 0.3552,
      "step": 11337
    },
    {
      "epoch": 22.812877263581488,
      "grad_norm": 0.6706894040107727,
      "learning_rate": 0.00015438977764362612,
      "loss": 0.3364,
      "step": 11338
    },
    {
      "epoch": 22.814889336016098,
      "grad_norm": 0.6678091287612915,
      "learning_rate": 0.00015438575309387263,
      "loss": 0.3612,
      "step": 11339
    },
    {
      "epoch": 22.816901408450704,
      "grad_norm": 0.745561957359314,
      "learning_rate": 0.00015438172854411914,
      "loss": 0.3696,
      "step": 11340
    },
    {
      "epoch": 22.81891348088531,
      "grad_norm": 0.6734577417373657,
      "learning_rate": 0.00015437770399436565,
      "loss": 0.3479,
      "step": 11341
    },
    {
      "epoch": 22.82092555331992,
      "grad_norm": 0.6573654413223267,
      "learning_rate": 0.00015437367944461214,
      "loss": 0.3457,
      "step": 11342
    },
    {
      "epoch": 22.822937625754527,
      "grad_norm": 0.671428918838501,
      "learning_rate": 0.00015436965489485865,
      "loss": 0.3576,
      "step": 11343
    },
    {
      "epoch": 22.824949698189133,
      "grad_norm": 0.7138749361038208,
      "learning_rate": 0.00015436563034510514,
      "loss": 0.3517,
      "step": 11344
    },
    {
      "epoch": 22.826961770623743,
      "grad_norm": 0.673026978969574,
      "learning_rate": 0.00015436160579535165,
      "loss": 0.3794,
      "step": 11345
    },
    {
      "epoch": 22.82897384305835,
      "grad_norm": 0.7383316159248352,
      "learning_rate": 0.00015435758124559816,
      "loss": 0.3552,
      "step": 11346
    },
    {
      "epoch": 22.830985915492956,
      "grad_norm": 0.6738818287849426,
      "learning_rate": 0.00015435355669584465,
      "loss": 0.3539,
      "step": 11347
    },
    {
      "epoch": 22.832997987927566,
      "grad_norm": 0.6644585728645325,
      "learning_rate": 0.00015434953214609116,
      "loss": 0.3409,
      "step": 11348
    },
    {
      "epoch": 22.835010060362173,
      "grad_norm": 0.6896158456802368,
      "learning_rate": 0.00015434550759633767,
      "loss": 0.3311,
      "step": 11349
    },
    {
      "epoch": 22.83702213279678,
      "grad_norm": 0.7160305976867676,
      "learning_rate": 0.00015434148304658418,
      "loss": 0.3654,
      "step": 11350
    },
    {
      "epoch": 22.83903420523139,
      "grad_norm": 0.6813358068466187,
      "learning_rate": 0.00015433745849683067,
      "loss": 0.3673,
      "step": 11351
    },
    {
      "epoch": 22.841046277665995,
      "grad_norm": 0.6883406043052673,
      "learning_rate": 0.00015433343394707718,
      "loss": 0.363,
      "step": 11352
    },
    {
      "epoch": 22.843058350100602,
      "grad_norm": 0.7846466302871704,
      "learning_rate": 0.00015432940939732366,
      "loss": 0.3758,
      "step": 11353
    },
    {
      "epoch": 22.845070422535212,
      "grad_norm": 0.686418890953064,
      "learning_rate": 0.00015432538484757018,
      "loss": 0.3181,
      "step": 11354
    },
    {
      "epoch": 22.84708249496982,
      "grad_norm": 0.6787753701210022,
      "learning_rate": 0.0001543213602978167,
      "loss": 0.3657,
      "step": 11355
    },
    {
      "epoch": 22.84909456740443,
      "grad_norm": 0.6641349196434021,
      "learning_rate": 0.0001543173357480632,
      "loss": 0.366,
      "step": 11356
    },
    {
      "epoch": 22.851106639839035,
      "grad_norm": 0.7011346817016602,
      "learning_rate": 0.0001543133111983097,
      "loss": 0.3957,
      "step": 11357
    },
    {
      "epoch": 22.85311871227364,
      "grad_norm": 0.6402190923690796,
      "learning_rate": 0.0001543092866485562,
      "loss": 0.3412,
      "step": 11358
    },
    {
      "epoch": 22.85513078470825,
      "grad_norm": 0.7481808662414551,
      "learning_rate": 0.00015430526209880268,
      "loss": 0.3667,
      "step": 11359
    },
    {
      "epoch": 22.857142857142858,
      "grad_norm": 0.735209584236145,
      "learning_rate": 0.00015430123754904922,
      "loss": 0.3402,
      "step": 11360
    },
    {
      "epoch": 22.859154929577464,
      "grad_norm": 0.7023525238037109,
      "learning_rate": 0.0001542972129992957,
      "loss": 0.3515,
      "step": 11361
    },
    {
      "epoch": 22.861167002012074,
      "grad_norm": 0.6817169785499573,
      "learning_rate": 0.00015429318844954222,
      "loss": 0.3452,
      "step": 11362
    },
    {
      "epoch": 22.86317907444668,
      "grad_norm": 0.7169330716133118,
      "learning_rate": 0.0001542891638997887,
      "loss": 0.3391,
      "step": 11363
    },
    {
      "epoch": 22.865191146881287,
      "grad_norm": 0.7280346155166626,
      "learning_rate": 0.00015428513935003522,
      "loss": 0.3837,
      "step": 11364
    },
    {
      "epoch": 22.867203219315897,
      "grad_norm": 0.6975522041320801,
      "learning_rate": 0.00015428111480028173,
      "loss": 0.346,
      "step": 11365
    },
    {
      "epoch": 22.869215291750503,
      "grad_norm": 0.6518831849098206,
      "learning_rate": 0.00015427709025052824,
      "loss": 0.3366,
      "step": 11366
    },
    {
      "epoch": 22.87122736418511,
      "grad_norm": 0.6782218813896179,
      "learning_rate": 0.00015427306570077473,
      "loss": 0.3348,
      "step": 11367
    },
    {
      "epoch": 22.87323943661972,
      "grad_norm": 0.7219122052192688,
      "learning_rate": 0.00015426904115102124,
      "loss": 0.3832,
      "step": 11368
    },
    {
      "epoch": 22.875251509054326,
      "grad_norm": 0.6794013977050781,
      "learning_rate": 0.00015426501660126772,
      "loss": 0.3414,
      "step": 11369
    },
    {
      "epoch": 22.877263581488933,
      "grad_norm": 0.677000105381012,
      "learning_rate": 0.00015426099205151426,
      "loss": 0.3691,
      "step": 11370
    },
    {
      "epoch": 22.879275653923543,
      "grad_norm": 0.7350112795829773,
      "learning_rate": 0.00015425696750176075,
      "loss": 0.3565,
      "step": 11371
    },
    {
      "epoch": 22.88128772635815,
      "grad_norm": 0.7285851836204529,
      "learning_rate": 0.00015425294295200726,
      "loss": 0.371,
      "step": 11372
    },
    {
      "epoch": 22.883299798792756,
      "grad_norm": 0.6824573278427124,
      "learning_rate": 0.00015424891840225375,
      "loss": 0.3178,
      "step": 11373
    },
    {
      "epoch": 22.885311871227366,
      "grad_norm": 0.7016588449478149,
      "learning_rate": 0.00015424489385250026,
      "loss": 0.3378,
      "step": 11374
    },
    {
      "epoch": 22.887323943661972,
      "grad_norm": 0.6773625612258911,
      "learning_rate": 0.00015424086930274677,
      "loss": 0.3454,
      "step": 11375
    },
    {
      "epoch": 22.88933601609658,
      "grad_norm": 0.6868624091148376,
      "learning_rate": 0.00015423684475299328,
      "loss": 0.359,
      "step": 11376
    },
    {
      "epoch": 22.89134808853119,
      "grad_norm": 0.6867939233779907,
      "learning_rate": 0.00015423282020323977,
      "loss": 0.3682,
      "step": 11377
    },
    {
      "epoch": 22.893360160965795,
      "grad_norm": 0.7207152843475342,
      "learning_rate": 0.00015422879565348628,
      "loss": 0.363,
      "step": 11378
    },
    {
      "epoch": 22.8953722334004,
      "grad_norm": 0.7021419405937195,
      "learning_rate": 0.00015422477110373277,
      "loss": 0.3409,
      "step": 11379
    },
    {
      "epoch": 22.89738430583501,
      "grad_norm": 0.7007200121879578,
      "learning_rate": 0.00015422074655397928,
      "loss": 0.3514,
      "step": 11380
    },
    {
      "epoch": 22.899396378269618,
      "grad_norm": 0.7086578607559204,
      "learning_rate": 0.0001542167220042258,
      "loss": 0.3462,
      "step": 11381
    },
    {
      "epoch": 22.901408450704224,
      "grad_norm": 0.6910988688468933,
      "learning_rate": 0.00015421269745447228,
      "loss": 0.3611,
      "step": 11382
    },
    {
      "epoch": 22.903420523138834,
      "grad_norm": 0.703908383846283,
      "learning_rate": 0.0001542086729047188,
      "loss": 0.3669,
      "step": 11383
    },
    {
      "epoch": 22.90543259557344,
      "grad_norm": 0.7251765727996826,
      "learning_rate": 0.0001542046483549653,
      "loss": 0.3538,
      "step": 11384
    },
    {
      "epoch": 22.907444668008047,
      "grad_norm": 0.6930004358291626,
      "learning_rate": 0.0001542006238052118,
      "loss": 0.3334,
      "step": 11385
    },
    {
      "epoch": 22.909456740442657,
      "grad_norm": 0.7251949906349182,
      "learning_rate": 0.0001541965992554583,
      "loss": 0.3888,
      "step": 11386
    },
    {
      "epoch": 22.911468812877263,
      "grad_norm": 0.678223729133606,
      "learning_rate": 0.0001541925747057048,
      "loss": 0.3648,
      "step": 11387
    },
    {
      "epoch": 22.91348088531187,
      "grad_norm": 0.692916214466095,
      "learning_rate": 0.0001541885501559513,
      "loss": 0.3589,
      "step": 11388
    },
    {
      "epoch": 22.91549295774648,
      "grad_norm": 0.6598613262176514,
      "learning_rate": 0.0001541845256061978,
      "loss": 0.348,
      "step": 11389
    },
    {
      "epoch": 22.917505030181086,
      "grad_norm": 0.6675928831100464,
      "learning_rate": 0.00015418050105644432,
      "loss": 0.3606,
      "step": 11390
    },
    {
      "epoch": 22.919517102615693,
      "grad_norm": 0.7258895039558411,
      "learning_rate": 0.00015417647650669083,
      "loss": 0.3732,
      "step": 11391
    },
    {
      "epoch": 22.921529175050303,
      "grad_norm": 0.6838187575340271,
      "learning_rate": 0.00015417245195693732,
      "loss": 0.3403,
      "step": 11392
    },
    {
      "epoch": 22.92354124748491,
      "grad_norm": 0.6832234263420105,
      "learning_rate": 0.00015416842740718383,
      "loss": 0.3659,
      "step": 11393
    },
    {
      "epoch": 22.925553319919516,
      "grad_norm": 0.7101696729660034,
      "learning_rate": 0.0001541644028574303,
      "loss": 0.396,
      "step": 11394
    },
    {
      "epoch": 22.927565392354126,
      "grad_norm": 0.6877366900444031,
      "learning_rate": 0.00015416037830767685,
      "loss": 0.3623,
      "step": 11395
    },
    {
      "epoch": 22.929577464788732,
      "grad_norm": 0.6821429133415222,
      "learning_rate": 0.00015415635375792334,
      "loss": 0.3628,
      "step": 11396
    },
    {
      "epoch": 22.93158953722334,
      "grad_norm": 0.6942313313484192,
      "learning_rate": 0.00015415232920816985,
      "loss": 0.3613,
      "step": 11397
    },
    {
      "epoch": 22.93360160965795,
      "grad_norm": 0.704281747341156,
      "learning_rate": 0.00015414830465841634,
      "loss": 0.3732,
      "step": 11398
    },
    {
      "epoch": 22.935613682092555,
      "grad_norm": 0.6752867698669434,
      "learning_rate": 0.00015414428010866285,
      "loss": 0.3539,
      "step": 11399
    },
    {
      "epoch": 22.93762575452716,
      "grad_norm": 0.7003076672554016,
      "learning_rate": 0.00015414025555890936,
      "loss": 0.3526,
      "step": 11400
    },
    {
      "epoch": 22.93963782696177,
      "grad_norm": 0.710323691368103,
      "learning_rate": 0.00015413623100915587,
      "loss": 0.3529,
      "step": 11401
    },
    {
      "epoch": 22.941649899396378,
      "grad_norm": 0.7145968079566956,
      "learning_rate": 0.00015413220645940236,
      "loss": 0.3445,
      "step": 11402
    },
    {
      "epoch": 22.943661971830984,
      "grad_norm": 0.6633100509643555,
      "learning_rate": 0.00015412818190964887,
      "loss": 0.3292,
      "step": 11403
    },
    {
      "epoch": 22.945674044265594,
      "grad_norm": 0.674527108669281,
      "learning_rate": 0.00015412415735989535,
      "loss": 0.3546,
      "step": 11404
    },
    {
      "epoch": 22.9476861167002,
      "grad_norm": 0.6880995035171509,
      "learning_rate": 0.0001541201328101419,
      "loss": 0.3311,
      "step": 11405
    },
    {
      "epoch": 22.949698189134807,
      "grad_norm": 0.6483495831489563,
      "learning_rate": 0.00015411610826038838,
      "loss": 0.3401,
      "step": 11406
    },
    {
      "epoch": 22.951710261569417,
      "grad_norm": 0.6456121802330017,
      "learning_rate": 0.0001541120837106349,
      "loss": 0.3365,
      "step": 11407
    },
    {
      "epoch": 22.953722334004024,
      "grad_norm": 0.6909138560295105,
      "learning_rate": 0.00015410805916088138,
      "loss": 0.3602,
      "step": 11408
    },
    {
      "epoch": 22.955734406438633,
      "grad_norm": 0.6691924929618835,
      "learning_rate": 0.0001541040346111279,
      "loss": 0.3632,
      "step": 11409
    },
    {
      "epoch": 22.95774647887324,
      "grad_norm": 0.7054941654205322,
      "learning_rate": 0.0001541000100613744,
      "loss": 0.3655,
      "step": 11410
    },
    {
      "epoch": 22.959758551307846,
      "grad_norm": 0.6863018274307251,
      "learning_rate": 0.0001540959855116209,
      "loss": 0.3436,
      "step": 11411
    },
    {
      "epoch": 22.961770623742456,
      "grad_norm": 0.689461886882782,
      "learning_rate": 0.0001540919609618674,
      "loss": 0.3484,
      "step": 11412
    },
    {
      "epoch": 22.963782696177063,
      "grad_norm": 0.6839672923088074,
      "learning_rate": 0.0001540879364121139,
      "loss": 0.3477,
      "step": 11413
    },
    {
      "epoch": 22.96579476861167,
      "grad_norm": 0.6873230934143066,
      "learning_rate": 0.0001540839118623604,
      "loss": 0.354,
      "step": 11414
    },
    {
      "epoch": 22.96780684104628,
      "grad_norm": 0.7137263417243958,
      "learning_rate": 0.0001540798873126069,
      "loss": 0.3814,
      "step": 11415
    },
    {
      "epoch": 22.969818913480886,
      "grad_norm": 0.743246853351593,
      "learning_rate": 0.00015407586276285342,
      "loss": 0.3584,
      "step": 11416
    },
    {
      "epoch": 22.971830985915492,
      "grad_norm": 0.6883520483970642,
      "learning_rate": 0.0001540718382130999,
      "loss": 0.3285,
      "step": 11417
    },
    {
      "epoch": 22.973843058350102,
      "grad_norm": 0.6950322389602661,
      "learning_rate": 0.00015406781366334642,
      "loss": 0.3509,
      "step": 11418
    },
    {
      "epoch": 22.97585513078471,
      "grad_norm": 0.6642948389053345,
      "learning_rate": 0.00015406378911359293,
      "loss": 0.3413,
      "step": 11419
    },
    {
      "epoch": 22.977867203219315,
      "grad_norm": 0.6877460479736328,
      "learning_rate": 0.00015405976456383944,
      "loss": 0.3663,
      "step": 11420
    },
    {
      "epoch": 22.979879275653925,
      "grad_norm": 0.6867579221725464,
      "learning_rate": 0.00015405574001408593,
      "loss": 0.3517,
      "step": 11421
    },
    {
      "epoch": 22.98189134808853,
      "grad_norm": 0.7481282949447632,
      "learning_rate": 0.00015405171546433244,
      "loss": 0.3342,
      "step": 11422
    },
    {
      "epoch": 22.983903420523138,
      "grad_norm": 0.7128684520721436,
      "learning_rate": 0.00015404769091457892,
      "loss": 0.34,
      "step": 11423
    },
    {
      "epoch": 22.985915492957748,
      "grad_norm": 0.6900715827941895,
      "learning_rate": 0.00015404366636482544,
      "loss": 0.3335,
      "step": 11424
    },
    {
      "epoch": 22.987927565392354,
      "grad_norm": 0.7459735870361328,
      "learning_rate": 0.00015403964181507195,
      "loss": 0.3657,
      "step": 11425
    },
    {
      "epoch": 22.98993963782696,
      "grad_norm": 0.68084317445755,
      "learning_rate": 0.00015403561726531846,
      "loss": 0.3631,
      "step": 11426
    },
    {
      "epoch": 22.99195171026157,
      "grad_norm": 0.6986505389213562,
      "learning_rate": 0.00015403159271556495,
      "loss": 0.3364,
      "step": 11427
    },
    {
      "epoch": 22.993963782696177,
      "grad_norm": 0.651006281375885,
      "learning_rate": 0.00015402756816581146,
      "loss": 0.3429,
      "step": 11428
    },
    {
      "epoch": 22.995975855130784,
      "grad_norm": 0.7081262469291687,
      "learning_rate": 0.00015402354361605794,
      "loss": 0.3532,
      "step": 11429
    },
    {
      "epoch": 22.997987927565394,
      "grad_norm": 0.7093890309333801,
      "learning_rate": 0.00015401951906630448,
      "loss": 0.3563,
      "step": 11430
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.6991812586784363,
      "learning_rate": 0.00015401549451655097,
      "loss": 0.3253,
      "step": 11431
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.0548417568206787,
      "eval_runtime": 49.8319,
      "eval_samples_per_second": 19.907,
      "eval_steps_per_second": 2.488,
      "step": 11431
    },
    {
      "epoch": 23.002012072434606,
      "grad_norm": 0.6094598174095154,
      "learning_rate": 0.00015401146996679748,
      "loss": 0.3092,
      "step": 11432
    },
    {
      "epoch": 23.004024144869216,
      "grad_norm": 0.5839487910270691,
      "learning_rate": 0.00015400744541704396,
      "loss": 0.2758,
      "step": 11433
    },
    {
      "epoch": 23.006036217303823,
      "grad_norm": 0.6044374108314514,
      "learning_rate": 0.00015400342086729048,
      "loss": 0.259,
      "step": 11434
    },
    {
      "epoch": 23.00804828973843,
      "grad_norm": 0.6839122176170349,
      "learning_rate": 0.000153999396317537,
      "loss": 0.3031,
      "step": 11435
    },
    {
      "epoch": 23.01006036217304,
      "grad_norm": 0.6979578733444214,
      "learning_rate": 0.0001539953717677835,
      "loss": 0.3238,
      "step": 11436
    },
    {
      "epoch": 23.012072434607646,
      "grad_norm": 0.7727548480033875,
      "learning_rate": 0.00015399134721802999,
      "loss": 0.283,
      "step": 11437
    },
    {
      "epoch": 23.014084507042252,
      "grad_norm": 0.6906452775001526,
      "learning_rate": 0.0001539873226682765,
      "loss": 0.2776,
      "step": 11438
    },
    {
      "epoch": 23.016096579476862,
      "grad_norm": 0.6942914724349976,
      "learning_rate": 0.00015398329811852298,
      "loss": 0.2988,
      "step": 11439
    },
    {
      "epoch": 23.01810865191147,
      "grad_norm": 0.6418495774269104,
      "learning_rate": 0.00015397927356876952,
      "loss": 0.3016,
      "step": 11440
    },
    {
      "epoch": 23.020120724346075,
      "grad_norm": 0.6502043604850769,
      "learning_rate": 0.000153975249019016,
      "loss": 0.3044,
      "step": 11441
    },
    {
      "epoch": 23.022132796780685,
      "grad_norm": 0.675111711025238,
      "learning_rate": 0.00015397122446926252,
      "loss": 0.3139,
      "step": 11442
    },
    {
      "epoch": 23.02414486921529,
      "grad_norm": 0.6352524757385254,
      "learning_rate": 0.000153967199919509,
      "loss": 0.2829,
      "step": 11443
    },
    {
      "epoch": 23.026156941649898,
      "grad_norm": 0.664005696773529,
      "learning_rate": 0.00015396317536975552,
      "loss": 0.2948,
      "step": 11444
    },
    {
      "epoch": 23.028169014084508,
      "grad_norm": 0.6570883393287659,
      "learning_rate": 0.00015395915082000203,
      "loss": 0.3015,
      "step": 11445
    },
    {
      "epoch": 23.030181086519114,
      "grad_norm": 0.6207672357559204,
      "learning_rate": 0.00015395512627024854,
      "loss": 0.2812,
      "step": 11446
    },
    {
      "epoch": 23.03219315895372,
      "grad_norm": 0.6871423125267029,
      "learning_rate": 0.00015395110172049503,
      "loss": 0.308,
      "step": 11447
    },
    {
      "epoch": 23.03420523138833,
      "grad_norm": 0.6899372339248657,
      "learning_rate": 0.00015394707717074154,
      "loss": 0.2799,
      "step": 11448
    },
    {
      "epoch": 23.036217303822937,
      "grad_norm": 0.6670516133308411,
      "learning_rate": 0.00015394305262098802,
      "loss": 0.3025,
      "step": 11449
    },
    {
      "epoch": 23.038229376257544,
      "grad_norm": 0.6464749574661255,
      "learning_rate": 0.00015393902807123454,
      "loss": 0.2746,
      "step": 11450
    },
    {
      "epoch": 23.040241448692154,
      "grad_norm": 0.6282873153686523,
      "learning_rate": 0.00015393500352148105,
      "loss": 0.2937,
      "step": 11451
    },
    {
      "epoch": 23.04225352112676,
      "grad_norm": 0.6244500875473022,
      "learning_rate": 0.00015393097897172753,
      "loss": 0.2782,
      "step": 11452
    },
    {
      "epoch": 23.044265593561367,
      "grad_norm": 0.6826412677764893,
      "learning_rate": 0.00015392695442197405,
      "loss": 0.3177,
      "step": 11453
    },
    {
      "epoch": 23.046277665995976,
      "grad_norm": 0.668639063835144,
      "learning_rate": 0.00015392292987222053,
      "loss": 0.2952,
      "step": 11454
    },
    {
      "epoch": 23.048289738430583,
      "grad_norm": 0.6727088093757629,
      "learning_rate": 0.00015391890532246707,
      "loss": 0.3079,
      "step": 11455
    },
    {
      "epoch": 23.050301810865193,
      "grad_norm": 0.6503890752792358,
      "learning_rate": 0.00015391488077271356,
      "loss": 0.281,
      "step": 11456
    },
    {
      "epoch": 23.0523138832998,
      "grad_norm": 0.633800208568573,
      "learning_rate": 0.00015391085622296007,
      "loss": 0.2921,
      "step": 11457
    },
    {
      "epoch": 23.054325955734406,
      "grad_norm": 0.6917956471443176,
      "learning_rate": 0.00015390683167320655,
      "loss": 0.3065,
      "step": 11458
    },
    {
      "epoch": 23.056338028169016,
      "grad_norm": 0.6477012634277344,
      "learning_rate": 0.00015390280712345307,
      "loss": 0.2819,
      "step": 11459
    },
    {
      "epoch": 23.058350100603622,
      "grad_norm": 0.6871299743652344,
      "learning_rate": 0.00015389878257369958,
      "loss": 0.2928,
      "step": 11460
    },
    {
      "epoch": 23.06036217303823,
      "grad_norm": 0.6617975234985352,
      "learning_rate": 0.0001538947580239461,
      "loss": 0.3005,
      "step": 11461
    },
    {
      "epoch": 23.06237424547284,
      "grad_norm": 0.6270269155502319,
      "learning_rate": 0.00015389073347419258,
      "loss": 0.286,
      "step": 11462
    },
    {
      "epoch": 23.064386317907445,
      "grad_norm": 0.631767213344574,
      "learning_rate": 0.0001538867089244391,
      "loss": 0.3057,
      "step": 11463
    },
    {
      "epoch": 23.06639839034205,
      "grad_norm": 0.6419333815574646,
      "learning_rate": 0.00015388268437468557,
      "loss": 0.2949,
      "step": 11464
    },
    {
      "epoch": 23.06841046277666,
      "grad_norm": 0.660772442817688,
      "learning_rate": 0.0001538786598249321,
      "loss": 0.2947,
      "step": 11465
    },
    {
      "epoch": 23.070422535211268,
      "grad_norm": 0.7193658947944641,
      "learning_rate": 0.0001538746352751786,
      "loss": 0.282,
      "step": 11466
    },
    {
      "epoch": 23.072434607645874,
      "grad_norm": 0.6999261975288391,
      "learning_rate": 0.0001538706107254251,
      "loss": 0.2937,
      "step": 11467
    },
    {
      "epoch": 23.074446680080484,
      "grad_norm": 0.6981185674667358,
      "learning_rate": 0.0001538665861756716,
      "loss": 0.2952,
      "step": 11468
    },
    {
      "epoch": 23.07645875251509,
      "grad_norm": 0.6598727107048035,
      "learning_rate": 0.0001538625616259181,
      "loss": 0.2953,
      "step": 11469
    },
    {
      "epoch": 23.078470824949697,
      "grad_norm": 0.6401817202568054,
      "learning_rate": 0.00015385853707616462,
      "loss": 0.2908,
      "step": 11470
    },
    {
      "epoch": 23.080482897384307,
      "grad_norm": 0.7078598737716675,
      "learning_rate": 0.00015385451252641113,
      "loss": 0.303,
      "step": 11471
    },
    {
      "epoch": 23.082494969818914,
      "grad_norm": 0.6657539010047913,
      "learning_rate": 0.00015385048797665762,
      "loss": 0.2879,
      "step": 11472
    },
    {
      "epoch": 23.08450704225352,
      "grad_norm": 0.6458894610404968,
      "learning_rate": 0.00015384646342690413,
      "loss": 0.3256,
      "step": 11473
    },
    {
      "epoch": 23.08651911468813,
      "grad_norm": 0.6692826747894287,
      "learning_rate": 0.0001538424388771506,
      "loss": 0.3106,
      "step": 11474
    },
    {
      "epoch": 23.088531187122737,
      "grad_norm": 0.6410059332847595,
      "learning_rate": 0.00015383841432739715,
      "loss": 0.2759,
      "step": 11475
    },
    {
      "epoch": 23.090543259557343,
      "grad_norm": 0.6400805711746216,
      "learning_rate": 0.00015383438977764364,
      "loss": 0.2666,
      "step": 11476
    },
    {
      "epoch": 23.092555331991953,
      "grad_norm": 0.6400387287139893,
      "learning_rate": 0.00015383036522789015,
      "loss": 0.2756,
      "step": 11477
    },
    {
      "epoch": 23.09456740442656,
      "grad_norm": 0.6778997182846069,
      "learning_rate": 0.00015382634067813663,
      "loss": 0.3169,
      "step": 11478
    },
    {
      "epoch": 23.096579476861166,
      "grad_norm": 0.6633602380752563,
      "learning_rate": 0.00015382231612838315,
      "loss": 0.301,
      "step": 11479
    },
    {
      "epoch": 23.098591549295776,
      "grad_norm": 0.7003610730171204,
      "learning_rate": 0.00015381829157862966,
      "loss": 0.305,
      "step": 11480
    },
    {
      "epoch": 23.100603621730382,
      "grad_norm": 0.6787207722663879,
      "learning_rate": 0.00015381426702887614,
      "loss": 0.302,
      "step": 11481
    },
    {
      "epoch": 23.10261569416499,
      "grad_norm": 0.6854997277259827,
      "learning_rate": 0.00015381024247912266,
      "loss": 0.3087,
      "step": 11482
    },
    {
      "epoch": 23.1046277665996,
      "grad_norm": 0.6127055883407593,
      "learning_rate": 0.00015380621792936917,
      "loss": 0.2904,
      "step": 11483
    },
    {
      "epoch": 23.106639839034205,
      "grad_norm": 0.7024217844009399,
      "learning_rate": 0.00015380219337961565,
      "loss": 0.3084,
      "step": 11484
    },
    {
      "epoch": 23.10865191146881,
      "grad_norm": 0.7322668433189392,
      "learning_rate": 0.00015379816882986217,
      "loss": 0.3279,
      "step": 11485
    },
    {
      "epoch": 23.11066398390342,
      "grad_norm": 0.6342503428459167,
      "learning_rate": 0.00015379414428010868,
      "loss": 0.27,
      "step": 11486
    },
    {
      "epoch": 23.112676056338028,
      "grad_norm": 0.6528477072715759,
      "learning_rate": 0.00015379011973035516,
      "loss": 0.3027,
      "step": 11487
    },
    {
      "epoch": 23.114688128772634,
      "grad_norm": 0.6398939490318298,
      "learning_rate": 0.00015378609518060168,
      "loss": 0.2899,
      "step": 11488
    },
    {
      "epoch": 23.116700201207244,
      "grad_norm": 0.6752629280090332,
      "learning_rate": 0.00015378207063084816,
      "loss": 0.2767,
      "step": 11489
    },
    {
      "epoch": 23.11871227364185,
      "grad_norm": 0.6569242477416992,
      "learning_rate": 0.0001537780460810947,
      "loss": 0.274,
      "step": 11490
    },
    {
      "epoch": 23.120724346076457,
      "grad_norm": 0.7013554573059082,
      "learning_rate": 0.00015377402153134119,
      "loss": 0.3086,
      "step": 11491
    },
    {
      "epoch": 23.122736418511067,
      "grad_norm": 0.7182376384735107,
      "learning_rate": 0.0001537699969815877,
      "loss": 0.305,
      "step": 11492
    },
    {
      "epoch": 23.124748490945674,
      "grad_norm": 0.6794382929801941,
      "learning_rate": 0.00015376597243183418,
      "loss": 0.2897,
      "step": 11493
    },
    {
      "epoch": 23.12676056338028,
      "grad_norm": 0.672661542892456,
      "learning_rate": 0.0001537619478820807,
      "loss": 0.2865,
      "step": 11494
    },
    {
      "epoch": 23.12877263581489,
      "grad_norm": 0.6684781312942505,
      "learning_rate": 0.0001537579233323272,
      "loss": 0.2807,
      "step": 11495
    },
    {
      "epoch": 23.130784708249497,
      "grad_norm": 0.609247624874115,
      "learning_rate": 0.00015375389878257372,
      "loss": 0.2845,
      "step": 11496
    },
    {
      "epoch": 23.132796780684103,
      "grad_norm": 0.6770791411399841,
      "learning_rate": 0.0001537498742328202,
      "loss": 0.3104,
      "step": 11497
    },
    {
      "epoch": 23.134808853118713,
      "grad_norm": 0.6184173822402954,
      "learning_rate": 0.00015374584968306672,
      "loss": 0.2777,
      "step": 11498
    },
    {
      "epoch": 23.13682092555332,
      "grad_norm": 0.6791446805000305,
      "learning_rate": 0.0001537418251333132,
      "loss": 0.2956,
      "step": 11499
    },
    {
      "epoch": 23.138832997987926,
      "grad_norm": 0.6874282360076904,
      "learning_rate": 0.00015373780058355974,
      "loss": 0.3371,
      "step": 11500
    },
    {
      "epoch": 23.140845070422536,
      "grad_norm": 0.6657772660255432,
      "learning_rate": 0.00015373377603380623,
      "loss": 0.2773,
      "step": 11501
    },
    {
      "epoch": 23.142857142857142,
      "grad_norm": 0.7149821519851685,
      "learning_rate": 0.00015372975148405274,
      "loss": 0.3126,
      "step": 11502
    },
    {
      "epoch": 23.14486921529175,
      "grad_norm": 0.6896481513977051,
      "learning_rate": 0.00015372572693429922,
      "loss": 0.3009,
      "step": 11503
    },
    {
      "epoch": 23.14688128772636,
      "grad_norm": 0.6345730423927307,
      "learning_rate": 0.00015372170238454574,
      "loss": 0.2858,
      "step": 11504
    },
    {
      "epoch": 23.148893360160965,
      "grad_norm": 0.6667121052742004,
      "learning_rate": 0.00015371767783479225,
      "loss": 0.311,
      "step": 11505
    },
    {
      "epoch": 23.15090543259557,
      "grad_norm": 0.6477221250534058,
      "learning_rate": 0.00015371365328503876,
      "loss": 0.2942,
      "step": 11506
    },
    {
      "epoch": 23.15291750503018,
      "grad_norm": 0.6471837759017944,
      "learning_rate": 0.00015370962873528525,
      "loss": 0.3012,
      "step": 11507
    },
    {
      "epoch": 23.154929577464788,
      "grad_norm": 0.6539730429649353,
      "learning_rate": 0.00015370560418553176,
      "loss": 0.2793,
      "step": 11508
    },
    {
      "epoch": 23.156941649899398,
      "grad_norm": 0.7137214541435242,
      "learning_rate": 0.00015370157963577824,
      "loss": 0.299,
      "step": 11509
    },
    {
      "epoch": 23.158953722334005,
      "grad_norm": 0.6742989420890808,
      "learning_rate": 0.00015369755508602478,
      "loss": 0.3042,
      "step": 11510
    },
    {
      "epoch": 23.16096579476861,
      "grad_norm": 0.7383151054382324,
      "learning_rate": 0.00015369353053627127,
      "loss": 0.3319,
      "step": 11511
    },
    {
      "epoch": 23.16297786720322,
      "grad_norm": 0.6721059679985046,
      "learning_rate": 0.00015368950598651778,
      "loss": 0.2988,
      "step": 11512
    },
    {
      "epoch": 23.164989939637827,
      "grad_norm": 0.6924513578414917,
      "learning_rate": 0.00015368548143676426,
      "loss": 0.3037,
      "step": 11513
    },
    {
      "epoch": 23.167002012072434,
      "grad_norm": 0.6577172875404358,
      "learning_rate": 0.00015368145688701078,
      "loss": 0.29,
      "step": 11514
    },
    {
      "epoch": 23.169014084507044,
      "grad_norm": 0.636726438999176,
      "learning_rate": 0.0001536774323372573,
      "loss": 0.2817,
      "step": 11515
    },
    {
      "epoch": 23.17102615694165,
      "grad_norm": 0.716666042804718,
      "learning_rate": 0.00015367340778750377,
      "loss": 0.3225,
      "step": 11516
    },
    {
      "epoch": 23.173038229376257,
      "grad_norm": 0.719394326210022,
      "learning_rate": 0.00015366938323775029,
      "loss": 0.2794,
      "step": 11517
    },
    {
      "epoch": 23.175050301810867,
      "grad_norm": 0.6962855458259583,
      "learning_rate": 0.0001536653586879968,
      "loss": 0.3066,
      "step": 11518
    },
    {
      "epoch": 23.177062374245473,
      "grad_norm": 0.6526123881340027,
      "learning_rate": 0.00015366133413824328,
      "loss": 0.2928,
      "step": 11519
    },
    {
      "epoch": 23.17907444668008,
      "grad_norm": 0.7237902879714966,
      "learning_rate": 0.0001536573095884898,
      "loss": 0.3303,
      "step": 11520
    },
    {
      "epoch": 23.18108651911469,
      "grad_norm": 0.6894334554672241,
      "learning_rate": 0.0001536532850387363,
      "loss": 0.3292,
      "step": 11521
    },
    {
      "epoch": 23.183098591549296,
      "grad_norm": 0.6595389246940613,
      "learning_rate": 0.0001536492604889828,
      "loss": 0.281,
      "step": 11522
    },
    {
      "epoch": 23.185110663983902,
      "grad_norm": 0.7029412388801575,
      "learning_rate": 0.0001536452359392293,
      "loss": 0.2957,
      "step": 11523
    },
    {
      "epoch": 23.187122736418512,
      "grad_norm": 0.799037754535675,
      "learning_rate": 0.0001536412113894758,
      "loss": 0.303,
      "step": 11524
    },
    {
      "epoch": 23.18913480885312,
      "grad_norm": 0.685408890247345,
      "learning_rate": 0.0001536371868397223,
      "loss": 0.3092,
      "step": 11525
    },
    {
      "epoch": 23.191146881287725,
      "grad_norm": 0.6662672758102417,
      "learning_rate": 0.00015363316228996881,
      "loss": 0.2974,
      "step": 11526
    },
    {
      "epoch": 23.193158953722335,
      "grad_norm": 0.6842378377914429,
      "learning_rate": 0.00015362913774021533,
      "loss": 0.33,
      "step": 11527
    },
    {
      "epoch": 23.19517102615694,
      "grad_norm": 0.6953609585762024,
      "learning_rate": 0.0001536251131904618,
      "loss": 0.3103,
      "step": 11528
    },
    {
      "epoch": 23.197183098591548,
      "grad_norm": 0.6748037338256836,
      "learning_rate": 0.00015362108864070832,
      "loss": 0.3067,
      "step": 11529
    },
    {
      "epoch": 23.199195171026158,
      "grad_norm": 0.745316207408905,
      "learning_rate": 0.0001536170640909548,
      "loss": 0.3058,
      "step": 11530
    },
    {
      "epoch": 23.201207243460765,
      "grad_norm": 0.7388917207717896,
      "learning_rate": 0.00015361303954120135,
      "loss": 0.312,
      "step": 11531
    },
    {
      "epoch": 23.20321931589537,
      "grad_norm": 0.7370019555091858,
      "learning_rate": 0.00015360901499144783,
      "loss": 0.3117,
      "step": 11532
    },
    {
      "epoch": 23.20523138832998,
      "grad_norm": 0.7168705463409424,
      "learning_rate": 0.00015360499044169435,
      "loss": 0.3299,
      "step": 11533
    },
    {
      "epoch": 23.207243460764587,
      "grad_norm": 0.7114486694335938,
      "learning_rate": 0.00015360096589194083,
      "loss": 0.2986,
      "step": 11534
    },
    {
      "epoch": 23.209255533199194,
      "grad_norm": 0.7938553094863892,
      "learning_rate": 0.00015359694134218734,
      "loss": 0.3248,
      "step": 11535
    },
    {
      "epoch": 23.211267605633804,
      "grad_norm": 0.6971988081932068,
      "learning_rate": 0.00015359291679243386,
      "loss": 0.3,
      "step": 11536
    },
    {
      "epoch": 23.21327967806841,
      "grad_norm": 0.7038567066192627,
      "learning_rate": 0.00015358889224268037,
      "loss": 0.3197,
      "step": 11537
    },
    {
      "epoch": 23.215291750503017,
      "grad_norm": 0.6554612517356873,
      "learning_rate": 0.00015358486769292685,
      "loss": 0.2907,
      "step": 11538
    },
    {
      "epoch": 23.217303822937627,
      "grad_norm": 0.7172286510467529,
      "learning_rate": 0.00015358084314317337,
      "loss": 0.3101,
      "step": 11539
    },
    {
      "epoch": 23.219315895372233,
      "grad_norm": 0.6611310839653015,
      "learning_rate": 0.00015357681859341985,
      "loss": 0.3054,
      "step": 11540
    },
    {
      "epoch": 23.22132796780684,
      "grad_norm": 0.7049728631973267,
      "learning_rate": 0.0001535727940436664,
      "loss": 0.3279,
      "step": 11541
    },
    {
      "epoch": 23.22334004024145,
      "grad_norm": 0.7137391567230225,
      "learning_rate": 0.00015356876949391287,
      "loss": 0.3196,
      "step": 11542
    },
    {
      "epoch": 23.225352112676056,
      "grad_norm": 0.6987155675888062,
      "learning_rate": 0.0001535647449441594,
      "loss": 0.2954,
      "step": 11543
    },
    {
      "epoch": 23.227364185110662,
      "grad_norm": 0.7115543484687805,
      "learning_rate": 0.00015356072039440587,
      "loss": 0.3212,
      "step": 11544
    },
    {
      "epoch": 23.229376257545272,
      "grad_norm": 0.6670013666152954,
      "learning_rate": 0.00015355669584465238,
      "loss": 0.301,
      "step": 11545
    },
    {
      "epoch": 23.23138832997988,
      "grad_norm": 0.7172277569770813,
      "learning_rate": 0.0001535526712948989,
      "loss": 0.2896,
      "step": 11546
    },
    {
      "epoch": 23.233400402414485,
      "grad_norm": 0.7285617589950562,
      "learning_rate": 0.0001535486467451454,
      "loss": 0.3305,
      "step": 11547
    },
    {
      "epoch": 23.235412474849095,
      "grad_norm": 0.6910629868507385,
      "learning_rate": 0.0001535446221953919,
      "loss": 0.2895,
      "step": 11548
    },
    {
      "epoch": 23.2374245472837,
      "grad_norm": 0.6795251369476318,
      "learning_rate": 0.0001535405976456384,
      "loss": 0.2869,
      "step": 11549
    },
    {
      "epoch": 23.239436619718308,
      "grad_norm": 0.7178763151168823,
      "learning_rate": 0.0001535365730958849,
      "loss": 0.2942,
      "step": 11550
    },
    {
      "epoch": 23.241448692152918,
      "grad_norm": 0.6720712184906006,
      "learning_rate": 0.0001535325485461314,
      "loss": 0.3041,
      "step": 11551
    },
    {
      "epoch": 23.243460764587525,
      "grad_norm": 0.6878038048744202,
      "learning_rate": 0.00015352852399637792,
      "loss": 0.3224,
      "step": 11552
    },
    {
      "epoch": 23.24547283702213,
      "grad_norm": 0.7080076336860657,
      "learning_rate": 0.00015352449944662443,
      "loss": 0.3098,
      "step": 11553
    },
    {
      "epoch": 23.24748490945674,
      "grad_norm": 0.7437824010848999,
      "learning_rate": 0.0001535204748968709,
      "loss": 0.3164,
      "step": 11554
    },
    {
      "epoch": 23.249496981891348,
      "grad_norm": 0.7348766922950745,
      "learning_rate": 0.00015351645034711743,
      "loss": 0.3398,
      "step": 11555
    },
    {
      "epoch": 23.251509054325957,
      "grad_norm": 0.7461655139923096,
      "learning_rate": 0.00015351242579736394,
      "loss": 0.3318,
      "step": 11556
    },
    {
      "epoch": 23.253521126760564,
      "grad_norm": 0.7149056792259216,
      "learning_rate": 0.00015350840124761042,
      "loss": 0.2942,
      "step": 11557
    },
    {
      "epoch": 23.25553319919517,
      "grad_norm": 0.7090538144111633,
      "learning_rate": 0.00015350437669785693,
      "loss": 0.3144,
      "step": 11558
    },
    {
      "epoch": 23.25754527162978,
      "grad_norm": 0.73050856590271,
      "learning_rate": 0.00015350035214810342,
      "loss": 0.3212,
      "step": 11559
    },
    {
      "epoch": 23.259557344064387,
      "grad_norm": 0.6819446086883545,
      "learning_rate": 0.00015349632759834993,
      "loss": 0.3169,
      "step": 11560
    },
    {
      "epoch": 23.261569416498993,
      "grad_norm": 0.6656051278114319,
      "learning_rate": 0.00015349230304859644,
      "loss": 0.2909,
      "step": 11561
    },
    {
      "epoch": 23.263581488933603,
      "grad_norm": 0.7098179459571838,
      "learning_rate": 0.00015348827849884296,
      "loss": 0.3255,
      "step": 11562
    },
    {
      "epoch": 23.26559356136821,
      "grad_norm": 0.7211844325065613,
      "learning_rate": 0.00015348425394908944,
      "loss": 0.315,
      "step": 11563
    },
    {
      "epoch": 23.267605633802816,
      "grad_norm": 0.7162256836891174,
      "learning_rate": 0.00015348022939933595,
      "loss": 0.3164,
      "step": 11564
    },
    {
      "epoch": 23.269617706237426,
      "grad_norm": 0.7428560256958008,
      "learning_rate": 0.00015347620484958244,
      "loss": 0.3263,
      "step": 11565
    },
    {
      "epoch": 23.271629778672033,
      "grad_norm": 0.7205720543861389,
      "learning_rate": 0.00015347218029982898,
      "loss": 0.3344,
      "step": 11566
    },
    {
      "epoch": 23.27364185110664,
      "grad_norm": 0.672454297542572,
      "learning_rate": 0.00015346815575007546,
      "loss": 0.298,
      "step": 11567
    },
    {
      "epoch": 23.27565392354125,
      "grad_norm": 0.7372660636901855,
      "learning_rate": 0.00015346413120032198,
      "loss": 0.3197,
      "step": 11568
    },
    {
      "epoch": 23.277665995975855,
      "grad_norm": 0.7163940668106079,
      "learning_rate": 0.00015346010665056846,
      "loss": 0.3219,
      "step": 11569
    },
    {
      "epoch": 23.279678068410462,
      "grad_norm": 0.7197881937026978,
      "learning_rate": 0.00015345608210081497,
      "loss": 0.302,
      "step": 11570
    },
    {
      "epoch": 23.281690140845072,
      "grad_norm": 0.7201470136642456,
      "learning_rate": 0.00015345205755106149,
      "loss": 0.2972,
      "step": 11571
    },
    {
      "epoch": 23.28370221327968,
      "grad_norm": 0.6949111819267273,
      "learning_rate": 0.000153448033001308,
      "loss": 0.3116,
      "step": 11572
    },
    {
      "epoch": 23.285714285714285,
      "grad_norm": 0.7018761038780212,
      "learning_rate": 0.00015344400845155448,
      "loss": 0.3118,
      "step": 11573
    },
    {
      "epoch": 23.287726358148895,
      "grad_norm": 0.6731342077255249,
      "learning_rate": 0.000153439983901801,
      "loss": 0.298,
      "step": 11574
    },
    {
      "epoch": 23.2897384305835,
      "grad_norm": 0.714542031288147,
      "learning_rate": 0.00015343595935204748,
      "loss": 0.3211,
      "step": 11575
    },
    {
      "epoch": 23.291750503018108,
      "grad_norm": 0.6724737882614136,
      "learning_rate": 0.00015343193480229402,
      "loss": 0.3256,
      "step": 11576
    },
    {
      "epoch": 23.293762575452718,
      "grad_norm": 0.7601713538169861,
      "learning_rate": 0.0001534279102525405,
      "loss": 0.3554,
      "step": 11577
    },
    {
      "epoch": 23.295774647887324,
      "grad_norm": 0.7343285083770752,
      "learning_rate": 0.00015342388570278702,
      "loss": 0.312,
      "step": 11578
    },
    {
      "epoch": 23.29778672032193,
      "grad_norm": 0.6822623014450073,
      "learning_rate": 0.0001534198611530335,
      "loss": 0.2809,
      "step": 11579
    },
    {
      "epoch": 23.29979879275654,
      "grad_norm": 0.7583218216896057,
      "learning_rate": 0.00015341583660328001,
      "loss": 0.3137,
      "step": 11580
    },
    {
      "epoch": 23.301810865191147,
      "grad_norm": 0.7159762978553772,
      "learning_rate": 0.00015341181205352653,
      "loss": 0.3413,
      "step": 11581
    },
    {
      "epoch": 23.303822937625753,
      "grad_norm": 0.7027924656867981,
      "learning_rate": 0.00015340778750377304,
      "loss": 0.2943,
      "step": 11582
    },
    {
      "epoch": 23.305835010060363,
      "grad_norm": 0.6859754920005798,
      "learning_rate": 0.00015340376295401952,
      "loss": 0.297,
      "step": 11583
    },
    {
      "epoch": 23.30784708249497,
      "grad_norm": 0.7138795256614685,
      "learning_rate": 0.00015339973840426604,
      "loss": 0.3276,
      "step": 11584
    },
    {
      "epoch": 23.309859154929576,
      "grad_norm": 0.6639937162399292,
      "learning_rate": 0.00015339571385451252,
      "loss": 0.284,
      "step": 11585
    },
    {
      "epoch": 23.311871227364186,
      "grad_norm": 0.685322642326355,
      "learning_rate": 0.00015339168930475903,
      "loss": 0.3187,
      "step": 11586
    },
    {
      "epoch": 23.313883299798793,
      "grad_norm": 0.6830961108207703,
      "learning_rate": 0.00015338766475500555,
      "loss": 0.3038,
      "step": 11587
    },
    {
      "epoch": 23.3158953722334,
      "grad_norm": 0.6896685361862183,
      "learning_rate": 0.00015338364020525206,
      "loss": 0.3298,
      "step": 11588
    },
    {
      "epoch": 23.31790744466801,
      "grad_norm": 0.7065136432647705,
      "learning_rate": 0.00015337961565549854,
      "loss": 0.3143,
      "step": 11589
    },
    {
      "epoch": 23.319919517102615,
      "grad_norm": 0.7054891586303711,
      "learning_rate": 0.00015337559110574505,
      "loss": 0.3176,
      "step": 11590
    },
    {
      "epoch": 23.321931589537222,
      "grad_norm": 0.735739529132843,
      "learning_rate": 0.00015337156655599157,
      "loss": 0.321,
      "step": 11591
    },
    {
      "epoch": 23.323943661971832,
      "grad_norm": 0.6951958537101746,
      "learning_rate": 0.00015336754200623805,
      "loss": 0.3277,
      "step": 11592
    },
    {
      "epoch": 23.32595573440644,
      "grad_norm": 0.70555579662323,
      "learning_rate": 0.00015336351745648456,
      "loss": 0.3261,
      "step": 11593
    },
    {
      "epoch": 23.327967806841045,
      "grad_norm": 0.7312043905258179,
      "learning_rate": 0.00015335949290673105,
      "loss": 0.3172,
      "step": 11594
    },
    {
      "epoch": 23.329979879275655,
      "grad_norm": 0.701907217502594,
      "learning_rate": 0.00015335546835697756,
      "loss": 0.2982,
      "step": 11595
    },
    {
      "epoch": 23.33199195171026,
      "grad_norm": 0.7111095786094666,
      "learning_rate": 0.00015335144380722407,
      "loss": 0.327,
      "step": 11596
    },
    {
      "epoch": 23.334004024144868,
      "grad_norm": 0.6987461447715759,
      "learning_rate": 0.00015334741925747059,
      "loss": 0.3482,
      "step": 11597
    },
    {
      "epoch": 23.336016096579478,
      "grad_norm": 0.7287110686302185,
      "learning_rate": 0.00015334339470771707,
      "loss": 0.3097,
      "step": 11598
    },
    {
      "epoch": 23.338028169014084,
      "grad_norm": 0.6933236122131348,
      "learning_rate": 0.00015333937015796358,
      "loss": 0.2953,
      "step": 11599
    },
    {
      "epoch": 23.34004024144869,
      "grad_norm": 0.7462372779846191,
      "learning_rate": 0.00015333534560821007,
      "loss": 0.3063,
      "step": 11600
    },
    {
      "epoch": 23.3420523138833,
      "grad_norm": 0.6891230344772339,
      "learning_rate": 0.0001533313210584566,
      "loss": 0.3233,
      "step": 11601
    },
    {
      "epoch": 23.344064386317907,
      "grad_norm": 0.6595258116722107,
      "learning_rate": 0.0001533272965087031,
      "loss": 0.3173,
      "step": 11602
    },
    {
      "epoch": 23.346076458752513,
      "grad_norm": 0.7245196104049683,
      "learning_rate": 0.0001533232719589496,
      "loss": 0.3309,
      "step": 11603
    },
    {
      "epoch": 23.348088531187123,
      "grad_norm": 0.6746394634246826,
      "learning_rate": 0.0001533192474091961,
      "loss": 0.304,
      "step": 11604
    },
    {
      "epoch": 23.35010060362173,
      "grad_norm": 0.6978152394294739,
      "learning_rate": 0.0001533152228594426,
      "loss": 0.3175,
      "step": 11605
    },
    {
      "epoch": 23.352112676056336,
      "grad_norm": 0.6832867860794067,
      "learning_rate": 0.00015331119830968911,
      "loss": 0.2919,
      "step": 11606
    },
    {
      "epoch": 23.354124748490946,
      "grad_norm": 0.6888858675956726,
      "learning_rate": 0.00015330717375993563,
      "loss": 0.3443,
      "step": 11607
    },
    {
      "epoch": 23.356136820925553,
      "grad_norm": 0.7352079749107361,
      "learning_rate": 0.0001533031492101821,
      "loss": 0.2948,
      "step": 11608
    },
    {
      "epoch": 23.358148893360163,
      "grad_norm": 0.695152997970581,
      "learning_rate": 0.00015329912466042862,
      "loss": 0.3216,
      "step": 11609
    },
    {
      "epoch": 23.36016096579477,
      "grad_norm": 0.7019186019897461,
      "learning_rate": 0.0001532951001106751,
      "loss": 0.325,
      "step": 11610
    },
    {
      "epoch": 23.362173038229376,
      "grad_norm": 0.7058271765708923,
      "learning_rate": 0.00015329107556092165,
      "loss": 0.3276,
      "step": 11611
    },
    {
      "epoch": 23.364185110663986,
      "grad_norm": 0.710513710975647,
      "learning_rate": 0.00015328705101116813,
      "loss": 0.3223,
      "step": 11612
    },
    {
      "epoch": 23.366197183098592,
      "grad_norm": 0.753208339214325,
      "learning_rate": 0.00015328302646141465,
      "loss": 0.323,
      "step": 11613
    },
    {
      "epoch": 23.3682092555332,
      "grad_norm": 0.7235543131828308,
      "learning_rate": 0.00015327900191166113,
      "loss": 0.3421,
      "step": 11614
    },
    {
      "epoch": 23.37022132796781,
      "grad_norm": 0.7173883318901062,
      "learning_rate": 0.00015327497736190764,
      "loss": 0.3509,
      "step": 11615
    },
    {
      "epoch": 23.372233400402415,
      "grad_norm": 0.7326058745384216,
      "learning_rate": 0.00015327095281215416,
      "loss": 0.3007,
      "step": 11616
    },
    {
      "epoch": 23.37424547283702,
      "grad_norm": 0.669211745262146,
      "learning_rate": 0.00015326692826240067,
      "loss": 0.3062,
      "step": 11617
    },
    {
      "epoch": 23.37625754527163,
      "grad_norm": 0.7314193844795227,
      "learning_rate": 0.00015326290371264715,
      "loss": 0.3128,
      "step": 11618
    },
    {
      "epoch": 23.378269617706238,
      "grad_norm": 0.6993354558944702,
      "learning_rate": 0.00015325887916289366,
      "loss": 0.34,
      "step": 11619
    },
    {
      "epoch": 23.380281690140844,
      "grad_norm": 0.6960909366607666,
      "learning_rate": 0.00015325485461314015,
      "loss": 0.3024,
      "step": 11620
    },
    {
      "epoch": 23.382293762575454,
      "grad_norm": 0.7380425930023193,
      "learning_rate": 0.00015325083006338666,
      "loss": 0.3363,
      "step": 11621
    },
    {
      "epoch": 23.38430583501006,
      "grad_norm": 0.7225700616836548,
      "learning_rate": 0.00015324680551363317,
      "loss": 0.324,
      "step": 11622
    },
    {
      "epoch": 23.386317907444667,
      "grad_norm": 0.7247011661529541,
      "learning_rate": 0.00015324278096387966,
      "loss": 0.3003,
      "step": 11623
    },
    {
      "epoch": 23.388329979879277,
      "grad_norm": 0.7429458498954773,
      "learning_rate": 0.00015323875641412617,
      "loss": 0.3022,
      "step": 11624
    },
    {
      "epoch": 23.390342052313883,
      "grad_norm": 0.7123589515686035,
      "learning_rate": 0.00015323473186437268,
      "loss": 0.2932,
      "step": 11625
    },
    {
      "epoch": 23.39235412474849,
      "grad_norm": 0.7132243514060974,
      "learning_rate": 0.0001532307073146192,
      "loss": 0.3479,
      "step": 11626
    },
    {
      "epoch": 23.3943661971831,
      "grad_norm": 0.7131863832473755,
      "learning_rate": 0.00015322668276486568,
      "loss": 0.2862,
      "step": 11627
    },
    {
      "epoch": 23.396378269617706,
      "grad_norm": 0.7072470188140869,
      "learning_rate": 0.0001532226582151122,
      "loss": 0.3251,
      "step": 11628
    },
    {
      "epoch": 23.398390342052313,
      "grad_norm": 0.7250308990478516,
      "learning_rate": 0.00015321863366535868,
      "loss": 0.302,
      "step": 11629
    },
    {
      "epoch": 23.400402414486923,
      "grad_norm": 0.7658237814903259,
      "learning_rate": 0.0001532146091156052,
      "loss": 0.328,
      "step": 11630
    },
    {
      "epoch": 23.40241448692153,
      "grad_norm": 0.7033230662345886,
      "learning_rate": 0.0001532105845658517,
      "loss": 0.3176,
      "step": 11631
    },
    {
      "epoch": 23.404426559356136,
      "grad_norm": 0.6826803684234619,
      "learning_rate": 0.00015320656001609822,
      "loss": 0.3114,
      "step": 11632
    },
    {
      "epoch": 23.406438631790746,
      "grad_norm": 0.7135636806488037,
      "learning_rate": 0.0001532025354663447,
      "loss": 0.3349,
      "step": 11633
    },
    {
      "epoch": 23.408450704225352,
      "grad_norm": 0.7062028050422668,
      "learning_rate": 0.0001531985109165912,
      "loss": 0.3086,
      "step": 11634
    },
    {
      "epoch": 23.41046277665996,
      "grad_norm": 0.7027314305305481,
      "learning_rate": 0.0001531944863668377,
      "loss": 0.3352,
      "step": 11635
    },
    {
      "epoch": 23.41247484909457,
      "grad_norm": 0.7095014452934265,
      "learning_rate": 0.00015319046181708424,
      "loss": 0.3142,
      "step": 11636
    },
    {
      "epoch": 23.414486921529175,
      "grad_norm": 0.692950427532196,
      "learning_rate": 0.00015318643726733072,
      "loss": 0.3075,
      "step": 11637
    },
    {
      "epoch": 23.41649899396378,
      "grad_norm": 0.681089460849762,
      "learning_rate": 0.00015318241271757723,
      "loss": 0.3206,
      "step": 11638
    },
    {
      "epoch": 23.41851106639839,
      "grad_norm": 0.7375121712684631,
      "learning_rate": 0.00015317838816782372,
      "loss": 0.3114,
      "step": 11639
    },
    {
      "epoch": 23.420523138832998,
      "grad_norm": 0.7026407718658447,
      "learning_rate": 0.00015317436361807023,
      "loss": 0.3228,
      "step": 11640
    },
    {
      "epoch": 23.422535211267604,
      "grad_norm": 0.7007401585578918,
      "learning_rate": 0.00015317033906831674,
      "loss": 0.3326,
      "step": 11641
    },
    {
      "epoch": 23.424547283702214,
      "grad_norm": 0.7237646579742432,
      "learning_rate": 0.00015316631451856326,
      "loss": 0.332,
      "step": 11642
    },
    {
      "epoch": 23.42655935613682,
      "grad_norm": 0.7024527788162231,
      "learning_rate": 0.00015316228996880974,
      "loss": 0.3138,
      "step": 11643
    },
    {
      "epoch": 23.428571428571427,
      "grad_norm": 0.7664797902107239,
      "learning_rate": 0.00015315826541905625,
      "loss": 0.3426,
      "step": 11644
    },
    {
      "epoch": 23.430583501006037,
      "grad_norm": 0.7022839784622192,
      "learning_rate": 0.00015315424086930274,
      "loss": 0.3126,
      "step": 11645
    },
    {
      "epoch": 23.432595573440643,
      "grad_norm": 0.7668628692626953,
      "learning_rate": 0.00015315021631954928,
      "loss": 0.3381,
      "step": 11646
    },
    {
      "epoch": 23.43460764587525,
      "grad_norm": 0.6659148931503296,
      "learning_rate": 0.00015314619176979576,
      "loss": 0.3065,
      "step": 11647
    },
    {
      "epoch": 23.43661971830986,
      "grad_norm": 0.6989742517471313,
      "learning_rate": 0.00015314216722004228,
      "loss": 0.3419,
      "step": 11648
    },
    {
      "epoch": 23.438631790744466,
      "grad_norm": 0.673751175403595,
      "learning_rate": 0.00015313814267028876,
      "loss": 0.3125,
      "step": 11649
    },
    {
      "epoch": 23.440643863179073,
      "grad_norm": 0.6791531443595886,
      "learning_rate": 0.00015313411812053527,
      "loss": 0.3037,
      "step": 11650
    },
    {
      "epoch": 23.442655935613683,
      "grad_norm": 0.7178449034690857,
      "learning_rate": 0.00015313009357078178,
      "loss": 0.3472,
      "step": 11651
    },
    {
      "epoch": 23.44466800804829,
      "grad_norm": 0.7106472253799438,
      "learning_rate": 0.0001531260690210283,
      "loss": 0.3124,
      "step": 11652
    },
    {
      "epoch": 23.446680080482896,
      "grad_norm": 0.7101139426231384,
      "learning_rate": 0.00015312204447127478,
      "loss": 0.3543,
      "step": 11653
    },
    {
      "epoch": 23.448692152917506,
      "grad_norm": 0.7154449820518494,
      "learning_rate": 0.0001531180199215213,
      "loss": 0.3236,
      "step": 11654
    },
    {
      "epoch": 23.450704225352112,
      "grad_norm": 0.7524784207344055,
      "learning_rate": 0.00015311399537176778,
      "loss": 0.3348,
      "step": 11655
    },
    {
      "epoch": 23.452716297786722,
      "grad_norm": 0.6844543814659119,
      "learning_rate": 0.0001531099708220143,
      "loss": 0.3303,
      "step": 11656
    },
    {
      "epoch": 23.45472837022133,
      "grad_norm": 0.7447679042816162,
      "learning_rate": 0.0001531059462722608,
      "loss": 0.3577,
      "step": 11657
    },
    {
      "epoch": 23.456740442655935,
      "grad_norm": 0.7148292660713196,
      "learning_rate": 0.0001531019217225073,
      "loss": 0.3219,
      "step": 11658
    },
    {
      "epoch": 23.458752515090545,
      "grad_norm": 0.7775566577911377,
      "learning_rate": 0.0001530978971727538,
      "loss": 0.3325,
      "step": 11659
    },
    {
      "epoch": 23.46076458752515,
      "grad_norm": 0.7354713678359985,
      "learning_rate": 0.0001530938726230003,
      "loss": 0.3291,
      "step": 11660
    },
    {
      "epoch": 23.462776659959758,
      "grad_norm": 0.7063568234443665,
      "learning_rate": 0.00015308984807324683,
      "loss": 0.3267,
      "step": 11661
    },
    {
      "epoch": 23.464788732394368,
      "grad_norm": 0.7891770601272583,
      "learning_rate": 0.0001530858235234933,
      "loss": 0.3297,
      "step": 11662
    },
    {
      "epoch": 23.466800804828974,
      "grad_norm": 0.7483665347099304,
      "learning_rate": 0.00015308179897373982,
      "loss": 0.321,
      "step": 11663
    },
    {
      "epoch": 23.46881287726358,
      "grad_norm": 0.6737709641456604,
      "learning_rate": 0.0001530777744239863,
      "loss": 0.3007,
      "step": 11664
    },
    {
      "epoch": 23.47082494969819,
      "grad_norm": 0.704155683517456,
      "learning_rate": 0.00015307374987423282,
      "loss": 0.3426,
      "step": 11665
    },
    {
      "epoch": 23.472837022132797,
      "grad_norm": 0.7393774390220642,
      "learning_rate": 0.00015306972532447933,
      "loss": 0.3554,
      "step": 11666
    },
    {
      "epoch": 23.474849094567404,
      "grad_norm": 0.7264236211776733,
      "learning_rate": 0.00015306570077472584,
      "loss": 0.3263,
      "step": 11667
    },
    {
      "epoch": 23.476861167002014,
      "grad_norm": 0.7411661744117737,
      "learning_rate": 0.00015306167622497233,
      "loss": 0.329,
      "step": 11668
    },
    {
      "epoch": 23.47887323943662,
      "grad_norm": 0.7336040735244751,
      "learning_rate": 0.00015305765167521884,
      "loss": 0.3022,
      "step": 11669
    },
    {
      "epoch": 23.480885311871226,
      "grad_norm": 0.7476624846458435,
      "learning_rate": 0.00015305362712546533,
      "loss": 0.3244,
      "step": 11670
    },
    {
      "epoch": 23.482897384305836,
      "grad_norm": 0.7297756671905518,
      "learning_rate": 0.00015304960257571187,
      "loss": 0.3128,
      "step": 11671
    },
    {
      "epoch": 23.484909456740443,
      "grad_norm": 0.7162652611732483,
      "learning_rate": 0.00015304557802595835,
      "loss": 0.3255,
      "step": 11672
    },
    {
      "epoch": 23.48692152917505,
      "grad_norm": 0.700537383556366,
      "learning_rate": 0.00015304155347620486,
      "loss": 0.3185,
      "step": 11673
    },
    {
      "epoch": 23.48893360160966,
      "grad_norm": 0.7733809947967529,
      "learning_rate": 0.00015303752892645135,
      "loss": 0.3262,
      "step": 11674
    },
    {
      "epoch": 23.490945674044266,
      "grad_norm": 0.7000248432159424,
      "learning_rate": 0.00015303350437669786,
      "loss": 0.3207,
      "step": 11675
    },
    {
      "epoch": 23.492957746478872,
      "grad_norm": 0.7103180289268494,
      "learning_rate": 0.00015302947982694437,
      "loss": 0.3217,
      "step": 11676
    },
    {
      "epoch": 23.494969818913482,
      "grad_norm": 0.7108844518661499,
      "learning_rate": 0.00015302545527719089,
      "loss": 0.329,
      "step": 11677
    },
    {
      "epoch": 23.49698189134809,
      "grad_norm": 0.6806929707527161,
      "learning_rate": 0.00015302143072743737,
      "loss": 0.3322,
      "step": 11678
    },
    {
      "epoch": 23.498993963782695,
      "grad_norm": 0.7185617089271545,
      "learning_rate": 0.00015301740617768388,
      "loss": 0.3287,
      "step": 11679
    },
    {
      "epoch": 23.501006036217305,
      "grad_norm": 0.7644613981246948,
      "learning_rate": 0.00015301338162793037,
      "loss": 0.3439,
      "step": 11680
    },
    {
      "epoch": 23.50301810865191,
      "grad_norm": 0.737933874130249,
      "learning_rate": 0.0001530093570781769,
      "loss": 0.3242,
      "step": 11681
    },
    {
      "epoch": 23.505030181086518,
      "grad_norm": 0.6993870735168457,
      "learning_rate": 0.0001530053325284234,
      "loss": 0.2994,
      "step": 11682
    },
    {
      "epoch": 23.507042253521128,
      "grad_norm": 0.7576984763145447,
      "learning_rate": 0.0001530013079786699,
      "loss": 0.3531,
      "step": 11683
    },
    {
      "epoch": 23.509054325955734,
      "grad_norm": 0.741198718547821,
      "learning_rate": 0.0001529972834289164,
      "loss": 0.3403,
      "step": 11684
    },
    {
      "epoch": 23.51106639839034,
      "grad_norm": 0.7684404850006104,
      "learning_rate": 0.0001529932588791629,
      "loss": 0.3219,
      "step": 11685
    },
    {
      "epoch": 23.51307847082495,
      "grad_norm": 0.7592297196388245,
      "learning_rate": 0.00015298923432940941,
      "loss": 0.3298,
      "step": 11686
    },
    {
      "epoch": 23.515090543259557,
      "grad_norm": 0.7175806164741516,
      "learning_rate": 0.00015298520977965593,
      "loss": 0.3143,
      "step": 11687
    },
    {
      "epoch": 23.517102615694164,
      "grad_norm": 0.7352234721183777,
      "learning_rate": 0.0001529811852299024,
      "loss": 0.3561,
      "step": 11688
    },
    {
      "epoch": 23.519114688128774,
      "grad_norm": 0.7024334073066711,
      "learning_rate": 0.00015297716068014892,
      "loss": 0.3318,
      "step": 11689
    },
    {
      "epoch": 23.52112676056338,
      "grad_norm": 0.7077447175979614,
      "learning_rate": 0.0001529731361303954,
      "loss": 0.3339,
      "step": 11690
    },
    {
      "epoch": 23.523138832997986,
      "grad_norm": 0.7397972345352173,
      "learning_rate": 0.00015296911158064192,
      "loss": 0.3108,
      "step": 11691
    },
    {
      "epoch": 23.525150905432596,
      "grad_norm": 0.7878843545913696,
      "learning_rate": 0.00015296508703088843,
      "loss": 0.3367,
      "step": 11692
    },
    {
      "epoch": 23.527162977867203,
      "grad_norm": 0.7239310145378113,
      "learning_rate": 0.00015296106248113492,
      "loss": 0.3578,
      "step": 11693
    },
    {
      "epoch": 23.52917505030181,
      "grad_norm": 0.7114993929862976,
      "learning_rate": 0.00015295703793138143,
      "loss": 0.31,
      "step": 11694
    },
    {
      "epoch": 23.53118712273642,
      "grad_norm": 0.7402968406677246,
      "learning_rate": 0.00015295301338162794,
      "loss": 0.3379,
      "step": 11695
    },
    {
      "epoch": 23.533199195171026,
      "grad_norm": 0.7070579528808594,
      "learning_rate": 0.00015294898883187446,
      "loss": 0.3086,
      "step": 11696
    },
    {
      "epoch": 23.535211267605632,
      "grad_norm": 0.6938126683235168,
      "learning_rate": 0.00015294496428212094,
      "loss": 0.3317,
      "step": 11697
    },
    {
      "epoch": 23.537223340040242,
      "grad_norm": 0.6695038676261902,
      "learning_rate": 0.00015294093973236745,
      "loss": 0.3065,
      "step": 11698
    },
    {
      "epoch": 23.53923541247485,
      "grad_norm": 0.7094067931175232,
      "learning_rate": 0.00015293691518261394,
      "loss": 0.3283,
      "step": 11699
    },
    {
      "epoch": 23.541247484909455,
      "grad_norm": 0.7449498772621155,
      "learning_rate": 0.00015293289063286045,
      "loss": 0.2942,
      "step": 11700
    },
    {
      "epoch": 23.543259557344065,
      "grad_norm": 0.7209089398384094,
      "learning_rate": 0.00015292886608310696,
      "loss": 0.3276,
      "step": 11701
    },
    {
      "epoch": 23.54527162977867,
      "grad_norm": 0.7633793950080872,
      "learning_rate": 0.00015292484153335347,
      "loss": 0.3564,
      "step": 11702
    },
    {
      "epoch": 23.547283702213278,
      "grad_norm": 0.7133073806762695,
      "learning_rate": 0.00015292081698359996,
      "loss": 0.3102,
      "step": 11703
    },
    {
      "epoch": 23.549295774647888,
      "grad_norm": 0.7606337666511536,
      "learning_rate": 0.00015291679243384647,
      "loss": 0.3683,
      "step": 11704
    },
    {
      "epoch": 23.551307847082494,
      "grad_norm": 0.7145707011222839,
      "learning_rate": 0.00015291276788409296,
      "loss": 0.3281,
      "step": 11705
    },
    {
      "epoch": 23.5533199195171,
      "grad_norm": 0.6927719116210938,
      "learning_rate": 0.0001529087433343395,
      "loss": 0.3166,
      "step": 11706
    },
    {
      "epoch": 23.55533199195171,
      "grad_norm": 0.6762166619300842,
      "learning_rate": 0.00015290471878458598,
      "loss": 0.3086,
      "step": 11707
    },
    {
      "epoch": 23.557344064386317,
      "grad_norm": 0.6707113981246948,
      "learning_rate": 0.0001529006942348325,
      "loss": 0.3153,
      "step": 11708
    },
    {
      "epoch": 23.559356136820927,
      "grad_norm": 0.7542104721069336,
      "learning_rate": 0.00015289666968507898,
      "loss": 0.3213,
      "step": 11709
    },
    {
      "epoch": 23.561368209255534,
      "grad_norm": 0.7429811358451843,
      "learning_rate": 0.0001528926451353255,
      "loss": 0.3622,
      "step": 11710
    },
    {
      "epoch": 23.56338028169014,
      "grad_norm": 0.6652865409851074,
      "learning_rate": 0.000152888620585572,
      "loss": 0.3082,
      "step": 11711
    },
    {
      "epoch": 23.56539235412475,
      "grad_norm": 0.6890062093734741,
      "learning_rate": 0.00015288459603581852,
      "loss": 0.3161,
      "step": 11712
    },
    {
      "epoch": 23.567404426559357,
      "grad_norm": 0.6658168435096741,
      "learning_rate": 0.000152880571486065,
      "loss": 0.3029,
      "step": 11713
    },
    {
      "epoch": 23.569416498993963,
      "grad_norm": 0.7222420573234558,
      "learning_rate": 0.0001528765469363115,
      "loss": 0.3337,
      "step": 11714
    },
    {
      "epoch": 23.571428571428573,
      "grad_norm": 0.7183624505996704,
      "learning_rate": 0.000152872522386558,
      "loss": 0.3334,
      "step": 11715
    },
    {
      "epoch": 23.57344064386318,
      "grad_norm": 0.7098060846328735,
      "learning_rate": 0.00015286849783680454,
      "loss": 0.3201,
      "step": 11716
    },
    {
      "epoch": 23.575452716297786,
      "grad_norm": 0.7153651118278503,
      "learning_rate": 0.00015286447328705102,
      "loss": 0.3257,
      "step": 11717
    },
    {
      "epoch": 23.577464788732396,
      "grad_norm": 0.7091646194458008,
      "learning_rate": 0.00015286044873729753,
      "loss": 0.2987,
      "step": 11718
    },
    {
      "epoch": 23.579476861167002,
      "grad_norm": 0.7461481094360352,
      "learning_rate": 0.00015285642418754402,
      "loss": 0.3316,
      "step": 11719
    },
    {
      "epoch": 23.58148893360161,
      "grad_norm": 0.6892188191413879,
      "learning_rate": 0.00015285239963779053,
      "loss": 0.3213,
      "step": 11720
    },
    {
      "epoch": 23.58350100603622,
      "grad_norm": 0.6768197417259216,
      "learning_rate": 0.00015284837508803704,
      "loss": 0.3138,
      "step": 11721
    },
    {
      "epoch": 23.585513078470825,
      "grad_norm": 0.7588341236114502,
      "learning_rate": 0.00015284435053828356,
      "loss": 0.3595,
      "step": 11722
    },
    {
      "epoch": 23.58752515090543,
      "grad_norm": 0.7556760311126709,
      "learning_rate": 0.00015284032598853004,
      "loss": 0.3293,
      "step": 11723
    },
    {
      "epoch": 23.58953722334004,
      "grad_norm": 0.6805586814880371,
      "learning_rate": 0.00015283630143877655,
      "loss": 0.3562,
      "step": 11724
    },
    {
      "epoch": 23.591549295774648,
      "grad_norm": 0.7160045504570007,
      "learning_rate": 0.00015283227688902304,
      "loss": 0.3304,
      "step": 11725
    },
    {
      "epoch": 23.593561368209254,
      "grad_norm": 0.6735777854919434,
      "learning_rate": 0.00015282825233926955,
      "loss": 0.3308,
      "step": 11726
    },
    {
      "epoch": 23.595573440643864,
      "grad_norm": 0.6744943261146545,
      "learning_rate": 0.00015282422778951606,
      "loss": 0.3188,
      "step": 11727
    },
    {
      "epoch": 23.59758551307847,
      "grad_norm": 0.7014045119285583,
      "learning_rate": 0.00015282020323976255,
      "loss": 0.3528,
      "step": 11728
    },
    {
      "epoch": 23.599597585513077,
      "grad_norm": 0.7181946039199829,
      "learning_rate": 0.00015281617869000906,
      "loss": 0.3338,
      "step": 11729
    },
    {
      "epoch": 23.601609657947687,
      "grad_norm": 0.6930010914802551,
      "learning_rate": 0.00015281215414025557,
      "loss": 0.3428,
      "step": 11730
    },
    {
      "epoch": 23.603621730382294,
      "grad_norm": 0.7130388617515564,
      "learning_rate": 0.00015280812959050208,
      "loss": 0.3154,
      "step": 11731
    },
    {
      "epoch": 23.6056338028169,
      "grad_norm": 0.7458776235580444,
      "learning_rate": 0.00015280410504074857,
      "loss": 0.3319,
      "step": 11732
    },
    {
      "epoch": 23.60764587525151,
      "grad_norm": 0.7225326299667358,
      "learning_rate": 0.00015280008049099508,
      "loss": 0.3549,
      "step": 11733
    },
    {
      "epoch": 23.609657947686117,
      "grad_norm": 0.7165305018424988,
      "learning_rate": 0.00015279605594124157,
      "loss": 0.3267,
      "step": 11734
    },
    {
      "epoch": 23.611670020120723,
      "grad_norm": 0.6755000352859497,
      "learning_rate": 0.00015279203139148808,
      "loss": 0.3062,
      "step": 11735
    },
    {
      "epoch": 23.613682092555333,
      "grad_norm": 0.6812062859535217,
      "learning_rate": 0.0001527880068417346,
      "loss": 0.346,
      "step": 11736
    },
    {
      "epoch": 23.61569416498994,
      "grad_norm": 0.6992103457450867,
      "learning_rate": 0.0001527839822919811,
      "loss": 0.3236,
      "step": 11737
    },
    {
      "epoch": 23.617706237424546,
      "grad_norm": 0.6916127800941467,
      "learning_rate": 0.0001527799577422276,
      "loss": 0.3281,
      "step": 11738
    },
    {
      "epoch": 23.619718309859156,
      "grad_norm": 0.722420334815979,
      "learning_rate": 0.0001527759331924741,
      "loss": 0.3359,
      "step": 11739
    },
    {
      "epoch": 23.621730382293762,
      "grad_norm": 0.7334840297698975,
      "learning_rate": 0.00015277190864272059,
      "loss": 0.3216,
      "step": 11740
    },
    {
      "epoch": 23.62374245472837,
      "grad_norm": 0.6737117767333984,
      "learning_rate": 0.00015276788409296713,
      "loss": 0.3312,
      "step": 11741
    },
    {
      "epoch": 23.62575452716298,
      "grad_norm": 0.737775981426239,
      "learning_rate": 0.0001527638595432136,
      "loss": 0.3285,
      "step": 11742
    },
    {
      "epoch": 23.627766599597585,
      "grad_norm": 0.7270727753639221,
      "learning_rate": 0.00015275983499346012,
      "loss": 0.3584,
      "step": 11743
    },
    {
      "epoch": 23.62977867203219,
      "grad_norm": 0.7008688449859619,
      "learning_rate": 0.0001527558104437066,
      "loss": 0.3478,
      "step": 11744
    },
    {
      "epoch": 23.6317907444668,
      "grad_norm": 0.7050579786300659,
      "learning_rate": 0.00015275178589395312,
      "loss": 0.3177,
      "step": 11745
    },
    {
      "epoch": 23.633802816901408,
      "grad_norm": 0.6726342439651489,
      "learning_rate": 0.00015274776134419963,
      "loss": 0.3052,
      "step": 11746
    },
    {
      "epoch": 23.635814889336014,
      "grad_norm": 0.7056395411491394,
      "learning_rate": 0.00015274373679444614,
      "loss": 0.3309,
      "step": 11747
    },
    {
      "epoch": 23.637826961770624,
      "grad_norm": 0.7560240626335144,
      "learning_rate": 0.00015273971224469263,
      "loss": 0.3124,
      "step": 11748
    },
    {
      "epoch": 23.63983903420523,
      "grad_norm": 0.6880835294723511,
      "learning_rate": 0.00015273568769493914,
      "loss": 0.3219,
      "step": 11749
    },
    {
      "epoch": 23.641851106639837,
      "grad_norm": 0.7047333717346191,
      "learning_rate": 0.00015273166314518563,
      "loss": 0.3246,
      "step": 11750
    },
    {
      "epoch": 23.643863179074447,
      "grad_norm": 0.687501847743988,
      "learning_rate": 0.00015272763859543217,
      "loss": 0.3535,
      "step": 11751
    },
    {
      "epoch": 23.645875251509054,
      "grad_norm": 0.7156075835227966,
      "learning_rate": 0.00015272361404567865,
      "loss": 0.327,
      "step": 11752
    },
    {
      "epoch": 23.647887323943664,
      "grad_norm": 0.6992800235748291,
      "learning_rate": 0.00015271958949592516,
      "loss": 0.3317,
      "step": 11753
    },
    {
      "epoch": 23.64989939637827,
      "grad_norm": 0.6937195062637329,
      "learning_rate": 0.00015271556494617165,
      "loss": 0.3431,
      "step": 11754
    },
    {
      "epoch": 23.651911468812877,
      "grad_norm": 0.7027531862258911,
      "learning_rate": 0.00015271154039641816,
      "loss": 0.3119,
      "step": 11755
    },
    {
      "epoch": 23.653923541247487,
      "grad_norm": 0.7285314202308655,
      "learning_rate": 0.00015270751584666467,
      "loss": 0.347,
      "step": 11756
    },
    {
      "epoch": 23.655935613682093,
      "grad_norm": 0.7802127599716187,
      "learning_rate": 0.00015270349129691119,
      "loss": 0.3526,
      "step": 11757
    },
    {
      "epoch": 23.6579476861167,
      "grad_norm": 0.698186457157135,
      "learning_rate": 0.00015269946674715767,
      "loss": 0.3198,
      "step": 11758
    },
    {
      "epoch": 23.65995975855131,
      "grad_norm": 0.7376866340637207,
      "learning_rate": 0.00015269544219740418,
      "loss": 0.3779,
      "step": 11759
    },
    {
      "epoch": 23.661971830985916,
      "grad_norm": 0.7613179087638855,
      "learning_rate": 0.00015269141764765067,
      "loss": 0.3836,
      "step": 11760
    },
    {
      "epoch": 23.663983903420522,
      "grad_norm": 0.7198779582977295,
      "learning_rate": 0.00015268739309789718,
      "loss": 0.3325,
      "step": 11761
    },
    {
      "epoch": 23.665995975855132,
      "grad_norm": 0.7046909928321838,
      "learning_rate": 0.0001526833685481437,
      "loss": 0.3346,
      "step": 11762
    },
    {
      "epoch": 23.66800804828974,
      "grad_norm": 0.7388893365859985,
      "learning_rate": 0.00015267934399839018,
      "loss": 0.3345,
      "step": 11763
    },
    {
      "epoch": 23.670020120724345,
      "grad_norm": 0.7065784931182861,
      "learning_rate": 0.0001526753194486367,
      "loss": 0.3255,
      "step": 11764
    },
    {
      "epoch": 23.672032193158955,
      "grad_norm": 0.698753297328949,
      "learning_rate": 0.00015267129489888317,
      "loss": 0.3378,
      "step": 11765
    },
    {
      "epoch": 23.67404426559356,
      "grad_norm": 0.769438624382019,
      "learning_rate": 0.0001526672703491297,
      "loss": 0.3515,
      "step": 11766
    },
    {
      "epoch": 23.676056338028168,
      "grad_norm": 0.7136096954345703,
      "learning_rate": 0.0001526632457993762,
      "loss": 0.327,
      "step": 11767
    },
    {
      "epoch": 23.678068410462778,
      "grad_norm": 0.6841196417808533,
      "learning_rate": 0.0001526592212496227,
      "loss": 0.3444,
      "step": 11768
    },
    {
      "epoch": 23.680080482897385,
      "grad_norm": 0.7316516041755676,
      "learning_rate": 0.0001526551966998692,
      "loss": 0.3431,
      "step": 11769
    },
    {
      "epoch": 23.68209255533199,
      "grad_norm": 0.6823691129684448,
      "learning_rate": 0.0001526511721501157,
      "loss": 0.3262,
      "step": 11770
    },
    {
      "epoch": 23.6841046277666,
      "grad_norm": 0.7077459692955017,
      "learning_rate": 0.0001526471476003622,
      "loss": 0.3573,
      "step": 11771
    },
    {
      "epoch": 23.686116700201207,
      "grad_norm": 0.7234579920768738,
      "learning_rate": 0.00015264312305060873,
      "loss": 0.3364,
      "step": 11772
    },
    {
      "epoch": 23.688128772635814,
      "grad_norm": 0.71760493516922,
      "learning_rate": 0.00015263909850085522,
      "loss": 0.337,
      "step": 11773
    },
    {
      "epoch": 23.690140845070424,
      "grad_norm": 0.7252403497695923,
      "learning_rate": 0.00015263507395110173,
      "loss": 0.3105,
      "step": 11774
    },
    {
      "epoch": 23.69215291750503,
      "grad_norm": 0.7241045236587524,
      "learning_rate": 0.00015263104940134822,
      "loss": 0.3444,
      "step": 11775
    },
    {
      "epoch": 23.694164989939637,
      "grad_norm": 0.6996423006057739,
      "learning_rate": 0.00015262702485159473,
      "loss": 0.3305,
      "step": 11776
    },
    {
      "epoch": 23.696177062374247,
      "grad_norm": 0.7544245719909668,
      "learning_rate": 0.00015262300030184124,
      "loss": 0.3316,
      "step": 11777
    },
    {
      "epoch": 23.698189134808853,
      "grad_norm": 0.7241209745407104,
      "learning_rate": 0.00015261897575208775,
      "loss": 0.3223,
      "step": 11778
    },
    {
      "epoch": 23.70020120724346,
      "grad_norm": 0.7273854613304138,
      "learning_rate": 0.00015261495120233424,
      "loss": 0.3524,
      "step": 11779
    },
    {
      "epoch": 23.70221327967807,
      "grad_norm": 0.7183482646942139,
      "learning_rate": 0.00015261092665258075,
      "loss": 0.3305,
      "step": 11780
    },
    {
      "epoch": 23.704225352112676,
      "grad_norm": 0.7565269470214844,
      "learning_rate": 0.00015260690210282723,
      "loss": 0.3585,
      "step": 11781
    },
    {
      "epoch": 23.706237424547282,
      "grad_norm": 0.6994756460189819,
      "learning_rate": 0.00015260287755307377,
      "loss": 0.3227,
      "step": 11782
    },
    {
      "epoch": 23.708249496981892,
      "grad_norm": 0.7118232250213623,
      "learning_rate": 0.00015259885300332026,
      "loss": 0.3466,
      "step": 11783
    },
    {
      "epoch": 23.7102615694165,
      "grad_norm": 0.7310753464698792,
      "learning_rate": 0.00015259482845356677,
      "loss": 0.3792,
      "step": 11784
    },
    {
      "epoch": 23.712273641851105,
      "grad_norm": 0.7507861256599426,
      "learning_rate": 0.00015259080390381326,
      "loss": 0.3691,
      "step": 11785
    },
    {
      "epoch": 23.714285714285715,
      "grad_norm": 0.7186687588691711,
      "learning_rate": 0.00015258677935405977,
      "loss": 0.3273,
      "step": 11786
    },
    {
      "epoch": 23.71629778672032,
      "grad_norm": 0.7381108403205872,
      "learning_rate": 0.00015258275480430628,
      "loss": 0.3493,
      "step": 11787
    },
    {
      "epoch": 23.718309859154928,
      "grad_norm": 0.7373563051223755,
      "learning_rate": 0.0001525787302545528,
      "loss": 0.3444,
      "step": 11788
    },
    {
      "epoch": 23.720321931589538,
      "grad_norm": 0.7434813976287842,
      "learning_rate": 0.00015257470570479928,
      "loss": 0.3628,
      "step": 11789
    },
    {
      "epoch": 23.722334004024145,
      "grad_norm": 0.737284243106842,
      "learning_rate": 0.0001525706811550458,
      "loss": 0.3274,
      "step": 11790
    },
    {
      "epoch": 23.72434607645875,
      "grad_norm": 0.7059479355812073,
      "learning_rate": 0.00015256665660529228,
      "loss": 0.3161,
      "step": 11791
    },
    {
      "epoch": 23.72635814889336,
      "grad_norm": 0.7257578372955322,
      "learning_rate": 0.0001525626320555388,
      "loss": 0.3454,
      "step": 11792
    },
    {
      "epoch": 23.728370221327967,
      "grad_norm": 0.7277202010154724,
      "learning_rate": 0.0001525586075057853,
      "loss": 0.3395,
      "step": 11793
    },
    {
      "epoch": 23.730382293762574,
      "grad_norm": 0.7000309824943542,
      "learning_rate": 0.0001525545829560318,
      "loss": 0.3494,
      "step": 11794
    },
    {
      "epoch": 23.732394366197184,
      "grad_norm": 0.7257574200630188,
      "learning_rate": 0.0001525505584062783,
      "loss": 0.3418,
      "step": 11795
    },
    {
      "epoch": 23.73440643863179,
      "grad_norm": 0.7356066703796387,
      "learning_rate": 0.0001525465338565248,
      "loss": 0.3383,
      "step": 11796
    },
    {
      "epoch": 23.736418511066397,
      "grad_norm": 0.7326110601425171,
      "learning_rate": 0.00015254250930677132,
      "loss": 0.3426,
      "step": 11797
    },
    {
      "epoch": 23.738430583501007,
      "grad_norm": 0.6798749566078186,
      "learning_rate": 0.0001525384847570178,
      "loss": 0.3314,
      "step": 11798
    },
    {
      "epoch": 23.740442655935613,
      "grad_norm": 0.7177590131759644,
      "learning_rate": 0.00015253446020726432,
      "loss": 0.3224,
      "step": 11799
    },
    {
      "epoch": 23.74245472837022,
      "grad_norm": 0.7117706537246704,
      "learning_rate": 0.0001525304356575108,
      "loss": 0.3511,
      "step": 11800
    },
    {
      "epoch": 23.74446680080483,
      "grad_norm": 0.6890246272087097,
      "learning_rate": 0.00015252641110775732,
      "loss": 0.3223,
      "step": 11801
    },
    {
      "epoch": 23.746478873239436,
      "grad_norm": 0.7817685604095459,
      "learning_rate": 0.00015252238655800383,
      "loss": 0.3417,
      "step": 11802
    },
    {
      "epoch": 23.748490945674043,
      "grad_norm": 0.7238191962242126,
      "learning_rate": 0.00015251836200825034,
      "loss": 0.3335,
      "step": 11803
    },
    {
      "epoch": 23.750503018108652,
      "grad_norm": 0.7423034310340881,
      "learning_rate": 0.00015251433745849683,
      "loss": 0.3246,
      "step": 11804
    },
    {
      "epoch": 23.75251509054326,
      "grad_norm": 0.681686520576477,
      "learning_rate": 0.00015251031290874334,
      "loss": 0.3305,
      "step": 11805
    },
    {
      "epoch": 23.754527162977865,
      "grad_norm": 0.7333422899246216,
      "learning_rate": 0.00015250628835898982,
      "loss": 0.3258,
      "step": 11806
    },
    {
      "epoch": 23.756539235412475,
      "grad_norm": 0.7148303985595703,
      "learning_rate": 0.00015250226380923636,
      "loss": 0.3402,
      "step": 11807
    },
    {
      "epoch": 23.758551307847082,
      "grad_norm": 0.7113479971885681,
      "learning_rate": 0.00015249823925948285,
      "loss": 0.341,
      "step": 11808
    },
    {
      "epoch": 23.760563380281692,
      "grad_norm": 0.704411506652832,
      "learning_rate": 0.00015249421470972936,
      "loss": 0.3531,
      "step": 11809
    },
    {
      "epoch": 23.7625754527163,
      "grad_norm": 0.6958748698234558,
      "learning_rate": 0.00015249019015997584,
      "loss": 0.3374,
      "step": 11810
    },
    {
      "epoch": 23.764587525150905,
      "grad_norm": 0.7124940156936646,
      "learning_rate": 0.00015248616561022236,
      "loss": 0.3358,
      "step": 11811
    },
    {
      "epoch": 23.766599597585515,
      "grad_norm": 0.7332504987716675,
      "learning_rate": 0.00015248214106046887,
      "loss": 0.3385,
      "step": 11812
    },
    {
      "epoch": 23.76861167002012,
      "grad_norm": 0.6900618076324463,
      "learning_rate": 0.00015247811651071538,
      "loss": 0.3575,
      "step": 11813
    },
    {
      "epoch": 23.770623742454728,
      "grad_norm": 0.7012134790420532,
      "learning_rate": 0.00015247409196096187,
      "loss": 0.3367,
      "step": 11814
    },
    {
      "epoch": 23.772635814889338,
      "grad_norm": 0.7073686122894287,
      "learning_rate": 0.00015247006741120838,
      "loss": 0.3304,
      "step": 11815
    },
    {
      "epoch": 23.774647887323944,
      "grad_norm": 0.7451171875,
      "learning_rate": 0.00015246604286145486,
      "loss": 0.3323,
      "step": 11816
    },
    {
      "epoch": 23.77665995975855,
      "grad_norm": 0.7497454285621643,
      "learning_rate": 0.0001524620183117014,
      "loss": 0.3624,
      "step": 11817
    },
    {
      "epoch": 23.77867203219316,
      "grad_norm": 0.7513874769210815,
      "learning_rate": 0.0001524579937619479,
      "loss": 0.355,
      "step": 11818
    },
    {
      "epoch": 23.780684104627767,
      "grad_norm": 0.706588089466095,
      "learning_rate": 0.0001524539692121944,
      "loss": 0.3277,
      "step": 11819
    },
    {
      "epoch": 23.782696177062373,
      "grad_norm": 0.7219257354736328,
      "learning_rate": 0.00015244994466244089,
      "loss": 0.3257,
      "step": 11820
    },
    {
      "epoch": 23.784708249496983,
      "grad_norm": 0.7177141904830933,
      "learning_rate": 0.0001524459201126874,
      "loss": 0.3434,
      "step": 11821
    },
    {
      "epoch": 23.78672032193159,
      "grad_norm": 0.6961566805839539,
      "learning_rate": 0.0001524418955629339,
      "loss": 0.3489,
      "step": 11822
    },
    {
      "epoch": 23.788732394366196,
      "grad_norm": 0.706829309463501,
      "learning_rate": 0.00015243787101318042,
      "loss": 0.3329,
      "step": 11823
    },
    {
      "epoch": 23.790744466800806,
      "grad_norm": 0.7465378642082214,
      "learning_rate": 0.0001524338464634269,
      "loss": 0.3377,
      "step": 11824
    },
    {
      "epoch": 23.792756539235413,
      "grad_norm": 0.724871814250946,
      "learning_rate": 0.00015242982191367342,
      "loss": 0.3201,
      "step": 11825
    },
    {
      "epoch": 23.79476861167002,
      "grad_norm": 0.722377598285675,
      "learning_rate": 0.0001524257973639199,
      "loss": 0.3391,
      "step": 11826
    },
    {
      "epoch": 23.79678068410463,
      "grad_norm": 0.6889148950576782,
      "learning_rate": 0.00015242177281416642,
      "loss": 0.3394,
      "step": 11827
    },
    {
      "epoch": 23.798792756539235,
      "grad_norm": 0.7351746559143066,
      "learning_rate": 0.00015241774826441293,
      "loss": 0.3523,
      "step": 11828
    },
    {
      "epoch": 23.800804828973842,
      "grad_norm": 0.701239287853241,
      "learning_rate": 0.00015241372371465944,
      "loss": 0.3644,
      "step": 11829
    },
    {
      "epoch": 23.802816901408452,
      "grad_norm": 0.7547321319580078,
      "learning_rate": 0.00015240969916490593,
      "loss": 0.3329,
      "step": 11830
    },
    {
      "epoch": 23.80482897384306,
      "grad_norm": 0.733135461807251,
      "learning_rate": 0.00015240567461515244,
      "loss": 0.333,
      "step": 11831
    },
    {
      "epoch": 23.806841046277665,
      "grad_norm": 0.708530068397522,
      "learning_rate": 0.00015240165006539895,
      "loss": 0.3189,
      "step": 11832
    },
    {
      "epoch": 23.808853118712275,
      "grad_norm": 0.7299152612686157,
      "learning_rate": 0.00015239762551564544,
      "loss": 0.3809,
      "step": 11833
    },
    {
      "epoch": 23.81086519114688,
      "grad_norm": 0.6588891744613647,
      "learning_rate": 0.00015239360096589195,
      "loss": 0.3087,
      "step": 11834
    },
    {
      "epoch": 23.812877263581488,
      "grad_norm": 0.7442063689231873,
      "learning_rate": 0.00015238957641613843,
      "loss": 0.3519,
      "step": 11835
    },
    {
      "epoch": 23.814889336016098,
      "grad_norm": 0.7305464148521423,
      "learning_rate": 0.00015238555186638495,
      "loss": 0.3566,
      "step": 11836
    },
    {
      "epoch": 23.816901408450704,
      "grad_norm": 0.7245694994926453,
      "learning_rate": 0.00015238152731663146,
      "loss": 0.3272,
      "step": 11837
    },
    {
      "epoch": 23.81891348088531,
      "grad_norm": 0.6822986602783203,
      "learning_rate": 0.00015237750276687797,
      "loss": 0.3163,
      "step": 11838
    },
    {
      "epoch": 23.82092555331992,
      "grad_norm": 0.748174250125885,
      "learning_rate": 0.00015237347821712446,
      "loss": 0.3648,
      "step": 11839
    },
    {
      "epoch": 23.822937625754527,
      "grad_norm": 0.7176750302314758,
      "learning_rate": 0.00015236945366737097,
      "loss": 0.3253,
      "step": 11840
    },
    {
      "epoch": 23.824949698189133,
      "grad_norm": 0.7435678839683533,
      "learning_rate": 0.00015236542911761745,
      "loss": 0.3647,
      "step": 11841
    },
    {
      "epoch": 23.826961770623743,
      "grad_norm": 0.7076730728149414,
      "learning_rate": 0.000152361404567864,
      "loss": 0.3627,
      "step": 11842
    },
    {
      "epoch": 23.82897384305835,
      "grad_norm": 0.7078703045845032,
      "learning_rate": 0.00015235738001811048,
      "loss": 0.3548,
      "step": 11843
    },
    {
      "epoch": 23.830985915492956,
      "grad_norm": 0.697201132774353,
      "learning_rate": 0.000152353355468357,
      "loss": 0.3335,
      "step": 11844
    },
    {
      "epoch": 23.832997987927566,
      "grad_norm": 0.7421503663063049,
      "learning_rate": 0.00015234933091860347,
      "loss": 0.3499,
      "step": 11845
    },
    {
      "epoch": 23.835010060362173,
      "grad_norm": 0.6886475682258606,
      "learning_rate": 0.00015234530636885,
      "loss": 0.3452,
      "step": 11846
    },
    {
      "epoch": 23.83702213279678,
      "grad_norm": 0.7090550065040588,
      "learning_rate": 0.0001523412818190965,
      "loss": 0.3531,
      "step": 11847
    },
    {
      "epoch": 23.83903420523139,
      "grad_norm": 0.6997308731079102,
      "learning_rate": 0.000152337257269343,
      "loss": 0.3439,
      "step": 11848
    },
    {
      "epoch": 23.841046277665995,
      "grad_norm": 0.7294878363609314,
      "learning_rate": 0.0001523332327195895,
      "loss": 0.3183,
      "step": 11849
    },
    {
      "epoch": 23.843058350100602,
      "grad_norm": 0.700761616230011,
      "learning_rate": 0.000152329208169836,
      "loss": 0.3233,
      "step": 11850
    },
    {
      "epoch": 23.845070422535212,
      "grad_norm": 0.7175967693328857,
      "learning_rate": 0.0001523251836200825,
      "loss": 0.3383,
      "step": 11851
    },
    {
      "epoch": 23.84708249496982,
      "grad_norm": 0.7918674349784851,
      "learning_rate": 0.00015232115907032903,
      "loss": 0.3468,
      "step": 11852
    },
    {
      "epoch": 23.84909456740443,
      "grad_norm": 0.7428923845291138,
      "learning_rate": 0.00015231713452057552,
      "loss": 0.3512,
      "step": 11853
    },
    {
      "epoch": 23.851106639839035,
      "grad_norm": 0.7573670744895935,
      "learning_rate": 0.00015231310997082203,
      "loss": 0.3199,
      "step": 11854
    },
    {
      "epoch": 23.85311871227364,
      "grad_norm": 0.7045107483863831,
      "learning_rate": 0.00015230908542106852,
      "loss": 0.3415,
      "step": 11855
    },
    {
      "epoch": 23.85513078470825,
      "grad_norm": 0.7003766298294067,
      "learning_rate": 0.00015230506087131503,
      "loss": 0.3512,
      "step": 11856
    },
    {
      "epoch": 23.857142857142858,
      "grad_norm": 0.7242188453674316,
      "learning_rate": 0.00015230103632156154,
      "loss": 0.3029,
      "step": 11857
    },
    {
      "epoch": 23.859154929577464,
      "grad_norm": 0.7001927495002747,
      "learning_rate": 0.00015229701177180805,
      "loss": 0.3381,
      "step": 11858
    },
    {
      "epoch": 23.861167002012074,
      "grad_norm": 0.7222447991371155,
      "learning_rate": 0.00015229298722205454,
      "loss": 0.3679,
      "step": 11859
    },
    {
      "epoch": 23.86317907444668,
      "grad_norm": 0.7310866713523865,
      "learning_rate": 0.00015228896267230105,
      "loss": 0.35,
      "step": 11860
    },
    {
      "epoch": 23.865191146881287,
      "grad_norm": 0.6945509910583496,
      "learning_rate": 0.00015228493812254753,
      "loss": 0.3645,
      "step": 11861
    },
    {
      "epoch": 23.867203219315897,
      "grad_norm": 0.788266122341156,
      "learning_rate": 0.00015228091357279405,
      "loss": 0.3309,
      "step": 11862
    },
    {
      "epoch": 23.869215291750503,
      "grad_norm": 0.7220718264579773,
      "learning_rate": 0.00015227688902304056,
      "loss": 0.3433,
      "step": 11863
    },
    {
      "epoch": 23.87122736418511,
      "grad_norm": 0.7239168882369995,
      "learning_rate": 0.00015227286447328707,
      "loss": 0.3715,
      "step": 11864
    },
    {
      "epoch": 23.87323943661972,
      "grad_norm": 0.7289465069770813,
      "learning_rate": 0.00015226883992353356,
      "loss": 0.3489,
      "step": 11865
    },
    {
      "epoch": 23.875251509054326,
      "grad_norm": 0.708146870136261,
      "learning_rate": 0.00015226481537378007,
      "loss": 0.3534,
      "step": 11866
    },
    {
      "epoch": 23.877263581488933,
      "grad_norm": 0.7117997407913208,
      "learning_rate": 0.00015226079082402658,
      "loss": 0.3513,
      "step": 11867
    },
    {
      "epoch": 23.879275653923543,
      "grad_norm": 0.6972165107727051,
      "learning_rate": 0.00015225676627427307,
      "loss": 0.3134,
      "step": 11868
    },
    {
      "epoch": 23.88128772635815,
      "grad_norm": 0.7240472435951233,
      "learning_rate": 0.00015225274172451958,
      "loss": 0.3494,
      "step": 11869
    },
    {
      "epoch": 23.883299798792756,
      "grad_norm": 0.7595029473304749,
      "learning_rate": 0.00015224871717476606,
      "loss": 0.3563,
      "step": 11870
    },
    {
      "epoch": 23.885311871227366,
      "grad_norm": 0.7428645491600037,
      "learning_rate": 0.00015224469262501258,
      "loss": 0.3535,
      "step": 11871
    },
    {
      "epoch": 23.887323943661972,
      "grad_norm": 0.7214735746383667,
      "learning_rate": 0.0001522406680752591,
      "loss": 0.3082,
      "step": 11872
    },
    {
      "epoch": 23.88933601609658,
      "grad_norm": 0.7553540468215942,
      "learning_rate": 0.0001522366435255056,
      "loss": 0.3686,
      "step": 11873
    },
    {
      "epoch": 23.89134808853119,
      "grad_norm": 0.7257389426231384,
      "learning_rate": 0.00015223261897575208,
      "loss": 0.3531,
      "step": 11874
    },
    {
      "epoch": 23.893360160965795,
      "grad_norm": 0.8331692814826965,
      "learning_rate": 0.0001522285944259986,
      "loss": 0.3694,
      "step": 11875
    },
    {
      "epoch": 23.8953722334004,
      "grad_norm": 0.7176533341407776,
      "learning_rate": 0.00015222456987624508,
      "loss": 0.3484,
      "step": 11876
    },
    {
      "epoch": 23.89738430583501,
      "grad_norm": 0.6702543497085571,
      "learning_rate": 0.00015222054532649162,
      "loss": 0.3131,
      "step": 11877
    },
    {
      "epoch": 23.899396378269618,
      "grad_norm": 0.7235310673713684,
      "learning_rate": 0.0001522165207767381,
      "loss": 0.3585,
      "step": 11878
    },
    {
      "epoch": 23.901408450704224,
      "grad_norm": 0.7125055193901062,
      "learning_rate": 0.00015221249622698462,
      "loss": 0.3389,
      "step": 11879
    },
    {
      "epoch": 23.903420523138834,
      "grad_norm": 0.7141635417938232,
      "learning_rate": 0.0001522084716772311,
      "loss": 0.3424,
      "step": 11880
    },
    {
      "epoch": 23.90543259557344,
      "grad_norm": 0.7209523320198059,
      "learning_rate": 0.00015220444712747762,
      "loss": 0.3547,
      "step": 11881
    },
    {
      "epoch": 23.907444668008047,
      "grad_norm": 0.7069675922393799,
      "learning_rate": 0.00015220042257772413,
      "loss": 0.3825,
      "step": 11882
    },
    {
      "epoch": 23.909456740442657,
      "grad_norm": 0.7250085473060608,
      "learning_rate": 0.00015219639802797064,
      "loss": 0.3183,
      "step": 11883
    },
    {
      "epoch": 23.911468812877263,
      "grad_norm": 0.7360610365867615,
      "learning_rate": 0.00015219237347821713,
      "loss": 0.3146,
      "step": 11884
    },
    {
      "epoch": 23.91348088531187,
      "grad_norm": 0.7247750759124756,
      "learning_rate": 0.00015218834892846364,
      "loss": 0.3289,
      "step": 11885
    },
    {
      "epoch": 23.91549295774648,
      "grad_norm": 0.6783906817436218,
      "learning_rate": 0.00015218432437871012,
      "loss": 0.3369,
      "step": 11886
    },
    {
      "epoch": 23.917505030181086,
      "grad_norm": 0.7101420164108276,
      "learning_rate": 0.00015218029982895666,
      "loss": 0.3481,
      "step": 11887
    },
    {
      "epoch": 23.919517102615693,
      "grad_norm": 0.7439361214637756,
      "learning_rate": 0.00015217627527920315,
      "loss": 0.3501,
      "step": 11888
    },
    {
      "epoch": 23.921529175050303,
      "grad_norm": 0.7407023906707764,
      "learning_rate": 0.00015217225072944966,
      "loss": 0.373,
      "step": 11889
    },
    {
      "epoch": 23.92354124748491,
      "grad_norm": 0.7217239737510681,
      "learning_rate": 0.00015216822617969614,
      "loss": 0.3319,
      "step": 11890
    },
    {
      "epoch": 23.925553319919516,
      "grad_norm": 0.7691692113876343,
      "learning_rate": 0.00015216420162994266,
      "loss": 0.3659,
      "step": 11891
    },
    {
      "epoch": 23.927565392354126,
      "grad_norm": 0.7666828632354736,
      "learning_rate": 0.00015216017708018917,
      "loss": 0.3346,
      "step": 11892
    },
    {
      "epoch": 23.929577464788732,
      "grad_norm": 0.7390941381454468,
      "learning_rate": 0.00015215615253043568,
      "loss": 0.3363,
      "step": 11893
    },
    {
      "epoch": 23.93158953722334,
      "grad_norm": 0.7355985641479492,
      "learning_rate": 0.00015215212798068217,
      "loss": 0.3241,
      "step": 11894
    },
    {
      "epoch": 23.93360160965795,
      "grad_norm": 0.7114761471748352,
      "learning_rate": 0.00015214810343092868,
      "loss": 0.3734,
      "step": 11895
    },
    {
      "epoch": 23.935613682092555,
      "grad_norm": 0.6908443570137024,
      "learning_rate": 0.00015214407888117516,
      "loss": 0.3554,
      "step": 11896
    },
    {
      "epoch": 23.93762575452716,
      "grad_norm": 0.6995319128036499,
      "learning_rate": 0.00015214005433142168,
      "loss": 0.3252,
      "step": 11897
    },
    {
      "epoch": 23.93963782696177,
      "grad_norm": 0.722920298576355,
      "learning_rate": 0.0001521360297816682,
      "loss": 0.354,
      "step": 11898
    },
    {
      "epoch": 23.941649899396378,
      "grad_norm": 0.7512055039405823,
      "learning_rate": 0.0001521320052319147,
      "loss": 0.3709,
      "step": 11899
    },
    {
      "epoch": 23.943661971830984,
      "grad_norm": 0.7138617634773254,
      "learning_rate": 0.00015212798068216119,
      "loss": 0.3555,
      "step": 11900
    },
    {
      "epoch": 23.945674044265594,
      "grad_norm": 0.7272707223892212,
      "learning_rate": 0.0001521239561324077,
      "loss": 0.3314,
      "step": 11901
    },
    {
      "epoch": 23.9476861167002,
      "grad_norm": 0.7241880893707275,
      "learning_rate": 0.0001521199315826542,
      "loss": 0.354,
      "step": 11902
    },
    {
      "epoch": 23.949698189134807,
      "grad_norm": 0.71498042345047,
      "learning_rate": 0.0001521159070329007,
      "loss": 0.359,
      "step": 11903
    },
    {
      "epoch": 23.951710261569417,
      "grad_norm": 0.7170587778091431,
      "learning_rate": 0.0001521118824831472,
      "loss": 0.3452,
      "step": 11904
    },
    {
      "epoch": 23.953722334004024,
      "grad_norm": 0.7035375237464905,
      "learning_rate": 0.0001521078579333937,
      "loss": 0.3372,
      "step": 11905
    },
    {
      "epoch": 23.955734406438633,
      "grad_norm": 0.7258347868919373,
      "learning_rate": 0.0001521038333836402,
      "loss": 0.3745,
      "step": 11906
    },
    {
      "epoch": 23.95774647887324,
      "grad_norm": 0.7305789589881897,
      "learning_rate": 0.00015209980883388672,
      "loss": 0.3513,
      "step": 11907
    },
    {
      "epoch": 23.959758551307846,
      "grad_norm": 0.7174922227859497,
      "learning_rate": 0.00015209578428413323,
      "loss": 0.3348,
      "step": 11908
    },
    {
      "epoch": 23.961770623742456,
      "grad_norm": 0.6955404877662659,
      "learning_rate": 0.00015209175973437971,
      "loss": 0.3638,
      "step": 11909
    },
    {
      "epoch": 23.963782696177063,
      "grad_norm": 0.6633679866790771,
      "learning_rate": 0.00015208773518462623,
      "loss": 0.3285,
      "step": 11910
    },
    {
      "epoch": 23.96579476861167,
      "grad_norm": 0.7588973641395569,
      "learning_rate": 0.0001520837106348727,
      "loss": 0.3464,
      "step": 11911
    },
    {
      "epoch": 23.96780684104628,
      "grad_norm": 0.7033340334892273,
      "learning_rate": 0.00015207968608511925,
      "loss": 0.3131,
      "step": 11912
    },
    {
      "epoch": 23.969818913480886,
      "grad_norm": 0.7376243472099304,
      "learning_rate": 0.00015207566153536574,
      "loss": 0.3729,
      "step": 11913
    },
    {
      "epoch": 23.971830985915492,
      "grad_norm": 0.7178745269775391,
      "learning_rate": 0.00015207163698561225,
      "loss": 0.3607,
      "step": 11914
    },
    {
      "epoch": 23.973843058350102,
      "grad_norm": 0.7219484448432922,
      "learning_rate": 0.00015206761243585873,
      "loss": 0.3431,
      "step": 11915
    },
    {
      "epoch": 23.97585513078471,
      "grad_norm": 0.7032880783081055,
      "learning_rate": 0.00015206358788610525,
      "loss": 0.3241,
      "step": 11916
    },
    {
      "epoch": 23.977867203219315,
      "grad_norm": 0.7200008034706116,
      "learning_rate": 0.00015205956333635176,
      "loss": 0.3485,
      "step": 11917
    },
    {
      "epoch": 23.979879275653925,
      "grad_norm": 0.7312802076339722,
      "learning_rate": 0.00015205553878659827,
      "loss": 0.3627,
      "step": 11918
    },
    {
      "epoch": 23.98189134808853,
      "grad_norm": 0.7247442603111267,
      "learning_rate": 0.00015205151423684475,
      "loss": 0.3466,
      "step": 11919
    },
    {
      "epoch": 23.983903420523138,
      "grad_norm": 0.7200998067855835,
      "learning_rate": 0.00015204748968709127,
      "loss": 0.3228,
      "step": 11920
    },
    {
      "epoch": 23.985915492957748,
      "grad_norm": 0.7299699187278748,
      "learning_rate": 0.00015204346513733775,
      "loss": 0.349,
      "step": 11921
    },
    {
      "epoch": 23.987927565392354,
      "grad_norm": 0.729826807975769,
      "learning_rate": 0.0001520394405875843,
      "loss": 0.3399,
      "step": 11922
    },
    {
      "epoch": 23.98993963782696,
      "grad_norm": 0.7013491988182068,
      "learning_rate": 0.00015203541603783078,
      "loss": 0.3406,
      "step": 11923
    },
    {
      "epoch": 23.99195171026157,
      "grad_norm": 0.7171909809112549,
      "learning_rate": 0.0001520313914880773,
      "loss": 0.3533,
      "step": 11924
    },
    {
      "epoch": 23.993963782696177,
      "grad_norm": 0.7117586731910706,
      "learning_rate": 0.00015202736693832377,
      "loss": 0.3691,
      "step": 11925
    },
    {
      "epoch": 23.995975855130784,
      "grad_norm": 0.7129229307174683,
      "learning_rate": 0.00015202334238857029,
      "loss": 0.3086,
      "step": 11926
    },
    {
      "epoch": 23.997987927565394,
      "grad_norm": 0.7204023599624634,
      "learning_rate": 0.0001520193178388168,
      "loss": 0.3838,
      "step": 11927
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.7378948330879211,
      "learning_rate": 0.0001520152932890633,
      "loss": 0.342,
      "step": 11928
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.0893325805664062,
      "eval_runtime": 49.816,
      "eval_samples_per_second": 19.913,
      "eval_steps_per_second": 2.489,
      "step": 11928
    },
    {
      "epoch": 24.002012072434606,
      "grad_norm": 0.6301829218864441,
      "learning_rate": 0.0001520112687393098,
      "loss": 0.3062,
      "step": 11929
    },
    {
      "epoch": 24.004024144869216,
      "grad_norm": 0.5863799452781677,
      "learning_rate": 0.0001520072441895563,
      "loss": 0.2542,
      "step": 11930
    },
    {
      "epoch": 24.006036217303823,
      "grad_norm": 0.6483327746391296,
      "learning_rate": 0.0001520032196398028,
      "loss": 0.2828,
      "step": 11931
    },
    {
      "epoch": 24.00804828973843,
      "grad_norm": 0.6680594682693481,
      "learning_rate": 0.0001519991950900493,
      "loss": 0.2837,
      "step": 11932
    },
    {
      "epoch": 24.01006036217304,
      "grad_norm": 0.7117071747779846,
      "learning_rate": 0.00015199517054029582,
      "loss": 0.2712,
      "step": 11933
    },
    {
      "epoch": 24.012072434607646,
      "grad_norm": 0.7396734952926636,
      "learning_rate": 0.00015199114599054233,
      "loss": 0.318,
      "step": 11934
    },
    {
      "epoch": 24.014084507042252,
      "grad_norm": 0.6911728978157043,
      "learning_rate": 0.00015198712144078881,
      "loss": 0.2855,
      "step": 11935
    },
    {
      "epoch": 24.016096579476862,
      "grad_norm": 0.6852837204933167,
      "learning_rate": 0.00015198309689103533,
      "loss": 0.2874,
      "step": 11936
    },
    {
      "epoch": 24.01810865191147,
      "grad_norm": 0.6646714806556702,
      "learning_rate": 0.00015197907234128184,
      "loss": 0.2677,
      "step": 11937
    },
    {
      "epoch": 24.020120724346075,
      "grad_norm": 0.649090588092804,
      "learning_rate": 0.00015197504779152832,
      "loss": 0.2824,
      "step": 11938
    },
    {
      "epoch": 24.022132796780685,
      "grad_norm": 0.674278974533081,
      "learning_rate": 0.00015197102324177484,
      "loss": 0.2749,
      "step": 11939
    },
    {
      "epoch": 24.02414486921529,
      "grad_norm": 0.6661338210105896,
      "learning_rate": 0.00015196699869202132,
      "loss": 0.2822,
      "step": 11940
    },
    {
      "epoch": 24.026156941649898,
      "grad_norm": 0.648328959941864,
      "learning_rate": 0.00015196297414226783,
      "loss": 0.2697,
      "step": 11941
    },
    {
      "epoch": 24.028169014084508,
      "grad_norm": 0.6764125227928162,
      "learning_rate": 0.00015195894959251435,
      "loss": 0.2898,
      "step": 11942
    },
    {
      "epoch": 24.030181086519114,
      "grad_norm": 0.6986427307128906,
      "learning_rate": 0.00015195492504276086,
      "loss": 0.2732,
      "step": 11943
    },
    {
      "epoch": 24.03219315895372,
      "grad_norm": 0.7595362663269043,
      "learning_rate": 0.00015195090049300734,
      "loss": 0.3178,
      "step": 11944
    },
    {
      "epoch": 24.03420523138833,
      "grad_norm": 0.693606972694397,
      "learning_rate": 0.00015194687594325386,
      "loss": 0.277,
      "step": 11945
    },
    {
      "epoch": 24.036217303822937,
      "grad_norm": 0.7253089547157288,
      "learning_rate": 0.00015194285139350034,
      "loss": 0.2783,
      "step": 11946
    },
    {
      "epoch": 24.038229376257544,
      "grad_norm": 0.6864703297615051,
      "learning_rate": 0.00015193882684374688,
      "loss": 0.2882,
      "step": 11947
    },
    {
      "epoch": 24.040241448692154,
      "grad_norm": 0.6395546793937683,
      "learning_rate": 0.00015193480229399337,
      "loss": 0.2544,
      "step": 11948
    },
    {
      "epoch": 24.04225352112676,
      "grad_norm": 0.642488956451416,
      "learning_rate": 0.00015193077774423988,
      "loss": 0.2783,
      "step": 11949
    },
    {
      "epoch": 24.044265593561367,
      "grad_norm": 0.7012825608253479,
      "learning_rate": 0.00015192675319448636,
      "loss": 0.2955,
      "step": 11950
    },
    {
      "epoch": 24.046277665995976,
      "grad_norm": 0.7021870613098145,
      "learning_rate": 0.00015192272864473287,
      "loss": 0.2943,
      "step": 11951
    },
    {
      "epoch": 24.048289738430583,
      "grad_norm": 0.7230234742164612,
      "learning_rate": 0.0001519187040949794,
      "loss": 0.2721,
      "step": 11952
    },
    {
      "epoch": 24.050301810865193,
      "grad_norm": 0.6948973536491394,
      "learning_rate": 0.0001519146795452259,
      "loss": 0.2786,
      "step": 11953
    },
    {
      "epoch": 24.0523138832998,
      "grad_norm": 0.7148271799087524,
      "learning_rate": 0.00015191065499547238,
      "loss": 0.3051,
      "step": 11954
    },
    {
      "epoch": 24.054325955734406,
      "grad_norm": 0.6526502370834351,
      "learning_rate": 0.0001519066304457189,
      "loss": 0.2839,
      "step": 11955
    },
    {
      "epoch": 24.056338028169016,
      "grad_norm": 0.6712836027145386,
      "learning_rate": 0.00015190260589596538,
      "loss": 0.2614,
      "step": 11956
    },
    {
      "epoch": 24.058350100603622,
      "grad_norm": 0.64942467212677,
      "learning_rate": 0.00015189858134621192,
      "loss": 0.2645,
      "step": 11957
    },
    {
      "epoch": 24.06036217303823,
      "grad_norm": 0.65217524766922,
      "learning_rate": 0.0001518945567964584,
      "loss": 0.2802,
      "step": 11958
    },
    {
      "epoch": 24.06237424547284,
      "grad_norm": 0.6487560868263245,
      "learning_rate": 0.00015189053224670492,
      "loss": 0.2904,
      "step": 11959
    },
    {
      "epoch": 24.064386317907445,
      "grad_norm": 0.6499915719032288,
      "learning_rate": 0.0001518865076969514,
      "loss": 0.2703,
      "step": 11960
    },
    {
      "epoch": 24.06639839034205,
      "grad_norm": 0.7019907832145691,
      "learning_rate": 0.00015188248314719792,
      "loss": 0.2811,
      "step": 11961
    },
    {
      "epoch": 24.06841046277666,
      "grad_norm": 0.6738559603691101,
      "learning_rate": 0.00015187845859744443,
      "loss": 0.3228,
      "step": 11962
    },
    {
      "epoch": 24.070422535211268,
      "grad_norm": 0.6952863335609436,
      "learning_rate": 0.00015187443404769094,
      "loss": 0.3056,
      "step": 11963
    },
    {
      "epoch": 24.072434607645874,
      "grad_norm": 0.666534960269928,
      "learning_rate": 0.00015187040949793743,
      "loss": 0.2805,
      "step": 11964
    },
    {
      "epoch": 24.074446680080484,
      "grad_norm": 0.6901322603225708,
      "learning_rate": 0.00015186638494818394,
      "loss": 0.2782,
      "step": 11965
    },
    {
      "epoch": 24.07645875251509,
      "grad_norm": 0.6752840280532837,
      "learning_rate": 0.00015186236039843042,
      "loss": 0.2963,
      "step": 11966
    },
    {
      "epoch": 24.078470824949697,
      "grad_norm": 0.6531842947006226,
      "learning_rate": 0.00015185833584867693,
      "loss": 0.2723,
      "step": 11967
    },
    {
      "epoch": 24.080482897384307,
      "grad_norm": 0.6851419806480408,
      "learning_rate": 0.00015185431129892345,
      "loss": 0.2767,
      "step": 11968
    },
    {
      "epoch": 24.082494969818914,
      "grad_norm": 0.7229326963424683,
      "learning_rate": 0.00015185028674916993,
      "loss": 0.2781,
      "step": 11969
    },
    {
      "epoch": 24.08450704225352,
      "grad_norm": 0.6778721213340759,
      "learning_rate": 0.00015184626219941644,
      "loss": 0.3084,
      "step": 11970
    },
    {
      "epoch": 24.08651911468813,
      "grad_norm": 0.7212467193603516,
      "learning_rate": 0.00015184223764966296,
      "loss": 0.3079,
      "step": 11971
    },
    {
      "epoch": 24.088531187122737,
      "grad_norm": 0.7051218152046204,
      "learning_rate": 0.00015183821309990947,
      "loss": 0.2746,
      "step": 11972
    },
    {
      "epoch": 24.090543259557343,
      "grad_norm": 0.6552504301071167,
      "learning_rate": 0.00015183418855015595,
      "loss": 0.2617,
      "step": 11973
    },
    {
      "epoch": 24.092555331991953,
      "grad_norm": 0.6734322905540466,
      "learning_rate": 0.00015183016400040247,
      "loss": 0.2918,
      "step": 11974
    },
    {
      "epoch": 24.09456740442656,
      "grad_norm": 0.6613065600395203,
      "learning_rate": 0.00015182613945064895,
      "loss": 0.2575,
      "step": 11975
    },
    {
      "epoch": 24.096579476861166,
      "grad_norm": 0.7036720514297485,
      "learning_rate": 0.00015182211490089546,
      "loss": 0.2927,
      "step": 11976
    },
    {
      "epoch": 24.098591549295776,
      "grad_norm": 0.6938209533691406,
      "learning_rate": 0.00015181809035114198,
      "loss": 0.3084,
      "step": 11977
    },
    {
      "epoch": 24.100603621730382,
      "grad_norm": 0.6762889623641968,
      "learning_rate": 0.0001518140658013885,
      "loss": 0.2931,
      "step": 11978
    },
    {
      "epoch": 24.10261569416499,
      "grad_norm": 0.6909311413764954,
      "learning_rate": 0.00015181004125163497,
      "loss": 0.316,
      "step": 11979
    },
    {
      "epoch": 24.1046277665996,
      "grad_norm": 0.7147007584571838,
      "learning_rate": 0.00015180601670188149,
      "loss": 0.2883,
      "step": 11980
    },
    {
      "epoch": 24.106639839034205,
      "grad_norm": 0.694676399230957,
      "learning_rate": 0.00015180199215212797,
      "loss": 0.275,
      "step": 11981
    },
    {
      "epoch": 24.10865191146881,
      "grad_norm": 0.6657323837280273,
      "learning_rate": 0.0001517979676023745,
      "loss": 0.2959,
      "step": 11982
    },
    {
      "epoch": 24.11066398390342,
      "grad_norm": 0.6371727585792542,
      "learning_rate": 0.000151793943052621,
      "loss": 0.2648,
      "step": 11983
    },
    {
      "epoch": 24.112676056338028,
      "grad_norm": 0.7226030230522156,
      "learning_rate": 0.0001517899185028675,
      "loss": 0.3015,
      "step": 11984
    },
    {
      "epoch": 24.114688128772634,
      "grad_norm": 0.7020863890647888,
      "learning_rate": 0.000151785893953114,
      "loss": 0.2887,
      "step": 11985
    },
    {
      "epoch": 24.116700201207244,
      "grad_norm": 0.7740837335586548,
      "learning_rate": 0.0001517818694033605,
      "loss": 0.2798,
      "step": 11986
    },
    {
      "epoch": 24.11871227364185,
      "grad_norm": 0.6974823474884033,
      "learning_rate": 0.00015177784485360702,
      "loss": 0.3056,
      "step": 11987
    },
    {
      "epoch": 24.120724346076457,
      "grad_norm": 0.7117509245872498,
      "learning_rate": 0.00015177382030385353,
      "loss": 0.2585,
      "step": 11988
    },
    {
      "epoch": 24.122736418511067,
      "grad_norm": 0.6587185263633728,
      "learning_rate": 0.00015176979575410001,
      "loss": 0.2696,
      "step": 11989
    },
    {
      "epoch": 24.124748490945674,
      "grad_norm": 0.7170827984809875,
      "learning_rate": 0.00015176577120434653,
      "loss": 0.3145,
      "step": 11990
    },
    {
      "epoch": 24.12676056338028,
      "grad_norm": 0.7349199056625366,
      "learning_rate": 0.000151761746654593,
      "loss": 0.2995,
      "step": 11991
    },
    {
      "epoch": 24.12877263581489,
      "grad_norm": 0.6727410554885864,
      "learning_rate": 0.00015175772210483955,
      "loss": 0.2749,
      "step": 11992
    },
    {
      "epoch": 24.130784708249497,
      "grad_norm": 0.6647413372993469,
      "learning_rate": 0.00015175369755508604,
      "loss": 0.2705,
      "step": 11993
    },
    {
      "epoch": 24.132796780684103,
      "grad_norm": 0.6636438369750977,
      "learning_rate": 0.00015174967300533255,
      "loss": 0.2763,
      "step": 11994
    },
    {
      "epoch": 24.134808853118713,
      "grad_norm": 0.6970105767250061,
      "learning_rate": 0.00015174564845557903,
      "loss": 0.295,
      "step": 11995
    },
    {
      "epoch": 24.13682092555332,
      "grad_norm": 0.6774687170982361,
      "learning_rate": 0.00015174162390582555,
      "loss": 0.2791,
      "step": 11996
    },
    {
      "epoch": 24.138832997987926,
      "grad_norm": 0.6457561254501343,
      "learning_rate": 0.00015173759935607206,
      "loss": 0.2816,
      "step": 11997
    },
    {
      "epoch": 24.140845070422536,
      "grad_norm": 0.7243793606758118,
      "learning_rate": 0.00015173357480631857,
      "loss": 0.2947,
      "step": 11998
    },
    {
      "epoch": 24.142857142857142,
      "grad_norm": 0.6840384602546692,
      "learning_rate": 0.00015172955025656505,
      "loss": 0.2899,
      "step": 11999
    },
    {
      "epoch": 24.14486921529175,
      "grad_norm": 0.6759147047996521,
      "learning_rate": 0.00015172552570681157,
      "loss": 0.2965,
      "step": 12000
    },
    {
      "epoch": 24.14688128772636,
      "grad_norm": 0.6687803268432617,
      "learning_rate": 0.00015172150115705805,
      "loss": 0.2683,
      "step": 12001
    },
    {
      "epoch": 24.148893360160965,
      "grad_norm": 0.7265843749046326,
      "learning_rate": 0.00015171747660730456,
      "loss": 0.2824,
      "step": 12002
    },
    {
      "epoch": 24.15090543259557,
      "grad_norm": 0.6941784620285034,
      "learning_rate": 0.00015171345205755108,
      "loss": 0.2822,
      "step": 12003
    },
    {
      "epoch": 24.15291750503018,
      "grad_norm": 0.7097207903862,
      "learning_rate": 0.00015170942750779756,
      "loss": 0.2875,
      "step": 12004
    },
    {
      "epoch": 24.154929577464788,
      "grad_norm": 0.7331015467643738,
      "learning_rate": 0.00015170540295804407,
      "loss": 0.294,
      "step": 12005
    },
    {
      "epoch": 24.156941649899398,
      "grad_norm": 0.6812459230422974,
      "learning_rate": 0.00015170137840829059,
      "loss": 0.2687,
      "step": 12006
    },
    {
      "epoch": 24.158953722334005,
      "grad_norm": 0.7252210974693298,
      "learning_rate": 0.00015169735385853707,
      "loss": 0.2911,
      "step": 12007
    },
    {
      "epoch": 24.16096579476861,
      "grad_norm": 0.7955394387245178,
      "learning_rate": 0.00015169332930878358,
      "loss": 0.3005,
      "step": 12008
    },
    {
      "epoch": 24.16297786720322,
      "grad_norm": 0.7387984991073608,
      "learning_rate": 0.0001516893047590301,
      "loss": 0.2732,
      "step": 12009
    },
    {
      "epoch": 24.164989939637827,
      "grad_norm": 0.6939184665679932,
      "learning_rate": 0.00015168528020927658,
      "loss": 0.2853,
      "step": 12010
    },
    {
      "epoch": 24.167002012072434,
      "grad_norm": 0.643705427646637,
      "learning_rate": 0.0001516812556595231,
      "loss": 0.2741,
      "step": 12011
    },
    {
      "epoch": 24.169014084507044,
      "grad_norm": 0.7507577538490295,
      "learning_rate": 0.00015167723110976958,
      "loss": 0.2988,
      "step": 12012
    },
    {
      "epoch": 24.17102615694165,
      "grad_norm": 0.7552784085273743,
      "learning_rate": 0.00015167320656001612,
      "loss": 0.2791,
      "step": 12013
    },
    {
      "epoch": 24.173038229376257,
      "grad_norm": 0.7353069186210632,
      "learning_rate": 0.0001516691820102626,
      "loss": 0.2883,
      "step": 12014
    },
    {
      "epoch": 24.175050301810867,
      "grad_norm": 0.7772298455238342,
      "learning_rate": 0.00015166515746050911,
      "loss": 0.2821,
      "step": 12015
    },
    {
      "epoch": 24.177062374245473,
      "grad_norm": 0.729865550994873,
      "learning_rate": 0.0001516611329107556,
      "loss": 0.2914,
      "step": 12016
    },
    {
      "epoch": 24.17907444668008,
      "grad_norm": 0.7333180904388428,
      "learning_rate": 0.0001516571083610021,
      "loss": 0.3033,
      "step": 12017
    },
    {
      "epoch": 24.18108651911469,
      "grad_norm": 0.7408413887023926,
      "learning_rate": 0.00015165308381124862,
      "loss": 0.2964,
      "step": 12018
    },
    {
      "epoch": 24.183098591549296,
      "grad_norm": 0.6911740899085999,
      "learning_rate": 0.00015164905926149514,
      "loss": 0.2973,
      "step": 12019
    },
    {
      "epoch": 24.185110663983902,
      "grad_norm": 0.7557368278503418,
      "learning_rate": 0.00015164503471174162,
      "loss": 0.3335,
      "step": 12020
    },
    {
      "epoch": 24.187122736418512,
      "grad_norm": 0.6987971663475037,
      "learning_rate": 0.00015164101016198813,
      "loss": 0.2959,
      "step": 12021
    },
    {
      "epoch": 24.18913480885312,
      "grad_norm": 0.6859366297721863,
      "learning_rate": 0.00015163698561223462,
      "loss": 0.2858,
      "step": 12022
    },
    {
      "epoch": 24.191146881287725,
      "grad_norm": 0.6748249530792236,
      "learning_rate": 0.00015163296106248116,
      "loss": 0.2824,
      "step": 12023
    },
    {
      "epoch": 24.193158953722335,
      "grad_norm": 0.7133845686912537,
      "learning_rate": 0.00015162893651272764,
      "loss": 0.2609,
      "step": 12024
    },
    {
      "epoch": 24.19517102615694,
      "grad_norm": 0.7042149901390076,
      "learning_rate": 0.00015162491196297416,
      "loss": 0.2826,
      "step": 12025
    },
    {
      "epoch": 24.197183098591548,
      "grad_norm": 0.7238988280296326,
      "learning_rate": 0.00015162088741322064,
      "loss": 0.309,
      "step": 12026
    },
    {
      "epoch": 24.199195171026158,
      "grad_norm": 0.7131995558738708,
      "learning_rate": 0.00015161686286346715,
      "loss": 0.268,
      "step": 12027
    },
    {
      "epoch": 24.201207243460765,
      "grad_norm": 0.7479349970817566,
      "learning_rate": 0.00015161283831371367,
      "loss": 0.2918,
      "step": 12028
    },
    {
      "epoch": 24.20321931589537,
      "grad_norm": 0.6945937275886536,
      "learning_rate": 0.00015160881376396018,
      "loss": 0.2696,
      "step": 12029
    },
    {
      "epoch": 24.20523138832998,
      "grad_norm": 0.6792636513710022,
      "learning_rate": 0.00015160478921420666,
      "loss": 0.292,
      "step": 12030
    },
    {
      "epoch": 24.207243460764587,
      "grad_norm": 0.6770317554473877,
      "learning_rate": 0.00015160076466445317,
      "loss": 0.2925,
      "step": 12031
    },
    {
      "epoch": 24.209255533199194,
      "grad_norm": 0.7184740304946899,
      "learning_rate": 0.00015159674011469966,
      "loss": 0.2878,
      "step": 12032
    },
    {
      "epoch": 24.211267605633804,
      "grad_norm": 0.735264241695404,
      "learning_rate": 0.0001515927155649462,
      "loss": 0.3075,
      "step": 12033
    },
    {
      "epoch": 24.21327967806841,
      "grad_norm": 0.7211546301841736,
      "learning_rate": 0.00015158869101519268,
      "loss": 0.3076,
      "step": 12034
    },
    {
      "epoch": 24.215291750503017,
      "grad_norm": 0.7780429124832153,
      "learning_rate": 0.0001515846664654392,
      "loss": 0.3215,
      "step": 12035
    },
    {
      "epoch": 24.217303822937627,
      "grad_norm": 0.7254534363746643,
      "learning_rate": 0.00015158064191568568,
      "loss": 0.3183,
      "step": 12036
    },
    {
      "epoch": 24.219315895372233,
      "grad_norm": 0.7839496731758118,
      "learning_rate": 0.0001515766173659322,
      "loss": 0.3163,
      "step": 12037
    },
    {
      "epoch": 24.22132796780684,
      "grad_norm": 0.6797224879264832,
      "learning_rate": 0.0001515725928161787,
      "loss": 0.2741,
      "step": 12038
    },
    {
      "epoch": 24.22334004024145,
      "grad_norm": 0.776827871799469,
      "learning_rate": 0.0001515685682664252,
      "loss": 0.2992,
      "step": 12039
    },
    {
      "epoch": 24.225352112676056,
      "grad_norm": 0.7394207715988159,
      "learning_rate": 0.0001515645437166717,
      "loss": 0.3183,
      "step": 12040
    },
    {
      "epoch": 24.227364185110662,
      "grad_norm": 0.7146490216255188,
      "learning_rate": 0.00015156051916691822,
      "loss": 0.2878,
      "step": 12041
    },
    {
      "epoch": 24.229376257545272,
      "grad_norm": 0.7289959192276001,
      "learning_rate": 0.0001515564946171647,
      "loss": 0.2875,
      "step": 12042
    },
    {
      "epoch": 24.23138832997988,
      "grad_norm": 0.7215208411216736,
      "learning_rate": 0.0001515524700674112,
      "loss": 0.3029,
      "step": 12043
    },
    {
      "epoch": 24.233400402414485,
      "grad_norm": 0.6806948184967041,
      "learning_rate": 0.00015154844551765773,
      "loss": 0.2795,
      "step": 12044
    },
    {
      "epoch": 24.235412474849095,
      "grad_norm": 0.7666381597518921,
      "learning_rate": 0.0001515444209679042,
      "loss": 0.2825,
      "step": 12045
    },
    {
      "epoch": 24.2374245472837,
      "grad_norm": 0.7292503118515015,
      "learning_rate": 0.00015154039641815072,
      "loss": 0.2976,
      "step": 12046
    },
    {
      "epoch": 24.239436619718308,
      "grad_norm": 0.7006520628929138,
      "learning_rate": 0.0001515363718683972,
      "loss": 0.3007,
      "step": 12047
    },
    {
      "epoch": 24.241448692152918,
      "grad_norm": 0.7491462230682373,
      "learning_rate": 0.00015153234731864375,
      "loss": 0.2897,
      "step": 12048
    },
    {
      "epoch": 24.243460764587525,
      "grad_norm": 0.6994215846061707,
      "learning_rate": 0.00015152832276889023,
      "loss": 0.3021,
      "step": 12049
    },
    {
      "epoch": 24.24547283702213,
      "grad_norm": 0.711111307144165,
      "learning_rate": 0.00015152429821913674,
      "loss": 0.3015,
      "step": 12050
    },
    {
      "epoch": 24.24748490945674,
      "grad_norm": 0.7239264249801636,
      "learning_rate": 0.00015152027366938323,
      "loss": 0.3095,
      "step": 12051
    },
    {
      "epoch": 24.249496981891348,
      "grad_norm": 0.7194272875785828,
      "learning_rate": 0.00015151624911962974,
      "loss": 0.2968,
      "step": 12052
    },
    {
      "epoch": 24.251509054325957,
      "grad_norm": 0.7141730189323425,
      "learning_rate": 0.00015151222456987625,
      "loss": 0.2872,
      "step": 12053
    },
    {
      "epoch": 24.253521126760564,
      "grad_norm": 0.7097998261451721,
      "learning_rate": 0.00015150820002012277,
      "loss": 0.3047,
      "step": 12054
    },
    {
      "epoch": 24.25553319919517,
      "grad_norm": 0.7204509377479553,
      "learning_rate": 0.00015150417547036925,
      "loss": 0.2949,
      "step": 12055
    },
    {
      "epoch": 24.25754527162978,
      "grad_norm": 0.6983287334442139,
      "learning_rate": 0.00015150015092061576,
      "loss": 0.2757,
      "step": 12056
    },
    {
      "epoch": 24.259557344064387,
      "grad_norm": 0.6871880292892456,
      "learning_rate": 0.00015149612637086225,
      "loss": 0.2857,
      "step": 12057
    },
    {
      "epoch": 24.261569416498993,
      "grad_norm": 0.7246146202087402,
      "learning_rate": 0.0001514921018211088,
      "loss": 0.2856,
      "step": 12058
    },
    {
      "epoch": 24.263581488933603,
      "grad_norm": 0.7112438678741455,
      "learning_rate": 0.00015148807727135527,
      "loss": 0.2834,
      "step": 12059
    },
    {
      "epoch": 24.26559356136821,
      "grad_norm": 0.7643530368804932,
      "learning_rate": 0.00015148405272160178,
      "loss": 0.3147,
      "step": 12060
    },
    {
      "epoch": 24.267605633802816,
      "grad_norm": 0.7101681232452393,
      "learning_rate": 0.00015148002817184827,
      "loss": 0.3052,
      "step": 12061
    },
    {
      "epoch": 24.269617706237426,
      "grad_norm": 0.6990728378295898,
      "learning_rate": 0.00015147600362209478,
      "loss": 0.2853,
      "step": 12062
    },
    {
      "epoch": 24.271629778672033,
      "grad_norm": 0.6870096921920776,
      "learning_rate": 0.0001514719790723413,
      "loss": 0.2892,
      "step": 12063
    },
    {
      "epoch": 24.27364185110664,
      "grad_norm": 0.7160815000534058,
      "learning_rate": 0.0001514679545225878,
      "loss": 0.2772,
      "step": 12064
    },
    {
      "epoch": 24.27565392354125,
      "grad_norm": 0.7769171595573425,
      "learning_rate": 0.0001514639299728343,
      "loss": 0.3014,
      "step": 12065
    },
    {
      "epoch": 24.277665995975855,
      "grad_norm": 0.7402498722076416,
      "learning_rate": 0.0001514599054230808,
      "loss": 0.3025,
      "step": 12066
    },
    {
      "epoch": 24.279678068410462,
      "grad_norm": 0.7598696947097778,
      "learning_rate": 0.0001514558808733273,
      "loss": 0.2824,
      "step": 12067
    },
    {
      "epoch": 24.281690140845072,
      "grad_norm": 0.6908634901046753,
      "learning_rate": 0.00015145185632357383,
      "loss": 0.2913,
      "step": 12068
    },
    {
      "epoch": 24.28370221327968,
      "grad_norm": 0.6573587656021118,
      "learning_rate": 0.00015144783177382031,
      "loss": 0.2857,
      "step": 12069
    },
    {
      "epoch": 24.285714285714285,
      "grad_norm": 0.7498697638511658,
      "learning_rate": 0.00015144380722406683,
      "loss": 0.2894,
      "step": 12070
    },
    {
      "epoch": 24.287726358148895,
      "grad_norm": 0.7253624796867371,
      "learning_rate": 0.0001514397826743133,
      "loss": 0.3127,
      "step": 12071
    },
    {
      "epoch": 24.2897384305835,
      "grad_norm": 0.7379684448242188,
      "learning_rate": 0.00015143575812455982,
      "loss": 0.3061,
      "step": 12072
    },
    {
      "epoch": 24.291750503018108,
      "grad_norm": 0.703594982624054,
      "learning_rate": 0.00015143173357480634,
      "loss": 0.3115,
      "step": 12073
    },
    {
      "epoch": 24.293762575452718,
      "grad_norm": 0.7308061718940735,
      "learning_rate": 0.00015142770902505282,
      "loss": 0.2947,
      "step": 12074
    },
    {
      "epoch": 24.295774647887324,
      "grad_norm": 0.727994441986084,
      "learning_rate": 0.00015142368447529933,
      "loss": 0.3026,
      "step": 12075
    },
    {
      "epoch": 24.29778672032193,
      "grad_norm": 0.7249877452850342,
      "learning_rate": 0.00015141965992554584,
      "loss": 0.3175,
      "step": 12076
    },
    {
      "epoch": 24.29979879275654,
      "grad_norm": 0.7109878659248352,
      "learning_rate": 0.00015141563537579233,
      "loss": 0.2964,
      "step": 12077
    },
    {
      "epoch": 24.301810865191147,
      "grad_norm": 0.7242693901062012,
      "learning_rate": 0.00015141161082603884,
      "loss": 0.3029,
      "step": 12078
    },
    {
      "epoch": 24.303822937625753,
      "grad_norm": 0.6873992681503296,
      "learning_rate": 0.00015140758627628535,
      "loss": 0.3128,
      "step": 12079
    },
    {
      "epoch": 24.305835010060363,
      "grad_norm": 0.7293295860290527,
      "learning_rate": 0.00015140356172653184,
      "loss": 0.2995,
      "step": 12080
    },
    {
      "epoch": 24.30784708249497,
      "grad_norm": 0.7299755811691284,
      "learning_rate": 0.00015139953717677835,
      "loss": 0.2933,
      "step": 12081
    },
    {
      "epoch": 24.309859154929576,
      "grad_norm": 0.686576783657074,
      "learning_rate": 0.00015139551262702484,
      "loss": 0.2965,
      "step": 12082
    },
    {
      "epoch": 24.311871227364186,
      "grad_norm": 0.7299288511276245,
      "learning_rate": 0.00015139148807727138,
      "loss": 0.3033,
      "step": 12083
    },
    {
      "epoch": 24.313883299798793,
      "grad_norm": 0.7423938512802124,
      "learning_rate": 0.00015138746352751786,
      "loss": 0.3125,
      "step": 12084
    },
    {
      "epoch": 24.3158953722334,
      "grad_norm": 0.7205292582511902,
      "learning_rate": 0.00015138343897776437,
      "loss": 0.2974,
      "step": 12085
    },
    {
      "epoch": 24.31790744466801,
      "grad_norm": 0.745502233505249,
      "learning_rate": 0.00015137941442801086,
      "loss": 0.29,
      "step": 12086
    },
    {
      "epoch": 24.319919517102615,
      "grad_norm": 0.7299871444702148,
      "learning_rate": 0.00015137538987825737,
      "loss": 0.3005,
      "step": 12087
    },
    {
      "epoch": 24.321931589537222,
      "grad_norm": 0.7214446663856506,
      "learning_rate": 0.00015137136532850388,
      "loss": 0.3221,
      "step": 12088
    },
    {
      "epoch": 24.323943661971832,
      "grad_norm": 0.757942259311676,
      "learning_rate": 0.0001513673407787504,
      "loss": 0.3171,
      "step": 12089
    },
    {
      "epoch": 24.32595573440644,
      "grad_norm": 0.7325930595397949,
      "learning_rate": 0.00015136331622899688,
      "loss": 0.2864,
      "step": 12090
    },
    {
      "epoch": 24.327967806841045,
      "grad_norm": 0.7143069505691528,
      "learning_rate": 0.0001513592916792434,
      "loss": 0.2953,
      "step": 12091
    },
    {
      "epoch": 24.329979879275655,
      "grad_norm": 0.7355509996414185,
      "learning_rate": 0.00015135526712948988,
      "loss": 0.3143,
      "step": 12092
    },
    {
      "epoch": 24.33199195171026,
      "grad_norm": 0.7469138503074646,
      "learning_rate": 0.00015135124257973642,
      "loss": 0.2837,
      "step": 12093
    },
    {
      "epoch": 24.334004024144868,
      "grad_norm": 0.7711400389671326,
      "learning_rate": 0.0001513472180299829,
      "loss": 0.295,
      "step": 12094
    },
    {
      "epoch": 24.336016096579478,
      "grad_norm": 0.7518177032470703,
      "learning_rate": 0.00015134319348022941,
      "loss": 0.3097,
      "step": 12095
    },
    {
      "epoch": 24.338028169014084,
      "grad_norm": 0.7396571636199951,
      "learning_rate": 0.0001513391689304759,
      "loss": 0.3045,
      "step": 12096
    },
    {
      "epoch": 24.34004024144869,
      "grad_norm": 0.7538929581642151,
      "learning_rate": 0.0001513351443807224,
      "loss": 0.3271,
      "step": 12097
    },
    {
      "epoch": 24.3420523138833,
      "grad_norm": 0.690602719783783,
      "learning_rate": 0.00015133111983096892,
      "loss": 0.2989,
      "step": 12098
    },
    {
      "epoch": 24.344064386317907,
      "grad_norm": 0.6973938345909119,
      "learning_rate": 0.00015132709528121544,
      "loss": 0.3092,
      "step": 12099
    },
    {
      "epoch": 24.346076458752513,
      "grad_norm": 0.7287271618843079,
      "learning_rate": 0.00015132307073146192,
      "loss": 0.3067,
      "step": 12100
    },
    {
      "epoch": 24.348088531187123,
      "grad_norm": 0.7361352443695068,
      "learning_rate": 0.00015131904618170843,
      "loss": 0.3217,
      "step": 12101
    },
    {
      "epoch": 24.35010060362173,
      "grad_norm": 0.7974953055381775,
      "learning_rate": 0.00015131502163195492,
      "loss": 0.3093,
      "step": 12102
    },
    {
      "epoch": 24.352112676056336,
      "grad_norm": 0.7329221963882446,
      "learning_rate": 0.00015131099708220146,
      "loss": 0.3181,
      "step": 12103
    },
    {
      "epoch": 24.354124748490946,
      "grad_norm": 0.7016322016716003,
      "learning_rate": 0.00015130697253244794,
      "loss": 0.2765,
      "step": 12104
    },
    {
      "epoch": 24.356136820925553,
      "grad_norm": 0.754241943359375,
      "learning_rate": 0.00015130294798269446,
      "loss": 0.3278,
      "step": 12105
    },
    {
      "epoch": 24.358148893360163,
      "grad_norm": 0.7885466814041138,
      "learning_rate": 0.00015129892343294094,
      "loss": 0.3167,
      "step": 12106
    },
    {
      "epoch": 24.36016096579477,
      "grad_norm": 0.7541194558143616,
      "learning_rate": 0.00015129489888318745,
      "loss": 0.3292,
      "step": 12107
    },
    {
      "epoch": 24.362173038229376,
      "grad_norm": 0.7378572225570679,
      "learning_rate": 0.00015129087433343396,
      "loss": 0.3321,
      "step": 12108
    },
    {
      "epoch": 24.364185110663986,
      "grad_norm": 0.69471675157547,
      "learning_rate": 0.00015128684978368045,
      "loss": 0.28,
      "step": 12109
    },
    {
      "epoch": 24.366197183098592,
      "grad_norm": 0.7581503391265869,
      "learning_rate": 0.00015128282523392696,
      "loss": 0.3184,
      "step": 12110
    },
    {
      "epoch": 24.3682092555332,
      "grad_norm": 0.7382146120071411,
      "learning_rate": 0.00015127880068417345,
      "loss": 0.324,
      "step": 12111
    },
    {
      "epoch": 24.37022132796781,
      "grad_norm": 0.7076342105865479,
      "learning_rate": 0.00015127477613441996,
      "loss": 0.2954,
      "step": 12112
    },
    {
      "epoch": 24.372233400402415,
      "grad_norm": 0.737766683101654,
      "learning_rate": 0.00015127075158466647,
      "loss": 0.3075,
      "step": 12113
    },
    {
      "epoch": 24.37424547283702,
      "grad_norm": 0.7364397644996643,
      "learning_rate": 0.00015126672703491298,
      "loss": 0.3019,
      "step": 12114
    },
    {
      "epoch": 24.37625754527163,
      "grad_norm": 0.7485324740409851,
      "learning_rate": 0.00015126270248515947,
      "loss": 0.3047,
      "step": 12115
    },
    {
      "epoch": 24.378269617706238,
      "grad_norm": 0.7549731135368347,
      "learning_rate": 0.00015125867793540598,
      "loss": 0.3016,
      "step": 12116
    },
    {
      "epoch": 24.380281690140844,
      "grad_norm": 0.7129833698272705,
      "learning_rate": 0.00015125465338565247,
      "loss": 0.2934,
      "step": 12117
    },
    {
      "epoch": 24.382293762575454,
      "grad_norm": 0.7112675905227661,
      "learning_rate": 0.000151250628835899,
      "loss": 0.2876,
      "step": 12118
    },
    {
      "epoch": 24.38430583501006,
      "grad_norm": 0.7043889164924622,
      "learning_rate": 0.0001512466042861455,
      "loss": 0.3081,
      "step": 12119
    },
    {
      "epoch": 24.386317907444667,
      "grad_norm": 0.7178258895874023,
      "learning_rate": 0.000151242579736392,
      "loss": 0.2798,
      "step": 12120
    },
    {
      "epoch": 24.388329979879277,
      "grad_norm": 0.7235902547836304,
      "learning_rate": 0.0001512385551866385,
      "loss": 0.2884,
      "step": 12121
    },
    {
      "epoch": 24.390342052313883,
      "grad_norm": 0.7435410022735596,
      "learning_rate": 0.000151234530636885,
      "loss": 0.2998,
      "step": 12122
    },
    {
      "epoch": 24.39235412474849,
      "grad_norm": 0.7437736988067627,
      "learning_rate": 0.0001512305060871315,
      "loss": 0.2847,
      "step": 12123
    },
    {
      "epoch": 24.3943661971831,
      "grad_norm": 0.7322354912757874,
      "learning_rate": 0.00015122648153737802,
      "loss": 0.3228,
      "step": 12124
    },
    {
      "epoch": 24.396378269617706,
      "grad_norm": 0.7791087627410889,
      "learning_rate": 0.0001512224569876245,
      "loss": 0.308,
      "step": 12125
    },
    {
      "epoch": 24.398390342052313,
      "grad_norm": 0.7306824922561646,
      "learning_rate": 0.00015121843243787102,
      "loss": 0.2982,
      "step": 12126
    },
    {
      "epoch": 24.400402414486923,
      "grad_norm": 0.7110111713409424,
      "learning_rate": 0.0001512144078881175,
      "loss": 0.3001,
      "step": 12127
    },
    {
      "epoch": 24.40241448692153,
      "grad_norm": 0.7509645819664001,
      "learning_rate": 0.00015121038333836405,
      "loss": 0.3118,
      "step": 12128
    },
    {
      "epoch": 24.404426559356136,
      "grad_norm": 0.6767935752868652,
      "learning_rate": 0.00015120635878861053,
      "loss": 0.2882,
      "step": 12129
    },
    {
      "epoch": 24.406438631790746,
      "grad_norm": 0.7506594657897949,
      "learning_rate": 0.00015120233423885704,
      "loss": 0.318,
      "step": 12130
    },
    {
      "epoch": 24.408450704225352,
      "grad_norm": 0.6876040101051331,
      "learning_rate": 0.00015119830968910353,
      "loss": 0.2592,
      "step": 12131
    },
    {
      "epoch": 24.41046277665996,
      "grad_norm": 0.8020637631416321,
      "learning_rate": 0.00015119428513935004,
      "loss": 0.3077,
      "step": 12132
    },
    {
      "epoch": 24.41247484909457,
      "grad_norm": 0.7484276294708252,
      "learning_rate": 0.00015119026058959655,
      "loss": 0.3174,
      "step": 12133
    },
    {
      "epoch": 24.414486921529175,
      "grad_norm": 0.7412312030792236,
      "learning_rate": 0.00015118623603984307,
      "loss": 0.3142,
      "step": 12134
    },
    {
      "epoch": 24.41649899396378,
      "grad_norm": 0.7763605713844299,
      "learning_rate": 0.00015118221149008955,
      "loss": 0.3022,
      "step": 12135
    },
    {
      "epoch": 24.41851106639839,
      "grad_norm": 0.7119798064231873,
      "learning_rate": 0.00015117818694033606,
      "loss": 0.2975,
      "step": 12136
    },
    {
      "epoch": 24.420523138832998,
      "grad_norm": 0.7488093376159668,
      "learning_rate": 0.00015117416239058255,
      "loss": 0.3225,
      "step": 12137
    },
    {
      "epoch": 24.422535211267604,
      "grad_norm": 0.7380436062812805,
      "learning_rate": 0.00015117013784082906,
      "loss": 0.3269,
      "step": 12138
    },
    {
      "epoch": 24.424547283702214,
      "grad_norm": 0.6906956434249878,
      "learning_rate": 0.00015116611329107557,
      "loss": 0.2892,
      "step": 12139
    },
    {
      "epoch": 24.42655935613682,
      "grad_norm": 0.7212496995925903,
      "learning_rate": 0.00015116208874132208,
      "loss": 0.2881,
      "step": 12140
    },
    {
      "epoch": 24.428571428571427,
      "grad_norm": 0.7325553894042969,
      "learning_rate": 0.00015115806419156857,
      "loss": 0.2967,
      "step": 12141
    },
    {
      "epoch": 24.430583501006037,
      "grad_norm": 0.7140612006187439,
      "learning_rate": 0.00015115403964181508,
      "loss": 0.3014,
      "step": 12142
    },
    {
      "epoch": 24.432595573440643,
      "grad_norm": 0.7246152758598328,
      "learning_rate": 0.0001511500150920616,
      "loss": 0.3013,
      "step": 12143
    },
    {
      "epoch": 24.43460764587525,
      "grad_norm": 0.7420009970664978,
      "learning_rate": 0.00015114599054230808,
      "loss": 0.3398,
      "step": 12144
    },
    {
      "epoch": 24.43661971830986,
      "grad_norm": 0.7270588874816895,
      "learning_rate": 0.0001511419659925546,
      "loss": 0.2857,
      "step": 12145
    },
    {
      "epoch": 24.438631790744466,
      "grad_norm": 0.7500238418579102,
      "learning_rate": 0.00015113794144280108,
      "loss": 0.3172,
      "step": 12146
    },
    {
      "epoch": 24.440643863179073,
      "grad_norm": 0.7308666706085205,
      "learning_rate": 0.0001511339168930476,
      "loss": 0.3114,
      "step": 12147
    },
    {
      "epoch": 24.442655935613683,
      "grad_norm": 0.8115085959434509,
      "learning_rate": 0.0001511298923432941,
      "loss": 0.308,
      "step": 12148
    },
    {
      "epoch": 24.44466800804829,
      "grad_norm": 0.7234784960746765,
      "learning_rate": 0.0001511258677935406,
      "loss": 0.3328,
      "step": 12149
    },
    {
      "epoch": 24.446680080482896,
      "grad_norm": 0.7036240696907043,
      "learning_rate": 0.0001511218432437871,
      "loss": 0.3067,
      "step": 12150
    },
    {
      "epoch": 24.448692152917506,
      "grad_norm": 0.7203144431114197,
      "learning_rate": 0.0001511178186940336,
      "loss": 0.3015,
      "step": 12151
    },
    {
      "epoch": 24.450704225352112,
      "grad_norm": 0.7255135774612427,
      "learning_rate": 0.0001511137941442801,
      "loss": 0.304,
      "step": 12152
    },
    {
      "epoch": 24.452716297786722,
      "grad_norm": 0.7071714997291565,
      "learning_rate": 0.00015110976959452664,
      "loss": 0.3127,
      "step": 12153
    },
    {
      "epoch": 24.45472837022133,
      "grad_norm": 0.719207227230072,
      "learning_rate": 0.00015110574504477312,
      "loss": 0.288,
      "step": 12154
    },
    {
      "epoch": 24.456740442655935,
      "grad_norm": 0.7218745946884155,
      "learning_rate": 0.00015110172049501963,
      "loss": 0.3196,
      "step": 12155
    },
    {
      "epoch": 24.458752515090545,
      "grad_norm": 0.7423855066299438,
      "learning_rate": 0.00015109769594526612,
      "loss": 0.2913,
      "step": 12156
    },
    {
      "epoch": 24.46076458752515,
      "grad_norm": 0.7404402494430542,
      "learning_rate": 0.00015109367139551263,
      "loss": 0.3044,
      "step": 12157
    },
    {
      "epoch": 24.462776659959758,
      "grad_norm": 0.7137224078178406,
      "learning_rate": 0.00015108964684575914,
      "loss": 0.3086,
      "step": 12158
    },
    {
      "epoch": 24.464788732394368,
      "grad_norm": 0.7000699639320374,
      "learning_rate": 0.00015108562229600565,
      "loss": 0.3073,
      "step": 12159
    },
    {
      "epoch": 24.466800804828974,
      "grad_norm": 0.7457308769226074,
      "learning_rate": 0.00015108159774625214,
      "loss": 0.315,
      "step": 12160
    },
    {
      "epoch": 24.46881287726358,
      "grad_norm": 0.799562394618988,
      "learning_rate": 0.00015107757319649865,
      "loss": 0.3306,
      "step": 12161
    },
    {
      "epoch": 24.47082494969819,
      "grad_norm": 0.7047982215881348,
      "learning_rate": 0.00015107354864674514,
      "loss": 0.2957,
      "step": 12162
    },
    {
      "epoch": 24.472837022132797,
      "grad_norm": 0.715465247631073,
      "learning_rate": 0.00015106952409699168,
      "loss": 0.302,
      "step": 12163
    },
    {
      "epoch": 24.474849094567404,
      "grad_norm": 0.7644521594047546,
      "learning_rate": 0.00015106549954723816,
      "loss": 0.33,
      "step": 12164
    },
    {
      "epoch": 24.476861167002014,
      "grad_norm": 0.7604140043258667,
      "learning_rate": 0.00015106147499748467,
      "loss": 0.3121,
      "step": 12165
    },
    {
      "epoch": 24.47887323943662,
      "grad_norm": 0.7532273530960083,
      "learning_rate": 0.00015105745044773116,
      "loss": 0.333,
      "step": 12166
    },
    {
      "epoch": 24.480885311871226,
      "grad_norm": 0.7526782751083374,
      "learning_rate": 0.00015105342589797767,
      "loss": 0.3172,
      "step": 12167
    },
    {
      "epoch": 24.482897384305836,
      "grad_norm": 0.775200605392456,
      "learning_rate": 0.00015104940134822418,
      "loss": 0.3159,
      "step": 12168
    },
    {
      "epoch": 24.484909456740443,
      "grad_norm": 0.7233854532241821,
      "learning_rate": 0.0001510453767984707,
      "loss": 0.3017,
      "step": 12169
    },
    {
      "epoch": 24.48692152917505,
      "grad_norm": 0.780607283115387,
      "learning_rate": 0.00015104135224871718,
      "loss": 0.3293,
      "step": 12170
    },
    {
      "epoch": 24.48893360160966,
      "grad_norm": 0.7782673835754395,
      "learning_rate": 0.0001510373276989637,
      "loss": 0.3155,
      "step": 12171
    },
    {
      "epoch": 24.490945674044266,
      "grad_norm": 0.7992463111877441,
      "learning_rate": 0.00015103330314921018,
      "loss": 0.3168,
      "step": 12172
    },
    {
      "epoch": 24.492957746478872,
      "grad_norm": 0.7098711133003235,
      "learning_rate": 0.0001510292785994567,
      "loss": 0.2994,
      "step": 12173
    },
    {
      "epoch": 24.494969818913482,
      "grad_norm": 0.7542244791984558,
      "learning_rate": 0.0001510252540497032,
      "loss": 0.3063,
      "step": 12174
    },
    {
      "epoch": 24.49698189134809,
      "grad_norm": 0.7386408448219299,
      "learning_rate": 0.00015102122949994971,
      "loss": 0.3148,
      "step": 12175
    },
    {
      "epoch": 24.498993963782695,
      "grad_norm": 0.7105128765106201,
      "learning_rate": 0.0001510172049501962,
      "loss": 0.2961,
      "step": 12176
    },
    {
      "epoch": 24.501006036217305,
      "grad_norm": 0.7172641158103943,
      "learning_rate": 0.0001510131804004427,
      "loss": 0.2716,
      "step": 12177
    },
    {
      "epoch": 24.50301810865191,
      "grad_norm": 0.7591462135314941,
      "learning_rate": 0.00015100915585068922,
      "loss": 0.3171,
      "step": 12178
    },
    {
      "epoch": 24.505030181086518,
      "grad_norm": 0.7015443444252014,
      "learning_rate": 0.0001510051313009357,
      "loss": 0.294,
      "step": 12179
    },
    {
      "epoch": 24.507042253521128,
      "grad_norm": 0.7656649351119995,
      "learning_rate": 0.00015100110675118222,
      "loss": 0.3457,
      "step": 12180
    },
    {
      "epoch": 24.509054325955734,
      "grad_norm": 0.7494566440582275,
      "learning_rate": 0.0001509970822014287,
      "loss": 0.3279,
      "step": 12181
    },
    {
      "epoch": 24.51106639839034,
      "grad_norm": 0.7534368634223938,
      "learning_rate": 0.00015099305765167522,
      "loss": 0.3232,
      "step": 12182
    },
    {
      "epoch": 24.51307847082495,
      "grad_norm": 0.7186279296875,
      "learning_rate": 0.00015098903310192173,
      "loss": 0.319,
      "step": 12183
    },
    {
      "epoch": 24.515090543259557,
      "grad_norm": 0.697129487991333,
      "learning_rate": 0.00015098500855216824,
      "loss": 0.3199,
      "step": 12184
    },
    {
      "epoch": 24.517102615694164,
      "grad_norm": 0.7311272621154785,
      "learning_rate": 0.00015098098400241473,
      "loss": 0.3059,
      "step": 12185
    },
    {
      "epoch": 24.519114688128774,
      "grad_norm": 0.727104663848877,
      "learning_rate": 0.00015097695945266124,
      "loss": 0.3303,
      "step": 12186
    },
    {
      "epoch": 24.52112676056338,
      "grad_norm": 0.7074832916259766,
      "learning_rate": 0.00015097293490290773,
      "loss": 0.2894,
      "step": 12187
    },
    {
      "epoch": 24.523138832997986,
      "grad_norm": 0.7227476239204407,
      "learning_rate": 0.00015096891035315426,
      "loss": 0.3094,
      "step": 12188
    },
    {
      "epoch": 24.525150905432596,
      "grad_norm": 0.7495831847190857,
      "learning_rate": 0.00015096488580340075,
      "loss": 0.3119,
      "step": 12189
    },
    {
      "epoch": 24.527162977867203,
      "grad_norm": 0.7214986085891724,
      "learning_rate": 0.00015096086125364726,
      "loss": 0.3101,
      "step": 12190
    },
    {
      "epoch": 24.52917505030181,
      "grad_norm": 0.7495114207267761,
      "learning_rate": 0.00015095683670389375,
      "loss": 0.2789,
      "step": 12191
    },
    {
      "epoch": 24.53118712273642,
      "grad_norm": 0.7592225670814514,
      "learning_rate": 0.00015095281215414026,
      "loss": 0.3195,
      "step": 12192
    },
    {
      "epoch": 24.533199195171026,
      "grad_norm": 0.7252066731452942,
      "learning_rate": 0.00015094878760438677,
      "loss": 0.3168,
      "step": 12193
    },
    {
      "epoch": 24.535211267605632,
      "grad_norm": 0.7820566892623901,
      "learning_rate": 0.00015094476305463328,
      "loss": 0.336,
      "step": 12194
    },
    {
      "epoch": 24.537223340040242,
      "grad_norm": 0.7454491853713989,
      "learning_rate": 0.00015094073850487977,
      "loss": 0.3133,
      "step": 12195
    },
    {
      "epoch": 24.53923541247485,
      "grad_norm": 0.6765353083610535,
      "learning_rate": 0.00015093671395512628,
      "loss": 0.2874,
      "step": 12196
    },
    {
      "epoch": 24.541247484909455,
      "grad_norm": 0.8122062683105469,
      "learning_rate": 0.00015093268940537277,
      "loss": 0.362,
      "step": 12197
    },
    {
      "epoch": 24.543259557344065,
      "grad_norm": 0.754425048828125,
      "learning_rate": 0.0001509286648556193,
      "loss": 0.3098,
      "step": 12198
    },
    {
      "epoch": 24.54527162977867,
      "grad_norm": 0.7945981025695801,
      "learning_rate": 0.0001509246403058658,
      "loss": 0.3031,
      "step": 12199
    },
    {
      "epoch": 24.547283702213278,
      "grad_norm": 0.7417620420455933,
      "learning_rate": 0.0001509206157561123,
      "loss": 0.3212,
      "step": 12200
    },
    {
      "epoch": 24.549295774647888,
      "grad_norm": 0.7522269487380981,
      "learning_rate": 0.0001509165912063588,
      "loss": 0.3015,
      "step": 12201
    },
    {
      "epoch": 24.551307847082494,
      "grad_norm": 0.7614904642105103,
      "learning_rate": 0.0001509125666566053,
      "loss": 0.3368,
      "step": 12202
    },
    {
      "epoch": 24.5533199195171,
      "grad_norm": 0.7184723019599915,
      "learning_rate": 0.0001509085421068518,
      "loss": 0.3117,
      "step": 12203
    },
    {
      "epoch": 24.55533199195171,
      "grad_norm": 0.7821248769760132,
      "learning_rate": 0.00015090451755709832,
      "loss": 0.3429,
      "step": 12204
    },
    {
      "epoch": 24.557344064386317,
      "grad_norm": 0.7967586517333984,
      "learning_rate": 0.0001509004930073448,
      "loss": 0.3252,
      "step": 12205
    },
    {
      "epoch": 24.559356136820927,
      "grad_norm": 0.738575279712677,
      "learning_rate": 0.00015089646845759132,
      "loss": 0.3206,
      "step": 12206
    },
    {
      "epoch": 24.561368209255534,
      "grad_norm": 0.6981000304222107,
      "learning_rate": 0.0001508924439078378,
      "loss": 0.3312,
      "step": 12207
    },
    {
      "epoch": 24.56338028169014,
      "grad_norm": 0.7301288843154907,
      "learning_rate": 0.00015088841935808432,
      "loss": 0.3404,
      "step": 12208
    },
    {
      "epoch": 24.56539235412475,
      "grad_norm": 0.7405298948287964,
      "learning_rate": 0.00015088439480833083,
      "loss": 0.3044,
      "step": 12209
    },
    {
      "epoch": 24.567404426559357,
      "grad_norm": 0.8190590739250183,
      "learning_rate": 0.00015088037025857734,
      "loss": 0.3197,
      "step": 12210
    },
    {
      "epoch": 24.569416498993963,
      "grad_norm": 0.7774151563644409,
      "learning_rate": 0.00015087634570882383,
      "loss": 0.2947,
      "step": 12211
    },
    {
      "epoch": 24.571428571428573,
      "grad_norm": 0.7009093165397644,
      "learning_rate": 0.00015087232115907034,
      "loss": 0.2978,
      "step": 12212
    },
    {
      "epoch": 24.57344064386318,
      "grad_norm": 0.7417309880256653,
      "learning_rate": 0.00015086829660931685,
      "loss": 0.33,
      "step": 12213
    },
    {
      "epoch": 24.575452716297786,
      "grad_norm": 0.7566366195678711,
      "learning_rate": 0.00015086427205956334,
      "loss": 0.3322,
      "step": 12214
    },
    {
      "epoch": 24.577464788732396,
      "grad_norm": 0.745061457157135,
      "learning_rate": 0.00015086024750980985,
      "loss": 0.3342,
      "step": 12215
    },
    {
      "epoch": 24.579476861167002,
      "grad_norm": 0.7443752884864807,
      "learning_rate": 0.00015085622296005634,
      "loss": 0.3104,
      "step": 12216
    },
    {
      "epoch": 24.58148893360161,
      "grad_norm": 0.7501783967018127,
      "learning_rate": 0.00015085219841030285,
      "loss": 0.3284,
      "step": 12217
    },
    {
      "epoch": 24.58350100603622,
      "grad_norm": 0.7179058194160461,
      "learning_rate": 0.00015084817386054936,
      "loss": 0.3172,
      "step": 12218
    },
    {
      "epoch": 24.585513078470825,
      "grad_norm": 0.7728108167648315,
      "learning_rate": 0.00015084414931079587,
      "loss": 0.3237,
      "step": 12219
    },
    {
      "epoch": 24.58752515090543,
      "grad_norm": 0.7692198157310486,
      "learning_rate": 0.00015084012476104236,
      "loss": 0.3304,
      "step": 12220
    },
    {
      "epoch": 24.58953722334004,
      "grad_norm": 0.736916720867157,
      "learning_rate": 0.00015083610021128887,
      "loss": 0.2999,
      "step": 12221
    },
    {
      "epoch": 24.591549295774648,
      "grad_norm": 0.7678619027137756,
      "learning_rate": 0.00015083207566153535,
      "loss": 0.3359,
      "step": 12222
    },
    {
      "epoch": 24.593561368209254,
      "grad_norm": 0.7748491764068604,
      "learning_rate": 0.0001508280511117819,
      "loss": 0.3244,
      "step": 12223
    },
    {
      "epoch": 24.595573440643864,
      "grad_norm": 0.7230483889579773,
      "learning_rate": 0.00015082402656202838,
      "loss": 0.3017,
      "step": 12224
    },
    {
      "epoch": 24.59758551307847,
      "grad_norm": 0.7275878190994263,
      "learning_rate": 0.0001508200020122749,
      "loss": 0.3154,
      "step": 12225
    },
    {
      "epoch": 24.599597585513077,
      "grad_norm": 0.7540652751922607,
      "learning_rate": 0.00015081597746252138,
      "loss": 0.3219,
      "step": 12226
    },
    {
      "epoch": 24.601609657947687,
      "grad_norm": 0.7101641893386841,
      "learning_rate": 0.0001508119529127679,
      "loss": 0.3189,
      "step": 12227
    },
    {
      "epoch": 24.603621730382294,
      "grad_norm": 0.728662371635437,
      "learning_rate": 0.0001508079283630144,
      "loss": 0.3463,
      "step": 12228
    },
    {
      "epoch": 24.6056338028169,
      "grad_norm": 0.7740670442581177,
      "learning_rate": 0.0001508039038132609,
      "loss": 0.372,
      "step": 12229
    },
    {
      "epoch": 24.60764587525151,
      "grad_norm": 0.7228216528892517,
      "learning_rate": 0.0001507998792635074,
      "loss": 0.3042,
      "step": 12230
    },
    {
      "epoch": 24.609657947686117,
      "grad_norm": 0.8104901313781738,
      "learning_rate": 0.0001507958547137539,
      "loss": 0.3212,
      "step": 12231
    },
    {
      "epoch": 24.611670020120723,
      "grad_norm": 0.7372739911079407,
      "learning_rate": 0.0001507918301640004,
      "loss": 0.3169,
      "step": 12232
    },
    {
      "epoch": 24.613682092555333,
      "grad_norm": 0.7818816900253296,
      "learning_rate": 0.00015078780561424693,
      "loss": 0.3323,
      "step": 12233
    },
    {
      "epoch": 24.61569416498994,
      "grad_norm": 0.7570009231567383,
      "learning_rate": 0.00015078378106449342,
      "loss": 0.3326,
      "step": 12234
    },
    {
      "epoch": 24.617706237424546,
      "grad_norm": 0.7564584612846375,
      "learning_rate": 0.00015077975651473993,
      "loss": 0.3193,
      "step": 12235
    },
    {
      "epoch": 24.619718309859156,
      "grad_norm": 0.8133289217948914,
      "learning_rate": 0.00015077573196498642,
      "loss": 0.347,
      "step": 12236
    },
    {
      "epoch": 24.621730382293762,
      "grad_norm": 0.7259110808372498,
      "learning_rate": 0.00015077170741523293,
      "loss": 0.3164,
      "step": 12237
    },
    {
      "epoch": 24.62374245472837,
      "grad_norm": 0.7234811782836914,
      "learning_rate": 0.00015076768286547944,
      "loss": 0.3336,
      "step": 12238
    },
    {
      "epoch": 24.62575452716298,
      "grad_norm": 0.7206404209136963,
      "learning_rate": 0.00015076365831572595,
      "loss": 0.3122,
      "step": 12239
    },
    {
      "epoch": 24.627766599597585,
      "grad_norm": 0.7424774765968323,
      "learning_rate": 0.00015075963376597244,
      "loss": 0.3097,
      "step": 12240
    },
    {
      "epoch": 24.62977867203219,
      "grad_norm": 0.8274885416030884,
      "learning_rate": 0.00015075560921621895,
      "loss": 0.3192,
      "step": 12241
    },
    {
      "epoch": 24.6317907444668,
      "grad_norm": 0.8079763650894165,
      "learning_rate": 0.00015075158466646544,
      "loss": 0.3131,
      "step": 12242
    },
    {
      "epoch": 24.633802816901408,
      "grad_norm": 0.7321892380714417,
      "learning_rate": 0.00015074756011671195,
      "loss": 0.3063,
      "step": 12243
    },
    {
      "epoch": 24.635814889336014,
      "grad_norm": 0.744353175163269,
      "learning_rate": 0.00015074353556695846,
      "loss": 0.2971,
      "step": 12244
    },
    {
      "epoch": 24.637826961770624,
      "grad_norm": 0.7363063097000122,
      "learning_rate": 0.00015073951101720497,
      "loss": 0.3118,
      "step": 12245
    },
    {
      "epoch": 24.63983903420523,
      "grad_norm": 0.7537973523139954,
      "learning_rate": 0.00015073548646745146,
      "loss": 0.3287,
      "step": 12246
    },
    {
      "epoch": 24.641851106639837,
      "grad_norm": 0.7606743574142456,
      "learning_rate": 0.00015073146191769797,
      "loss": 0.3374,
      "step": 12247
    },
    {
      "epoch": 24.643863179074447,
      "grad_norm": 0.7509438395500183,
      "learning_rate": 0.00015072743736794446,
      "loss": 0.345,
      "step": 12248
    },
    {
      "epoch": 24.645875251509054,
      "grad_norm": 0.7297613620758057,
      "learning_rate": 0.00015072341281819097,
      "loss": 0.3069,
      "step": 12249
    },
    {
      "epoch": 24.647887323943664,
      "grad_norm": 0.7276255488395691,
      "learning_rate": 0.00015071938826843748,
      "loss": 0.3017,
      "step": 12250
    },
    {
      "epoch": 24.64989939637827,
      "grad_norm": 0.7615132927894592,
      "learning_rate": 0.00015071536371868396,
      "loss": 0.3511,
      "step": 12251
    },
    {
      "epoch": 24.651911468812877,
      "grad_norm": 0.8003718256950378,
      "learning_rate": 0.00015071133916893048,
      "loss": 0.3397,
      "step": 12252
    },
    {
      "epoch": 24.653923541247487,
      "grad_norm": 0.7505269646644592,
      "learning_rate": 0.00015070731461917696,
      "loss": 0.3162,
      "step": 12253
    },
    {
      "epoch": 24.655935613682093,
      "grad_norm": 0.7284643650054932,
      "learning_rate": 0.0001507032900694235,
      "loss": 0.323,
      "step": 12254
    },
    {
      "epoch": 24.6579476861167,
      "grad_norm": 0.7160601615905762,
      "learning_rate": 0.00015069926551967,
      "loss": 0.3106,
      "step": 12255
    },
    {
      "epoch": 24.65995975855131,
      "grad_norm": 0.7470363974571228,
      "learning_rate": 0.0001506952409699165,
      "loss": 0.3497,
      "step": 12256
    },
    {
      "epoch": 24.661971830985916,
      "grad_norm": 0.7365906238555908,
      "learning_rate": 0.00015069121642016298,
      "loss": 0.3466,
      "step": 12257
    },
    {
      "epoch": 24.663983903420522,
      "grad_norm": 0.7347458600997925,
      "learning_rate": 0.0001506871918704095,
      "loss": 0.3175,
      "step": 12258
    },
    {
      "epoch": 24.665995975855132,
      "grad_norm": 0.7274937033653259,
      "learning_rate": 0.000150683167320656,
      "loss": 0.308,
      "step": 12259
    },
    {
      "epoch": 24.66800804828974,
      "grad_norm": 0.7168903946876526,
      "learning_rate": 0.00015067914277090252,
      "loss": 0.3125,
      "step": 12260
    },
    {
      "epoch": 24.670020120724345,
      "grad_norm": 0.7495428323745728,
      "learning_rate": 0.000150675118221149,
      "loss": 0.33,
      "step": 12261
    },
    {
      "epoch": 24.672032193158955,
      "grad_norm": 0.8012233972549438,
      "learning_rate": 0.00015067109367139552,
      "loss": 0.3634,
      "step": 12262
    },
    {
      "epoch": 24.67404426559356,
      "grad_norm": 0.7374268174171448,
      "learning_rate": 0.000150667069121642,
      "loss": 0.3223,
      "step": 12263
    },
    {
      "epoch": 24.676056338028168,
      "grad_norm": 0.721488356590271,
      "learning_rate": 0.00015066304457188854,
      "loss": 0.3191,
      "step": 12264
    },
    {
      "epoch": 24.678068410462778,
      "grad_norm": 0.6962931752204895,
      "learning_rate": 0.00015065902002213503,
      "loss": 0.3216,
      "step": 12265
    },
    {
      "epoch": 24.680080482897385,
      "grad_norm": 0.7525436878204346,
      "learning_rate": 0.00015065499547238154,
      "loss": 0.3106,
      "step": 12266
    },
    {
      "epoch": 24.68209255533199,
      "grad_norm": 0.748278021812439,
      "learning_rate": 0.00015065097092262802,
      "loss": 0.3209,
      "step": 12267
    },
    {
      "epoch": 24.6841046277666,
      "grad_norm": 0.7500854134559631,
      "learning_rate": 0.00015064694637287454,
      "loss": 0.313,
      "step": 12268
    },
    {
      "epoch": 24.686116700201207,
      "grad_norm": 0.7658982276916504,
      "learning_rate": 0.00015064292182312105,
      "loss": 0.3303,
      "step": 12269
    },
    {
      "epoch": 24.688128772635814,
      "grad_norm": 0.6987708210945129,
      "learning_rate": 0.00015063889727336756,
      "loss": 0.3008,
      "step": 12270
    },
    {
      "epoch": 24.690140845070424,
      "grad_norm": 0.7246288061141968,
      "learning_rate": 0.00015063487272361405,
      "loss": 0.318,
      "step": 12271
    },
    {
      "epoch": 24.69215291750503,
      "grad_norm": 0.7243268489837646,
      "learning_rate": 0.00015063084817386056,
      "loss": 0.3169,
      "step": 12272
    },
    {
      "epoch": 24.694164989939637,
      "grad_norm": 0.7302104234695435,
      "learning_rate": 0.00015062682362410704,
      "loss": 0.3377,
      "step": 12273
    },
    {
      "epoch": 24.696177062374247,
      "grad_norm": 0.7585814595222473,
      "learning_rate": 0.00015062279907435358,
      "loss": 0.301,
      "step": 12274
    },
    {
      "epoch": 24.698189134808853,
      "grad_norm": 0.7347202897071838,
      "learning_rate": 0.00015061877452460007,
      "loss": 0.3096,
      "step": 12275
    },
    {
      "epoch": 24.70020120724346,
      "grad_norm": 0.7766970992088318,
      "learning_rate": 0.00015061474997484658,
      "loss": 0.3426,
      "step": 12276
    },
    {
      "epoch": 24.70221327967807,
      "grad_norm": 0.775227963924408,
      "learning_rate": 0.00015061072542509307,
      "loss": 0.3294,
      "step": 12277
    },
    {
      "epoch": 24.704225352112676,
      "grad_norm": 0.74126797914505,
      "learning_rate": 0.00015060670087533958,
      "loss": 0.3424,
      "step": 12278
    },
    {
      "epoch": 24.706237424547282,
      "grad_norm": 0.7560505867004395,
      "learning_rate": 0.0001506026763255861,
      "loss": 0.3648,
      "step": 12279
    },
    {
      "epoch": 24.708249496981892,
      "grad_norm": 0.7173388004302979,
      "learning_rate": 0.00015059865177583258,
      "loss": 0.3125,
      "step": 12280
    },
    {
      "epoch": 24.7102615694165,
      "grad_norm": 0.6832101345062256,
      "learning_rate": 0.0001505946272260791,
      "loss": 0.3131,
      "step": 12281
    },
    {
      "epoch": 24.712273641851105,
      "grad_norm": 0.7880868315696716,
      "learning_rate": 0.0001505906026763256,
      "loss": 0.341,
      "step": 12282
    },
    {
      "epoch": 24.714285714285715,
      "grad_norm": 0.7070892453193665,
      "learning_rate": 0.00015058657812657208,
      "loss": 0.3057,
      "step": 12283
    },
    {
      "epoch": 24.71629778672032,
      "grad_norm": 0.6999089121818542,
      "learning_rate": 0.0001505825535768186,
      "loss": 0.3112,
      "step": 12284
    },
    {
      "epoch": 24.718309859154928,
      "grad_norm": 0.7324985861778259,
      "learning_rate": 0.0001505785290270651,
      "loss": 0.3203,
      "step": 12285
    },
    {
      "epoch": 24.720321931589538,
      "grad_norm": 0.7238649725914001,
      "learning_rate": 0.0001505745044773116,
      "loss": 0.3178,
      "step": 12286
    },
    {
      "epoch": 24.722334004024145,
      "grad_norm": 0.7592558264732361,
      "learning_rate": 0.0001505704799275581,
      "loss": 0.3555,
      "step": 12287
    },
    {
      "epoch": 24.72434607645875,
      "grad_norm": 0.7540932893753052,
      "learning_rate": 0.0001505664553778046,
      "loss": 0.341,
      "step": 12288
    },
    {
      "epoch": 24.72635814889336,
      "grad_norm": 0.7334125638008118,
      "learning_rate": 0.00015056243082805113,
      "loss": 0.3134,
      "step": 12289
    },
    {
      "epoch": 24.728370221327967,
      "grad_norm": 0.7344465255737305,
      "learning_rate": 0.00015055840627829762,
      "loss": 0.3248,
      "step": 12290
    },
    {
      "epoch": 24.730382293762574,
      "grad_norm": 0.7395020127296448,
      "learning_rate": 0.00015055438172854413,
      "loss": 0.3249,
      "step": 12291
    },
    {
      "epoch": 24.732394366197184,
      "grad_norm": 0.7200827598571777,
      "learning_rate": 0.0001505503571787906,
      "loss": 0.3099,
      "step": 12292
    },
    {
      "epoch": 24.73440643863179,
      "grad_norm": 0.7562025785446167,
      "learning_rate": 0.00015054633262903713,
      "loss": 0.3335,
      "step": 12293
    },
    {
      "epoch": 24.736418511066397,
      "grad_norm": 0.7269295454025269,
      "learning_rate": 0.00015054230807928364,
      "loss": 0.2963,
      "step": 12294
    },
    {
      "epoch": 24.738430583501007,
      "grad_norm": 0.8251277208328247,
      "learning_rate": 0.00015053828352953015,
      "loss": 0.3345,
      "step": 12295
    },
    {
      "epoch": 24.740442655935613,
      "grad_norm": 0.7175019383430481,
      "learning_rate": 0.00015053425897977664,
      "loss": 0.3296,
      "step": 12296
    },
    {
      "epoch": 24.74245472837022,
      "grad_norm": 0.7890556454658508,
      "learning_rate": 0.00015053023443002315,
      "loss": 0.3194,
      "step": 12297
    },
    {
      "epoch": 24.74446680080483,
      "grad_norm": 0.7340860962867737,
      "learning_rate": 0.00015052620988026963,
      "loss": 0.3242,
      "step": 12298
    },
    {
      "epoch": 24.746478873239436,
      "grad_norm": 0.7522873282432556,
      "learning_rate": 0.00015052218533051617,
      "loss": 0.3264,
      "step": 12299
    },
    {
      "epoch": 24.748490945674043,
      "grad_norm": 0.7707334160804749,
      "learning_rate": 0.00015051816078076266,
      "loss": 0.3594,
      "step": 12300
    },
    {
      "epoch": 24.750503018108652,
      "grad_norm": 0.7189320921897888,
      "learning_rate": 0.00015051413623100917,
      "loss": 0.3039,
      "step": 12301
    },
    {
      "epoch": 24.75251509054326,
      "grad_norm": 0.777741014957428,
      "learning_rate": 0.00015051011168125565,
      "loss": 0.3178,
      "step": 12302
    },
    {
      "epoch": 24.754527162977865,
      "grad_norm": 0.754302978515625,
      "learning_rate": 0.00015050608713150217,
      "loss": 0.3177,
      "step": 12303
    },
    {
      "epoch": 24.756539235412475,
      "grad_norm": 0.77350252866745,
      "learning_rate": 0.00015050206258174868,
      "loss": 0.3242,
      "step": 12304
    },
    {
      "epoch": 24.758551307847082,
      "grad_norm": 0.7293684482574463,
      "learning_rate": 0.0001504980380319952,
      "loss": 0.327,
      "step": 12305
    },
    {
      "epoch": 24.760563380281692,
      "grad_norm": 0.7555789351463318,
      "learning_rate": 0.00015049401348224168,
      "loss": 0.3461,
      "step": 12306
    },
    {
      "epoch": 24.7625754527163,
      "grad_norm": 0.8293488621711731,
      "learning_rate": 0.0001504899889324882,
      "loss": 0.3344,
      "step": 12307
    },
    {
      "epoch": 24.764587525150905,
      "grad_norm": 0.7366231679916382,
      "learning_rate": 0.00015048596438273467,
      "loss": 0.3409,
      "step": 12308
    },
    {
      "epoch": 24.766599597585515,
      "grad_norm": 0.749095618724823,
      "learning_rate": 0.0001504819398329812,
      "loss": 0.3215,
      "step": 12309
    },
    {
      "epoch": 24.76861167002012,
      "grad_norm": 0.7224053740501404,
      "learning_rate": 0.0001504779152832277,
      "loss": 0.3348,
      "step": 12310
    },
    {
      "epoch": 24.770623742454728,
      "grad_norm": 0.7518449425697327,
      "learning_rate": 0.0001504738907334742,
      "loss": 0.3514,
      "step": 12311
    },
    {
      "epoch": 24.772635814889338,
      "grad_norm": 0.7405693531036377,
      "learning_rate": 0.0001504698661837207,
      "loss": 0.3249,
      "step": 12312
    },
    {
      "epoch": 24.774647887323944,
      "grad_norm": 0.8007651567459106,
      "learning_rate": 0.0001504658416339672,
      "loss": 0.3402,
      "step": 12313
    },
    {
      "epoch": 24.77665995975855,
      "grad_norm": 0.7690573334693909,
      "learning_rate": 0.00015046181708421372,
      "loss": 0.3658,
      "step": 12314
    },
    {
      "epoch": 24.77867203219316,
      "grad_norm": 0.731516420841217,
      "learning_rate": 0.0001504577925344602,
      "loss": 0.3232,
      "step": 12315
    },
    {
      "epoch": 24.780684104627767,
      "grad_norm": 0.764882504940033,
      "learning_rate": 0.00015045376798470672,
      "loss": 0.3338,
      "step": 12316
    },
    {
      "epoch": 24.782696177062373,
      "grad_norm": 0.7107548713684082,
      "learning_rate": 0.00015044974343495323,
      "loss": 0.3187,
      "step": 12317
    },
    {
      "epoch": 24.784708249496983,
      "grad_norm": 0.7418256998062134,
      "learning_rate": 0.00015044571888519971,
      "loss": 0.3246,
      "step": 12318
    },
    {
      "epoch": 24.78672032193159,
      "grad_norm": 0.7121483683586121,
      "learning_rate": 0.00015044169433544623,
      "loss": 0.336,
      "step": 12319
    },
    {
      "epoch": 24.788732394366196,
      "grad_norm": 0.7433841824531555,
      "learning_rate": 0.00015043766978569274,
      "loss": 0.3331,
      "step": 12320
    },
    {
      "epoch": 24.790744466800806,
      "grad_norm": 0.7798544764518738,
      "learning_rate": 0.00015043364523593922,
      "loss": 0.3192,
      "step": 12321
    },
    {
      "epoch": 24.792756539235413,
      "grad_norm": 0.7263286709785461,
      "learning_rate": 0.00015042962068618574,
      "loss": 0.3159,
      "step": 12322
    },
    {
      "epoch": 24.79476861167002,
      "grad_norm": 0.780392587184906,
      "learning_rate": 0.00015042559613643222,
      "loss": 0.3491,
      "step": 12323
    },
    {
      "epoch": 24.79678068410463,
      "grad_norm": 0.7301477789878845,
      "learning_rate": 0.00015042157158667876,
      "loss": 0.3293,
      "step": 12324
    },
    {
      "epoch": 24.798792756539235,
      "grad_norm": 0.7976919412612915,
      "learning_rate": 0.00015041754703692525,
      "loss": 0.3073,
      "step": 12325
    },
    {
      "epoch": 24.800804828973842,
      "grad_norm": 0.7520653605461121,
      "learning_rate": 0.00015041352248717176,
      "loss": 0.2858,
      "step": 12326
    },
    {
      "epoch": 24.802816901408452,
      "grad_norm": 0.7269344925880432,
      "learning_rate": 0.00015040949793741824,
      "loss": 0.3162,
      "step": 12327
    },
    {
      "epoch": 24.80482897384306,
      "grad_norm": 0.7147478461265564,
      "learning_rate": 0.00015040547338766476,
      "loss": 0.3043,
      "step": 12328
    },
    {
      "epoch": 24.806841046277665,
      "grad_norm": 0.8043241500854492,
      "learning_rate": 0.00015040144883791127,
      "loss": 0.3579,
      "step": 12329
    },
    {
      "epoch": 24.808853118712275,
      "grad_norm": 0.7381619811058044,
      "learning_rate": 0.00015039742428815778,
      "loss": 0.321,
      "step": 12330
    },
    {
      "epoch": 24.81086519114688,
      "grad_norm": 0.7480506896972656,
      "learning_rate": 0.00015039339973840426,
      "loss": 0.3661,
      "step": 12331
    },
    {
      "epoch": 24.812877263581488,
      "grad_norm": 0.7364161014556885,
      "learning_rate": 0.00015038937518865078,
      "loss": 0.3356,
      "step": 12332
    },
    {
      "epoch": 24.814889336016098,
      "grad_norm": 0.7047799229621887,
      "learning_rate": 0.00015038535063889726,
      "loss": 0.3021,
      "step": 12333
    },
    {
      "epoch": 24.816901408450704,
      "grad_norm": 0.7570348381996155,
      "learning_rate": 0.0001503813260891438,
      "loss": 0.3351,
      "step": 12334
    },
    {
      "epoch": 24.81891348088531,
      "grad_norm": 0.7365961074829102,
      "learning_rate": 0.0001503773015393903,
      "loss": 0.3424,
      "step": 12335
    },
    {
      "epoch": 24.82092555331992,
      "grad_norm": 0.7548682689666748,
      "learning_rate": 0.0001503732769896368,
      "loss": 0.3451,
      "step": 12336
    },
    {
      "epoch": 24.822937625754527,
      "grad_norm": 0.776388943195343,
      "learning_rate": 0.00015036925243988328,
      "loss": 0.3536,
      "step": 12337
    },
    {
      "epoch": 24.824949698189133,
      "grad_norm": 0.7217773199081421,
      "learning_rate": 0.0001503652278901298,
      "loss": 0.333,
      "step": 12338
    },
    {
      "epoch": 24.826961770623743,
      "grad_norm": 0.7537906765937805,
      "learning_rate": 0.0001503612033403763,
      "loss": 0.3408,
      "step": 12339
    },
    {
      "epoch": 24.82897384305835,
      "grad_norm": 0.7662443518638611,
      "learning_rate": 0.00015035717879062282,
      "loss": 0.3502,
      "step": 12340
    },
    {
      "epoch": 24.830985915492956,
      "grad_norm": 0.7795235514640808,
      "learning_rate": 0.0001503531542408693,
      "loss": 0.3442,
      "step": 12341
    },
    {
      "epoch": 24.832997987927566,
      "grad_norm": 0.7656625509262085,
      "learning_rate": 0.00015034912969111582,
      "loss": 0.3731,
      "step": 12342
    },
    {
      "epoch": 24.835010060362173,
      "grad_norm": 0.7248599529266357,
      "learning_rate": 0.0001503451051413623,
      "loss": 0.3397,
      "step": 12343
    },
    {
      "epoch": 24.83702213279678,
      "grad_norm": 0.7468461394309998,
      "learning_rate": 0.00015034108059160884,
      "loss": 0.3368,
      "step": 12344
    },
    {
      "epoch": 24.83903420523139,
      "grad_norm": 0.7150283455848694,
      "learning_rate": 0.00015033705604185533,
      "loss": 0.3178,
      "step": 12345
    },
    {
      "epoch": 24.841046277665995,
      "grad_norm": 0.730399489402771,
      "learning_rate": 0.00015033303149210184,
      "loss": 0.335,
      "step": 12346
    },
    {
      "epoch": 24.843058350100602,
      "grad_norm": 0.7818757891654968,
      "learning_rate": 0.00015032900694234832,
      "loss": 0.3519,
      "step": 12347
    },
    {
      "epoch": 24.845070422535212,
      "grad_norm": 0.7563718557357788,
      "learning_rate": 0.00015032498239259484,
      "loss": 0.3154,
      "step": 12348
    },
    {
      "epoch": 24.84708249496982,
      "grad_norm": 0.7758439779281616,
      "learning_rate": 0.00015032095784284135,
      "loss": 0.3268,
      "step": 12349
    },
    {
      "epoch": 24.84909456740443,
      "grad_norm": 0.7383113503456116,
      "learning_rate": 0.00015031693329308783,
      "loss": 0.3453,
      "step": 12350
    },
    {
      "epoch": 24.851106639839035,
      "grad_norm": 0.7395036816596985,
      "learning_rate": 0.00015031290874333435,
      "loss": 0.3451,
      "step": 12351
    },
    {
      "epoch": 24.85311871227364,
      "grad_norm": 0.7293384075164795,
      "learning_rate": 0.00015030888419358086,
      "loss": 0.2998,
      "step": 12352
    },
    {
      "epoch": 24.85513078470825,
      "grad_norm": 0.7801175117492676,
      "learning_rate": 0.00015030485964382734,
      "loss": 0.3356,
      "step": 12353
    },
    {
      "epoch": 24.857142857142858,
      "grad_norm": 0.7505655288696289,
      "learning_rate": 0.00015030083509407386,
      "loss": 0.3485,
      "step": 12354
    },
    {
      "epoch": 24.859154929577464,
      "grad_norm": 0.7103930711746216,
      "learning_rate": 0.00015029681054432037,
      "loss": 0.3348,
      "step": 12355
    },
    {
      "epoch": 24.861167002012074,
      "grad_norm": 0.7535672187805176,
      "learning_rate": 0.00015029278599456685,
      "loss": 0.3401,
      "step": 12356
    },
    {
      "epoch": 24.86317907444668,
      "grad_norm": 0.7628243565559387,
      "learning_rate": 0.00015028876144481337,
      "loss": 0.3406,
      "step": 12357
    },
    {
      "epoch": 24.865191146881287,
      "grad_norm": 0.7684636116027832,
      "learning_rate": 0.00015028473689505985,
      "loss": 0.3203,
      "step": 12358
    },
    {
      "epoch": 24.867203219315897,
      "grad_norm": 0.7414256930351257,
      "learning_rate": 0.0001502807123453064,
      "loss": 0.3368,
      "step": 12359
    },
    {
      "epoch": 24.869215291750503,
      "grad_norm": 0.8557103276252747,
      "learning_rate": 0.00015027668779555288,
      "loss": 0.323,
      "step": 12360
    },
    {
      "epoch": 24.87122736418511,
      "grad_norm": 0.7242356538772583,
      "learning_rate": 0.0001502726632457994,
      "loss": 0.3125,
      "step": 12361
    },
    {
      "epoch": 24.87323943661972,
      "grad_norm": 0.7398971319198608,
      "learning_rate": 0.00015026863869604587,
      "loss": 0.3498,
      "step": 12362
    },
    {
      "epoch": 24.875251509054326,
      "grad_norm": 0.7386958003044128,
      "learning_rate": 0.00015026461414629238,
      "loss": 0.3594,
      "step": 12363
    },
    {
      "epoch": 24.877263581488933,
      "grad_norm": 0.7562692165374756,
      "learning_rate": 0.0001502605895965389,
      "loss": 0.3715,
      "step": 12364
    },
    {
      "epoch": 24.879275653923543,
      "grad_norm": 0.7416185140609741,
      "learning_rate": 0.0001502565650467854,
      "loss": 0.3276,
      "step": 12365
    },
    {
      "epoch": 24.88128772635815,
      "grad_norm": 0.7149054408073425,
      "learning_rate": 0.0001502525404970319,
      "loss": 0.3024,
      "step": 12366
    },
    {
      "epoch": 24.883299798792756,
      "grad_norm": 0.7646487355232239,
      "learning_rate": 0.0001502485159472784,
      "loss": 0.3129,
      "step": 12367
    },
    {
      "epoch": 24.885311871227366,
      "grad_norm": 0.8206384181976318,
      "learning_rate": 0.0001502444913975249,
      "loss": 0.3222,
      "step": 12368
    },
    {
      "epoch": 24.887323943661972,
      "grad_norm": 0.7132487893104553,
      "learning_rate": 0.00015024046684777143,
      "loss": 0.3087,
      "step": 12369
    },
    {
      "epoch": 24.88933601609658,
      "grad_norm": 0.7516809105873108,
      "learning_rate": 0.00015023644229801792,
      "loss": 0.3334,
      "step": 12370
    },
    {
      "epoch": 24.89134808853119,
      "grad_norm": 0.7755746245384216,
      "learning_rate": 0.00015023241774826443,
      "loss": 0.3648,
      "step": 12371
    },
    {
      "epoch": 24.893360160965795,
      "grad_norm": 0.7280237674713135,
      "learning_rate": 0.0001502283931985109,
      "loss": 0.3108,
      "step": 12372
    },
    {
      "epoch": 24.8953722334004,
      "grad_norm": 0.7463529109954834,
      "learning_rate": 0.00015022436864875743,
      "loss": 0.3267,
      "step": 12373
    },
    {
      "epoch": 24.89738430583501,
      "grad_norm": 0.7680657505989075,
      "learning_rate": 0.00015022034409900394,
      "loss": 0.3389,
      "step": 12374
    },
    {
      "epoch": 24.899396378269618,
      "grad_norm": 0.7303561568260193,
      "learning_rate": 0.00015021631954925045,
      "loss": 0.3299,
      "step": 12375
    },
    {
      "epoch": 24.901408450704224,
      "grad_norm": 0.7337952852249146,
      "learning_rate": 0.00015021229499949693,
      "loss": 0.326,
      "step": 12376
    },
    {
      "epoch": 24.903420523138834,
      "grad_norm": 0.707404375076294,
      "learning_rate": 0.00015020827044974345,
      "loss": 0.3293,
      "step": 12377
    },
    {
      "epoch": 24.90543259557344,
      "grad_norm": 0.7109960913658142,
      "learning_rate": 0.00015020424589998993,
      "loss": 0.319,
      "step": 12378
    },
    {
      "epoch": 24.907444668008047,
      "grad_norm": 0.7674646973609924,
      "learning_rate": 0.00015020022135023647,
      "loss": 0.3385,
      "step": 12379
    },
    {
      "epoch": 24.909456740442657,
      "grad_norm": 0.70293790102005,
      "learning_rate": 0.00015019619680048296,
      "loss": 0.3358,
      "step": 12380
    },
    {
      "epoch": 24.911468812877263,
      "grad_norm": 0.7868365049362183,
      "learning_rate": 0.00015019217225072947,
      "loss": 0.341,
      "step": 12381
    },
    {
      "epoch": 24.91348088531187,
      "grad_norm": 0.7745285630226135,
      "learning_rate": 0.00015018814770097595,
      "loss": 0.3296,
      "step": 12382
    },
    {
      "epoch": 24.91549295774648,
      "grad_norm": 0.7141907215118408,
      "learning_rate": 0.00015018412315122247,
      "loss": 0.3315,
      "step": 12383
    },
    {
      "epoch": 24.917505030181086,
      "grad_norm": 0.75490802526474,
      "learning_rate": 0.00015018009860146898,
      "loss": 0.3468,
      "step": 12384
    },
    {
      "epoch": 24.919517102615693,
      "grad_norm": 0.7707579135894775,
      "learning_rate": 0.00015017607405171546,
      "loss": 0.3517,
      "step": 12385
    },
    {
      "epoch": 24.921529175050303,
      "grad_norm": 0.7824519872665405,
      "learning_rate": 0.00015017204950196198,
      "loss": 0.3416,
      "step": 12386
    },
    {
      "epoch": 24.92354124748491,
      "grad_norm": 0.7765389084815979,
      "learning_rate": 0.0001501680249522085,
      "loss": 0.3666,
      "step": 12387
    },
    {
      "epoch": 24.925553319919516,
      "grad_norm": 0.7357140183448792,
      "learning_rate": 0.00015016400040245497,
      "loss": 0.3293,
      "step": 12388
    },
    {
      "epoch": 24.927565392354126,
      "grad_norm": 0.7945161461830139,
      "learning_rate": 0.00015015997585270149,
      "loss": 0.3378,
      "step": 12389
    },
    {
      "epoch": 24.929577464788732,
      "grad_norm": 0.7843195199966431,
      "learning_rate": 0.000150155951302948,
      "loss": 0.3652,
      "step": 12390
    },
    {
      "epoch": 24.93158953722334,
      "grad_norm": 0.7064261436462402,
      "learning_rate": 0.00015015192675319448,
      "loss": 0.3144,
      "step": 12391
    },
    {
      "epoch": 24.93360160965795,
      "grad_norm": 0.7532206177711487,
      "learning_rate": 0.000150147902203441,
      "loss": 0.3627,
      "step": 12392
    },
    {
      "epoch": 24.935613682092555,
      "grad_norm": 0.7544792890548706,
      "learning_rate": 0.00015014387765368748,
      "loss": 0.3233,
      "step": 12393
    },
    {
      "epoch": 24.93762575452716,
      "grad_norm": 0.7474142909049988,
      "learning_rate": 0.00015013985310393402,
      "loss": 0.3341,
      "step": 12394
    },
    {
      "epoch": 24.93963782696177,
      "grad_norm": 0.7518942952156067,
      "learning_rate": 0.0001501358285541805,
      "loss": 0.3204,
      "step": 12395
    },
    {
      "epoch": 24.941649899396378,
      "grad_norm": 0.7426102161407471,
      "learning_rate": 0.00015013180400442702,
      "loss": 0.351,
      "step": 12396
    },
    {
      "epoch": 24.943661971830984,
      "grad_norm": 0.775115966796875,
      "learning_rate": 0.0001501277794546735,
      "loss": 0.3538,
      "step": 12397
    },
    {
      "epoch": 24.945674044265594,
      "grad_norm": 0.7103366851806641,
      "learning_rate": 0.00015012375490492001,
      "loss": 0.3072,
      "step": 12398
    },
    {
      "epoch": 24.9476861167002,
      "grad_norm": 0.7558114528656006,
      "learning_rate": 0.00015011973035516653,
      "loss": 0.3405,
      "step": 12399
    },
    {
      "epoch": 24.949698189134807,
      "grad_norm": 0.7542030811309814,
      "learning_rate": 0.00015011570580541304,
      "loss": 0.358,
      "step": 12400
    },
    {
      "epoch": 24.951710261569417,
      "grad_norm": 0.7542465329170227,
      "learning_rate": 0.00015011168125565952,
      "loss": 0.3231,
      "step": 12401
    },
    {
      "epoch": 24.953722334004024,
      "grad_norm": 0.7578826546669006,
      "learning_rate": 0.00015010765670590604,
      "loss": 0.3107,
      "step": 12402
    },
    {
      "epoch": 24.955734406438633,
      "grad_norm": 0.7282606959342957,
      "learning_rate": 0.00015010363215615252,
      "loss": 0.3382,
      "step": 12403
    },
    {
      "epoch": 24.95774647887324,
      "grad_norm": 0.74522864818573,
      "learning_rate": 0.00015009960760639906,
      "loss": 0.3487,
      "step": 12404
    },
    {
      "epoch": 24.959758551307846,
      "grad_norm": 0.725838303565979,
      "learning_rate": 0.00015009558305664555,
      "loss": 0.3283,
      "step": 12405
    },
    {
      "epoch": 24.961770623742456,
      "grad_norm": 0.752124547958374,
      "learning_rate": 0.00015009155850689206,
      "loss": 0.3347,
      "step": 12406
    },
    {
      "epoch": 24.963782696177063,
      "grad_norm": 0.7611011266708374,
      "learning_rate": 0.00015008753395713854,
      "loss": 0.3313,
      "step": 12407
    },
    {
      "epoch": 24.96579476861167,
      "grad_norm": 0.7714781761169434,
      "learning_rate": 0.00015008350940738505,
      "loss": 0.3211,
      "step": 12408
    },
    {
      "epoch": 24.96780684104628,
      "grad_norm": 0.7487471103668213,
      "learning_rate": 0.00015007948485763157,
      "loss": 0.3531,
      "step": 12409
    },
    {
      "epoch": 24.969818913480886,
      "grad_norm": 0.6989200711250305,
      "learning_rate": 0.00015007546030787808,
      "loss": 0.3159,
      "step": 12410
    },
    {
      "epoch": 24.971830985915492,
      "grad_norm": 0.7674663066864014,
      "learning_rate": 0.00015007143575812456,
      "loss": 0.3133,
      "step": 12411
    },
    {
      "epoch": 24.973843058350102,
      "grad_norm": 0.7352102398872375,
      "learning_rate": 0.00015006741120837108,
      "loss": 0.3565,
      "step": 12412
    },
    {
      "epoch": 24.97585513078471,
      "grad_norm": 0.7546566128730774,
      "learning_rate": 0.00015006338665861756,
      "loss": 0.3228,
      "step": 12413
    },
    {
      "epoch": 24.977867203219315,
      "grad_norm": 0.7272361516952515,
      "learning_rate": 0.0001500593621088641,
      "loss": 0.3285,
      "step": 12414
    },
    {
      "epoch": 24.979879275653925,
      "grad_norm": 0.7593162655830383,
      "learning_rate": 0.00015005533755911059,
      "loss": 0.3587,
      "step": 12415
    },
    {
      "epoch": 24.98189134808853,
      "grad_norm": 0.7368446588516235,
      "learning_rate": 0.0001500513130093571,
      "loss": 0.3203,
      "step": 12416
    },
    {
      "epoch": 24.983903420523138,
      "grad_norm": 0.771240770816803,
      "learning_rate": 0.00015004728845960358,
      "loss": 0.3423,
      "step": 12417
    },
    {
      "epoch": 24.985915492957748,
      "grad_norm": 0.7420585751533508,
      "learning_rate": 0.0001500432639098501,
      "loss": 0.3353,
      "step": 12418
    },
    {
      "epoch": 24.987927565392354,
      "grad_norm": 0.7520308494567871,
      "learning_rate": 0.0001500392393600966,
      "loss": 0.3395,
      "step": 12419
    },
    {
      "epoch": 24.98993963782696,
      "grad_norm": 0.7467088103294373,
      "learning_rate": 0.0001500352148103431,
      "loss": 0.3438,
      "step": 12420
    },
    {
      "epoch": 24.99195171026157,
      "grad_norm": 0.7743856906890869,
      "learning_rate": 0.0001500311902605896,
      "loss": 0.3705,
      "step": 12421
    },
    {
      "epoch": 24.993963782696177,
      "grad_norm": 0.7375678420066833,
      "learning_rate": 0.00015002716571083612,
      "loss": 0.3455,
      "step": 12422
    },
    {
      "epoch": 24.995975855130784,
      "grad_norm": 0.7211930751800537,
      "learning_rate": 0.0001500231411610826,
      "loss": 0.3328,
      "step": 12423
    },
    {
      "epoch": 24.997987927565394,
      "grad_norm": 0.7213833928108215,
      "learning_rate": 0.00015001911661132911,
      "loss": 0.3191,
      "step": 12424
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.7327308058738708,
      "learning_rate": 0.00015001509206157563,
      "loss": 0.3242,
      "step": 12425
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.121000051498413,
      "eval_runtime": 49.8343,
      "eval_samples_per_second": 19.906,
      "eval_steps_per_second": 2.488,
      "step": 12425
    },
    {
      "epoch": 25.002012072434606,
      "grad_norm": 0.6233192682266235,
      "learning_rate": 0.0001500110675118221,
      "loss": 0.2744,
      "step": 12426
    },
    {
      "epoch": 25.004024144869216,
      "grad_norm": 0.6681527495384216,
      "learning_rate": 0.00015000704296206862,
      "loss": 0.2648,
      "step": 12427
    },
    {
      "epoch": 25.006036217303823,
      "grad_norm": 0.6789575219154358,
      "learning_rate": 0.0001500030184123151,
      "loss": 0.2711,
      "step": 12428
    },
    {
      "epoch": 25.00804828973843,
      "grad_norm": 0.6779306530952454,
      "learning_rate": 0.00014999899386256165,
      "loss": 0.2629,
      "step": 12429
    },
    {
      "epoch": 25.01006036217304,
      "grad_norm": 0.7286157011985779,
      "learning_rate": 0.00014999496931280813,
      "loss": 0.2374,
      "step": 12430
    },
    {
      "epoch": 25.012072434607646,
      "grad_norm": 0.7199414968490601,
      "learning_rate": 0.00014999094476305465,
      "loss": 0.2807,
      "step": 12431
    },
    {
      "epoch": 25.014084507042252,
      "grad_norm": 0.7428425550460815,
      "learning_rate": 0.00014998692021330113,
      "loss": 0.2691,
      "step": 12432
    },
    {
      "epoch": 25.016096579476862,
      "grad_norm": 0.7112762331962585,
      "learning_rate": 0.00014998289566354764,
      "loss": 0.2547,
      "step": 12433
    },
    {
      "epoch": 25.01810865191147,
      "grad_norm": 0.6845287084579468,
      "learning_rate": 0.00014997887111379416,
      "loss": 0.2754,
      "step": 12434
    },
    {
      "epoch": 25.020120724346075,
      "grad_norm": 0.6514660120010376,
      "learning_rate": 0.00014997484656404067,
      "loss": 0.2792,
      "step": 12435
    },
    {
      "epoch": 25.022132796780685,
      "grad_norm": 0.6375842094421387,
      "learning_rate": 0.00014997082201428715,
      "loss": 0.2556,
      "step": 12436
    },
    {
      "epoch": 25.02414486921529,
      "grad_norm": 0.6717283129692078,
      "learning_rate": 0.00014996679746453367,
      "loss": 0.2514,
      "step": 12437
    },
    {
      "epoch": 25.026156941649898,
      "grad_norm": 0.6927056908607483,
      "learning_rate": 0.00014996277291478015,
      "loss": 0.2518,
      "step": 12438
    },
    {
      "epoch": 25.028169014084508,
      "grad_norm": 0.7976531386375427,
      "learning_rate": 0.0001499587483650267,
      "loss": 0.2851,
      "step": 12439
    },
    {
      "epoch": 25.030181086519114,
      "grad_norm": 0.7141213417053223,
      "learning_rate": 0.00014995472381527317,
      "loss": 0.2511,
      "step": 12440
    },
    {
      "epoch": 25.03219315895372,
      "grad_norm": 0.7123416662216187,
      "learning_rate": 0.0001499506992655197,
      "loss": 0.2742,
      "step": 12441
    },
    {
      "epoch": 25.03420523138833,
      "grad_norm": 0.732246458530426,
      "learning_rate": 0.00014994667471576617,
      "loss": 0.2696,
      "step": 12442
    },
    {
      "epoch": 25.036217303822937,
      "grad_norm": 0.6928271651268005,
      "learning_rate": 0.00014994265016601268,
      "loss": 0.2813,
      "step": 12443
    },
    {
      "epoch": 25.038229376257544,
      "grad_norm": 0.703521728515625,
      "learning_rate": 0.0001499386256162592,
      "loss": 0.2637,
      "step": 12444
    },
    {
      "epoch": 25.040241448692154,
      "grad_norm": 0.7352011203765869,
      "learning_rate": 0.0001499346010665057,
      "loss": 0.285,
      "step": 12445
    },
    {
      "epoch": 25.04225352112676,
      "grad_norm": 0.7471358180046082,
      "learning_rate": 0.0001499305765167522,
      "loss": 0.2596,
      "step": 12446
    },
    {
      "epoch": 25.044265593561367,
      "grad_norm": 0.7118780612945557,
      "learning_rate": 0.0001499265519669987,
      "loss": 0.2787,
      "step": 12447
    },
    {
      "epoch": 25.046277665995976,
      "grad_norm": 0.7361120581626892,
      "learning_rate": 0.0001499225274172452,
      "loss": 0.2874,
      "step": 12448
    },
    {
      "epoch": 25.048289738430583,
      "grad_norm": 0.7513343691825867,
      "learning_rate": 0.0001499185028674917,
      "loss": 0.2851,
      "step": 12449
    },
    {
      "epoch": 25.050301810865193,
      "grad_norm": 0.704010546207428,
      "learning_rate": 0.00014991447831773822,
      "loss": 0.2621,
      "step": 12450
    },
    {
      "epoch": 25.0523138832998,
      "grad_norm": 0.6807287931442261,
      "learning_rate": 0.00014991045376798473,
      "loss": 0.2605,
      "step": 12451
    },
    {
      "epoch": 25.054325955734406,
      "grad_norm": 0.8408420085906982,
      "learning_rate": 0.0001499064292182312,
      "loss": 0.2732,
      "step": 12452
    },
    {
      "epoch": 25.056338028169016,
      "grad_norm": 0.6671488285064697,
      "learning_rate": 0.00014990240466847773,
      "loss": 0.2529,
      "step": 12453
    },
    {
      "epoch": 25.058350100603622,
      "grad_norm": 0.6915692090988159,
      "learning_rate": 0.00014989838011872424,
      "loss": 0.2686,
      "step": 12454
    },
    {
      "epoch": 25.06036217303823,
      "grad_norm": 0.7559558153152466,
      "learning_rate": 0.00014989435556897072,
      "loss": 0.253,
      "step": 12455
    },
    {
      "epoch": 25.06237424547284,
      "grad_norm": 0.6908515691757202,
      "learning_rate": 0.00014989033101921723,
      "loss": 0.2511,
      "step": 12456
    },
    {
      "epoch": 25.064386317907445,
      "grad_norm": 0.6860114932060242,
      "learning_rate": 0.00014988630646946372,
      "loss": 0.2762,
      "step": 12457
    },
    {
      "epoch": 25.06639839034205,
      "grad_norm": 0.8041237592697144,
      "learning_rate": 0.00014988228191971023,
      "loss": 0.2764,
      "step": 12458
    },
    {
      "epoch": 25.06841046277666,
      "grad_norm": 0.755319356918335,
      "learning_rate": 0.00014987825736995674,
      "loss": 0.2737,
      "step": 12459
    },
    {
      "epoch": 25.070422535211268,
      "grad_norm": 0.7152116298675537,
      "learning_rate": 0.00014987423282020326,
      "loss": 0.2868,
      "step": 12460
    },
    {
      "epoch": 25.072434607645874,
      "grad_norm": 0.7428475618362427,
      "learning_rate": 0.00014987020827044974,
      "loss": 0.2758,
      "step": 12461
    },
    {
      "epoch": 25.074446680080484,
      "grad_norm": 0.7076974511146545,
      "learning_rate": 0.00014986618372069625,
      "loss": 0.2848,
      "step": 12462
    },
    {
      "epoch": 25.07645875251509,
      "grad_norm": 0.6718488335609436,
      "learning_rate": 0.00014986215917094274,
      "loss": 0.2519,
      "step": 12463
    },
    {
      "epoch": 25.078470824949697,
      "grad_norm": 0.6898497343063354,
      "learning_rate": 0.00014985813462118928,
      "loss": 0.2833,
      "step": 12464
    },
    {
      "epoch": 25.080482897384307,
      "grad_norm": 0.738292396068573,
      "learning_rate": 0.00014985411007143576,
      "loss": 0.2559,
      "step": 12465
    },
    {
      "epoch": 25.082494969818914,
      "grad_norm": 0.8191055655479431,
      "learning_rate": 0.00014985008552168228,
      "loss": 0.2984,
      "step": 12466
    },
    {
      "epoch": 25.08450704225352,
      "grad_norm": 0.6685302257537842,
      "learning_rate": 0.00014984606097192876,
      "loss": 0.2619,
      "step": 12467
    },
    {
      "epoch": 25.08651911468813,
      "grad_norm": 0.6998794078826904,
      "learning_rate": 0.00014984203642217527,
      "loss": 0.2756,
      "step": 12468
    },
    {
      "epoch": 25.088531187122737,
      "grad_norm": 0.7405049204826355,
      "learning_rate": 0.00014983801187242179,
      "loss": 0.3053,
      "step": 12469
    },
    {
      "epoch": 25.090543259557343,
      "grad_norm": 0.6982327103614807,
      "learning_rate": 0.0001498339873226683,
      "loss": 0.2673,
      "step": 12470
    },
    {
      "epoch": 25.092555331991953,
      "grad_norm": 0.7327657341957092,
      "learning_rate": 0.00014982996277291478,
      "loss": 0.2933,
      "step": 12471
    },
    {
      "epoch": 25.09456740442656,
      "grad_norm": 0.6889193654060364,
      "learning_rate": 0.0001498259382231613,
      "loss": 0.266,
      "step": 12472
    },
    {
      "epoch": 25.096579476861166,
      "grad_norm": 0.6829889416694641,
      "learning_rate": 0.00014982191367340778,
      "loss": 0.2621,
      "step": 12473
    },
    {
      "epoch": 25.098591549295776,
      "grad_norm": 0.723504364490509,
      "learning_rate": 0.00014981788912365432,
      "loss": 0.2745,
      "step": 12474
    },
    {
      "epoch": 25.100603621730382,
      "grad_norm": 0.7326918244361877,
      "learning_rate": 0.0001498138645739008,
      "loss": 0.2783,
      "step": 12475
    },
    {
      "epoch": 25.10261569416499,
      "grad_norm": 0.7693808674812317,
      "learning_rate": 0.00014980984002414732,
      "loss": 0.2801,
      "step": 12476
    },
    {
      "epoch": 25.1046277665996,
      "grad_norm": 0.6991908550262451,
      "learning_rate": 0.0001498058154743938,
      "loss": 0.2788,
      "step": 12477
    },
    {
      "epoch": 25.106639839034205,
      "grad_norm": 0.7858260869979858,
      "learning_rate": 0.00014980179092464031,
      "loss": 0.3057,
      "step": 12478
    },
    {
      "epoch": 25.10865191146881,
      "grad_norm": 0.7377750873565674,
      "learning_rate": 0.00014979776637488683,
      "loss": 0.2725,
      "step": 12479
    },
    {
      "epoch": 25.11066398390342,
      "grad_norm": 0.6704604029655457,
      "learning_rate": 0.00014979374182513334,
      "loss": 0.2673,
      "step": 12480
    },
    {
      "epoch": 25.112676056338028,
      "grad_norm": 0.7220227122306824,
      "learning_rate": 0.00014978971727537982,
      "loss": 0.2881,
      "step": 12481
    },
    {
      "epoch": 25.114688128772634,
      "grad_norm": 0.7214204668998718,
      "learning_rate": 0.00014978569272562634,
      "loss": 0.2546,
      "step": 12482
    },
    {
      "epoch": 25.116700201207244,
      "grad_norm": 0.7257176041603088,
      "learning_rate": 0.00014978166817587282,
      "loss": 0.2786,
      "step": 12483
    },
    {
      "epoch": 25.11871227364185,
      "grad_norm": 0.7312638759613037,
      "learning_rate": 0.00014977764362611933,
      "loss": 0.2717,
      "step": 12484
    },
    {
      "epoch": 25.120724346076457,
      "grad_norm": 0.6879067420959473,
      "learning_rate": 0.00014977361907636585,
      "loss": 0.2793,
      "step": 12485
    },
    {
      "epoch": 25.122736418511067,
      "grad_norm": 0.7388235330581665,
      "learning_rate": 0.00014976959452661236,
      "loss": 0.2529,
      "step": 12486
    },
    {
      "epoch": 25.124748490945674,
      "grad_norm": 0.7300503849983215,
      "learning_rate": 0.00014976556997685884,
      "loss": 0.2756,
      "step": 12487
    },
    {
      "epoch": 25.12676056338028,
      "grad_norm": 0.7350810170173645,
      "learning_rate": 0.00014976154542710535,
      "loss": 0.2791,
      "step": 12488
    },
    {
      "epoch": 25.12877263581489,
      "grad_norm": 0.7520520687103271,
      "learning_rate": 0.00014975752087735187,
      "loss": 0.2781,
      "step": 12489
    },
    {
      "epoch": 25.130784708249497,
      "grad_norm": 0.7540848851203918,
      "learning_rate": 0.00014975349632759835,
      "loss": 0.2683,
      "step": 12490
    },
    {
      "epoch": 25.132796780684103,
      "grad_norm": 0.7196670174598694,
      "learning_rate": 0.00014974947177784486,
      "loss": 0.2994,
      "step": 12491
    },
    {
      "epoch": 25.134808853118713,
      "grad_norm": 0.6920243501663208,
      "learning_rate": 0.00014974544722809135,
      "loss": 0.2863,
      "step": 12492
    },
    {
      "epoch": 25.13682092555332,
      "grad_norm": 0.6896801590919495,
      "learning_rate": 0.00014974142267833786,
      "loss": 0.2549,
      "step": 12493
    },
    {
      "epoch": 25.138832997987926,
      "grad_norm": 0.6710941195487976,
      "learning_rate": 0.00014973739812858437,
      "loss": 0.2512,
      "step": 12494
    },
    {
      "epoch": 25.140845070422536,
      "grad_norm": 0.7534072995185852,
      "learning_rate": 0.00014973337357883089,
      "loss": 0.2754,
      "step": 12495
    },
    {
      "epoch": 25.142857142857142,
      "grad_norm": 0.7241532206535339,
      "learning_rate": 0.00014972934902907737,
      "loss": 0.2867,
      "step": 12496
    },
    {
      "epoch": 25.14486921529175,
      "grad_norm": 0.6783238649368286,
      "learning_rate": 0.00014972532447932388,
      "loss": 0.266,
      "step": 12497
    },
    {
      "epoch": 25.14688128772636,
      "grad_norm": 0.7142060399055481,
      "learning_rate": 0.00014972129992957037,
      "loss": 0.2864,
      "step": 12498
    },
    {
      "epoch": 25.148893360160965,
      "grad_norm": 0.6789306998252869,
      "learning_rate": 0.00014971727537981688,
      "loss": 0.281,
      "step": 12499
    },
    {
      "epoch": 25.15090543259557,
      "grad_norm": 0.6806023120880127,
      "learning_rate": 0.0001497132508300634,
      "loss": 0.2584,
      "step": 12500
    },
    {
      "epoch": 25.15291750503018,
      "grad_norm": 0.7341625094413757,
      "learning_rate": 0.0001497092262803099,
      "loss": 0.2657,
      "step": 12501
    },
    {
      "epoch": 25.154929577464788,
      "grad_norm": 0.7176445722579956,
      "learning_rate": 0.0001497052017305564,
      "loss": 0.2912,
      "step": 12502
    },
    {
      "epoch": 25.156941649899398,
      "grad_norm": 0.7097330093383789,
      "learning_rate": 0.0001497011771808029,
      "loss": 0.2741,
      "step": 12503
    },
    {
      "epoch": 25.158953722334005,
      "grad_norm": 0.6877988576889038,
      "learning_rate": 0.0001496971526310494,
      "loss": 0.2568,
      "step": 12504
    },
    {
      "epoch": 25.16096579476861,
      "grad_norm": 0.7054125666618347,
      "learning_rate": 0.00014969312808129593,
      "loss": 0.2597,
      "step": 12505
    },
    {
      "epoch": 25.16297786720322,
      "grad_norm": 0.7318902015686035,
      "learning_rate": 0.0001496891035315424,
      "loss": 0.2762,
      "step": 12506
    },
    {
      "epoch": 25.164989939637827,
      "grad_norm": 0.7577462196350098,
      "learning_rate": 0.00014968507898178892,
      "loss": 0.295,
      "step": 12507
    },
    {
      "epoch": 25.167002012072434,
      "grad_norm": 0.700888991355896,
      "learning_rate": 0.0001496810544320354,
      "loss": 0.2403,
      "step": 12508
    },
    {
      "epoch": 25.169014084507044,
      "grad_norm": 0.7328513264656067,
      "learning_rate": 0.00014967702988228192,
      "loss": 0.2993,
      "step": 12509
    },
    {
      "epoch": 25.17102615694165,
      "grad_norm": 0.702289879322052,
      "learning_rate": 0.00014967300533252843,
      "loss": 0.2827,
      "step": 12510
    },
    {
      "epoch": 25.173038229376257,
      "grad_norm": 0.766890823841095,
      "learning_rate": 0.00014966898078277495,
      "loss": 0.2968,
      "step": 12511
    },
    {
      "epoch": 25.175050301810867,
      "grad_norm": 0.7329024076461792,
      "learning_rate": 0.00014966495623302143,
      "loss": 0.2753,
      "step": 12512
    },
    {
      "epoch": 25.177062374245473,
      "grad_norm": 0.7307842373847961,
      "learning_rate": 0.00014966093168326794,
      "loss": 0.2857,
      "step": 12513
    },
    {
      "epoch": 25.17907444668008,
      "grad_norm": 0.6860194802284241,
      "learning_rate": 0.00014965690713351443,
      "loss": 0.2528,
      "step": 12514
    },
    {
      "epoch": 25.18108651911469,
      "grad_norm": 0.704128623008728,
      "learning_rate": 0.00014965288258376097,
      "loss": 0.2593,
      "step": 12515
    },
    {
      "epoch": 25.183098591549296,
      "grad_norm": 0.6883770227432251,
      "learning_rate": 0.00014964885803400745,
      "loss": 0.2602,
      "step": 12516
    },
    {
      "epoch": 25.185110663983902,
      "grad_norm": 0.7146812081336975,
      "learning_rate": 0.00014964483348425396,
      "loss": 0.2594,
      "step": 12517
    },
    {
      "epoch": 25.187122736418512,
      "grad_norm": 0.7593772411346436,
      "learning_rate": 0.00014964080893450045,
      "loss": 0.2527,
      "step": 12518
    },
    {
      "epoch": 25.18913480885312,
      "grad_norm": 0.7646350860595703,
      "learning_rate": 0.00014963678438474696,
      "loss": 0.3083,
      "step": 12519
    },
    {
      "epoch": 25.191146881287725,
      "grad_norm": 0.7440564632415771,
      "learning_rate": 0.00014963275983499347,
      "loss": 0.2757,
      "step": 12520
    },
    {
      "epoch": 25.193158953722335,
      "grad_norm": 0.7827219367027283,
      "learning_rate": 0.00014962873528524,
      "loss": 0.2819,
      "step": 12521
    },
    {
      "epoch": 25.19517102615694,
      "grad_norm": 0.728007972240448,
      "learning_rate": 0.00014962471073548647,
      "loss": 0.289,
      "step": 12522
    },
    {
      "epoch": 25.197183098591548,
      "grad_norm": 0.7515888214111328,
      "learning_rate": 0.00014962068618573298,
      "loss": 0.2822,
      "step": 12523
    },
    {
      "epoch": 25.199195171026158,
      "grad_norm": 0.74831223487854,
      "learning_rate": 0.00014961666163597947,
      "loss": 0.3024,
      "step": 12524
    },
    {
      "epoch": 25.201207243460765,
      "grad_norm": 0.7525311708450317,
      "learning_rate": 0.00014961263708622598,
      "loss": 0.2651,
      "step": 12525
    },
    {
      "epoch": 25.20321931589537,
      "grad_norm": 0.778780996799469,
      "learning_rate": 0.0001496086125364725,
      "loss": 0.2938,
      "step": 12526
    },
    {
      "epoch": 25.20523138832998,
      "grad_norm": 0.7922792434692383,
      "learning_rate": 0.00014960458798671898,
      "loss": 0.2941,
      "step": 12527
    },
    {
      "epoch": 25.207243460764587,
      "grad_norm": 0.7024485468864441,
      "learning_rate": 0.0001496005634369655,
      "loss": 0.2551,
      "step": 12528
    },
    {
      "epoch": 25.209255533199194,
      "grad_norm": 0.7816585302352905,
      "learning_rate": 0.000149596538887212,
      "loss": 0.3012,
      "step": 12529
    },
    {
      "epoch": 25.211267605633804,
      "grad_norm": 0.7251011729240417,
      "learning_rate": 0.00014959251433745852,
      "loss": 0.268,
      "step": 12530
    },
    {
      "epoch": 25.21327967806841,
      "grad_norm": 0.7598355412483215,
      "learning_rate": 0.000149588489787705,
      "loss": 0.2916,
      "step": 12531
    },
    {
      "epoch": 25.215291750503017,
      "grad_norm": 0.7792380452156067,
      "learning_rate": 0.0001495844652379515,
      "loss": 0.2996,
      "step": 12532
    },
    {
      "epoch": 25.217303822937627,
      "grad_norm": 0.6961737871170044,
      "learning_rate": 0.000149580440688198,
      "loss": 0.2554,
      "step": 12533
    },
    {
      "epoch": 25.219315895372233,
      "grad_norm": 0.7435798645019531,
      "learning_rate": 0.0001495764161384445,
      "loss": 0.2603,
      "step": 12534
    },
    {
      "epoch": 25.22132796780684,
      "grad_norm": 0.7545207738876343,
      "learning_rate": 0.00014957239158869102,
      "loss": 0.2829,
      "step": 12535
    },
    {
      "epoch": 25.22334004024145,
      "grad_norm": 0.720274031162262,
      "learning_rate": 0.00014956836703893753,
      "loss": 0.2734,
      "step": 12536
    },
    {
      "epoch": 25.225352112676056,
      "grad_norm": 0.8318952918052673,
      "learning_rate": 0.00014956434248918402,
      "loss": 0.3238,
      "step": 12537
    },
    {
      "epoch": 25.227364185110662,
      "grad_norm": 0.7440587878227234,
      "learning_rate": 0.00014956031793943053,
      "loss": 0.2955,
      "step": 12538
    },
    {
      "epoch": 25.229376257545272,
      "grad_norm": 0.7200428247451782,
      "learning_rate": 0.00014955629338967702,
      "loss": 0.2764,
      "step": 12539
    },
    {
      "epoch": 25.23138832997988,
      "grad_norm": 0.7336071133613586,
      "learning_rate": 0.00014955226883992356,
      "loss": 0.2803,
      "step": 12540
    },
    {
      "epoch": 25.233400402414485,
      "grad_norm": 0.724120020866394,
      "learning_rate": 0.00014954824429017004,
      "loss": 0.2913,
      "step": 12541
    },
    {
      "epoch": 25.235412474849095,
      "grad_norm": 0.7197484970092773,
      "learning_rate": 0.00014954421974041655,
      "loss": 0.2888,
      "step": 12542
    },
    {
      "epoch": 25.2374245472837,
      "grad_norm": 0.7913755774497986,
      "learning_rate": 0.00014954019519066304,
      "loss": 0.2813,
      "step": 12543
    },
    {
      "epoch": 25.239436619718308,
      "grad_norm": 0.7910248041152954,
      "learning_rate": 0.00014953617064090955,
      "loss": 0.2868,
      "step": 12544
    },
    {
      "epoch": 25.241448692152918,
      "grad_norm": 0.7649182081222534,
      "learning_rate": 0.00014953214609115606,
      "loss": 0.2943,
      "step": 12545
    },
    {
      "epoch": 25.243460764587525,
      "grad_norm": 0.7031248807907104,
      "learning_rate": 0.00014952812154140258,
      "loss": 0.2891,
      "step": 12546
    },
    {
      "epoch": 25.24547283702213,
      "grad_norm": 0.720101535320282,
      "learning_rate": 0.00014952409699164906,
      "loss": 0.2737,
      "step": 12547
    },
    {
      "epoch": 25.24748490945674,
      "grad_norm": 0.6926406621932983,
      "learning_rate": 0.00014952007244189557,
      "loss": 0.2937,
      "step": 12548
    },
    {
      "epoch": 25.249496981891348,
      "grad_norm": 0.7923745512962341,
      "learning_rate": 0.00014951604789214206,
      "loss": 0.2738,
      "step": 12549
    },
    {
      "epoch": 25.251509054325957,
      "grad_norm": 0.7487855553627014,
      "learning_rate": 0.0001495120233423886,
      "loss": 0.2806,
      "step": 12550
    },
    {
      "epoch": 25.253521126760564,
      "grad_norm": 0.7176336050033569,
      "learning_rate": 0.00014950799879263508,
      "loss": 0.2862,
      "step": 12551
    },
    {
      "epoch": 25.25553319919517,
      "grad_norm": 0.7407572865486145,
      "learning_rate": 0.0001495039742428816,
      "loss": 0.2838,
      "step": 12552
    },
    {
      "epoch": 25.25754527162978,
      "grad_norm": 0.7713232040405273,
      "learning_rate": 0.00014949994969312808,
      "loss": 0.3257,
      "step": 12553
    },
    {
      "epoch": 25.259557344064387,
      "grad_norm": 0.7985947132110596,
      "learning_rate": 0.0001494959251433746,
      "loss": 0.3181,
      "step": 12554
    },
    {
      "epoch": 25.261569416498993,
      "grad_norm": 0.7114023566246033,
      "learning_rate": 0.0001494919005936211,
      "loss": 0.3109,
      "step": 12555
    },
    {
      "epoch": 25.263581488933603,
      "grad_norm": 0.7781051397323608,
      "learning_rate": 0.00014948787604386762,
      "loss": 0.308,
      "step": 12556
    },
    {
      "epoch": 25.26559356136821,
      "grad_norm": 0.7378551363945007,
      "learning_rate": 0.0001494838514941141,
      "loss": 0.2756,
      "step": 12557
    },
    {
      "epoch": 25.267605633802816,
      "grad_norm": 0.6997926831245422,
      "learning_rate": 0.0001494798269443606,
      "loss": 0.2681,
      "step": 12558
    },
    {
      "epoch": 25.269617706237426,
      "grad_norm": 0.778114378452301,
      "learning_rate": 0.0001494758023946071,
      "loss": 0.2854,
      "step": 12559
    },
    {
      "epoch": 25.271629778672033,
      "grad_norm": 0.7354649305343628,
      "learning_rate": 0.0001494717778448536,
      "loss": 0.2695,
      "step": 12560
    },
    {
      "epoch": 25.27364185110664,
      "grad_norm": 0.792713463306427,
      "learning_rate": 0.00014946775329510012,
      "loss": 0.3092,
      "step": 12561
    },
    {
      "epoch": 25.27565392354125,
      "grad_norm": 0.7184954285621643,
      "learning_rate": 0.0001494637287453466,
      "loss": 0.2775,
      "step": 12562
    },
    {
      "epoch": 25.277665995975855,
      "grad_norm": 0.7491767406463623,
      "learning_rate": 0.00014945970419559312,
      "loss": 0.2869,
      "step": 12563
    },
    {
      "epoch": 25.279678068410462,
      "grad_norm": 0.7243937849998474,
      "learning_rate": 0.00014945567964583963,
      "loss": 0.2811,
      "step": 12564
    },
    {
      "epoch": 25.281690140845072,
      "grad_norm": 0.7543990612030029,
      "learning_rate": 0.00014945165509608614,
      "loss": 0.2935,
      "step": 12565
    },
    {
      "epoch": 25.28370221327968,
      "grad_norm": 0.707844078540802,
      "learning_rate": 0.00014944763054633263,
      "loss": 0.2828,
      "step": 12566
    },
    {
      "epoch": 25.285714285714285,
      "grad_norm": 0.7561047077178955,
      "learning_rate": 0.00014944360599657914,
      "loss": 0.2922,
      "step": 12567
    },
    {
      "epoch": 25.287726358148895,
      "grad_norm": 0.80531907081604,
      "learning_rate": 0.00014943958144682563,
      "loss": 0.2746,
      "step": 12568
    },
    {
      "epoch": 25.2897384305835,
      "grad_norm": 0.7798625230789185,
      "learning_rate": 0.00014943555689707214,
      "loss": 0.3268,
      "step": 12569
    },
    {
      "epoch": 25.291750503018108,
      "grad_norm": 0.7281212210655212,
      "learning_rate": 0.00014943153234731865,
      "loss": 0.2902,
      "step": 12570
    },
    {
      "epoch": 25.293762575452718,
      "grad_norm": 0.717074990272522,
      "learning_rate": 0.00014942750779756516,
      "loss": 0.2758,
      "step": 12571
    },
    {
      "epoch": 25.295774647887324,
      "grad_norm": 0.7769760489463806,
      "learning_rate": 0.00014942348324781165,
      "loss": 0.3002,
      "step": 12572
    },
    {
      "epoch": 25.29778672032193,
      "grad_norm": 0.7412327527999878,
      "learning_rate": 0.00014941945869805816,
      "loss": 0.2821,
      "step": 12573
    },
    {
      "epoch": 25.29979879275654,
      "grad_norm": 0.7004685997962952,
      "learning_rate": 0.00014941543414830465,
      "loss": 0.2808,
      "step": 12574
    },
    {
      "epoch": 25.301810865191147,
      "grad_norm": 0.7302052974700928,
      "learning_rate": 0.00014941140959855119,
      "loss": 0.2895,
      "step": 12575
    },
    {
      "epoch": 25.303822937625753,
      "grad_norm": 0.7352930903434753,
      "learning_rate": 0.00014940738504879767,
      "loss": 0.2777,
      "step": 12576
    },
    {
      "epoch": 25.305835010060363,
      "grad_norm": 0.7227272987365723,
      "learning_rate": 0.00014940336049904418,
      "loss": 0.2928,
      "step": 12577
    },
    {
      "epoch": 25.30784708249497,
      "grad_norm": 0.7483616471290588,
      "learning_rate": 0.00014939933594929067,
      "loss": 0.2844,
      "step": 12578
    },
    {
      "epoch": 25.309859154929576,
      "grad_norm": 0.7160911560058594,
      "learning_rate": 0.00014939531139953718,
      "loss": 0.2855,
      "step": 12579
    },
    {
      "epoch": 25.311871227364186,
      "grad_norm": 0.7357147932052612,
      "learning_rate": 0.0001493912868497837,
      "loss": 0.2967,
      "step": 12580
    },
    {
      "epoch": 25.313883299798793,
      "grad_norm": 0.7323766946792603,
      "learning_rate": 0.0001493872623000302,
      "loss": 0.3015,
      "step": 12581
    },
    {
      "epoch": 25.3158953722334,
      "grad_norm": 0.7382836937904358,
      "learning_rate": 0.0001493832377502767,
      "loss": 0.2775,
      "step": 12582
    },
    {
      "epoch": 25.31790744466801,
      "grad_norm": 0.736565351486206,
      "learning_rate": 0.0001493792132005232,
      "loss": 0.2713,
      "step": 12583
    },
    {
      "epoch": 25.319919517102615,
      "grad_norm": 0.7519351840019226,
      "learning_rate": 0.0001493751886507697,
      "loss": 0.3007,
      "step": 12584
    },
    {
      "epoch": 25.321931589537222,
      "grad_norm": 0.7727394700050354,
      "learning_rate": 0.00014937116410101623,
      "loss": 0.3239,
      "step": 12585
    },
    {
      "epoch": 25.323943661971832,
      "grad_norm": 0.790638267993927,
      "learning_rate": 0.0001493671395512627,
      "loss": 0.2987,
      "step": 12586
    },
    {
      "epoch": 25.32595573440644,
      "grad_norm": 0.7438164949417114,
      "learning_rate": 0.00014936311500150922,
      "loss": 0.2772,
      "step": 12587
    },
    {
      "epoch": 25.327967806841045,
      "grad_norm": 0.7738990783691406,
      "learning_rate": 0.0001493590904517557,
      "loss": 0.3128,
      "step": 12588
    },
    {
      "epoch": 25.329979879275655,
      "grad_norm": 0.7709720730781555,
      "learning_rate": 0.00014935506590200222,
      "loss": 0.3269,
      "step": 12589
    },
    {
      "epoch": 25.33199195171026,
      "grad_norm": 0.7310389876365662,
      "learning_rate": 0.00014935104135224873,
      "loss": 0.2918,
      "step": 12590
    },
    {
      "epoch": 25.334004024144868,
      "grad_norm": 0.7048205137252808,
      "learning_rate": 0.00014934701680249525,
      "loss": 0.2852,
      "step": 12591
    },
    {
      "epoch": 25.336016096579478,
      "grad_norm": 0.7016087174415588,
      "learning_rate": 0.00014934299225274173,
      "loss": 0.2866,
      "step": 12592
    },
    {
      "epoch": 25.338028169014084,
      "grad_norm": 0.7392110228538513,
      "learning_rate": 0.00014933896770298824,
      "loss": 0.2809,
      "step": 12593
    },
    {
      "epoch": 25.34004024144869,
      "grad_norm": 0.7465314865112305,
      "learning_rate": 0.00014933494315323473,
      "loss": 0.2977,
      "step": 12594
    },
    {
      "epoch": 25.3420523138833,
      "grad_norm": 0.7534603476524353,
      "learning_rate": 0.00014933091860348124,
      "loss": 0.3064,
      "step": 12595
    },
    {
      "epoch": 25.344064386317907,
      "grad_norm": 0.7110104560852051,
      "learning_rate": 0.00014932689405372775,
      "loss": 0.2812,
      "step": 12596
    },
    {
      "epoch": 25.346076458752513,
      "grad_norm": 0.7600091099739075,
      "learning_rate": 0.00014932286950397424,
      "loss": 0.2814,
      "step": 12597
    },
    {
      "epoch": 25.348088531187123,
      "grad_norm": 0.7343291640281677,
      "learning_rate": 0.00014931884495422075,
      "loss": 0.3097,
      "step": 12598
    },
    {
      "epoch": 25.35010060362173,
      "grad_norm": 0.7425734996795654,
      "learning_rate": 0.00014931482040446723,
      "loss": 0.2956,
      "step": 12599
    },
    {
      "epoch": 25.352112676056336,
      "grad_norm": 0.7583165764808655,
      "learning_rate": 0.00014931079585471377,
      "loss": 0.2951,
      "step": 12600
    },
    {
      "epoch": 25.354124748490946,
      "grad_norm": 0.7209164500236511,
      "learning_rate": 0.00014930677130496026,
      "loss": 0.2885,
      "step": 12601
    },
    {
      "epoch": 25.356136820925553,
      "grad_norm": 0.7976799607276917,
      "learning_rate": 0.00014930274675520677,
      "loss": 0.3127,
      "step": 12602
    },
    {
      "epoch": 25.358148893360163,
      "grad_norm": 0.7231213450431824,
      "learning_rate": 0.00014929872220545326,
      "loss": 0.2797,
      "step": 12603
    },
    {
      "epoch": 25.36016096579477,
      "grad_norm": 0.7271547913551331,
      "learning_rate": 0.00014929469765569977,
      "loss": 0.275,
      "step": 12604
    },
    {
      "epoch": 25.362173038229376,
      "grad_norm": 0.7764450311660767,
      "learning_rate": 0.00014929067310594628,
      "loss": 0.3048,
      "step": 12605
    },
    {
      "epoch": 25.364185110663986,
      "grad_norm": 0.7351914644241333,
      "learning_rate": 0.0001492866485561928,
      "loss": 0.3017,
      "step": 12606
    },
    {
      "epoch": 25.366197183098592,
      "grad_norm": 0.7426241040229797,
      "learning_rate": 0.00014928262400643928,
      "loss": 0.2872,
      "step": 12607
    },
    {
      "epoch": 25.3682092555332,
      "grad_norm": 0.7479576468467712,
      "learning_rate": 0.0001492785994566858,
      "loss": 0.2837,
      "step": 12608
    },
    {
      "epoch": 25.37022132796781,
      "grad_norm": 0.7922371029853821,
      "learning_rate": 0.00014927457490693228,
      "loss": 0.2991,
      "step": 12609
    },
    {
      "epoch": 25.372233400402415,
      "grad_norm": 0.7628862261772156,
      "learning_rate": 0.00014927055035717882,
      "loss": 0.3117,
      "step": 12610
    },
    {
      "epoch": 25.37424547283702,
      "grad_norm": 0.7084946632385254,
      "learning_rate": 0.0001492665258074253,
      "loss": 0.2836,
      "step": 12611
    },
    {
      "epoch": 25.37625754527163,
      "grad_norm": 0.7591384053230286,
      "learning_rate": 0.0001492625012576718,
      "loss": 0.3031,
      "step": 12612
    },
    {
      "epoch": 25.378269617706238,
      "grad_norm": 0.7513222694396973,
      "learning_rate": 0.0001492584767079183,
      "loss": 0.3067,
      "step": 12613
    },
    {
      "epoch": 25.380281690140844,
      "grad_norm": 0.750910758972168,
      "learning_rate": 0.0001492544521581648,
      "loss": 0.2934,
      "step": 12614
    },
    {
      "epoch": 25.382293762575454,
      "grad_norm": 0.7482917308807373,
      "learning_rate": 0.00014925042760841132,
      "loss": 0.2864,
      "step": 12615
    },
    {
      "epoch": 25.38430583501006,
      "grad_norm": 0.7749627828598022,
      "learning_rate": 0.00014924640305865783,
      "loss": 0.3243,
      "step": 12616
    },
    {
      "epoch": 25.386317907444667,
      "grad_norm": 0.7268716096878052,
      "learning_rate": 0.00014924237850890432,
      "loss": 0.2806,
      "step": 12617
    },
    {
      "epoch": 25.388329979879277,
      "grad_norm": 0.7611731290817261,
      "learning_rate": 0.00014923835395915083,
      "loss": 0.2871,
      "step": 12618
    },
    {
      "epoch": 25.390342052313883,
      "grad_norm": 0.817055344581604,
      "learning_rate": 0.00014923432940939732,
      "loss": 0.3202,
      "step": 12619
    },
    {
      "epoch": 25.39235412474849,
      "grad_norm": 0.7565743327140808,
      "learning_rate": 0.00014923030485964386,
      "loss": 0.3102,
      "step": 12620
    },
    {
      "epoch": 25.3943661971831,
      "grad_norm": 0.7481357455253601,
      "learning_rate": 0.00014922628030989034,
      "loss": 0.2708,
      "step": 12621
    },
    {
      "epoch": 25.396378269617706,
      "grad_norm": 0.7521777153015137,
      "learning_rate": 0.00014922225576013685,
      "loss": 0.296,
      "step": 12622
    },
    {
      "epoch": 25.398390342052313,
      "grad_norm": 0.7322132587432861,
      "learning_rate": 0.00014921823121038334,
      "loss": 0.2941,
      "step": 12623
    },
    {
      "epoch": 25.400402414486923,
      "grad_norm": 0.7607394456863403,
      "learning_rate": 0.00014921420666062985,
      "loss": 0.2894,
      "step": 12624
    },
    {
      "epoch": 25.40241448692153,
      "grad_norm": 0.7760601043701172,
      "learning_rate": 0.00014921018211087636,
      "loss": 0.2809,
      "step": 12625
    },
    {
      "epoch": 25.404426559356136,
      "grad_norm": 0.8103588819503784,
      "learning_rate": 0.00014920615756112285,
      "loss": 0.2898,
      "step": 12626
    },
    {
      "epoch": 25.406438631790746,
      "grad_norm": 0.7527974843978882,
      "learning_rate": 0.00014920213301136936,
      "loss": 0.2973,
      "step": 12627
    },
    {
      "epoch": 25.408450704225352,
      "grad_norm": 0.7644853591918945,
      "learning_rate": 0.00014919810846161587,
      "loss": 0.3218,
      "step": 12628
    },
    {
      "epoch": 25.41046277665996,
      "grad_norm": 0.7820338606834412,
      "learning_rate": 0.00014919408391186236,
      "loss": 0.2852,
      "step": 12629
    },
    {
      "epoch": 25.41247484909457,
      "grad_norm": 0.7878532409667969,
      "learning_rate": 0.00014919005936210887,
      "loss": 0.3002,
      "step": 12630
    },
    {
      "epoch": 25.414486921529175,
      "grad_norm": 0.7726133465766907,
      "learning_rate": 0.00014918603481235538,
      "loss": 0.3065,
      "step": 12631
    },
    {
      "epoch": 25.41649899396378,
      "grad_norm": 0.8179880976676941,
      "learning_rate": 0.00014918201026260187,
      "loss": 0.3113,
      "step": 12632
    },
    {
      "epoch": 25.41851106639839,
      "grad_norm": 0.7548496723175049,
      "learning_rate": 0.00014917798571284838,
      "loss": 0.2954,
      "step": 12633
    },
    {
      "epoch": 25.420523138832998,
      "grad_norm": 0.7698736786842346,
      "learning_rate": 0.00014917396116309486,
      "loss": 0.3095,
      "step": 12634
    },
    {
      "epoch": 25.422535211267604,
      "grad_norm": 0.8025214672088623,
      "learning_rate": 0.0001491699366133414,
      "loss": 0.2886,
      "step": 12635
    },
    {
      "epoch": 25.424547283702214,
      "grad_norm": 0.7098907232284546,
      "learning_rate": 0.0001491659120635879,
      "loss": 0.2976,
      "step": 12636
    },
    {
      "epoch": 25.42655935613682,
      "grad_norm": 0.7154008746147156,
      "learning_rate": 0.0001491618875138344,
      "loss": 0.2851,
      "step": 12637
    },
    {
      "epoch": 25.428571428571427,
      "grad_norm": 0.7904534339904785,
      "learning_rate": 0.00014915786296408089,
      "loss": 0.3117,
      "step": 12638
    },
    {
      "epoch": 25.430583501006037,
      "grad_norm": 0.7576499581336975,
      "learning_rate": 0.0001491538384143274,
      "loss": 0.3078,
      "step": 12639
    },
    {
      "epoch": 25.432595573440643,
      "grad_norm": 0.7854071855545044,
      "learning_rate": 0.0001491498138645739,
      "loss": 0.3173,
      "step": 12640
    },
    {
      "epoch": 25.43460764587525,
      "grad_norm": 0.7512726187705994,
      "learning_rate": 0.00014914578931482042,
      "loss": 0.3196,
      "step": 12641
    },
    {
      "epoch": 25.43661971830986,
      "grad_norm": 0.7707062363624573,
      "learning_rate": 0.0001491417647650669,
      "loss": 0.2767,
      "step": 12642
    },
    {
      "epoch": 25.438631790744466,
      "grad_norm": 0.7223821878433228,
      "learning_rate": 0.00014913774021531342,
      "loss": 0.2758,
      "step": 12643
    },
    {
      "epoch": 25.440643863179073,
      "grad_norm": 0.7739947438240051,
      "learning_rate": 0.0001491337156655599,
      "loss": 0.3078,
      "step": 12644
    },
    {
      "epoch": 25.442655935613683,
      "grad_norm": 0.764611542224884,
      "learning_rate": 0.00014912969111580644,
      "loss": 0.269,
      "step": 12645
    },
    {
      "epoch": 25.44466800804829,
      "grad_norm": 0.7179149985313416,
      "learning_rate": 0.00014912566656605293,
      "loss": 0.2914,
      "step": 12646
    },
    {
      "epoch": 25.446680080482896,
      "grad_norm": 0.7823951840400696,
      "learning_rate": 0.00014912164201629944,
      "loss": 0.3084,
      "step": 12647
    },
    {
      "epoch": 25.448692152917506,
      "grad_norm": 0.7696221470832825,
      "learning_rate": 0.00014911761746654593,
      "loss": 0.3057,
      "step": 12648
    },
    {
      "epoch": 25.450704225352112,
      "grad_norm": 0.7386270761489868,
      "learning_rate": 0.00014911359291679244,
      "loss": 0.2841,
      "step": 12649
    },
    {
      "epoch": 25.452716297786722,
      "grad_norm": 0.7653788924217224,
      "learning_rate": 0.00014910956836703895,
      "loss": 0.3133,
      "step": 12650
    },
    {
      "epoch": 25.45472837022133,
      "grad_norm": 0.7506179213523865,
      "learning_rate": 0.00014910554381728546,
      "loss": 0.2674,
      "step": 12651
    },
    {
      "epoch": 25.456740442655935,
      "grad_norm": 0.7864285707473755,
      "learning_rate": 0.00014910151926753195,
      "loss": 0.3138,
      "step": 12652
    },
    {
      "epoch": 25.458752515090545,
      "grad_norm": 0.8034141659736633,
      "learning_rate": 0.00014909749471777846,
      "loss": 0.2927,
      "step": 12653
    },
    {
      "epoch": 25.46076458752515,
      "grad_norm": 0.7935285568237305,
      "learning_rate": 0.00014909347016802495,
      "loss": 0.2702,
      "step": 12654
    },
    {
      "epoch": 25.462776659959758,
      "grad_norm": 0.8166313171386719,
      "learning_rate": 0.00014908944561827149,
      "loss": 0.3253,
      "step": 12655
    },
    {
      "epoch": 25.464788732394368,
      "grad_norm": 0.7424972057342529,
      "learning_rate": 0.00014908542106851797,
      "loss": 0.2996,
      "step": 12656
    },
    {
      "epoch": 25.466800804828974,
      "grad_norm": 0.7485877871513367,
      "learning_rate": 0.00014908139651876448,
      "loss": 0.2955,
      "step": 12657
    },
    {
      "epoch": 25.46881287726358,
      "grad_norm": 0.8138995170593262,
      "learning_rate": 0.00014907737196901097,
      "loss": 0.293,
      "step": 12658
    },
    {
      "epoch": 25.47082494969819,
      "grad_norm": 0.7657267451286316,
      "learning_rate": 0.00014907334741925748,
      "loss": 0.3295,
      "step": 12659
    },
    {
      "epoch": 25.472837022132797,
      "grad_norm": 0.7402837872505188,
      "learning_rate": 0.000149069322869504,
      "loss": 0.2667,
      "step": 12660
    },
    {
      "epoch": 25.474849094567404,
      "grad_norm": 0.8163408041000366,
      "learning_rate": 0.00014906529831975048,
      "loss": 0.3167,
      "step": 12661
    },
    {
      "epoch": 25.476861167002014,
      "grad_norm": 0.8199633955955505,
      "learning_rate": 0.000149061273769997,
      "loss": 0.2864,
      "step": 12662
    },
    {
      "epoch": 25.47887323943662,
      "grad_norm": 0.797267496585846,
      "learning_rate": 0.0001490572492202435,
      "loss": 0.3155,
      "step": 12663
    },
    {
      "epoch": 25.480885311871226,
      "grad_norm": 0.78936767578125,
      "learning_rate": 0.00014905322467049,
      "loss": 0.287,
      "step": 12664
    },
    {
      "epoch": 25.482897384305836,
      "grad_norm": 0.7703359723091125,
      "learning_rate": 0.0001490492001207365,
      "loss": 0.3078,
      "step": 12665
    },
    {
      "epoch": 25.484909456740443,
      "grad_norm": 0.8103490471839905,
      "learning_rate": 0.000149045175570983,
      "loss": 0.3006,
      "step": 12666
    },
    {
      "epoch": 25.48692152917505,
      "grad_norm": 0.7628520131111145,
      "learning_rate": 0.0001490411510212295,
      "loss": 0.2746,
      "step": 12667
    },
    {
      "epoch": 25.48893360160966,
      "grad_norm": 0.7485412359237671,
      "learning_rate": 0.000149037126471476,
      "loss": 0.2912,
      "step": 12668
    },
    {
      "epoch": 25.490945674044266,
      "grad_norm": 0.7368710041046143,
      "learning_rate": 0.0001490331019217225,
      "loss": 0.2846,
      "step": 12669
    },
    {
      "epoch": 25.492957746478872,
      "grad_norm": 0.8103664517402649,
      "learning_rate": 0.00014902907737196903,
      "loss": 0.3099,
      "step": 12670
    },
    {
      "epoch": 25.494969818913482,
      "grad_norm": 0.7547658681869507,
      "learning_rate": 0.00014902505282221552,
      "loss": 0.3104,
      "step": 12671
    },
    {
      "epoch": 25.49698189134809,
      "grad_norm": 0.7239664196968079,
      "learning_rate": 0.00014902102827246203,
      "loss": 0.3052,
      "step": 12672
    },
    {
      "epoch": 25.498993963782695,
      "grad_norm": 0.7840759754180908,
      "learning_rate": 0.00014901700372270852,
      "loss": 0.3223,
      "step": 12673
    },
    {
      "epoch": 25.501006036217305,
      "grad_norm": 0.763150155544281,
      "learning_rate": 0.00014901297917295503,
      "loss": 0.2959,
      "step": 12674
    },
    {
      "epoch": 25.50301810865191,
      "grad_norm": 0.7410596013069153,
      "learning_rate": 0.00014900895462320154,
      "loss": 0.3086,
      "step": 12675
    },
    {
      "epoch": 25.505030181086518,
      "grad_norm": 0.7485603094100952,
      "learning_rate": 0.00014900493007344805,
      "loss": 0.3001,
      "step": 12676
    },
    {
      "epoch": 25.507042253521128,
      "grad_norm": 0.7434661388397217,
      "learning_rate": 0.00014900090552369454,
      "loss": 0.3178,
      "step": 12677
    },
    {
      "epoch": 25.509054325955734,
      "grad_norm": 0.7279055714607239,
      "learning_rate": 0.00014899688097394105,
      "loss": 0.2742,
      "step": 12678
    },
    {
      "epoch": 25.51106639839034,
      "grad_norm": 0.7370700240135193,
      "learning_rate": 0.00014899285642418753,
      "loss": 0.3022,
      "step": 12679
    },
    {
      "epoch": 25.51307847082495,
      "grad_norm": 0.7391393780708313,
      "learning_rate": 0.00014898883187443407,
      "loss": 0.3075,
      "step": 12680
    },
    {
      "epoch": 25.515090543259557,
      "grad_norm": 0.7431974411010742,
      "learning_rate": 0.00014898480732468056,
      "loss": 0.283,
      "step": 12681
    },
    {
      "epoch": 25.517102615694164,
      "grad_norm": 0.7764418721199036,
      "learning_rate": 0.00014898078277492707,
      "loss": 0.3067,
      "step": 12682
    },
    {
      "epoch": 25.519114688128774,
      "grad_norm": 0.767610490322113,
      "learning_rate": 0.00014897675822517356,
      "loss": 0.304,
      "step": 12683
    },
    {
      "epoch": 25.52112676056338,
      "grad_norm": 0.7832509875297546,
      "learning_rate": 0.00014897273367542007,
      "loss": 0.299,
      "step": 12684
    },
    {
      "epoch": 25.523138832997986,
      "grad_norm": 0.732815146446228,
      "learning_rate": 0.00014896870912566658,
      "loss": 0.2843,
      "step": 12685
    },
    {
      "epoch": 25.525150905432596,
      "grad_norm": 0.7417379021644592,
      "learning_rate": 0.0001489646845759131,
      "loss": 0.3092,
      "step": 12686
    },
    {
      "epoch": 25.527162977867203,
      "grad_norm": 0.7563036680221558,
      "learning_rate": 0.00014896066002615958,
      "loss": 0.3091,
      "step": 12687
    },
    {
      "epoch": 25.52917505030181,
      "grad_norm": 0.7342371940612793,
      "learning_rate": 0.0001489566354764061,
      "loss": 0.2853,
      "step": 12688
    },
    {
      "epoch": 25.53118712273642,
      "grad_norm": 0.7892100214958191,
      "learning_rate": 0.00014895261092665258,
      "loss": 0.3099,
      "step": 12689
    },
    {
      "epoch": 25.533199195171026,
      "grad_norm": 0.7464552521705627,
      "learning_rate": 0.00014894858637689911,
      "loss": 0.3031,
      "step": 12690
    },
    {
      "epoch": 25.535211267605632,
      "grad_norm": 0.7218373417854309,
      "learning_rate": 0.0001489445618271456,
      "loss": 0.2998,
      "step": 12691
    },
    {
      "epoch": 25.537223340040242,
      "grad_norm": 0.7679755091667175,
      "learning_rate": 0.0001489405372773921,
      "loss": 0.2987,
      "step": 12692
    },
    {
      "epoch": 25.53923541247485,
      "grad_norm": 0.7934405207633972,
      "learning_rate": 0.0001489365127276386,
      "loss": 0.3388,
      "step": 12693
    },
    {
      "epoch": 25.541247484909455,
      "grad_norm": 0.7312623262405396,
      "learning_rate": 0.0001489324881778851,
      "loss": 0.302,
      "step": 12694
    },
    {
      "epoch": 25.543259557344065,
      "grad_norm": 0.7371435761451721,
      "learning_rate": 0.00014892846362813162,
      "loss": 0.3173,
      "step": 12695
    },
    {
      "epoch": 25.54527162977867,
      "grad_norm": 0.7535746097564697,
      "learning_rate": 0.0001489244390783781,
      "loss": 0.2945,
      "step": 12696
    },
    {
      "epoch": 25.547283702213278,
      "grad_norm": 0.77696293592453,
      "learning_rate": 0.00014892041452862462,
      "loss": 0.3014,
      "step": 12697
    },
    {
      "epoch": 25.549295774647888,
      "grad_norm": 0.8023607134819031,
      "learning_rate": 0.00014891638997887113,
      "loss": 0.3334,
      "step": 12698
    },
    {
      "epoch": 25.551307847082494,
      "grad_norm": 0.7578599452972412,
      "learning_rate": 0.00014891236542911762,
      "loss": 0.3027,
      "step": 12699
    },
    {
      "epoch": 25.5533199195171,
      "grad_norm": 0.7520911693572998,
      "learning_rate": 0.00014890834087936413,
      "loss": 0.3053,
      "step": 12700
    },
    {
      "epoch": 25.55533199195171,
      "grad_norm": 0.7311790585517883,
      "learning_rate": 0.00014890431632961064,
      "loss": 0.2829,
      "step": 12701
    },
    {
      "epoch": 25.557344064386317,
      "grad_norm": 0.797208309173584,
      "learning_rate": 0.00014890029177985713,
      "loss": 0.3433,
      "step": 12702
    },
    {
      "epoch": 25.559356136820927,
      "grad_norm": 0.7584307193756104,
      "learning_rate": 0.00014889626723010364,
      "loss": 0.2959,
      "step": 12703
    },
    {
      "epoch": 25.561368209255534,
      "grad_norm": 0.7528484463691711,
      "learning_rate": 0.00014889224268035012,
      "loss": 0.3037,
      "step": 12704
    },
    {
      "epoch": 25.56338028169014,
      "grad_norm": 0.7604131102561951,
      "learning_rate": 0.00014888821813059666,
      "loss": 0.3076,
      "step": 12705
    },
    {
      "epoch": 25.56539235412475,
      "grad_norm": 0.7659698724746704,
      "learning_rate": 0.00014888419358084315,
      "loss": 0.3348,
      "step": 12706
    },
    {
      "epoch": 25.567404426559357,
      "grad_norm": 0.794037401676178,
      "learning_rate": 0.00014888016903108966,
      "loss": 0.3172,
      "step": 12707
    },
    {
      "epoch": 25.569416498993963,
      "grad_norm": 0.7730584144592285,
      "learning_rate": 0.00014887614448133614,
      "loss": 0.3112,
      "step": 12708
    },
    {
      "epoch": 25.571428571428573,
      "grad_norm": 0.7530660629272461,
      "learning_rate": 0.00014887211993158266,
      "loss": 0.3254,
      "step": 12709
    },
    {
      "epoch": 25.57344064386318,
      "grad_norm": 0.7498179078102112,
      "learning_rate": 0.00014886809538182917,
      "loss": 0.3135,
      "step": 12710
    },
    {
      "epoch": 25.575452716297786,
      "grad_norm": 0.7776290774345398,
      "learning_rate": 0.00014886407083207568,
      "loss": 0.311,
      "step": 12711
    },
    {
      "epoch": 25.577464788732396,
      "grad_norm": 0.7381739616394043,
      "learning_rate": 0.00014886004628232217,
      "loss": 0.3095,
      "step": 12712
    },
    {
      "epoch": 25.579476861167002,
      "grad_norm": 0.7652016878128052,
      "learning_rate": 0.00014885602173256868,
      "loss": 0.3049,
      "step": 12713
    },
    {
      "epoch": 25.58148893360161,
      "grad_norm": 0.731243908405304,
      "learning_rate": 0.00014885199718281516,
      "loss": 0.3043,
      "step": 12714
    },
    {
      "epoch": 25.58350100603622,
      "grad_norm": 0.7686164379119873,
      "learning_rate": 0.0001488479726330617,
      "loss": 0.311,
      "step": 12715
    },
    {
      "epoch": 25.585513078470825,
      "grad_norm": 0.7446792125701904,
      "learning_rate": 0.0001488439480833082,
      "loss": 0.2992,
      "step": 12716
    },
    {
      "epoch": 25.58752515090543,
      "grad_norm": 0.7459002137184143,
      "learning_rate": 0.0001488399235335547,
      "loss": 0.302,
      "step": 12717
    },
    {
      "epoch": 25.58953722334004,
      "grad_norm": 0.7792922258377075,
      "learning_rate": 0.00014883589898380119,
      "loss": 0.3089,
      "step": 12718
    },
    {
      "epoch": 25.591549295774648,
      "grad_norm": 0.7614293098449707,
      "learning_rate": 0.0001488318744340477,
      "loss": 0.2906,
      "step": 12719
    },
    {
      "epoch": 25.593561368209254,
      "grad_norm": 0.7838300466537476,
      "learning_rate": 0.0001488278498842942,
      "loss": 0.3068,
      "step": 12720
    },
    {
      "epoch": 25.595573440643864,
      "grad_norm": 0.7970613241195679,
      "learning_rate": 0.00014882382533454072,
      "loss": 0.3239,
      "step": 12721
    },
    {
      "epoch": 25.59758551307847,
      "grad_norm": 0.7457935214042664,
      "learning_rate": 0.0001488198007847872,
      "loss": 0.3095,
      "step": 12722
    },
    {
      "epoch": 25.599597585513077,
      "grad_norm": 0.8125738501548767,
      "learning_rate": 0.00014881577623503372,
      "loss": 0.3253,
      "step": 12723
    },
    {
      "epoch": 25.601609657947687,
      "grad_norm": 0.8262999653816223,
      "learning_rate": 0.0001488117516852802,
      "loss": 0.3004,
      "step": 12724
    },
    {
      "epoch": 25.603621730382294,
      "grad_norm": 0.756684422492981,
      "learning_rate": 0.00014880772713552674,
      "loss": 0.3,
      "step": 12725
    },
    {
      "epoch": 25.6056338028169,
      "grad_norm": 0.7698533535003662,
      "learning_rate": 0.00014880370258577323,
      "loss": 0.2979,
      "step": 12726
    },
    {
      "epoch": 25.60764587525151,
      "grad_norm": 0.7532048225402832,
      "learning_rate": 0.00014879967803601974,
      "loss": 0.3026,
      "step": 12727
    },
    {
      "epoch": 25.609657947686117,
      "grad_norm": 0.8146986961364746,
      "learning_rate": 0.00014879565348626623,
      "loss": 0.2982,
      "step": 12728
    },
    {
      "epoch": 25.611670020120723,
      "grad_norm": 0.7987308502197266,
      "learning_rate": 0.00014879162893651274,
      "loss": 0.3101,
      "step": 12729
    },
    {
      "epoch": 25.613682092555333,
      "grad_norm": 0.7894943356513977,
      "learning_rate": 0.00014878760438675925,
      "loss": 0.3207,
      "step": 12730
    },
    {
      "epoch": 25.61569416498994,
      "grad_norm": 0.7736330628395081,
      "learning_rate": 0.00014878357983700574,
      "loss": 0.3031,
      "step": 12731
    },
    {
      "epoch": 25.617706237424546,
      "grad_norm": 0.795005202293396,
      "learning_rate": 0.00014877955528725225,
      "loss": 0.3183,
      "step": 12732
    },
    {
      "epoch": 25.619718309859156,
      "grad_norm": 0.7567329406738281,
      "learning_rate": 0.00014877553073749876,
      "loss": 0.3138,
      "step": 12733
    },
    {
      "epoch": 25.621730382293762,
      "grad_norm": 0.7556832432746887,
      "learning_rate": 0.00014877150618774525,
      "loss": 0.3009,
      "step": 12734
    },
    {
      "epoch": 25.62374245472837,
      "grad_norm": 0.7593536376953125,
      "learning_rate": 0.00014876748163799176,
      "loss": 0.3231,
      "step": 12735
    },
    {
      "epoch": 25.62575452716298,
      "grad_norm": 0.7642289996147156,
      "learning_rate": 0.00014876345708823827,
      "loss": 0.3429,
      "step": 12736
    },
    {
      "epoch": 25.627766599597585,
      "grad_norm": 0.7396149635314941,
      "learning_rate": 0.00014875943253848476,
      "loss": 0.3154,
      "step": 12737
    },
    {
      "epoch": 25.62977867203219,
      "grad_norm": 0.7383669018745422,
      "learning_rate": 0.00014875540798873127,
      "loss": 0.3242,
      "step": 12738
    },
    {
      "epoch": 25.6317907444668,
      "grad_norm": 0.8117443323135376,
      "learning_rate": 0.00014875138343897775,
      "loss": 0.314,
      "step": 12739
    },
    {
      "epoch": 25.633802816901408,
      "grad_norm": 0.8173210620880127,
      "learning_rate": 0.00014874735888922426,
      "loss": 0.3143,
      "step": 12740
    },
    {
      "epoch": 25.635814889336014,
      "grad_norm": 0.7293760180473328,
      "learning_rate": 0.00014874333433947078,
      "loss": 0.2955,
      "step": 12741
    },
    {
      "epoch": 25.637826961770624,
      "grad_norm": 0.8049161434173584,
      "learning_rate": 0.0001487393097897173,
      "loss": 0.3177,
      "step": 12742
    },
    {
      "epoch": 25.63983903420523,
      "grad_norm": 0.7751445174217224,
      "learning_rate": 0.00014873528523996377,
      "loss": 0.3229,
      "step": 12743
    },
    {
      "epoch": 25.641851106639837,
      "grad_norm": 0.7405526638031006,
      "learning_rate": 0.0001487312606902103,
      "loss": 0.2907,
      "step": 12744
    },
    {
      "epoch": 25.643863179074447,
      "grad_norm": 0.876790463924408,
      "learning_rate": 0.00014872723614045677,
      "loss": 0.3323,
      "step": 12745
    },
    {
      "epoch": 25.645875251509054,
      "grad_norm": 0.7589742541313171,
      "learning_rate": 0.0001487232115907033,
      "loss": 0.2984,
      "step": 12746
    },
    {
      "epoch": 25.647887323943664,
      "grad_norm": 0.7219444513320923,
      "learning_rate": 0.0001487191870409498,
      "loss": 0.3108,
      "step": 12747
    },
    {
      "epoch": 25.64989939637827,
      "grad_norm": 0.7786858081817627,
      "learning_rate": 0.0001487151624911963,
      "loss": 0.3419,
      "step": 12748
    },
    {
      "epoch": 25.651911468812877,
      "grad_norm": 0.7655324935913086,
      "learning_rate": 0.0001487111379414428,
      "loss": 0.3079,
      "step": 12749
    },
    {
      "epoch": 25.653923541247487,
      "grad_norm": 0.7678663730621338,
      "learning_rate": 0.0001487071133916893,
      "loss": 0.3245,
      "step": 12750
    },
    {
      "epoch": 25.655935613682093,
      "grad_norm": 0.7715423107147217,
      "learning_rate": 0.00014870308884193582,
      "loss": 0.3309,
      "step": 12751
    },
    {
      "epoch": 25.6579476861167,
      "grad_norm": 0.7717233300209045,
      "learning_rate": 0.00014869906429218233,
      "loss": 0.3258,
      "step": 12752
    },
    {
      "epoch": 25.65995975855131,
      "grad_norm": 0.8304096460342407,
      "learning_rate": 0.00014869503974242882,
      "loss": 0.3131,
      "step": 12753
    },
    {
      "epoch": 25.661971830985916,
      "grad_norm": 0.8015127182006836,
      "learning_rate": 0.00014869101519267533,
      "loss": 0.3047,
      "step": 12754
    },
    {
      "epoch": 25.663983903420522,
      "grad_norm": 0.7962571978569031,
      "learning_rate": 0.0001486869906429218,
      "loss": 0.329,
      "step": 12755
    },
    {
      "epoch": 25.665995975855132,
      "grad_norm": 0.786737322807312,
      "learning_rate": 0.00014868296609316835,
      "loss": 0.3195,
      "step": 12756
    },
    {
      "epoch": 25.66800804828974,
      "grad_norm": 0.7425830364227295,
      "learning_rate": 0.00014867894154341484,
      "loss": 0.3087,
      "step": 12757
    },
    {
      "epoch": 25.670020120724345,
      "grad_norm": 0.751416802406311,
      "learning_rate": 0.00014867491699366135,
      "loss": 0.327,
      "step": 12758
    },
    {
      "epoch": 25.672032193158955,
      "grad_norm": 0.7882513403892517,
      "learning_rate": 0.00014867089244390783,
      "loss": 0.3121,
      "step": 12759
    },
    {
      "epoch": 25.67404426559356,
      "grad_norm": 0.7282527089118958,
      "learning_rate": 0.00014866686789415435,
      "loss": 0.2885,
      "step": 12760
    },
    {
      "epoch": 25.676056338028168,
      "grad_norm": 0.7722147703170776,
      "learning_rate": 0.00014866284334440086,
      "loss": 0.3203,
      "step": 12761
    },
    {
      "epoch": 25.678068410462778,
      "grad_norm": 0.7691181302070618,
      "learning_rate": 0.00014865881879464737,
      "loss": 0.3196,
      "step": 12762
    },
    {
      "epoch": 25.680080482897385,
      "grad_norm": 0.7749940156936646,
      "learning_rate": 0.00014865479424489386,
      "loss": 0.3079,
      "step": 12763
    },
    {
      "epoch": 25.68209255533199,
      "grad_norm": 0.7875524759292603,
      "learning_rate": 0.00014865076969514037,
      "loss": 0.3104,
      "step": 12764
    },
    {
      "epoch": 25.6841046277666,
      "grad_norm": 0.7531772255897522,
      "learning_rate": 0.00014864674514538685,
      "loss": 0.3195,
      "step": 12765
    },
    {
      "epoch": 25.686116700201207,
      "grad_norm": 0.7893822193145752,
      "learning_rate": 0.00014864272059563337,
      "loss": 0.3381,
      "step": 12766
    },
    {
      "epoch": 25.688128772635814,
      "grad_norm": 0.7794879674911499,
      "learning_rate": 0.00014863869604587988,
      "loss": 0.3205,
      "step": 12767
    },
    {
      "epoch": 25.690140845070424,
      "grad_norm": 0.7343724966049194,
      "learning_rate": 0.00014863467149612636,
      "loss": 0.2947,
      "step": 12768
    },
    {
      "epoch": 25.69215291750503,
      "grad_norm": 0.7302478551864624,
      "learning_rate": 0.00014863064694637288,
      "loss": 0.3236,
      "step": 12769
    },
    {
      "epoch": 25.694164989939637,
      "grad_norm": 0.7447224259376526,
      "learning_rate": 0.0001486266223966194,
      "loss": 0.3306,
      "step": 12770
    },
    {
      "epoch": 25.696177062374247,
      "grad_norm": 0.7576471567153931,
      "learning_rate": 0.0001486225978468659,
      "loss": 0.3105,
      "step": 12771
    },
    {
      "epoch": 25.698189134808853,
      "grad_norm": 0.780720591545105,
      "learning_rate": 0.00014861857329711238,
      "loss": 0.299,
      "step": 12772
    },
    {
      "epoch": 25.70020120724346,
      "grad_norm": 0.7662952542304993,
      "learning_rate": 0.0001486145487473589,
      "loss": 0.2899,
      "step": 12773
    },
    {
      "epoch": 25.70221327967807,
      "grad_norm": 0.7948300838470459,
      "learning_rate": 0.00014861052419760538,
      "loss": 0.3327,
      "step": 12774
    },
    {
      "epoch": 25.704225352112676,
      "grad_norm": 0.8057758808135986,
      "learning_rate": 0.0001486064996478519,
      "loss": 0.3232,
      "step": 12775
    },
    {
      "epoch": 25.706237424547282,
      "grad_norm": 0.7389733195304871,
      "learning_rate": 0.0001486024750980984,
      "loss": 0.3281,
      "step": 12776
    },
    {
      "epoch": 25.708249496981892,
      "grad_norm": 0.7605499625205994,
      "learning_rate": 0.00014859845054834492,
      "loss": 0.3249,
      "step": 12777
    },
    {
      "epoch": 25.7102615694165,
      "grad_norm": 0.7934542894363403,
      "learning_rate": 0.0001485944259985914,
      "loss": 0.3461,
      "step": 12778
    },
    {
      "epoch": 25.712273641851105,
      "grad_norm": 0.8078838586807251,
      "learning_rate": 0.00014859040144883792,
      "loss": 0.3014,
      "step": 12779
    },
    {
      "epoch": 25.714285714285715,
      "grad_norm": 0.7707463502883911,
      "learning_rate": 0.0001485863768990844,
      "loss": 0.3433,
      "step": 12780
    },
    {
      "epoch": 25.71629778672032,
      "grad_norm": 0.7425475716590881,
      "learning_rate": 0.00014858235234933094,
      "loss": 0.2933,
      "step": 12781
    },
    {
      "epoch": 25.718309859154928,
      "grad_norm": 0.8157984614372253,
      "learning_rate": 0.00014857832779957743,
      "loss": 0.3335,
      "step": 12782
    },
    {
      "epoch": 25.720321931589538,
      "grad_norm": 0.7516401410102844,
      "learning_rate": 0.00014857430324982394,
      "loss": 0.3094,
      "step": 12783
    },
    {
      "epoch": 25.722334004024145,
      "grad_norm": 0.7557669281959534,
      "learning_rate": 0.00014857027870007042,
      "loss": 0.291,
      "step": 12784
    },
    {
      "epoch": 25.72434607645875,
      "grad_norm": 0.7801432013511658,
      "learning_rate": 0.00014856625415031694,
      "loss": 0.3143,
      "step": 12785
    },
    {
      "epoch": 25.72635814889336,
      "grad_norm": 0.8749162554740906,
      "learning_rate": 0.00014856222960056345,
      "loss": 0.3161,
      "step": 12786
    },
    {
      "epoch": 25.728370221327967,
      "grad_norm": 0.7477852702140808,
      "learning_rate": 0.00014855820505080996,
      "loss": 0.3092,
      "step": 12787
    },
    {
      "epoch": 25.730382293762574,
      "grad_norm": 0.7878715395927429,
      "learning_rate": 0.00014855418050105644,
      "loss": 0.3149,
      "step": 12788
    },
    {
      "epoch": 25.732394366197184,
      "grad_norm": 0.7310857176780701,
      "learning_rate": 0.00014855015595130296,
      "loss": 0.3005,
      "step": 12789
    },
    {
      "epoch": 25.73440643863179,
      "grad_norm": 0.7779597043991089,
      "learning_rate": 0.00014854613140154944,
      "loss": 0.331,
      "step": 12790
    },
    {
      "epoch": 25.736418511066397,
      "grad_norm": 0.7408455014228821,
      "learning_rate": 0.00014854210685179598,
      "loss": 0.2963,
      "step": 12791
    },
    {
      "epoch": 25.738430583501007,
      "grad_norm": 0.7743439674377441,
      "learning_rate": 0.00014853808230204247,
      "loss": 0.2968,
      "step": 12792
    },
    {
      "epoch": 25.740442655935613,
      "grad_norm": 0.7765861749649048,
      "learning_rate": 0.00014853405775228898,
      "loss": 0.3205,
      "step": 12793
    },
    {
      "epoch": 25.74245472837022,
      "grad_norm": 0.7800893783569336,
      "learning_rate": 0.00014853003320253546,
      "loss": 0.3383,
      "step": 12794
    },
    {
      "epoch": 25.74446680080483,
      "grad_norm": 0.7724462151527405,
      "learning_rate": 0.00014852600865278198,
      "loss": 0.3033,
      "step": 12795
    },
    {
      "epoch": 25.746478873239436,
      "grad_norm": 0.7338029742240906,
      "learning_rate": 0.0001485219841030285,
      "loss": 0.2833,
      "step": 12796
    },
    {
      "epoch": 25.748490945674043,
      "grad_norm": 0.7491663694381714,
      "learning_rate": 0.000148517959553275,
      "loss": 0.3073,
      "step": 12797
    },
    {
      "epoch": 25.750503018108652,
      "grad_norm": 0.7636981010437012,
      "learning_rate": 0.00014851393500352149,
      "loss": 0.3116,
      "step": 12798
    },
    {
      "epoch": 25.75251509054326,
      "grad_norm": 0.7616022229194641,
      "learning_rate": 0.000148509910453768,
      "loss": 0.2984,
      "step": 12799
    },
    {
      "epoch": 25.754527162977865,
      "grad_norm": 0.7573460936546326,
      "learning_rate": 0.00014850588590401448,
      "loss": 0.2994,
      "step": 12800
    },
    {
      "epoch": 25.756539235412475,
      "grad_norm": 0.7752065658569336,
      "learning_rate": 0.000148501861354261,
      "loss": 0.3438,
      "step": 12801
    },
    {
      "epoch": 25.758551307847082,
      "grad_norm": 0.7504733800888062,
      "learning_rate": 0.0001484978368045075,
      "loss": 0.3008,
      "step": 12802
    },
    {
      "epoch": 25.760563380281692,
      "grad_norm": 0.7399901151657104,
      "learning_rate": 0.000148493812254754,
      "loss": 0.3331,
      "step": 12803
    },
    {
      "epoch": 25.7625754527163,
      "grad_norm": 0.7417151927947998,
      "learning_rate": 0.0001484897877050005,
      "loss": 0.2985,
      "step": 12804
    },
    {
      "epoch": 25.764587525150905,
      "grad_norm": 0.7417121529579163,
      "learning_rate": 0.00014848576315524702,
      "loss": 0.3156,
      "step": 12805
    },
    {
      "epoch": 25.766599597585515,
      "grad_norm": 0.7724179625511169,
      "learning_rate": 0.00014848173860549353,
      "loss": 0.3321,
      "step": 12806
    },
    {
      "epoch": 25.76861167002012,
      "grad_norm": 0.8247672915458679,
      "learning_rate": 0.00014847771405574001,
      "loss": 0.3189,
      "step": 12807
    },
    {
      "epoch": 25.770623742454728,
      "grad_norm": 0.7606241703033447,
      "learning_rate": 0.00014847368950598653,
      "loss": 0.2867,
      "step": 12808
    },
    {
      "epoch": 25.772635814889338,
      "grad_norm": 0.7171361446380615,
      "learning_rate": 0.000148469664956233,
      "loss": 0.3181,
      "step": 12809
    },
    {
      "epoch": 25.774647887323944,
      "grad_norm": 0.7891343235969543,
      "learning_rate": 0.00014846564040647952,
      "loss": 0.3222,
      "step": 12810
    },
    {
      "epoch": 25.77665995975855,
      "grad_norm": 0.748994767665863,
      "learning_rate": 0.00014846161585672604,
      "loss": 0.3266,
      "step": 12811
    },
    {
      "epoch": 25.77867203219316,
      "grad_norm": 0.7560004591941833,
      "learning_rate": 0.00014845759130697255,
      "loss": 0.3023,
      "step": 12812
    },
    {
      "epoch": 25.780684104627767,
      "grad_norm": 0.7813915014266968,
      "learning_rate": 0.00014845356675721903,
      "loss": 0.3305,
      "step": 12813
    },
    {
      "epoch": 25.782696177062373,
      "grad_norm": 0.7504043579101562,
      "learning_rate": 0.00014844954220746555,
      "loss": 0.3039,
      "step": 12814
    },
    {
      "epoch": 25.784708249496983,
      "grad_norm": 0.7846725583076477,
      "learning_rate": 0.00014844551765771203,
      "loss": 0.3239,
      "step": 12815
    },
    {
      "epoch": 25.78672032193159,
      "grad_norm": 0.7597638368606567,
      "learning_rate": 0.00014844149310795857,
      "loss": 0.3055,
      "step": 12816
    },
    {
      "epoch": 25.788732394366196,
      "grad_norm": 0.7726213932037354,
      "learning_rate": 0.00014843746855820506,
      "loss": 0.3385,
      "step": 12817
    },
    {
      "epoch": 25.790744466800806,
      "grad_norm": 0.7577328681945801,
      "learning_rate": 0.00014843344400845157,
      "loss": 0.3353,
      "step": 12818
    },
    {
      "epoch": 25.792756539235413,
      "grad_norm": 0.7528781294822693,
      "learning_rate": 0.00014842941945869805,
      "loss": 0.3042,
      "step": 12819
    },
    {
      "epoch": 25.79476861167002,
      "grad_norm": 0.7594466805458069,
      "learning_rate": 0.00014842539490894456,
      "loss": 0.3269,
      "step": 12820
    },
    {
      "epoch": 25.79678068410463,
      "grad_norm": 0.7709593176841736,
      "learning_rate": 0.00014842137035919108,
      "loss": 0.2932,
      "step": 12821
    },
    {
      "epoch": 25.798792756539235,
      "grad_norm": 0.7177034020423889,
      "learning_rate": 0.0001484173458094376,
      "loss": 0.3189,
      "step": 12822
    },
    {
      "epoch": 25.800804828973842,
      "grad_norm": 0.79048091173172,
      "learning_rate": 0.00014841332125968407,
      "loss": 0.3203,
      "step": 12823
    },
    {
      "epoch": 25.802816901408452,
      "grad_norm": 0.744866132736206,
      "learning_rate": 0.00014840929670993059,
      "loss": 0.2972,
      "step": 12824
    },
    {
      "epoch": 25.80482897384306,
      "grad_norm": 0.7229796648025513,
      "learning_rate": 0.00014840527216017707,
      "loss": 0.2972,
      "step": 12825
    },
    {
      "epoch": 25.806841046277665,
      "grad_norm": 0.7317700386047363,
      "learning_rate": 0.0001484012476104236,
      "loss": 0.2813,
      "step": 12826
    },
    {
      "epoch": 25.808853118712275,
      "grad_norm": 0.788009762763977,
      "learning_rate": 0.0001483972230606701,
      "loss": 0.3463,
      "step": 12827
    },
    {
      "epoch": 25.81086519114688,
      "grad_norm": 0.7616706490516663,
      "learning_rate": 0.0001483931985109166,
      "loss": 0.3254,
      "step": 12828
    },
    {
      "epoch": 25.812877263581488,
      "grad_norm": 0.7649217844009399,
      "learning_rate": 0.0001483891739611631,
      "loss": 0.3177,
      "step": 12829
    },
    {
      "epoch": 25.814889336016098,
      "grad_norm": 0.7546830177307129,
      "learning_rate": 0.0001483851494114096,
      "loss": 0.3194,
      "step": 12830
    },
    {
      "epoch": 25.816901408450704,
      "grad_norm": 0.7729432582855225,
      "learning_rate": 0.00014838112486165612,
      "loss": 0.3518,
      "step": 12831
    },
    {
      "epoch": 25.81891348088531,
      "grad_norm": 0.7736939787864685,
      "learning_rate": 0.00014837710031190263,
      "loss": 0.3158,
      "step": 12832
    },
    {
      "epoch": 25.82092555331992,
      "grad_norm": 0.7771941423416138,
      "learning_rate": 0.00014837307576214911,
      "loss": 0.3315,
      "step": 12833
    },
    {
      "epoch": 25.822937625754527,
      "grad_norm": 0.7643405199050903,
      "learning_rate": 0.00014836905121239563,
      "loss": 0.3137,
      "step": 12834
    },
    {
      "epoch": 25.824949698189133,
      "grad_norm": 0.7637131810188293,
      "learning_rate": 0.0001483650266626421,
      "loss": 0.3129,
      "step": 12835
    },
    {
      "epoch": 25.826961770623743,
      "grad_norm": 0.8043209314346313,
      "learning_rate": 0.00014836100211288862,
      "loss": 0.3256,
      "step": 12836
    },
    {
      "epoch": 25.82897384305835,
      "grad_norm": 0.7954819798469543,
      "learning_rate": 0.00014835697756313514,
      "loss": 0.3256,
      "step": 12837
    },
    {
      "epoch": 25.830985915492956,
      "grad_norm": 0.7773614525794983,
      "learning_rate": 0.00014835295301338162,
      "loss": 0.2992,
      "step": 12838
    },
    {
      "epoch": 25.832997987927566,
      "grad_norm": 0.7925337553024292,
      "learning_rate": 0.00014834892846362813,
      "loss": 0.3121,
      "step": 12839
    },
    {
      "epoch": 25.835010060362173,
      "grad_norm": 0.8033544421195984,
      "learning_rate": 0.00014834490391387465,
      "loss": 0.3285,
      "step": 12840
    },
    {
      "epoch": 25.83702213279678,
      "grad_norm": 0.7565152645111084,
      "learning_rate": 0.00014834087936412116,
      "loss": 0.319,
      "step": 12841
    },
    {
      "epoch": 25.83903420523139,
      "grad_norm": 0.7463234663009644,
      "learning_rate": 0.00014833685481436764,
      "loss": 0.3148,
      "step": 12842
    },
    {
      "epoch": 25.841046277665995,
      "grad_norm": 0.7769812941551208,
      "learning_rate": 0.00014833283026461416,
      "loss": 0.3316,
      "step": 12843
    },
    {
      "epoch": 25.843058350100602,
      "grad_norm": 0.7913336753845215,
      "learning_rate": 0.00014832880571486064,
      "loss": 0.3501,
      "step": 12844
    },
    {
      "epoch": 25.845070422535212,
      "grad_norm": 0.8472949266433716,
      "learning_rate": 0.00014832478116510715,
      "loss": 0.3444,
      "step": 12845
    },
    {
      "epoch": 25.84708249496982,
      "grad_norm": 0.7645046710968018,
      "learning_rate": 0.00014832075661535367,
      "loss": 0.3325,
      "step": 12846
    },
    {
      "epoch": 25.84909456740443,
      "grad_norm": 0.7297122478485107,
      "learning_rate": 0.00014831673206560018,
      "loss": 0.288,
      "step": 12847
    },
    {
      "epoch": 25.851106639839035,
      "grad_norm": 0.7745974063873291,
      "learning_rate": 0.00014831270751584666,
      "loss": 0.315,
      "step": 12848
    },
    {
      "epoch": 25.85311871227364,
      "grad_norm": 0.7874341011047363,
      "learning_rate": 0.00014830868296609317,
      "loss": 0.3227,
      "step": 12849
    },
    {
      "epoch": 25.85513078470825,
      "grad_norm": 0.7760207653045654,
      "learning_rate": 0.00014830465841633966,
      "loss": 0.3269,
      "step": 12850
    },
    {
      "epoch": 25.857142857142858,
      "grad_norm": 0.7595908045768738,
      "learning_rate": 0.0001483006338665862,
      "loss": 0.3082,
      "step": 12851
    },
    {
      "epoch": 25.859154929577464,
      "grad_norm": 0.7853286862373352,
      "learning_rate": 0.00014829660931683268,
      "loss": 0.3174,
      "step": 12852
    },
    {
      "epoch": 25.861167002012074,
      "grad_norm": 0.7469220161437988,
      "learning_rate": 0.0001482925847670792,
      "loss": 0.2711,
      "step": 12853
    },
    {
      "epoch": 25.86317907444668,
      "grad_norm": 0.747048556804657,
      "learning_rate": 0.00014828856021732568,
      "loss": 0.3107,
      "step": 12854
    },
    {
      "epoch": 25.865191146881287,
      "grad_norm": 0.7578316926956177,
      "learning_rate": 0.0001482845356675722,
      "loss": 0.3319,
      "step": 12855
    },
    {
      "epoch": 25.867203219315897,
      "grad_norm": 0.77269446849823,
      "learning_rate": 0.0001482805111178187,
      "loss": 0.3312,
      "step": 12856
    },
    {
      "epoch": 25.869215291750503,
      "grad_norm": 0.7600870132446289,
      "learning_rate": 0.00014827648656806522,
      "loss": 0.3268,
      "step": 12857
    },
    {
      "epoch": 25.87122736418511,
      "grad_norm": 0.7602713704109192,
      "learning_rate": 0.0001482724620183117,
      "loss": 0.3445,
      "step": 12858
    },
    {
      "epoch": 25.87323943661972,
      "grad_norm": 0.8050768971443176,
      "learning_rate": 0.00014826843746855822,
      "loss": 0.293,
      "step": 12859
    },
    {
      "epoch": 25.875251509054326,
      "grad_norm": 0.7634226083755493,
      "learning_rate": 0.0001482644129188047,
      "loss": 0.3218,
      "step": 12860
    },
    {
      "epoch": 25.877263581488933,
      "grad_norm": 0.7936758995056152,
      "learning_rate": 0.00014826038836905124,
      "loss": 0.3429,
      "step": 12861
    },
    {
      "epoch": 25.879275653923543,
      "grad_norm": 0.813112199306488,
      "learning_rate": 0.00014825636381929773,
      "loss": 0.326,
      "step": 12862
    },
    {
      "epoch": 25.88128772635815,
      "grad_norm": 0.7699612379074097,
      "learning_rate": 0.00014825233926954424,
      "loss": 0.3001,
      "step": 12863
    },
    {
      "epoch": 25.883299798792756,
      "grad_norm": 0.8233341574668884,
      "learning_rate": 0.00014824831471979072,
      "loss": 0.3589,
      "step": 12864
    },
    {
      "epoch": 25.885311871227366,
      "grad_norm": 0.7773371934890747,
      "learning_rate": 0.00014824429017003723,
      "loss": 0.3252,
      "step": 12865
    },
    {
      "epoch": 25.887323943661972,
      "grad_norm": 0.7935535311698914,
      "learning_rate": 0.00014824026562028375,
      "loss": 0.3131,
      "step": 12866
    },
    {
      "epoch": 25.88933601609658,
      "grad_norm": 0.781984806060791,
      "learning_rate": 0.00014823624107053026,
      "loss": 0.3238,
      "step": 12867
    },
    {
      "epoch": 25.89134808853119,
      "grad_norm": 0.7531278729438782,
      "learning_rate": 0.00014823221652077674,
      "loss": 0.2972,
      "step": 12868
    },
    {
      "epoch": 25.893360160965795,
      "grad_norm": 0.8313218951225281,
      "learning_rate": 0.00014822819197102326,
      "loss": 0.3186,
      "step": 12869
    },
    {
      "epoch": 25.8953722334004,
      "grad_norm": 0.8069764971733093,
      "learning_rate": 0.00014822416742126974,
      "loss": 0.3139,
      "step": 12870
    },
    {
      "epoch": 25.89738430583501,
      "grad_norm": 0.8178473114967346,
      "learning_rate": 0.00014822014287151625,
      "loss": 0.3196,
      "step": 12871
    },
    {
      "epoch": 25.899396378269618,
      "grad_norm": 0.7547165155410767,
      "learning_rate": 0.00014821611832176277,
      "loss": 0.3187,
      "step": 12872
    },
    {
      "epoch": 25.901408450704224,
      "grad_norm": 0.7696124315261841,
      "learning_rate": 0.00014821209377200925,
      "loss": 0.3445,
      "step": 12873
    },
    {
      "epoch": 25.903420523138834,
      "grad_norm": 0.7932848930358887,
      "learning_rate": 0.00014820806922225576,
      "loss": 0.3192,
      "step": 12874
    },
    {
      "epoch": 25.90543259557344,
      "grad_norm": 0.7778880000114441,
      "learning_rate": 0.00014820404467250228,
      "loss": 0.3277,
      "step": 12875
    },
    {
      "epoch": 25.907444668008047,
      "grad_norm": 0.7706320285797119,
      "learning_rate": 0.0001482000201227488,
      "loss": 0.317,
      "step": 12876
    },
    {
      "epoch": 25.909456740442657,
      "grad_norm": 0.8182021975517273,
      "learning_rate": 0.00014819599557299527,
      "loss": 0.3518,
      "step": 12877
    },
    {
      "epoch": 25.911468812877263,
      "grad_norm": 0.7469199895858765,
      "learning_rate": 0.00014819197102324179,
      "loss": 0.302,
      "step": 12878
    },
    {
      "epoch": 25.91348088531187,
      "grad_norm": 0.7736423015594482,
      "learning_rate": 0.00014818794647348827,
      "loss": 0.3264,
      "step": 12879
    },
    {
      "epoch": 25.91549295774648,
      "grad_norm": 0.7755715847015381,
      "learning_rate": 0.00014818392192373478,
      "loss": 0.3163,
      "step": 12880
    },
    {
      "epoch": 25.917505030181086,
      "grad_norm": 0.7637823224067688,
      "learning_rate": 0.0001481798973739813,
      "loss": 0.3277,
      "step": 12881
    },
    {
      "epoch": 25.919517102615693,
      "grad_norm": 0.7929440140724182,
      "learning_rate": 0.0001481758728242278,
      "loss": 0.3171,
      "step": 12882
    },
    {
      "epoch": 25.921529175050303,
      "grad_norm": 0.7785189151763916,
      "learning_rate": 0.0001481718482744743,
      "loss": 0.3226,
      "step": 12883
    },
    {
      "epoch": 25.92354124748491,
      "grad_norm": 0.7520563006401062,
      "learning_rate": 0.0001481678237247208,
      "loss": 0.3068,
      "step": 12884
    },
    {
      "epoch": 25.925553319919516,
      "grad_norm": 0.7534192800521851,
      "learning_rate": 0.0001481637991749673,
      "loss": 0.3289,
      "step": 12885
    },
    {
      "epoch": 25.927565392354126,
      "grad_norm": 0.7487070560455322,
      "learning_rate": 0.00014815977462521383,
      "loss": 0.304,
      "step": 12886
    },
    {
      "epoch": 25.929577464788732,
      "grad_norm": 0.780545175075531,
      "learning_rate": 0.00014815575007546031,
      "loss": 0.3354,
      "step": 12887
    },
    {
      "epoch": 25.93158953722334,
      "grad_norm": 0.7982826232910156,
      "learning_rate": 0.00014815172552570683,
      "loss": 0.3343,
      "step": 12888
    },
    {
      "epoch": 25.93360160965795,
      "grad_norm": 0.7084394097328186,
      "learning_rate": 0.0001481477009759533,
      "loss": 0.2891,
      "step": 12889
    },
    {
      "epoch": 25.935613682092555,
      "grad_norm": 0.7567017078399658,
      "learning_rate": 0.00014814367642619982,
      "loss": 0.3091,
      "step": 12890
    },
    {
      "epoch": 25.93762575452716,
      "grad_norm": 0.7360625267028809,
      "learning_rate": 0.00014813965187644634,
      "loss": 0.2979,
      "step": 12891
    },
    {
      "epoch": 25.93963782696177,
      "grad_norm": 0.7642601728439331,
      "learning_rate": 0.00014813562732669285,
      "loss": 0.3044,
      "step": 12892
    },
    {
      "epoch": 25.941649899396378,
      "grad_norm": 0.8120441436767578,
      "learning_rate": 0.00014813160277693933,
      "loss": 0.3284,
      "step": 12893
    },
    {
      "epoch": 25.943661971830984,
      "grad_norm": 0.778342604637146,
      "learning_rate": 0.00014812757822718585,
      "loss": 0.3301,
      "step": 12894
    },
    {
      "epoch": 25.945674044265594,
      "grad_norm": 0.8160938620567322,
      "learning_rate": 0.00014812355367743233,
      "loss": 0.3233,
      "step": 12895
    },
    {
      "epoch": 25.9476861167002,
      "grad_norm": 0.7669737339019775,
      "learning_rate": 0.00014811952912767887,
      "loss": 0.3141,
      "step": 12896
    },
    {
      "epoch": 25.949698189134807,
      "grad_norm": 0.7599229216575623,
      "learning_rate": 0.00014811550457792535,
      "loss": 0.3209,
      "step": 12897
    },
    {
      "epoch": 25.951710261569417,
      "grad_norm": 0.772338330745697,
      "learning_rate": 0.00014811148002817187,
      "loss": 0.3016,
      "step": 12898
    },
    {
      "epoch": 25.953722334004024,
      "grad_norm": 0.7711796164512634,
      "learning_rate": 0.00014810745547841835,
      "loss": 0.3193,
      "step": 12899
    },
    {
      "epoch": 25.955734406438633,
      "grad_norm": 0.8134213089942932,
      "learning_rate": 0.00014810343092866486,
      "loss": 0.3292,
      "step": 12900
    },
    {
      "epoch": 25.95774647887324,
      "grad_norm": 0.7348914742469788,
      "learning_rate": 0.00014809940637891138,
      "loss": 0.288,
      "step": 12901
    },
    {
      "epoch": 25.959758551307846,
      "grad_norm": 0.7602412700653076,
      "learning_rate": 0.0001480953818291579,
      "loss": 0.3113,
      "step": 12902
    },
    {
      "epoch": 25.961770623742456,
      "grad_norm": 0.7480610013008118,
      "learning_rate": 0.00014809135727940437,
      "loss": 0.3331,
      "step": 12903
    },
    {
      "epoch": 25.963782696177063,
      "grad_norm": 0.7372515797615051,
      "learning_rate": 0.00014808733272965089,
      "loss": 0.2961,
      "step": 12904
    },
    {
      "epoch": 25.96579476861167,
      "grad_norm": 0.7430140376091003,
      "learning_rate": 0.00014808330817989737,
      "loss": 0.3207,
      "step": 12905
    },
    {
      "epoch": 25.96780684104628,
      "grad_norm": 0.7572693824768066,
      "learning_rate": 0.00014807928363014388,
      "loss": 0.2947,
      "step": 12906
    },
    {
      "epoch": 25.969818913480886,
      "grad_norm": 0.7819284796714783,
      "learning_rate": 0.0001480752590803904,
      "loss": 0.3385,
      "step": 12907
    },
    {
      "epoch": 25.971830985915492,
      "grad_norm": 0.8120395541191101,
      "learning_rate": 0.00014807123453063688,
      "loss": 0.3529,
      "step": 12908
    },
    {
      "epoch": 25.973843058350102,
      "grad_norm": 0.7510977983474731,
      "learning_rate": 0.0001480672099808834,
      "loss": 0.339,
      "step": 12909
    },
    {
      "epoch": 25.97585513078471,
      "grad_norm": 0.8117560744285583,
      "learning_rate": 0.0001480631854311299,
      "loss": 0.3179,
      "step": 12910
    },
    {
      "epoch": 25.977867203219315,
      "grad_norm": 0.7529231905937195,
      "learning_rate": 0.00014805916088137642,
      "loss": 0.3158,
      "step": 12911
    },
    {
      "epoch": 25.979879275653925,
      "grad_norm": 0.7790277004241943,
      "learning_rate": 0.0001480551363316229,
      "loss": 0.3242,
      "step": 12912
    },
    {
      "epoch": 25.98189134808853,
      "grad_norm": 0.7863179445266724,
      "learning_rate": 0.00014805111178186941,
      "loss": 0.3124,
      "step": 12913
    },
    {
      "epoch": 25.983903420523138,
      "grad_norm": 0.74644535779953,
      "learning_rate": 0.0001480470872321159,
      "loss": 0.3119,
      "step": 12914
    },
    {
      "epoch": 25.985915492957748,
      "grad_norm": 0.7689803242683411,
      "learning_rate": 0.0001480430626823624,
      "loss": 0.3084,
      "step": 12915
    },
    {
      "epoch": 25.987927565392354,
      "grad_norm": 0.7495079636573792,
      "learning_rate": 0.00014803903813260892,
      "loss": 0.3001,
      "step": 12916
    },
    {
      "epoch": 25.98993963782696,
      "grad_norm": 0.7934852242469788,
      "learning_rate": 0.00014803501358285544,
      "loss": 0.3243,
      "step": 12917
    },
    {
      "epoch": 25.99195171026157,
      "grad_norm": 0.7768102884292603,
      "learning_rate": 0.00014803098903310192,
      "loss": 0.3085,
      "step": 12918
    },
    {
      "epoch": 25.993963782696177,
      "grad_norm": 0.8299734592437744,
      "learning_rate": 0.00014802696448334843,
      "loss": 0.3261,
      "step": 12919
    },
    {
      "epoch": 25.995975855130784,
      "grad_norm": 0.7645258903503418,
      "learning_rate": 0.00014802293993359492,
      "loss": 0.329,
      "step": 12920
    },
    {
      "epoch": 25.997987927565394,
      "grad_norm": 0.7418637871742249,
      "learning_rate": 0.00014801891538384146,
      "loss": 0.3268,
      "step": 12921
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.7763583064079285,
      "learning_rate": 0.00014801489083408794,
      "loss": 0.3438,
      "step": 12922
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.1480977535247803,
      "eval_runtime": 49.84,
      "eval_samples_per_second": 19.904,
      "eval_steps_per_second": 2.488,
      "step": 12922
    },
    {
      "epoch": 26.002012072434606,
      "grad_norm": 0.6681944727897644,
      "learning_rate": 0.00014801086628433446,
      "loss": 0.2644,
      "step": 12923
    },
    {
      "epoch": 26.004024144869216,
      "grad_norm": 0.694456160068512,
      "learning_rate": 0.00014800684173458094,
      "loss": 0.2664,
      "step": 12924
    },
    {
      "epoch": 26.006036217303823,
      "grad_norm": 0.6661875247955322,
      "learning_rate": 0.00014800281718482745,
      "loss": 0.2457,
      "step": 12925
    },
    {
      "epoch": 26.00804828973843,
      "grad_norm": 0.6930601596832275,
      "learning_rate": 0.00014799879263507397,
      "loss": 0.2561,
      "step": 12926
    },
    {
      "epoch": 26.01006036217304,
      "grad_norm": 0.7886000871658325,
      "learning_rate": 0.00014799476808532048,
      "loss": 0.265,
      "step": 12927
    },
    {
      "epoch": 26.012072434607646,
      "grad_norm": 0.7472360730171204,
      "learning_rate": 0.00014799074353556696,
      "loss": 0.2471,
      "step": 12928
    },
    {
      "epoch": 26.014084507042252,
      "grad_norm": 0.7114006280899048,
      "learning_rate": 0.00014798671898581347,
      "loss": 0.2291,
      "step": 12929
    },
    {
      "epoch": 26.016096579476862,
      "grad_norm": 0.7519867420196533,
      "learning_rate": 0.00014798269443605996,
      "loss": 0.2594,
      "step": 12930
    },
    {
      "epoch": 26.01810865191147,
      "grad_norm": 0.7126571536064148,
      "learning_rate": 0.0001479786698863065,
      "loss": 0.248,
      "step": 12931
    },
    {
      "epoch": 26.020120724346075,
      "grad_norm": 0.6795217990875244,
      "learning_rate": 0.00014797464533655298,
      "loss": 0.2588,
      "step": 12932
    },
    {
      "epoch": 26.022132796780685,
      "grad_norm": 0.7017645239830017,
      "learning_rate": 0.0001479706207867995,
      "loss": 0.2687,
      "step": 12933
    },
    {
      "epoch": 26.02414486921529,
      "grad_norm": 0.6807524561882019,
      "learning_rate": 0.00014796659623704598,
      "loss": 0.2634,
      "step": 12934
    },
    {
      "epoch": 26.026156941649898,
      "grad_norm": 0.6882534623146057,
      "learning_rate": 0.0001479625716872925,
      "loss": 0.2646,
      "step": 12935
    },
    {
      "epoch": 26.028169014084508,
      "grad_norm": 0.7014055252075195,
      "learning_rate": 0.000147958547137539,
      "loss": 0.2607,
      "step": 12936
    },
    {
      "epoch": 26.030181086519114,
      "grad_norm": 0.6899382472038269,
      "learning_rate": 0.0001479545225877855,
      "loss": 0.2453,
      "step": 12937
    },
    {
      "epoch": 26.03219315895372,
      "grad_norm": 0.7492496967315674,
      "learning_rate": 0.000147950498038032,
      "loss": 0.2628,
      "step": 12938
    },
    {
      "epoch": 26.03420523138833,
      "grad_norm": 0.7968458533287048,
      "learning_rate": 0.00014794647348827852,
      "loss": 0.2521,
      "step": 12939
    },
    {
      "epoch": 26.036217303822937,
      "grad_norm": 0.8411692380905151,
      "learning_rate": 0.000147942448938525,
      "loss": 0.2764,
      "step": 12940
    },
    {
      "epoch": 26.038229376257544,
      "grad_norm": 0.7146375775337219,
      "learning_rate": 0.0001479384243887715,
      "loss": 0.2488,
      "step": 12941
    },
    {
      "epoch": 26.040241448692154,
      "grad_norm": 0.7302873730659485,
      "learning_rate": 0.00014793439983901803,
      "loss": 0.2574,
      "step": 12942
    },
    {
      "epoch": 26.04225352112676,
      "grad_norm": 0.7366606593132019,
      "learning_rate": 0.0001479303752892645,
      "loss": 0.2612,
      "step": 12943
    },
    {
      "epoch": 26.044265593561367,
      "grad_norm": 0.6672359108924866,
      "learning_rate": 0.00014792635073951102,
      "loss": 0.2501,
      "step": 12944
    },
    {
      "epoch": 26.046277665995976,
      "grad_norm": 0.7123088240623474,
      "learning_rate": 0.0001479223261897575,
      "loss": 0.2445,
      "step": 12945
    },
    {
      "epoch": 26.048289738430583,
      "grad_norm": 0.7299110293388367,
      "learning_rate": 0.00014791830164000405,
      "loss": 0.2617,
      "step": 12946
    },
    {
      "epoch": 26.050301810865193,
      "grad_norm": 0.7355857491493225,
      "learning_rate": 0.00014791427709025053,
      "loss": 0.2789,
      "step": 12947
    },
    {
      "epoch": 26.0523138832998,
      "grad_norm": 0.7169825434684753,
      "learning_rate": 0.00014791025254049704,
      "loss": 0.2573,
      "step": 12948
    },
    {
      "epoch": 26.054325955734406,
      "grad_norm": 0.798857569694519,
      "learning_rate": 0.00014790622799074353,
      "loss": 0.2723,
      "step": 12949
    },
    {
      "epoch": 26.056338028169016,
      "grad_norm": 0.7121781706809998,
      "learning_rate": 0.00014790220344099004,
      "loss": 0.2418,
      "step": 12950
    },
    {
      "epoch": 26.058350100603622,
      "grad_norm": 0.7523046135902405,
      "learning_rate": 0.00014789817889123655,
      "loss": 0.2381,
      "step": 12951
    },
    {
      "epoch": 26.06036217303823,
      "grad_norm": 0.7291116714477539,
      "learning_rate": 0.00014789415434148307,
      "loss": 0.2669,
      "step": 12952
    },
    {
      "epoch": 26.06237424547284,
      "grad_norm": 0.7317084670066833,
      "learning_rate": 0.00014789012979172955,
      "loss": 0.2659,
      "step": 12953
    },
    {
      "epoch": 26.064386317907445,
      "grad_norm": 0.7603707909584045,
      "learning_rate": 0.00014788610524197606,
      "loss": 0.2668,
      "step": 12954
    },
    {
      "epoch": 26.06639839034205,
      "grad_norm": 0.6763049960136414,
      "learning_rate": 0.00014788208069222255,
      "loss": 0.2646,
      "step": 12955
    },
    {
      "epoch": 26.06841046277666,
      "grad_norm": 0.7092347741127014,
      "learning_rate": 0.0001478780561424691,
      "loss": 0.2523,
      "step": 12956
    },
    {
      "epoch": 26.070422535211268,
      "grad_norm": 0.7657468914985657,
      "learning_rate": 0.00014787403159271557,
      "loss": 0.2536,
      "step": 12957
    },
    {
      "epoch": 26.072434607645874,
      "grad_norm": 0.7053458094596863,
      "learning_rate": 0.00014787000704296208,
      "loss": 0.2558,
      "step": 12958
    },
    {
      "epoch": 26.074446680080484,
      "grad_norm": 0.7274516224861145,
      "learning_rate": 0.00014786598249320857,
      "loss": 0.2598,
      "step": 12959
    },
    {
      "epoch": 26.07645875251509,
      "grad_norm": 0.7145339846611023,
      "learning_rate": 0.00014786195794345508,
      "loss": 0.244,
      "step": 12960
    },
    {
      "epoch": 26.078470824949697,
      "grad_norm": 0.7929001450538635,
      "learning_rate": 0.0001478579333937016,
      "loss": 0.2622,
      "step": 12961
    },
    {
      "epoch": 26.080482897384307,
      "grad_norm": 0.7506895065307617,
      "learning_rate": 0.0001478539088439481,
      "loss": 0.2596,
      "step": 12962
    },
    {
      "epoch": 26.082494969818914,
      "grad_norm": 0.7291752696037292,
      "learning_rate": 0.0001478498842941946,
      "loss": 0.2649,
      "step": 12963
    },
    {
      "epoch": 26.08450704225352,
      "grad_norm": 0.6514825224876404,
      "learning_rate": 0.0001478458597444411,
      "loss": 0.249,
      "step": 12964
    },
    {
      "epoch": 26.08651911468813,
      "grad_norm": 0.724226176738739,
      "learning_rate": 0.0001478418351946876,
      "loss": 0.2648,
      "step": 12965
    },
    {
      "epoch": 26.088531187122737,
      "grad_norm": 0.7022350430488586,
      "learning_rate": 0.00014783781064493413,
      "loss": 0.246,
      "step": 12966
    },
    {
      "epoch": 26.090543259557343,
      "grad_norm": 0.6981362700462341,
      "learning_rate": 0.00014783378609518061,
      "loss": 0.2636,
      "step": 12967
    },
    {
      "epoch": 26.092555331991953,
      "grad_norm": 0.6834415793418884,
      "learning_rate": 0.00014782976154542713,
      "loss": 0.2432,
      "step": 12968
    },
    {
      "epoch": 26.09456740442656,
      "grad_norm": 0.690715491771698,
      "learning_rate": 0.0001478257369956736,
      "loss": 0.2412,
      "step": 12969
    },
    {
      "epoch": 26.096579476861166,
      "grad_norm": 0.7269278168678284,
      "learning_rate": 0.00014782171244592012,
      "loss": 0.2671,
      "step": 12970
    },
    {
      "epoch": 26.098591549295776,
      "grad_norm": 0.7625128626823425,
      "learning_rate": 0.00014781768789616664,
      "loss": 0.2755,
      "step": 12971
    },
    {
      "epoch": 26.100603621730382,
      "grad_norm": 0.7465184330940247,
      "learning_rate": 0.00014781366334641312,
      "loss": 0.2561,
      "step": 12972
    },
    {
      "epoch": 26.10261569416499,
      "grad_norm": 0.7881076335906982,
      "learning_rate": 0.00014780963879665963,
      "loss": 0.279,
      "step": 12973
    },
    {
      "epoch": 26.1046277665996,
      "grad_norm": 0.7122132778167725,
      "learning_rate": 0.00014780561424690614,
      "loss": 0.2713,
      "step": 12974
    },
    {
      "epoch": 26.106639839034205,
      "grad_norm": 0.6685882210731506,
      "learning_rate": 0.00014780158969715263,
      "loss": 0.2558,
      "step": 12975
    },
    {
      "epoch": 26.10865191146881,
      "grad_norm": 0.7208429574966431,
      "learning_rate": 0.00014779756514739914,
      "loss": 0.2739,
      "step": 12976
    },
    {
      "epoch": 26.11066398390342,
      "grad_norm": 0.7309777140617371,
      "learning_rate": 0.00014779354059764565,
      "loss": 0.2733,
      "step": 12977
    },
    {
      "epoch": 26.112676056338028,
      "grad_norm": 0.7658297419548035,
      "learning_rate": 0.00014778951604789214,
      "loss": 0.2662,
      "step": 12978
    },
    {
      "epoch": 26.114688128772634,
      "grad_norm": 0.7215204238891602,
      "learning_rate": 0.00014778549149813865,
      "loss": 0.2633,
      "step": 12979
    },
    {
      "epoch": 26.116700201207244,
      "grad_norm": 0.7330175638198853,
      "learning_rate": 0.00014778146694838514,
      "loss": 0.2678,
      "step": 12980
    },
    {
      "epoch": 26.11871227364185,
      "grad_norm": 0.7666594386100769,
      "learning_rate": 0.00014777744239863165,
      "loss": 0.2673,
      "step": 12981
    },
    {
      "epoch": 26.120724346076457,
      "grad_norm": 0.7157239317893982,
      "learning_rate": 0.00014777341784887816,
      "loss": 0.2711,
      "step": 12982
    },
    {
      "epoch": 26.122736418511067,
      "grad_norm": 0.7517174482345581,
      "learning_rate": 0.00014776939329912467,
      "loss": 0.2615,
      "step": 12983
    },
    {
      "epoch": 26.124748490945674,
      "grad_norm": 0.7806510925292969,
      "learning_rate": 0.00014776536874937116,
      "loss": 0.2867,
      "step": 12984
    },
    {
      "epoch": 26.12676056338028,
      "grad_norm": 0.6876134276390076,
      "learning_rate": 0.00014776134419961767,
      "loss": 0.2435,
      "step": 12985
    },
    {
      "epoch": 26.12877263581489,
      "grad_norm": 0.705402672290802,
      "learning_rate": 0.00014775731964986416,
      "loss": 0.2711,
      "step": 12986
    },
    {
      "epoch": 26.130784708249497,
      "grad_norm": 0.7219447493553162,
      "learning_rate": 0.0001477532951001107,
      "loss": 0.2608,
      "step": 12987
    },
    {
      "epoch": 26.132796780684103,
      "grad_norm": 0.7167076468467712,
      "learning_rate": 0.00014774927055035718,
      "loss": 0.265,
      "step": 12988
    },
    {
      "epoch": 26.134808853118713,
      "grad_norm": 0.7389134764671326,
      "learning_rate": 0.0001477452460006037,
      "loss": 0.2679,
      "step": 12989
    },
    {
      "epoch": 26.13682092555332,
      "grad_norm": 0.6907140016555786,
      "learning_rate": 0.00014774122145085018,
      "loss": 0.2561,
      "step": 12990
    },
    {
      "epoch": 26.138832997987926,
      "grad_norm": 0.7512580156326294,
      "learning_rate": 0.0001477371969010967,
      "loss": 0.2706,
      "step": 12991
    },
    {
      "epoch": 26.140845070422536,
      "grad_norm": 0.7835720181465149,
      "learning_rate": 0.0001477331723513432,
      "loss": 0.2836,
      "step": 12992
    },
    {
      "epoch": 26.142857142857142,
      "grad_norm": 0.710909903049469,
      "learning_rate": 0.00014772914780158971,
      "loss": 0.2614,
      "step": 12993
    },
    {
      "epoch": 26.14486921529175,
      "grad_norm": 0.7807016968727112,
      "learning_rate": 0.0001477251232518362,
      "loss": 0.2749,
      "step": 12994
    },
    {
      "epoch": 26.14688128772636,
      "grad_norm": 0.7439824938774109,
      "learning_rate": 0.0001477210987020827,
      "loss": 0.2626,
      "step": 12995
    },
    {
      "epoch": 26.148893360160965,
      "grad_norm": 0.7454997897148132,
      "learning_rate": 0.0001477170741523292,
      "loss": 0.264,
      "step": 12996
    },
    {
      "epoch": 26.15090543259557,
      "grad_norm": 0.7422453761100769,
      "learning_rate": 0.00014771304960257574,
      "loss": 0.2835,
      "step": 12997
    },
    {
      "epoch": 26.15291750503018,
      "grad_norm": 0.7093281149864197,
      "learning_rate": 0.00014770902505282222,
      "loss": 0.2854,
      "step": 12998
    },
    {
      "epoch": 26.154929577464788,
      "grad_norm": 0.7589631080627441,
      "learning_rate": 0.00014770500050306873,
      "loss": 0.2767,
      "step": 12999
    },
    {
      "epoch": 26.156941649899398,
      "grad_norm": 0.7453261613845825,
      "learning_rate": 0.00014770097595331522,
      "loss": 0.2841,
      "step": 13000
    },
    {
      "epoch": 26.158953722334005,
      "grad_norm": 0.7329050302505493,
      "learning_rate": 0.00014769695140356173,
      "loss": 0.2691,
      "step": 13001
    },
    {
      "epoch": 26.16096579476861,
      "grad_norm": 0.7561578154563904,
      "learning_rate": 0.00014769292685380824,
      "loss": 0.2631,
      "step": 13002
    },
    {
      "epoch": 26.16297786720322,
      "grad_norm": 0.7322248816490173,
      "learning_rate": 0.00014768890230405476,
      "loss": 0.2455,
      "step": 13003
    },
    {
      "epoch": 26.164989939637827,
      "grad_norm": 0.7414274215698242,
      "learning_rate": 0.00014768487775430124,
      "loss": 0.2667,
      "step": 13004
    },
    {
      "epoch": 26.167002012072434,
      "grad_norm": 0.776900589466095,
      "learning_rate": 0.00014768085320454775,
      "loss": 0.2976,
      "step": 13005
    },
    {
      "epoch": 26.169014084507044,
      "grad_norm": 0.7514392733573914,
      "learning_rate": 0.00014767682865479424,
      "loss": 0.2589,
      "step": 13006
    },
    {
      "epoch": 26.17102615694165,
      "grad_norm": 0.7097219824790955,
      "learning_rate": 0.00014767280410504075,
      "loss": 0.2554,
      "step": 13007
    },
    {
      "epoch": 26.173038229376257,
      "grad_norm": 0.706345796585083,
      "learning_rate": 0.00014766877955528726,
      "loss": 0.2664,
      "step": 13008
    },
    {
      "epoch": 26.175050301810867,
      "grad_norm": 0.7318750619888306,
      "learning_rate": 0.00014766475500553377,
      "loss": 0.2756,
      "step": 13009
    },
    {
      "epoch": 26.177062374245473,
      "grad_norm": 0.7574858069419861,
      "learning_rate": 0.00014766073045578026,
      "loss": 0.2777,
      "step": 13010
    },
    {
      "epoch": 26.17907444668008,
      "grad_norm": 0.7572870850563049,
      "learning_rate": 0.00014765670590602677,
      "loss": 0.2713,
      "step": 13011
    },
    {
      "epoch": 26.18108651911469,
      "grad_norm": 0.7555453181266785,
      "learning_rate": 0.00014765268135627328,
      "loss": 0.2812,
      "step": 13012
    },
    {
      "epoch": 26.183098591549296,
      "grad_norm": 0.7404857873916626,
      "learning_rate": 0.00014764865680651977,
      "loss": 0.2661,
      "step": 13013
    },
    {
      "epoch": 26.185110663983902,
      "grad_norm": 0.7265141010284424,
      "learning_rate": 0.00014764463225676628,
      "loss": 0.2733,
      "step": 13014
    },
    {
      "epoch": 26.187122736418512,
      "grad_norm": 0.6937199234962463,
      "learning_rate": 0.00014764060770701277,
      "loss": 0.2585,
      "step": 13015
    },
    {
      "epoch": 26.18913480885312,
      "grad_norm": 0.8311337828636169,
      "learning_rate": 0.00014763658315725928,
      "loss": 0.2996,
      "step": 13016
    },
    {
      "epoch": 26.191146881287725,
      "grad_norm": 0.779868483543396,
      "learning_rate": 0.0001476325586075058,
      "loss": 0.2737,
      "step": 13017
    },
    {
      "epoch": 26.193158953722335,
      "grad_norm": 0.8145507574081421,
      "learning_rate": 0.0001476285340577523,
      "loss": 0.2726,
      "step": 13018
    },
    {
      "epoch": 26.19517102615694,
      "grad_norm": 0.804129421710968,
      "learning_rate": 0.0001476245095079988,
      "loss": 0.2883,
      "step": 13019
    },
    {
      "epoch": 26.197183098591548,
      "grad_norm": 0.7749749422073364,
      "learning_rate": 0.0001476204849582453,
      "loss": 0.2695,
      "step": 13020
    },
    {
      "epoch": 26.199195171026158,
      "grad_norm": 0.7328868508338928,
      "learning_rate": 0.00014761646040849179,
      "loss": 0.2803,
      "step": 13021
    },
    {
      "epoch": 26.201207243460765,
      "grad_norm": 0.774524450302124,
      "learning_rate": 0.00014761243585873832,
      "loss": 0.2633,
      "step": 13022
    },
    {
      "epoch": 26.20321931589537,
      "grad_norm": 0.7270797491073608,
      "learning_rate": 0.0001476084113089848,
      "loss": 0.2543,
      "step": 13023
    },
    {
      "epoch": 26.20523138832998,
      "grad_norm": 0.7123876810073853,
      "learning_rate": 0.00014760438675923132,
      "loss": 0.2618,
      "step": 13024
    },
    {
      "epoch": 26.207243460764587,
      "grad_norm": 0.7556254267692566,
      "learning_rate": 0.0001476003622094778,
      "loss": 0.253,
      "step": 13025
    },
    {
      "epoch": 26.209255533199194,
      "grad_norm": 0.7695711255073547,
      "learning_rate": 0.00014759633765972432,
      "loss": 0.2832,
      "step": 13026
    },
    {
      "epoch": 26.211267605633804,
      "grad_norm": 0.832611620426178,
      "learning_rate": 0.00014759231310997083,
      "loss": 0.2798,
      "step": 13027
    },
    {
      "epoch": 26.21327967806841,
      "grad_norm": 0.7820475697517395,
      "learning_rate": 0.00014758828856021734,
      "loss": 0.2829,
      "step": 13028
    },
    {
      "epoch": 26.215291750503017,
      "grad_norm": 0.7479634284973145,
      "learning_rate": 0.00014758426401046383,
      "loss": 0.2524,
      "step": 13029
    },
    {
      "epoch": 26.217303822937627,
      "grad_norm": 0.7279050350189209,
      "learning_rate": 0.00014758023946071034,
      "loss": 0.2301,
      "step": 13030
    },
    {
      "epoch": 26.219315895372233,
      "grad_norm": 0.7754218578338623,
      "learning_rate": 0.00014757621491095683,
      "loss": 0.264,
      "step": 13031
    },
    {
      "epoch": 26.22132796780684,
      "grad_norm": 0.7231470942497253,
      "learning_rate": 0.00014757219036120337,
      "loss": 0.2577,
      "step": 13032
    },
    {
      "epoch": 26.22334004024145,
      "grad_norm": 0.7855563759803772,
      "learning_rate": 0.00014756816581144985,
      "loss": 0.2812,
      "step": 13033
    },
    {
      "epoch": 26.225352112676056,
      "grad_norm": 0.7642344832420349,
      "learning_rate": 0.00014756414126169636,
      "loss": 0.276,
      "step": 13034
    },
    {
      "epoch": 26.227364185110662,
      "grad_norm": 0.734287440776825,
      "learning_rate": 0.00014756011671194285,
      "loss": 0.2836,
      "step": 13035
    },
    {
      "epoch": 26.229376257545272,
      "grad_norm": 0.70108562707901,
      "learning_rate": 0.00014755609216218936,
      "loss": 0.2605,
      "step": 13036
    },
    {
      "epoch": 26.23138832997988,
      "grad_norm": 0.7559537887573242,
      "learning_rate": 0.00014755206761243587,
      "loss": 0.2684,
      "step": 13037
    },
    {
      "epoch": 26.233400402414485,
      "grad_norm": 0.7742811441421509,
      "learning_rate": 0.00014754804306268238,
      "loss": 0.2779,
      "step": 13038
    },
    {
      "epoch": 26.235412474849095,
      "grad_norm": 0.7575841546058655,
      "learning_rate": 0.00014754401851292887,
      "loss": 0.2777,
      "step": 13039
    },
    {
      "epoch": 26.2374245472837,
      "grad_norm": 0.7956749200820923,
      "learning_rate": 0.00014753999396317538,
      "loss": 0.2921,
      "step": 13040
    },
    {
      "epoch": 26.239436619718308,
      "grad_norm": 0.7657397985458374,
      "learning_rate": 0.00014753596941342187,
      "loss": 0.2832,
      "step": 13041
    },
    {
      "epoch": 26.241448692152918,
      "grad_norm": 0.7501651048660278,
      "learning_rate": 0.00014753194486366838,
      "loss": 0.2562,
      "step": 13042
    },
    {
      "epoch": 26.243460764587525,
      "grad_norm": 0.7325316667556763,
      "learning_rate": 0.0001475279203139149,
      "loss": 0.2638,
      "step": 13043
    },
    {
      "epoch": 26.24547283702213,
      "grad_norm": 0.7631219029426575,
      "learning_rate": 0.0001475238957641614,
      "loss": 0.2632,
      "step": 13044
    },
    {
      "epoch": 26.24748490945674,
      "grad_norm": 0.7819821834564209,
      "learning_rate": 0.0001475198712144079,
      "loss": 0.272,
      "step": 13045
    },
    {
      "epoch": 26.249496981891348,
      "grad_norm": 0.7423149347305298,
      "learning_rate": 0.0001475158466646544,
      "loss": 0.2684,
      "step": 13046
    },
    {
      "epoch": 26.251509054325957,
      "grad_norm": 0.8280622363090515,
      "learning_rate": 0.0001475118221149009,
      "loss": 0.291,
      "step": 13047
    },
    {
      "epoch": 26.253521126760564,
      "grad_norm": 0.7765055298805237,
      "learning_rate": 0.0001475077975651474,
      "loss": 0.2785,
      "step": 13048
    },
    {
      "epoch": 26.25553319919517,
      "grad_norm": 0.7505446076393127,
      "learning_rate": 0.0001475037730153939,
      "loss": 0.2668,
      "step": 13049
    },
    {
      "epoch": 26.25754527162978,
      "grad_norm": 0.818324625492096,
      "learning_rate": 0.0001474997484656404,
      "loss": 0.2715,
      "step": 13050
    },
    {
      "epoch": 26.259557344064387,
      "grad_norm": 0.7869277000427246,
      "learning_rate": 0.0001474957239158869,
      "loss": 0.2841,
      "step": 13051
    },
    {
      "epoch": 26.261569416498993,
      "grad_norm": 0.7863278388977051,
      "learning_rate": 0.00014749169936613342,
      "loss": 0.29,
      "step": 13052
    },
    {
      "epoch": 26.263581488933603,
      "grad_norm": 0.7783165574073792,
      "learning_rate": 0.00014748767481637993,
      "loss": 0.2649,
      "step": 13053
    },
    {
      "epoch": 26.26559356136821,
      "grad_norm": 0.7067632079124451,
      "learning_rate": 0.00014748365026662642,
      "loss": 0.2476,
      "step": 13054
    },
    {
      "epoch": 26.267605633802816,
      "grad_norm": 0.7398985028266907,
      "learning_rate": 0.00014747962571687293,
      "loss": 0.2758,
      "step": 13055
    },
    {
      "epoch": 26.269617706237426,
      "grad_norm": 0.7659304141998291,
      "learning_rate": 0.00014747560116711941,
      "loss": 0.2634,
      "step": 13056
    },
    {
      "epoch": 26.271629778672033,
      "grad_norm": 0.780996561050415,
      "learning_rate": 0.00014747157661736595,
      "loss": 0.2857,
      "step": 13057
    },
    {
      "epoch": 26.27364185110664,
      "grad_norm": 0.8323838710784912,
      "learning_rate": 0.00014746755206761244,
      "loss": 0.2985,
      "step": 13058
    },
    {
      "epoch": 26.27565392354125,
      "grad_norm": 0.7526282668113708,
      "learning_rate": 0.00014746352751785895,
      "loss": 0.2745,
      "step": 13059
    },
    {
      "epoch": 26.277665995975855,
      "grad_norm": 0.7758187651634216,
      "learning_rate": 0.00014745950296810544,
      "loss": 0.2663,
      "step": 13060
    },
    {
      "epoch": 26.279678068410462,
      "grad_norm": 0.8330177068710327,
      "learning_rate": 0.00014745547841835195,
      "loss": 0.3001,
      "step": 13061
    },
    {
      "epoch": 26.281690140845072,
      "grad_norm": 0.7514374256134033,
      "learning_rate": 0.00014745145386859846,
      "loss": 0.2671,
      "step": 13062
    },
    {
      "epoch": 26.28370221327968,
      "grad_norm": 0.7716953754425049,
      "learning_rate": 0.00014744742931884497,
      "loss": 0.2979,
      "step": 13063
    },
    {
      "epoch": 26.285714285714285,
      "grad_norm": 0.7537936568260193,
      "learning_rate": 0.00014744340476909146,
      "loss": 0.2592,
      "step": 13064
    },
    {
      "epoch": 26.287726358148895,
      "grad_norm": 0.8310991525650024,
      "learning_rate": 0.00014743938021933797,
      "loss": 0.2985,
      "step": 13065
    },
    {
      "epoch": 26.2897384305835,
      "grad_norm": 0.7496985197067261,
      "learning_rate": 0.00014743535566958446,
      "loss": 0.2813,
      "step": 13066
    },
    {
      "epoch": 26.291750503018108,
      "grad_norm": 0.7848328351974487,
      "learning_rate": 0.000147431331119831,
      "loss": 0.2719,
      "step": 13067
    },
    {
      "epoch": 26.293762575452718,
      "grad_norm": 0.8269574642181396,
      "learning_rate": 0.00014742730657007748,
      "loss": 0.3043,
      "step": 13068
    },
    {
      "epoch": 26.295774647887324,
      "grad_norm": 0.7544039487838745,
      "learning_rate": 0.000147423282020324,
      "loss": 0.2538,
      "step": 13069
    },
    {
      "epoch": 26.29778672032193,
      "grad_norm": 0.7807374000549316,
      "learning_rate": 0.00014741925747057048,
      "loss": 0.285,
      "step": 13070
    },
    {
      "epoch": 26.29979879275654,
      "grad_norm": 0.7829049229621887,
      "learning_rate": 0.000147415232920817,
      "loss": 0.2802,
      "step": 13071
    },
    {
      "epoch": 26.301810865191147,
      "grad_norm": 0.7717357873916626,
      "learning_rate": 0.0001474112083710635,
      "loss": 0.2867,
      "step": 13072
    },
    {
      "epoch": 26.303822937625753,
      "grad_norm": 0.7757534980773926,
      "learning_rate": 0.00014740718382131001,
      "loss": 0.2924,
      "step": 13073
    },
    {
      "epoch": 26.305835010060363,
      "grad_norm": 0.7474990487098694,
      "learning_rate": 0.0001474031592715565,
      "loss": 0.305,
      "step": 13074
    },
    {
      "epoch": 26.30784708249497,
      "grad_norm": 0.7478392124176025,
      "learning_rate": 0.000147399134721803,
      "loss": 0.2793,
      "step": 13075
    },
    {
      "epoch": 26.309859154929576,
      "grad_norm": 0.7343162894248962,
      "learning_rate": 0.0001473951101720495,
      "loss": 0.2759,
      "step": 13076
    },
    {
      "epoch": 26.311871227364186,
      "grad_norm": 0.7672490477561951,
      "learning_rate": 0.000147391085622296,
      "loss": 0.2918,
      "step": 13077
    },
    {
      "epoch": 26.313883299798793,
      "grad_norm": 0.7327914237976074,
      "learning_rate": 0.00014738706107254252,
      "loss": 0.285,
      "step": 13078
    },
    {
      "epoch": 26.3158953722334,
      "grad_norm": 0.7701133489608765,
      "learning_rate": 0.00014738303652278903,
      "loss": 0.2765,
      "step": 13079
    },
    {
      "epoch": 26.31790744466801,
      "grad_norm": 0.7688388824462891,
      "learning_rate": 0.00014737901197303552,
      "loss": 0.2869,
      "step": 13080
    },
    {
      "epoch": 26.319919517102615,
      "grad_norm": 0.7242066264152527,
      "learning_rate": 0.00014737498742328203,
      "loss": 0.2496,
      "step": 13081
    },
    {
      "epoch": 26.321931589537222,
      "grad_norm": 0.7355383038520813,
      "learning_rate": 0.00014737096287352854,
      "loss": 0.2784,
      "step": 13082
    },
    {
      "epoch": 26.323943661971832,
      "grad_norm": 0.7438602447509766,
      "learning_rate": 0.00014736693832377503,
      "loss": 0.2596,
      "step": 13083
    },
    {
      "epoch": 26.32595573440644,
      "grad_norm": 0.7716643810272217,
      "learning_rate": 0.00014736291377402154,
      "loss": 0.3142,
      "step": 13084
    },
    {
      "epoch": 26.327967806841045,
      "grad_norm": 0.7795588970184326,
      "learning_rate": 0.00014735888922426803,
      "loss": 0.2857,
      "step": 13085
    },
    {
      "epoch": 26.329979879275655,
      "grad_norm": 0.7294042110443115,
      "learning_rate": 0.00014735486467451454,
      "loss": 0.2586,
      "step": 13086
    },
    {
      "epoch": 26.33199195171026,
      "grad_norm": 0.7510015368461609,
      "learning_rate": 0.00014735084012476105,
      "loss": 0.2846,
      "step": 13087
    },
    {
      "epoch": 26.334004024144868,
      "grad_norm": 0.7755683660507202,
      "learning_rate": 0.00014734681557500756,
      "loss": 0.261,
      "step": 13088
    },
    {
      "epoch": 26.336016096579478,
      "grad_norm": 0.7740966081619263,
      "learning_rate": 0.00014734279102525405,
      "loss": 0.2979,
      "step": 13089
    },
    {
      "epoch": 26.338028169014084,
      "grad_norm": 0.7691321969032288,
      "learning_rate": 0.00014733876647550056,
      "loss": 0.2596,
      "step": 13090
    },
    {
      "epoch": 26.34004024144869,
      "grad_norm": 0.782035768032074,
      "learning_rate": 0.00014733474192574704,
      "loss": 0.2698,
      "step": 13091
    },
    {
      "epoch": 26.3420523138833,
      "grad_norm": 0.8013231754302979,
      "learning_rate": 0.00014733071737599358,
      "loss": 0.2674,
      "step": 13092
    },
    {
      "epoch": 26.344064386317907,
      "grad_norm": 0.7910330891609192,
      "learning_rate": 0.00014732669282624007,
      "loss": 0.2987,
      "step": 13093
    },
    {
      "epoch": 26.346076458752513,
      "grad_norm": 0.7267515659332275,
      "learning_rate": 0.00014732266827648658,
      "loss": 0.2787,
      "step": 13094
    },
    {
      "epoch": 26.348088531187123,
      "grad_norm": 0.7852758765220642,
      "learning_rate": 0.00014731864372673307,
      "loss": 0.2952,
      "step": 13095
    },
    {
      "epoch": 26.35010060362173,
      "grad_norm": 0.7644734978675842,
      "learning_rate": 0.00014731461917697958,
      "loss": 0.2707,
      "step": 13096
    },
    {
      "epoch": 26.352112676056336,
      "grad_norm": 0.7435241937637329,
      "learning_rate": 0.0001473105946272261,
      "loss": 0.2598,
      "step": 13097
    },
    {
      "epoch": 26.354124748490946,
      "grad_norm": 0.7812638878822327,
      "learning_rate": 0.0001473065700774726,
      "loss": 0.2743,
      "step": 13098
    },
    {
      "epoch": 26.356136820925553,
      "grad_norm": 0.789479672908783,
      "learning_rate": 0.0001473025455277191,
      "loss": 0.2775,
      "step": 13099
    },
    {
      "epoch": 26.358148893360163,
      "grad_norm": 0.721778929233551,
      "learning_rate": 0.0001472985209779656,
      "loss": 0.2417,
      "step": 13100
    },
    {
      "epoch": 26.36016096579477,
      "grad_norm": 0.7402350306510925,
      "learning_rate": 0.00014729449642821209,
      "loss": 0.2643,
      "step": 13101
    },
    {
      "epoch": 26.362173038229376,
      "grad_norm": 0.7299712300300598,
      "learning_rate": 0.00014729047187845862,
      "loss": 0.2643,
      "step": 13102
    },
    {
      "epoch": 26.364185110663986,
      "grad_norm": 0.8277723789215088,
      "learning_rate": 0.0001472864473287051,
      "loss": 0.2992,
      "step": 13103
    },
    {
      "epoch": 26.366197183098592,
      "grad_norm": 0.7568168044090271,
      "learning_rate": 0.00014728242277895162,
      "loss": 0.2895,
      "step": 13104
    },
    {
      "epoch": 26.3682092555332,
      "grad_norm": 0.7537091374397278,
      "learning_rate": 0.0001472783982291981,
      "loss": 0.2651,
      "step": 13105
    },
    {
      "epoch": 26.37022132796781,
      "grad_norm": 0.7651384472846985,
      "learning_rate": 0.00014727437367944462,
      "loss": 0.3056,
      "step": 13106
    },
    {
      "epoch": 26.372233400402415,
      "grad_norm": 0.8050675392150879,
      "learning_rate": 0.00014727034912969113,
      "loss": 0.2997,
      "step": 13107
    },
    {
      "epoch": 26.37424547283702,
      "grad_norm": 0.7871271967887878,
      "learning_rate": 0.00014726632457993764,
      "loss": 0.2749,
      "step": 13108
    },
    {
      "epoch": 26.37625754527163,
      "grad_norm": 0.8370619416236877,
      "learning_rate": 0.00014726230003018413,
      "loss": 0.3132,
      "step": 13109
    },
    {
      "epoch": 26.378269617706238,
      "grad_norm": 0.7800084352493286,
      "learning_rate": 0.00014725827548043064,
      "loss": 0.2988,
      "step": 13110
    },
    {
      "epoch": 26.380281690140844,
      "grad_norm": 0.7903749346733093,
      "learning_rate": 0.00014725425093067713,
      "loss": 0.3053,
      "step": 13111
    },
    {
      "epoch": 26.382293762575454,
      "grad_norm": 0.7464198470115662,
      "learning_rate": 0.00014725022638092364,
      "loss": 0.2778,
      "step": 13112
    },
    {
      "epoch": 26.38430583501006,
      "grad_norm": 0.77472323179245,
      "learning_rate": 0.00014724620183117015,
      "loss": 0.2782,
      "step": 13113
    },
    {
      "epoch": 26.386317907444667,
      "grad_norm": 0.7505125999450684,
      "learning_rate": 0.00014724217728141664,
      "loss": 0.2644,
      "step": 13114
    },
    {
      "epoch": 26.388329979879277,
      "grad_norm": 0.7618382573127747,
      "learning_rate": 0.00014723815273166315,
      "loss": 0.303,
      "step": 13115
    },
    {
      "epoch": 26.390342052313883,
      "grad_norm": 0.7726380825042725,
      "learning_rate": 0.00014723412818190966,
      "loss": 0.2861,
      "step": 13116
    },
    {
      "epoch": 26.39235412474849,
      "grad_norm": 0.7728067636489868,
      "learning_rate": 0.00014723010363215617,
      "loss": 0.2599,
      "step": 13117
    },
    {
      "epoch": 26.3943661971831,
      "grad_norm": 0.7724218964576721,
      "learning_rate": 0.00014722607908240266,
      "loss": 0.2831,
      "step": 13118
    },
    {
      "epoch": 26.396378269617706,
      "grad_norm": 0.8221718072891235,
      "learning_rate": 0.00014722205453264917,
      "loss": 0.277,
      "step": 13119
    },
    {
      "epoch": 26.398390342052313,
      "grad_norm": 0.7771269679069519,
      "learning_rate": 0.00014721802998289565,
      "loss": 0.2853,
      "step": 13120
    },
    {
      "epoch": 26.400402414486923,
      "grad_norm": 0.8048074245452881,
      "learning_rate": 0.00014721400543314217,
      "loss": 0.29,
      "step": 13121
    },
    {
      "epoch": 26.40241448692153,
      "grad_norm": 0.8412725925445557,
      "learning_rate": 0.00014720998088338868,
      "loss": 0.2918,
      "step": 13122
    },
    {
      "epoch": 26.404426559356136,
      "grad_norm": 0.7756786346435547,
      "learning_rate": 0.0001472059563336352,
      "loss": 0.3084,
      "step": 13123
    },
    {
      "epoch": 26.406438631790746,
      "grad_norm": 0.7637746334075928,
      "learning_rate": 0.00014720193178388168,
      "loss": 0.2687,
      "step": 13124
    },
    {
      "epoch": 26.408450704225352,
      "grad_norm": 0.8160767555236816,
      "learning_rate": 0.0001471979072341282,
      "loss": 0.2813,
      "step": 13125
    },
    {
      "epoch": 26.41046277665996,
      "grad_norm": 0.7853091359138489,
      "learning_rate": 0.00014719388268437467,
      "loss": 0.2853,
      "step": 13126
    },
    {
      "epoch": 26.41247484909457,
      "grad_norm": 0.8092280030250549,
      "learning_rate": 0.0001471898581346212,
      "loss": 0.2864,
      "step": 13127
    },
    {
      "epoch": 26.414486921529175,
      "grad_norm": 0.7936654090881348,
      "learning_rate": 0.0001471858335848677,
      "loss": 0.2731,
      "step": 13128
    },
    {
      "epoch": 26.41649899396378,
      "grad_norm": 0.788178026676178,
      "learning_rate": 0.0001471818090351142,
      "loss": 0.2919,
      "step": 13129
    },
    {
      "epoch": 26.41851106639839,
      "grad_norm": 0.7720156908035278,
      "learning_rate": 0.0001471777844853607,
      "loss": 0.2618,
      "step": 13130
    },
    {
      "epoch": 26.420523138832998,
      "grad_norm": 0.7451662421226501,
      "learning_rate": 0.0001471737599356072,
      "loss": 0.2736,
      "step": 13131
    },
    {
      "epoch": 26.422535211267604,
      "grad_norm": 0.814243733882904,
      "learning_rate": 0.00014716973538585372,
      "loss": 0.2805,
      "step": 13132
    },
    {
      "epoch": 26.424547283702214,
      "grad_norm": 0.846132755279541,
      "learning_rate": 0.00014716571083610023,
      "loss": 0.2718,
      "step": 13133
    },
    {
      "epoch": 26.42655935613682,
      "grad_norm": 0.7777964472770691,
      "learning_rate": 0.00014716168628634672,
      "loss": 0.2795,
      "step": 13134
    },
    {
      "epoch": 26.428571428571427,
      "grad_norm": 0.7798660397529602,
      "learning_rate": 0.00014715766173659323,
      "loss": 0.2859,
      "step": 13135
    },
    {
      "epoch": 26.430583501006037,
      "grad_norm": 0.7398980855941772,
      "learning_rate": 0.00014715363718683971,
      "loss": 0.2681,
      "step": 13136
    },
    {
      "epoch": 26.432595573440643,
      "grad_norm": 0.7908593416213989,
      "learning_rate": 0.00014714961263708625,
      "loss": 0.2983,
      "step": 13137
    },
    {
      "epoch": 26.43460764587525,
      "grad_norm": 0.7779492139816284,
      "learning_rate": 0.00014714558808733274,
      "loss": 0.29,
      "step": 13138
    },
    {
      "epoch": 26.43661971830986,
      "grad_norm": 0.7979037165641785,
      "learning_rate": 0.00014714156353757925,
      "loss": 0.2923,
      "step": 13139
    },
    {
      "epoch": 26.438631790744466,
      "grad_norm": 0.7601906061172485,
      "learning_rate": 0.00014713753898782574,
      "loss": 0.2741,
      "step": 13140
    },
    {
      "epoch": 26.440643863179073,
      "grad_norm": 0.782455325126648,
      "learning_rate": 0.00014713351443807225,
      "loss": 0.2999,
      "step": 13141
    },
    {
      "epoch": 26.442655935613683,
      "grad_norm": 0.8168824315071106,
      "learning_rate": 0.00014712948988831876,
      "loss": 0.2918,
      "step": 13142
    },
    {
      "epoch": 26.44466800804829,
      "grad_norm": 0.770389974117279,
      "learning_rate": 0.00014712546533856527,
      "loss": 0.2845,
      "step": 13143
    },
    {
      "epoch": 26.446680080482896,
      "grad_norm": 0.833723247051239,
      "learning_rate": 0.00014712144078881176,
      "loss": 0.3167,
      "step": 13144
    },
    {
      "epoch": 26.448692152917506,
      "grad_norm": 0.7769092917442322,
      "learning_rate": 0.00014711741623905827,
      "loss": 0.313,
      "step": 13145
    },
    {
      "epoch": 26.450704225352112,
      "grad_norm": 0.7918782234191895,
      "learning_rate": 0.00014711339168930476,
      "loss": 0.3115,
      "step": 13146
    },
    {
      "epoch": 26.452716297786722,
      "grad_norm": 0.7725641131401062,
      "learning_rate": 0.00014710936713955127,
      "loss": 0.2762,
      "step": 13147
    },
    {
      "epoch": 26.45472837022133,
      "grad_norm": 0.8009546399116516,
      "learning_rate": 0.00014710534258979778,
      "loss": 0.2884,
      "step": 13148
    },
    {
      "epoch": 26.456740442655935,
      "grad_norm": 0.875322699546814,
      "learning_rate": 0.00014710131804004427,
      "loss": 0.2957,
      "step": 13149
    },
    {
      "epoch": 26.458752515090545,
      "grad_norm": 0.8564671874046326,
      "learning_rate": 0.00014709729349029078,
      "loss": 0.2921,
      "step": 13150
    },
    {
      "epoch": 26.46076458752515,
      "grad_norm": 0.7664124965667725,
      "learning_rate": 0.0001470932689405373,
      "loss": 0.2636,
      "step": 13151
    },
    {
      "epoch": 26.462776659959758,
      "grad_norm": 0.7514269351959229,
      "learning_rate": 0.0001470892443907838,
      "loss": 0.278,
      "step": 13152
    },
    {
      "epoch": 26.464788732394368,
      "grad_norm": 0.8424152135848999,
      "learning_rate": 0.0001470852198410303,
      "loss": 0.3099,
      "step": 13153
    },
    {
      "epoch": 26.466800804828974,
      "grad_norm": 0.797106146812439,
      "learning_rate": 0.0001470811952912768,
      "loss": 0.2804,
      "step": 13154
    },
    {
      "epoch": 26.46881287726358,
      "grad_norm": 0.7662606835365295,
      "learning_rate": 0.00014707717074152328,
      "loss": 0.2954,
      "step": 13155
    },
    {
      "epoch": 26.47082494969819,
      "grad_norm": 0.7851083278656006,
      "learning_rate": 0.0001470731461917698,
      "loss": 0.2778,
      "step": 13156
    },
    {
      "epoch": 26.472837022132797,
      "grad_norm": 0.7675397396087646,
      "learning_rate": 0.0001470691216420163,
      "loss": 0.288,
      "step": 13157
    },
    {
      "epoch": 26.474849094567404,
      "grad_norm": 0.8262749314308167,
      "learning_rate": 0.00014706509709226282,
      "loss": 0.2983,
      "step": 13158
    },
    {
      "epoch": 26.476861167002014,
      "grad_norm": 0.8170936107635498,
      "learning_rate": 0.0001470610725425093,
      "loss": 0.2848,
      "step": 13159
    },
    {
      "epoch": 26.47887323943662,
      "grad_norm": 0.8473361134529114,
      "learning_rate": 0.00014705704799275582,
      "loss": 0.2952,
      "step": 13160
    },
    {
      "epoch": 26.480885311871226,
      "grad_norm": 0.7657138705253601,
      "learning_rate": 0.0001470530234430023,
      "loss": 0.2862,
      "step": 13161
    },
    {
      "epoch": 26.482897384305836,
      "grad_norm": 0.79189532995224,
      "learning_rate": 0.00014704899889324884,
      "loss": 0.2971,
      "step": 13162
    },
    {
      "epoch": 26.484909456740443,
      "grad_norm": 0.7881865501403809,
      "learning_rate": 0.00014704497434349533,
      "loss": 0.2927,
      "step": 13163
    },
    {
      "epoch": 26.48692152917505,
      "grad_norm": 0.7991331815719604,
      "learning_rate": 0.00014704094979374184,
      "loss": 0.2818,
      "step": 13164
    },
    {
      "epoch": 26.48893360160966,
      "grad_norm": 0.7799150943756104,
      "learning_rate": 0.00014703692524398832,
      "loss": 0.3123,
      "step": 13165
    },
    {
      "epoch": 26.490945674044266,
      "grad_norm": 0.7948707938194275,
      "learning_rate": 0.00014703290069423484,
      "loss": 0.2992,
      "step": 13166
    },
    {
      "epoch": 26.492957746478872,
      "grad_norm": 0.8184642791748047,
      "learning_rate": 0.00014702887614448135,
      "loss": 0.2918,
      "step": 13167
    },
    {
      "epoch": 26.494969818913482,
      "grad_norm": 0.7720207571983337,
      "learning_rate": 0.00014702485159472786,
      "loss": 0.2478,
      "step": 13168
    },
    {
      "epoch": 26.49698189134809,
      "grad_norm": 0.7537051439285278,
      "learning_rate": 0.00014702082704497435,
      "loss": 0.2804,
      "step": 13169
    },
    {
      "epoch": 26.498993963782695,
      "grad_norm": 0.8190615773200989,
      "learning_rate": 0.00014701680249522086,
      "loss": 0.2928,
      "step": 13170
    },
    {
      "epoch": 26.501006036217305,
      "grad_norm": 0.8120908737182617,
      "learning_rate": 0.00014701277794546734,
      "loss": 0.2985,
      "step": 13171
    },
    {
      "epoch": 26.50301810865191,
      "grad_norm": 0.789377748966217,
      "learning_rate": 0.00014700875339571388,
      "loss": 0.3186,
      "step": 13172
    },
    {
      "epoch": 26.505030181086518,
      "grad_norm": 0.7935675978660583,
      "learning_rate": 0.00014700472884596037,
      "loss": 0.2918,
      "step": 13173
    },
    {
      "epoch": 26.507042253521128,
      "grad_norm": 0.7935305833816528,
      "learning_rate": 0.00014700070429620688,
      "loss": 0.2986,
      "step": 13174
    },
    {
      "epoch": 26.509054325955734,
      "grad_norm": 0.772011399269104,
      "learning_rate": 0.00014699667974645337,
      "loss": 0.2939,
      "step": 13175
    },
    {
      "epoch": 26.51106639839034,
      "grad_norm": 0.7510167360305786,
      "learning_rate": 0.00014699265519669988,
      "loss": 0.3112,
      "step": 13176
    },
    {
      "epoch": 26.51307847082495,
      "grad_norm": 0.768649697303772,
      "learning_rate": 0.0001469886306469464,
      "loss": 0.2774,
      "step": 13177
    },
    {
      "epoch": 26.515090543259557,
      "grad_norm": 0.8124745488166809,
      "learning_rate": 0.0001469846060971929,
      "loss": 0.2917,
      "step": 13178
    },
    {
      "epoch": 26.517102615694164,
      "grad_norm": 0.7640717029571533,
      "learning_rate": 0.0001469805815474394,
      "loss": 0.2847,
      "step": 13179
    },
    {
      "epoch": 26.519114688128774,
      "grad_norm": 0.7930320501327515,
      "learning_rate": 0.0001469765569976859,
      "loss": 0.3057,
      "step": 13180
    },
    {
      "epoch": 26.52112676056338,
      "grad_norm": 0.7911421060562134,
      "learning_rate": 0.00014697253244793238,
      "loss": 0.2779,
      "step": 13181
    },
    {
      "epoch": 26.523138832997986,
      "grad_norm": 0.7870489954948425,
      "learning_rate": 0.0001469685078981789,
      "loss": 0.2962,
      "step": 13182
    },
    {
      "epoch": 26.525150905432596,
      "grad_norm": 0.7550835013389587,
      "learning_rate": 0.0001469644833484254,
      "loss": 0.3021,
      "step": 13183
    },
    {
      "epoch": 26.527162977867203,
      "grad_norm": 0.7318613529205322,
      "learning_rate": 0.0001469604587986719,
      "loss": 0.2873,
      "step": 13184
    },
    {
      "epoch": 26.52917505030181,
      "grad_norm": 0.7602049708366394,
      "learning_rate": 0.0001469564342489184,
      "loss": 0.2954,
      "step": 13185
    },
    {
      "epoch": 26.53118712273642,
      "grad_norm": 0.8247461915016174,
      "learning_rate": 0.00014695240969916492,
      "loss": 0.2865,
      "step": 13186
    },
    {
      "epoch": 26.533199195171026,
      "grad_norm": 0.7722492814064026,
      "learning_rate": 0.00014694838514941143,
      "loss": 0.3047,
      "step": 13187
    },
    {
      "epoch": 26.535211267605632,
      "grad_norm": 0.7865883708000183,
      "learning_rate": 0.00014694436059965792,
      "loss": 0.2822,
      "step": 13188
    },
    {
      "epoch": 26.537223340040242,
      "grad_norm": 0.8073568940162659,
      "learning_rate": 0.00014694033604990443,
      "loss": 0.3086,
      "step": 13189
    },
    {
      "epoch": 26.53923541247485,
      "grad_norm": 0.7830729484558105,
      "learning_rate": 0.0001469363115001509,
      "loss": 0.2864,
      "step": 13190
    },
    {
      "epoch": 26.541247484909455,
      "grad_norm": 0.7909783720970154,
      "learning_rate": 0.00014693228695039743,
      "loss": 0.2873,
      "step": 13191
    },
    {
      "epoch": 26.543259557344065,
      "grad_norm": 0.7649182081222534,
      "learning_rate": 0.00014692826240064394,
      "loss": 0.2895,
      "step": 13192
    },
    {
      "epoch": 26.54527162977867,
      "grad_norm": 0.7935715317726135,
      "learning_rate": 0.00014692423785089045,
      "loss": 0.2878,
      "step": 13193
    },
    {
      "epoch": 26.547283702213278,
      "grad_norm": 0.7860292196273804,
      "learning_rate": 0.00014692021330113694,
      "loss": 0.2991,
      "step": 13194
    },
    {
      "epoch": 26.549295774647888,
      "grad_norm": 0.8111977577209473,
      "learning_rate": 0.00014691618875138345,
      "loss": 0.3157,
      "step": 13195
    },
    {
      "epoch": 26.551307847082494,
      "grad_norm": 0.7747527956962585,
      "learning_rate": 0.00014691216420162993,
      "loss": 0.3044,
      "step": 13196
    },
    {
      "epoch": 26.5533199195171,
      "grad_norm": 0.7742440700531006,
      "learning_rate": 0.00014690813965187647,
      "loss": 0.2864,
      "step": 13197
    },
    {
      "epoch": 26.55533199195171,
      "grad_norm": 0.7897234559059143,
      "learning_rate": 0.00014690411510212296,
      "loss": 0.3145,
      "step": 13198
    },
    {
      "epoch": 26.557344064386317,
      "grad_norm": 0.8086734414100647,
      "learning_rate": 0.00014690009055236947,
      "loss": 0.2781,
      "step": 13199
    },
    {
      "epoch": 26.559356136820927,
      "grad_norm": 0.8318352699279785,
      "learning_rate": 0.00014689606600261595,
      "loss": 0.2769,
      "step": 13200
    },
    {
      "epoch": 26.561368209255534,
      "grad_norm": 0.9052484631538391,
      "learning_rate": 0.00014689204145286247,
      "loss": 0.3242,
      "step": 13201
    },
    {
      "epoch": 26.56338028169014,
      "grad_norm": 0.8302906155586243,
      "learning_rate": 0.00014688801690310898,
      "loss": 0.3013,
      "step": 13202
    },
    {
      "epoch": 26.56539235412475,
      "grad_norm": 0.8087891340255737,
      "learning_rate": 0.0001468839923533555,
      "loss": 0.3138,
      "step": 13203
    },
    {
      "epoch": 26.567404426559357,
      "grad_norm": 0.7780822515487671,
      "learning_rate": 0.00014687996780360198,
      "loss": 0.272,
      "step": 13204
    },
    {
      "epoch": 26.569416498993963,
      "grad_norm": 0.8217222094535828,
      "learning_rate": 0.0001468759432538485,
      "loss": 0.3051,
      "step": 13205
    },
    {
      "epoch": 26.571428571428573,
      "grad_norm": 0.7486802935600281,
      "learning_rate": 0.00014687191870409497,
      "loss": 0.2802,
      "step": 13206
    },
    {
      "epoch": 26.57344064386318,
      "grad_norm": 0.7800775766372681,
      "learning_rate": 0.0001468678941543415,
      "loss": 0.2934,
      "step": 13207
    },
    {
      "epoch": 26.575452716297786,
      "grad_norm": 0.771699070930481,
      "learning_rate": 0.000146863869604588,
      "loss": 0.3001,
      "step": 13208
    },
    {
      "epoch": 26.577464788732396,
      "grad_norm": 0.7848093509674072,
      "learning_rate": 0.0001468598450548345,
      "loss": 0.2774,
      "step": 13209
    },
    {
      "epoch": 26.579476861167002,
      "grad_norm": 0.8102303743362427,
      "learning_rate": 0.000146855820505081,
      "loss": 0.2933,
      "step": 13210
    },
    {
      "epoch": 26.58148893360161,
      "grad_norm": 0.8223831057548523,
      "learning_rate": 0.0001468517959553275,
      "loss": 0.3026,
      "step": 13211
    },
    {
      "epoch": 26.58350100603622,
      "grad_norm": 0.7844117879867554,
      "learning_rate": 0.00014684777140557402,
      "loss": 0.2882,
      "step": 13212
    },
    {
      "epoch": 26.585513078470825,
      "grad_norm": 0.7888693809509277,
      "learning_rate": 0.00014684374685582053,
      "loss": 0.2839,
      "step": 13213
    },
    {
      "epoch": 26.58752515090543,
      "grad_norm": 0.7837543487548828,
      "learning_rate": 0.00014683972230606702,
      "loss": 0.3025,
      "step": 13214
    },
    {
      "epoch": 26.58953722334004,
      "grad_norm": 0.7762694358825684,
      "learning_rate": 0.00014683569775631353,
      "loss": 0.3014,
      "step": 13215
    },
    {
      "epoch": 26.591549295774648,
      "grad_norm": 0.8021271824836731,
      "learning_rate": 0.00014683167320656001,
      "loss": 0.2966,
      "step": 13216
    },
    {
      "epoch": 26.593561368209254,
      "grad_norm": 0.8124188780784607,
      "learning_rate": 0.00014682764865680653,
      "loss": 0.2965,
      "step": 13217
    },
    {
      "epoch": 26.595573440643864,
      "grad_norm": 0.8279063701629639,
      "learning_rate": 0.00014682362410705304,
      "loss": 0.3251,
      "step": 13218
    },
    {
      "epoch": 26.59758551307847,
      "grad_norm": 0.7623620629310608,
      "learning_rate": 0.00014681959955729952,
      "loss": 0.2996,
      "step": 13219
    },
    {
      "epoch": 26.599597585513077,
      "grad_norm": 0.7073361873626709,
      "learning_rate": 0.00014681557500754604,
      "loss": 0.2547,
      "step": 13220
    },
    {
      "epoch": 26.601609657947687,
      "grad_norm": 0.7801571488380432,
      "learning_rate": 0.00014681155045779255,
      "loss": 0.3049,
      "step": 13221
    },
    {
      "epoch": 26.603621730382294,
      "grad_norm": 0.8076186180114746,
      "learning_rate": 0.00014680752590803903,
      "loss": 0.3068,
      "step": 13222
    },
    {
      "epoch": 26.6056338028169,
      "grad_norm": 0.8128951191902161,
      "learning_rate": 0.00014680350135828555,
      "loss": 0.2994,
      "step": 13223
    },
    {
      "epoch": 26.60764587525151,
      "grad_norm": 0.8029482364654541,
      "learning_rate": 0.00014679947680853206,
      "loss": 0.3023,
      "step": 13224
    },
    {
      "epoch": 26.609657947686117,
      "grad_norm": 0.8047863841056824,
      "learning_rate": 0.00014679545225877854,
      "loss": 0.3115,
      "step": 13225
    },
    {
      "epoch": 26.611670020120723,
      "grad_norm": 0.7807989716529846,
      "learning_rate": 0.00014679142770902506,
      "loss": 0.3196,
      "step": 13226
    },
    {
      "epoch": 26.613682092555333,
      "grad_norm": 0.7637170553207397,
      "learning_rate": 0.00014678740315927154,
      "loss": 0.3016,
      "step": 13227
    },
    {
      "epoch": 26.61569416498994,
      "grad_norm": 0.7614160776138306,
      "learning_rate": 0.00014678337860951808,
      "loss": 0.2942,
      "step": 13228
    },
    {
      "epoch": 26.617706237424546,
      "grad_norm": 0.7900060415267944,
      "learning_rate": 0.00014677935405976456,
      "loss": 0.2985,
      "step": 13229
    },
    {
      "epoch": 26.619718309859156,
      "grad_norm": 0.8123592734336853,
      "learning_rate": 0.00014677532951001108,
      "loss": 0.2967,
      "step": 13230
    },
    {
      "epoch": 26.621730382293762,
      "grad_norm": 0.7759292125701904,
      "learning_rate": 0.00014677130496025756,
      "loss": 0.2852,
      "step": 13231
    },
    {
      "epoch": 26.62374245472837,
      "grad_norm": 0.808627188205719,
      "learning_rate": 0.00014676728041050407,
      "loss": 0.3179,
      "step": 13232
    },
    {
      "epoch": 26.62575452716298,
      "grad_norm": 0.7750090956687927,
      "learning_rate": 0.0001467632558607506,
      "loss": 0.2923,
      "step": 13233
    },
    {
      "epoch": 26.627766599597585,
      "grad_norm": 0.8700830936431885,
      "learning_rate": 0.0001467592313109971,
      "loss": 0.3229,
      "step": 13234
    },
    {
      "epoch": 26.62977867203219,
      "grad_norm": 0.7696011066436768,
      "learning_rate": 0.00014675520676124358,
      "loss": 0.3142,
      "step": 13235
    },
    {
      "epoch": 26.6317907444668,
      "grad_norm": 0.7914555072784424,
      "learning_rate": 0.0001467511822114901,
      "loss": 0.3112,
      "step": 13236
    },
    {
      "epoch": 26.633802816901408,
      "grad_norm": 0.7816293239593506,
      "learning_rate": 0.00014674715766173658,
      "loss": 0.2987,
      "step": 13237
    },
    {
      "epoch": 26.635814889336014,
      "grad_norm": 0.7877796292304993,
      "learning_rate": 0.00014674313311198312,
      "loss": 0.3003,
      "step": 13238
    },
    {
      "epoch": 26.637826961770624,
      "grad_norm": 0.7886332273483276,
      "learning_rate": 0.0001467391085622296,
      "loss": 0.3045,
      "step": 13239
    },
    {
      "epoch": 26.63983903420523,
      "grad_norm": 0.7607572078704834,
      "learning_rate": 0.00014673508401247612,
      "loss": 0.2688,
      "step": 13240
    },
    {
      "epoch": 26.641851106639837,
      "grad_norm": 0.7838104367256165,
      "learning_rate": 0.0001467310594627226,
      "loss": 0.3217,
      "step": 13241
    },
    {
      "epoch": 26.643863179074447,
      "grad_norm": 0.7444566488265991,
      "learning_rate": 0.00014672703491296912,
      "loss": 0.2583,
      "step": 13242
    },
    {
      "epoch": 26.645875251509054,
      "grad_norm": 0.8090526461601257,
      "learning_rate": 0.00014672301036321563,
      "loss": 0.3005,
      "step": 13243
    },
    {
      "epoch": 26.647887323943664,
      "grad_norm": 0.7803933620452881,
      "learning_rate": 0.00014671898581346214,
      "loss": 0.2983,
      "step": 13244
    },
    {
      "epoch": 26.64989939637827,
      "grad_norm": 0.7570638060569763,
      "learning_rate": 0.00014671496126370862,
      "loss": 0.2792,
      "step": 13245
    },
    {
      "epoch": 26.651911468812877,
      "grad_norm": 0.8082075119018555,
      "learning_rate": 0.00014671093671395514,
      "loss": 0.2894,
      "step": 13246
    },
    {
      "epoch": 26.653923541247487,
      "grad_norm": 0.7749688625335693,
      "learning_rate": 0.00014670691216420162,
      "loss": 0.2897,
      "step": 13247
    },
    {
      "epoch": 26.655935613682093,
      "grad_norm": 0.7675034403800964,
      "learning_rate": 0.00014670288761444816,
      "loss": 0.2723,
      "step": 13248
    },
    {
      "epoch": 26.6579476861167,
      "grad_norm": 0.7713668942451477,
      "learning_rate": 0.00014669886306469465,
      "loss": 0.3131,
      "step": 13249
    },
    {
      "epoch": 26.65995975855131,
      "grad_norm": 0.8095852732658386,
      "learning_rate": 0.00014669483851494116,
      "loss": 0.2861,
      "step": 13250
    },
    {
      "epoch": 26.661971830985916,
      "grad_norm": 0.7947977781295776,
      "learning_rate": 0.00014669081396518764,
      "loss": 0.3221,
      "step": 13251
    },
    {
      "epoch": 26.663983903420522,
      "grad_norm": 0.7069615125656128,
      "learning_rate": 0.00014668678941543416,
      "loss": 0.2491,
      "step": 13252
    },
    {
      "epoch": 26.665995975855132,
      "grad_norm": 0.7826388478279114,
      "learning_rate": 0.00014668276486568067,
      "loss": 0.2899,
      "step": 13253
    },
    {
      "epoch": 26.66800804828974,
      "grad_norm": 0.8021399974822998,
      "learning_rate": 0.00014667874031592715,
      "loss": 0.2988,
      "step": 13254
    },
    {
      "epoch": 26.670020120724345,
      "grad_norm": 0.812099277973175,
      "learning_rate": 0.00014667471576617367,
      "loss": 0.3034,
      "step": 13255
    },
    {
      "epoch": 26.672032193158955,
      "grad_norm": 0.8013314008712769,
      "learning_rate": 0.00014667069121642015,
      "loss": 0.2887,
      "step": 13256
    },
    {
      "epoch": 26.67404426559356,
      "grad_norm": 0.8174330592155457,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.3116,
      "step": 13257
    },
    {
      "epoch": 26.676056338028168,
      "grad_norm": 0.8108050227165222,
      "learning_rate": 0.00014666264211691318,
      "loss": 0.3144,
      "step": 13258
    },
    {
      "epoch": 26.678068410462778,
      "grad_norm": 0.7377672791481018,
      "learning_rate": 0.0001466586175671597,
      "loss": 0.2608,
      "step": 13259
    },
    {
      "epoch": 26.680080482897385,
      "grad_norm": 0.7618456482887268,
      "learning_rate": 0.00014665459301740617,
      "loss": 0.2683,
      "step": 13260
    },
    {
      "epoch": 26.68209255533199,
      "grad_norm": 0.7988891005516052,
      "learning_rate": 0.00014665056846765268,
      "loss": 0.309,
      "step": 13261
    },
    {
      "epoch": 26.6841046277666,
      "grad_norm": 0.7651079297065735,
      "learning_rate": 0.00014664654391789917,
      "loss": 0.2994,
      "step": 13262
    },
    {
      "epoch": 26.686116700201207,
      "grad_norm": 0.7911989092826843,
      "learning_rate": 0.0001466425193681457,
      "loss": 0.2899,
      "step": 13263
    },
    {
      "epoch": 26.688128772635814,
      "grad_norm": 0.7633535861968994,
      "learning_rate": 0.0001466384948183922,
      "loss": 0.2844,
      "step": 13264
    },
    {
      "epoch": 26.690140845070424,
      "grad_norm": 0.7707257866859436,
      "learning_rate": 0.0001466344702686387,
      "loss": 0.2926,
      "step": 13265
    },
    {
      "epoch": 26.69215291750503,
      "grad_norm": 0.7976931929588318,
      "learning_rate": 0.0001466304457188852,
      "loss": 0.295,
      "step": 13266
    },
    {
      "epoch": 26.694164989939637,
      "grad_norm": 0.7600091099739075,
      "learning_rate": 0.0001466264211691317,
      "loss": 0.2908,
      "step": 13267
    },
    {
      "epoch": 26.696177062374247,
      "grad_norm": 0.801088809967041,
      "learning_rate": 0.00014662239661937822,
      "loss": 0.3116,
      "step": 13268
    },
    {
      "epoch": 26.698189134808853,
      "grad_norm": 0.8036469221115112,
      "learning_rate": 0.00014661837206962473,
      "loss": 0.3162,
      "step": 13269
    },
    {
      "epoch": 26.70020120724346,
      "grad_norm": 0.7784300446510315,
      "learning_rate": 0.0001466143475198712,
      "loss": 0.3095,
      "step": 13270
    },
    {
      "epoch": 26.70221327967807,
      "grad_norm": 0.7775819301605225,
      "learning_rate": 0.00014661032297011773,
      "loss": 0.3165,
      "step": 13271
    },
    {
      "epoch": 26.704225352112676,
      "grad_norm": 0.7966600060462952,
      "learning_rate": 0.0001466062984203642,
      "loss": 0.3233,
      "step": 13272
    },
    {
      "epoch": 26.706237424547282,
      "grad_norm": 0.7994416356086731,
      "learning_rate": 0.00014660227387061075,
      "loss": 0.3357,
      "step": 13273
    },
    {
      "epoch": 26.708249496981892,
      "grad_norm": 0.7891992330551147,
      "learning_rate": 0.00014659824932085724,
      "loss": 0.2758,
      "step": 13274
    },
    {
      "epoch": 26.7102615694165,
      "grad_norm": 0.7632381319999695,
      "learning_rate": 0.00014659422477110375,
      "loss": 0.3125,
      "step": 13275
    },
    {
      "epoch": 26.712273641851105,
      "grad_norm": 0.8036351799964905,
      "learning_rate": 0.00014659020022135023,
      "loss": 0.302,
      "step": 13276
    },
    {
      "epoch": 26.714285714285715,
      "grad_norm": 0.8380252122879028,
      "learning_rate": 0.00014658617567159674,
      "loss": 0.3166,
      "step": 13277
    },
    {
      "epoch": 26.71629778672032,
      "grad_norm": 0.7961813807487488,
      "learning_rate": 0.00014658215112184326,
      "loss": 0.3257,
      "step": 13278
    },
    {
      "epoch": 26.718309859154928,
      "grad_norm": 0.7535924315452576,
      "learning_rate": 0.00014657812657208977,
      "loss": 0.2971,
      "step": 13279
    },
    {
      "epoch": 26.720321931589538,
      "grad_norm": 0.7680404782295227,
      "learning_rate": 0.00014657410202233625,
      "loss": 0.3155,
      "step": 13280
    },
    {
      "epoch": 26.722334004024145,
      "grad_norm": 0.7627226114273071,
      "learning_rate": 0.00014657007747258277,
      "loss": 0.2886,
      "step": 13281
    },
    {
      "epoch": 26.72434607645875,
      "grad_norm": 0.7921867966651917,
      "learning_rate": 0.00014656605292282925,
      "loss": 0.3137,
      "step": 13282
    },
    {
      "epoch": 26.72635814889336,
      "grad_norm": 0.7721922993659973,
      "learning_rate": 0.00014656202837307576,
      "loss": 0.3026,
      "step": 13283
    },
    {
      "epoch": 26.728370221327967,
      "grad_norm": 0.8236112594604492,
      "learning_rate": 0.00014655800382332228,
      "loss": 0.3236,
      "step": 13284
    },
    {
      "epoch": 26.730382293762574,
      "grad_norm": 0.8409190773963928,
      "learning_rate": 0.0001465539792735688,
      "loss": 0.2968,
      "step": 13285
    },
    {
      "epoch": 26.732394366197184,
      "grad_norm": 0.8312535285949707,
      "learning_rate": 0.00014654995472381527,
      "loss": 0.309,
      "step": 13286
    },
    {
      "epoch": 26.73440643863179,
      "grad_norm": 0.7973951101303101,
      "learning_rate": 0.00014654593017406179,
      "loss": 0.3073,
      "step": 13287
    },
    {
      "epoch": 26.736418511066397,
      "grad_norm": 0.7266796231269836,
      "learning_rate": 0.0001465419056243083,
      "loss": 0.2823,
      "step": 13288
    },
    {
      "epoch": 26.738430583501007,
      "grad_norm": 0.7902964353561401,
      "learning_rate": 0.00014653788107455478,
      "loss": 0.317,
      "step": 13289
    },
    {
      "epoch": 26.740442655935613,
      "grad_norm": 0.8056845664978027,
      "learning_rate": 0.0001465338565248013,
      "loss": 0.3079,
      "step": 13290
    },
    {
      "epoch": 26.74245472837022,
      "grad_norm": 0.7769622802734375,
      "learning_rate": 0.00014652983197504778,
      "loss": 0.3009,
      "step": 13291
    },
    {
      "epoch": 26.74446680080483,
      "grad_norm": 0.7852623462677002,
      "learning_rate": 0.0001465258074252943,
      "loss": 0.3153,
      "step": 13292
    },
    {
      "epoch": 26.746478873239436,
      "grad_norm": 0.8078077435493469,
      "learning_rate": 0.0001465217828755408,
      "loss": 0.2945,
      "step": 13293
    },
    {
      "epoch": 26.748490945674043,
      "grad_norm": 0.7758417725563049,
      "learning_rate": 0.00014651775832578732,
      "loss": 0.2969,
      "step": 13294
    },
    {
      "epoch": 26.750503018108652,
      "grad_norm": 0.826145350933075,
      "learning_rate": 0.0001465137337760338,
      "loss": 0.3128,
      "step": 13295
    },
    {
      "epoch": 26.75251509054326,
      "grad_norm": 0.7900973558425903,
      "learning_rate": 0.00014650970922628031,
      "loss": 0.3158,
      "step": 13296
    },
    {
      "epoch": 26.754527162977865,
      "grad_norm": 0.7818374037742615,
      "learning_rate": 0.0001465056846765268,
      "loss": 0.2976,
      "step": 13297
    },
    {
      "epoch": 26.756539235412475,
      "grad_norm": 0.7758556008338928,
      "learning_rate": 0.00014650166012677334,
      "loss": 0.3142,
      "step": 13298
    },
    {
      "epoch": 26.758551307847082,
      "grad_norm": 0.7782140970230103,
      "learning_rate": 0.00014649763557701982,
      "loss": 0.2924,
      "step": 13299
    },
    {
      "epoch": 26.760563380281692,
      "grad_norm": 0.8259859681129456,
      "learning_rate": 0.00014649361102726634,
      "loss": 0.3167,
      "step": 13300
    },
    {
      "epoch": 26.7625754527163,
      "grad_norm": 0.7708352208137512,
      "learning_rate": 0.00014648958647751282,
      "loss": 0.3155,
      "step": 13301
    },
    {
      "epoch": 26.764587525150905,
      "grad_norm": 0.7892819046974182,
      "learning_rate": 0.00014648556192775933,
      "loss": 0.304,
      "step": 13302
    },
    {
      "epoch": 26.766599597585515,
      "grad_norm": 0.7706382274627686,
      "learning_rate": 0.00014648153737800585,
      "loss": 0.3298,
      "step": 13303
    },
    {
      "epoch": 26.76861167002012,
      "grad_norm": 0.7693836092948914,
      "learning_rate": 0.00014647751282825236,
      "loss": 0.2961,
      "step": 13304
    },
    {
      "epoch": 26.770623742454728,
      "grad_norm": 0.7666906118392944,
      "learning_rate": 0.00014647348827849884,
      "loss": 0.2958,
      "step": 13305
    },
    {
      "epoch": 26.772635814889338,
      "grad_norm": 0.7799569964408875,
      "learning_rate": 0.00014646946372874535,
      "loss": 0.2978,
      "step": 13306
    },
    {
      "epoch": 26.774647887323944,
      "grad_norm": 0.7877351641654968,
      "learning_rate": 0.00014646543917899184,
      "loss": 0.3074,
      "step": 13307
    },
    {
      "epoch": 26.77665995975855,
      "grad_norm": 0.766084611415863,
      "learning_rate": 0.00014646141462923838,
      "loss": 0.3122,
      "step": 13308
    },
    {
      "epoch": 26.77867203219316,
      "grad_norm": 0.7762923836708069,
      "learning_rate": 0.00014645739007948486,
      "loss": 0.2977,
      "step": 13309
    },
    {
      "epoch": 26.780684104627767,
      "grad_norm": 0.7694496512413025,
      "learning_rate": 0.00014645336552973138,
      "loss": 0.2918,
      "step": 13310
    },
    {
      "epoch": 26.782696177062373,
      "grad_norm": 0.8080064058303833,
      "learning_rate": 0.00014644934097997786,
      "loss": 0.2822,
      "step": 13311
    },
    {
      "epoch": 26.784708249496983,
      "grad_norm": 0.7533226013183594,
      "learning_rate": 0.00014644531643022437,
      "loss": 0.2937,
      "step": 13312
    },
    {
      "epoch": 26.78672032193159,
      "grad_norm": 0.8531554341316223,
      "learning_rate": 0.00014644129188047089,
      "loss": 0.3303,
      "step": 13313
    },
    {
      "epoch": 26.788732394366196,
      "grad_norm": 0.7884901762008667,
      "learning_rate": 0.0001464372673307174,
      "loss": 0.2925,
      "step": 13314
    },
    {
      "epoch": 26.790744466800806,
      "grad_norm": 0.7957705855369568,
      "learning_rate": 0.00014643324278096388,
      "loss": 0.305,
      "step": 13315
    },
    {
      "epoch": 26.792756539235413,
      "grad_norm": 0.8182687759399414,
      "learning_rate": 0.0001464292182312104,
      "loss": 0.3123,
      "step": 13316
    },
    {
      "epoch": 26.79476861167002,
      "grad_norm": 0.8195953965187073,
      "learning_rate": 0.00014642519368145688,
      "loss": 0.3287,
      "step": 13317
    },
    {
      "epoch": 26.79678068410463,
      "grad_norm": 0.7829766869544983,
      "learning_rate": 0.0001464211691317034,
      "loss": 0.2824,
      "step": 13318
    },
    {
      "epoch": 26.798792756539235,
      "grad_norm": 0.7582559585571289,
      "learning_rate": 0.0001464171445819499,
      "loss": 0.2767,
      "step": 13319
    },
    {
      "epoch": 26.800804828973842,
      "grad_norm": 0.7627456784248352,
      "learning_rate": 0.00014641312003219642,
      "loss": 0.2906,
      "step": 13320
    },
    {
      "epoch": 26.802816901408452,
      "grad_norm": 0.775836706161499,
      "learning_rate": 0.0001464090954824429,
      "loss": 0.299,
      "step": 13321
    },
    {
      "epoch": 26.80482897384306,
      "grad_norm": 0.763038694858551,
      "learning_rate": 0.00014640507093268941,
      "loss": 0.2891,
      "step": 13322
    },
    {
      "epoch": 26.806841046277665,
      "grad_norm": 0.8026639223098755,
      "learning_rate": 0.00014640104638293593,
      "loss": 0.3054,
      "step": 13323
    },
    {
      "epoch": 26.808853118712275,
      "grad_norm": 0.8107534050941467,
      "learning_rate": 0.0001463970218331824,
      "loss": 0.3044,
      "step": 13324
    },
    {
      "epoch": 26.81086519114688,
      "grad_norm": 0.786898672580719,
      "learning_rate": 0.00014639299728342892,
      "loss": 0.3103,
      "step": 13325
    },
    {
      "epoch": 26.812877263581488,
      "grad_norm": 0.8370281457901001,
      "learning_rate": 0.0001463889727336754,
      "loss": 0.3108,
      "step": 13326
    },
    {
      "epoch": 26.814889336016098,
      "grad_norm": 0.7970697283744812,
      "learning_rate": 0.00014638494818392192,
      "loss": 0.305,
      "step": 13327
    },
    {
      "epoch": 26.816901408450704,
      "grad_norm": 0.7530011534690857,
      "learning_rate": 0.00014638092363416843,
      "loss": 0.2848,
      "step": 13328
    },
    {
      "epoch": 26.81891348088531,
      "grad_norm": 0.7667437195777893,
      "learning_rate": 0.00014637689908441495,
      "loss": 0.3082,
      "step": 13329
    },
    {
      "epoch": 26.82092555331992,
      "grad_norm": 0.7674480676651001,
      "learning_rate": 0.00014637287453466143,
      "loss": 0.2951,
      "step": 13330
    },
    {
      "epoch": 26.822937625754527,
      "grad_norm": 0.7982487678527832,
      "learning_rate": 0.00014636884998490794,
      "loss": 0.3176,
      "step": 13331
    },
    {
      "epoch": 26.824949698189133,
      "grad_norm": 0.791543185710907,
      "learning_rate": 0.00014636482543515443,
      "loss": 0.2965,
      "step": 13332
    },
    {
      "epoch": 26.826961770623743,
      "grad_norm": 0.8228899836540222,
      "learning_rate": 0.00014636080088540097,
      "loss": 0.2914,
      "step": 13333
    },
    {
      "epoch": 26.82897384305835,
      "grad_norm": 0.7810763716697693,
      "learning_rate": 0.00014635677633564745,
      "loss": 0.3077,
      "step": 13334
    },
    {
      "epoch": 26.830985915492956,
      "grad_norm": 0.8255385756492615,
      "learning_rate": 0.00014635275178589397,
      "loss": 0.2982,
      "step": 13335
    },
    {
      "epoch": 26.832997987927566,
      "grad_norm": 0.8344732522964478,
      "learning_rate": 0.00014634872723614045,
      "loss": 0.3297,
      "step": 13336
    },
    {
      "epoch": 26.835010060362173,
      "grad_norm": 0.7707495093345642,
      "learning_rate": 0.00014634470268638696,
      "loss": 0.3237,
      "step": 13337
    },
    {
      "epoch": 26.83702213279678,
      "grad_norm": 0.7772203683853149,
      "learning_rate": 0.00014634067813663347,
      "loss": 0.275,
      "step": 13338
    },
    {
      "epoch": 26.83903420523139,
      "grad_norm": 0.7485299706459045,
      "learning_rate": 0.00014633665358688,
      "loss": 0.2886,
      "step": 13339
    },
    {
      "epoch": 26.841046277665995,
      "grad_norm": 0.7829325795173645,
      "learning_rate": 0.00014633262903712647,
      "loss": 0.3147,
      "step": 13340
    },
    {
      "epoch": 26.843058350100602,
      "grad_norm": 0.7814450860023499,
      "learning_rate": 0.00014632860448737298,
      "loss": 0.3147,
      "step": 13341
    },
    {
      "epoch": 26.845070422535212,
      "grad_norm": 0.8248367309570312,
      "learning_rate": 0.00014632457993761947,
      "loss": 0.319,
      "step": 13342
    },
    {
      "epoch": 26.84708249496982,
      "grad_norm": 0.8072260618209839,
      "learning_rate": 0.000146320555387866,
      "loss": 0.2934,
      "step": 13343
    },
    {
      "epoch": 26.84909456740443,
      "grad_norm": 0.7768098711967468,
      "learning_rate": 0.0001463165308381125,
      "loss": 0.2801,
      "step": 13344
    },
    {
      "epoch": 26.851106639839035,
      "grad_norm": 0.8647803664207458,
      "learning_rate": 0.000146312506288359,
      "loss": 0.328,
      "step": 13345
    },
    {
      "epoch": 26.85311871227364,
      "grad_norm": 0.8311938643455505,
      "learning_rate": 0.0001463084817386055,
      "loss": 0.3317,
      "step": 13346
    },
    {
      "epoch": 26.85513078470825,
      "grad_norm": 0.8529986143112183,
      "learning_rate": 0.000146304457188852,
      "loss": 0.299,
      "step": 13347
    },
    {
      "epoch": 26.857142857142858,
      "grad_norm": 0.8102489709854126,
      "learning_rate": 0.00014630043263909852,
      "loss": 0.2937,
      "step": 13348
    },
    {
      "epoch": 26.859154929577464,
      "grad_norm": 0.8505908846855164,
      "learning_rate": 0.00014629640808934503,
      "loss": 0.3023,
      "step": 13349
    },
    {
      "epoch": 26.861167002012074,
      "grad_norm": 0.8194571733474731,
      "learning_rate": 0.0001462923835395915,
      "loss": 0.3215,
      "step": 13350
    },
    {
      "epoch": 26.86317907444668,
      "grad_norm": 0.794169545173645,
      "learning_rate": 0.00014628835898983803,
      "loss": 0.3087,
      "step": 13351
    },
    {
      "epoch": 26.865191146881287,
      "grad_norm": 0.7896736860275269,
      "learning_rate": 0.0001462843344400845,
      "loss": 0.3145,
      "step": 13352
    },
    {
      "epoch": 26.867203219315897,
      "grad_norm": 0.8000636696815491,
      "learning_rate": 0.00014628030989033102,
      "loss": 0.3214,
      "step": 13353
    },
    {
      "epoch": 26.869215291750503,
      "grad_norm": 0.8701944947242737,
      "learning_rate": 0.00014627628534057753,
      "loss": 0.355,
      "step": 13354
    },
    {
      "epoch": 26.87122736418511,
      "grad_norm": 0.8238368630409241,
      "learning_rate": 0.00014627226079082405,
      "loss": 0.3068,
      "step": 13355
    },
    {
      "epoch": 26.87323943661972,
      "grad_norm": 0.7916585206985474,
      "learning_rate": 0.00014626823624107053,
      "loss": 0.3436,
      "step": 13356
    },
    {
      "epoch": 26.875251509054326,
      "grad_norm": 0.767417848110199,
      "learning_rate": 0.00014626421169131704,
      "loss": 0.3115,
      "step": 13357
    },
    {
      "epoch": 26.877263581488933,
      "grad_norm": 0.7823919653892517,
      "learning_rate": 0.00014626018714156356,
      "loss": 0.3344,
      "step": 13358
    },
    {
      "epoch": 26.879275653923543,
      "grad_norm": 0.8013547658920288,
      "learning_rate": 0.00014625616259181004,
      "loss": 0.3142,
      "step": 13359
    },
    {
      "epoch": 26.88128772635815,
      "grad_norm": 0.8410587310791016,
      "learning_rate": 0.00014625213804205655,
      "loss": 0.3187,
      "step": 13360
    },
    {
      "epoch": 26.883299798792756,
      "grad_norm": 0.8044654726982117,
      "learning_rate": 0.00014624811349230304,
      "loss": 0.3397,
      "step": 13361
    },
    {
      "epoch": 26.885311871227366,
      "grad_norm": 0.7955734729766846,
      "learning_rate": 0.00014624408894254955,
      "loss": 0.3038,
      "step": 13362
    },
    {
      "epoch": 26.887323943661972,
      "grad_norm": 0.7782137989997864,
      "learning_rate": 0.00014624006439279606,
      "loss": 0.2945,
      "step": 13363
    },
    {
      "epoch": 26.88933601609658,
      "grad_norm": 0.7978533506393433,
      "learning_rate": 0.00014623603984304258,
      "loss": 0.3296,
      "step": 13364
    },
    {
      "epoch": 26.89134808853119,
      "grad_norm": 0.7770602703094482,
      "learning_rate": 0.00014623201529328906,
      "loss": 0.2911,
      "step": 13365
    },
    {
      "epoch": 26.893360160965795,
      "grad_norm": 0.7991673350334167,
      "learning_rate": 0.00014622799074353557,
      "loss": 0.3076,
      "step": 13366
    },
    {
      "epoch": 26.8953722334004,
      "grad_norm": 0.7652735710144043,
      "learning_rate": 0.00014622396619378206,
      "loss": 0.2978,
      "step": 13367
    },
    {
      "epoch": 26.89738430583501,
      "grad_norm": 0.7934262752532959,
      "learning_rate": 0.0001462199416440286,
      "loss": 0.2946,
      "step": 13368
    },
    {
      "epoch": 26.899396378269618,
      "grad_norm": 0.8596733212471008,
      "learning_rate": 0.00014621591709427508,
      "loss": 0.3322,
      "step": 13369
    },
    {
      "epoch": 26.901408450704224,
      "grad_norm": 0.8449679017066956,
      "learning_rate": 0.0001462118925445216,
      "loss": 0.3121,
      "step": 13370
    },
    {
      "epoch": 26.903420523138834,
      "grad_norm": 0.831450343132019,
      "learning_rate": 0.00014620786799476808,
      "loss": 0.3114,
      "step": 13371
    },
    {
      "epoch": 26.90543259557344,
      "grad_norm": 0.8614221811294556,
      "learning_rate": 0.0001462038434450146,
      "loss": 0.2947,
      "step": 13372
    },
    {
      "epoch": 26.907444668008047,
      "grad_norm": 0.8345218896865845,
      "learning_rate": 0.0001461998188952611,
      "loss": 0.3431,
      "step": 13373
    },
    {
      "epoch": 26.909456740442657,
      "grad_norm": 0.8001013398170471,
      "learning_rate": 0.00014619579434550762,
      "loss": 0.3286,
      "step": 13374
    },
    {
      "epoch": 26.911468812877263,
      "grad_norm": 0.8136001229286194,
      "learning_rate": 0.0001461917697957541,
      "loss": 0.3091,
      "step": 13375
    },
    {
      "epoch": 26.91348088531187,
      "grad_norm": 0.7517308592796326,
      "learning_rate": 0.00014618774524600061,
      "loss": 0.2939,
      "step": 13376
    },
    {
      "epoch": 26.91549295774648,
      "grad_norm": 0.7868747115135193,
      "learning_rate": 0.0001461837206962471,
      "loss": 0.3122,
      "step": 13377
    },
    {
      "epoch": 26.917505030181086,
      "grad_norm": 0.8187644481658936,
      "learning_rate": 0.00014617969614649364,
      "loss": 0.3209,
      "step": 13378
    },
    {
      "epoch": 26.919517102615693,
      "grad_norm": 0.8217620849609375,
      "learning_rate": 0.00014617567159674012,
      "loss": 0.3015,
      "step": 13379
    },
    {
      "epoch": 26.921529175050303,
      "grad_norm": 0.8228644728660583,
      "learning_rate": 0.00014617164704698664,
      "loss": 0.3055,
      "step": 13380
    },
    {
      "epoch": 26.92354124748491,
      "grad_norm": 0.775921106338501,
      "learning_rate": 0.00014616762249723312,
      "loss": 0.32,
      "step": 13381
    },
    {
      "epoch": 26.925553319919516,
      "grad_norm": 0.7899811863899231,
      "learning_rate": 0.00014616359794747963,
      "loss": 0.2958,
      "step": 13382
    },
    {
      "epoch": 26.927565392354126,
      "grad_norm": 0.734868049621582,
      "learning_rate": 0.00014615957339772615,
      "loss": 0.2744,
      "step": 13383
    },
    {
      "epoch": 26.929577464788732,
      "grad_norm": 0.8022879958152771,
      "learning_rate": 0.00014615554884797266,
      "loss": 0.3344,
      "step": 13384
    },
    {
      "epoch": 26.93158953722334,
      "grad_norm": 0.8551884293556213,
      "learning_rate": 0.00014615152429821914,
      "loss": 0.3364,
      "step": 13385
    },
    {
      "epoch": 26.93360160965795,
      "grad_norm": 0.8030979633331299,
      "learning_rate": 0.00014614749974846565,
      "loss": 0.313,
      "step": 13386
    },
    {
      "epoch": 26.935613682092555,
      "grad_norm": 0.7541840672492981,
      "learning_rate": 0.00014614347519871214,
      "loss": 0.2751,
      "step": 13387
    },
    {
      "epoch": 26.93762575452716,
      "grad_norm": 0.7685481309890747,
      "learning_rate": 0.00014613945064895865,
      "loss": 0.2943,
      "step": 13388
    },
    {
      "epoch": 26.93963782696177,
      "grad_norm": 0.7487296462059021,
      "learning_rate": 0.00014613542609920516,
      "loss": 0.2895,
      "step": 13389
    },
    {
      "epoch": 26.941649899396378,
      "grad_norm": 0.8219616413116455,
      "learning_rate": 0.00014613140154945168,
      "loss": 0.3161,
      "step": 13390
    },
    {
      "epoch": 26.943661971830984,
      "grad_norm": 0.8243628144264221,
      "learning_rate": 0.00014612737699969816,
      "loss": 0.3017,
      "step": 13391
    },
    {
      "epoch": 26.945674044265594,
      "grad_norm": 0.7676599025726318,
      "learning_rate": 0.00014612335244994467,
      "loss": 0.2964,
      "step": 13392
    },
    {
      "epoch": 26.9476861167002,
      "grad_norm": 0.7910570502281189,
      "learning_rate": 0.00014611932790019119,
      "loss": 0.3161,
      "step": 13393
    },
    {
      "epoch": 26.949698189134807,
      "grad_norm": 0.7847491502761841,
      "learning_rate": 0.00014611530335043767,
      "loss": 0.3149,
      "step": 13394
    },
    {
      "epoch": 26.951710261569417,
      "grad_norm": 0.7947232723236084,
      "learning_rate": 0.00014611127880068418,
      "loss": 0.314,
      "step": 13395
    },
    {
      "epoch": 26.953722334004024,
      "grad_norm": 0.7892351150512695,
      "learning_rate": 0.00014610725425093067,
      "loss": 0.297,
      "step": 13396
    },
    {
      "epoch": 26.955734406438633,
      "grad_norm": 0.8074767589569092,
      "learning_rate": 0.00014610322970117718,
      "loss": 0.317,
      "step": 13397
    },
    {
      "epoch": 26.95774647887324,
      "grad_norm": 0.78666752576828,
      "learning_rate": 0.0001460992051514237,
      "loss": 0.3082,
      "step": 13398
    },
    {
      "epoch": 26.959758551307846,
      "grad_norm": 0.796024739742279,
      "learning_rate": 0.0001460951806016702,
      "loss": 0.3244,
      "step": 13399
    },
    {
      "epoch": 26.961770623742456,
      "grad_norm": 0.7864651083946228,
      "learning_rate": 0.0001460911560519167,
      "loss": 0.3068,
      "step": 13400
    },
    {
      "epoch": 26.963782696177063,
      "grad_norm": 0.7479676604270935,
      "learning_rate": 0.0001460871315021632,
      "loss": 0.3095,
      "step": 13401
    },
    {
      "epoch": 26.96579476861167,
      "grad_norm": 0.8053016662597656,
      "learning_rate": 0.0001460831069524097,
      "loss": 0.3018,
      "step": 13402
    },
    {
      "epoch": 26.96780684104628,
      "grad_norm": 0.813685417175293,
      "learning_rate": 0.00014607908240265623,
      "loss": 0.324,
      "step": 13403
    },
    {
      "epoch": 26.969818913480886,
      "grad_norm": 0.7502779364585876,
      "learning_rate": 0.0001460750578529027,
      "loss": 0.3046,
      "step": 13404
    },
    {
      "epoch": 26.971830985915492,
      "grad_norm": 0.8479347229003906,
      "learning_rate": 0.00014607103330314922,
      "loss": 0.3201,
      "step": 13405
    },
    {
      "epoch": 26.973843058350102,
      "grad_norm": 0.7702706456184387,
      "learning_rate": 0.0001460670087533957,
      "loss": 0.2846,
      "step": 13406
    },
    {
      "epoch": 26.97585513078471,
      "grad_norm": 0.7851144671440125,
      "learning_rate": 0.00014606298420364222,
      "loss": 0.3164,
      "step": 13407
    },
    {
      "epoch": 26.977867203219315,
      "grad_norm": 0.843172550201416,
      "learning_rate": 0.00014605895965388873,
      "loss": 0.2979,
      "step": 13408
    },
    {
      "epoch": 26.979879275653925,
      "grad_norm": 0.7594695091247559,
      "learning_rate": 0.00014605493510413525,
      "loss": 0.3379,
      "step": 13409
    },
    {
      "epoch": 26.98189134808853,
      "grad_norm": 0.7962122559547424,
      "learning_rate": 0.00014605091055438173,
      "loss": 0.3112,
      "step": 13410
    },
    {
      "epoch": 26.983903420523138,
      "grad_norm": 0.7470353841781616,
      "learning_rate": 0.00014604688600462824,
      "loss": 0.2957,
      "step": 13411
    },
    {
      "epoch": 26.985915492957748,
      "grad_norm": 0.7759186029434204,
      "learning_rate": 0.00014604286145487473,
      "loss": 0.3117,
      "step": 13412
    },
    {
      "epoch": 26.987927565392354,
      "grad_norm": 0.7950389981269836,
      "learning_rate": 0.00014603883690512127,
      "loss": 0.3068,
      "step": 13413
    },
    {
      "epoch": 26.98993963782696,
      "grad_norm": 0.7938560247421265,
      "learning_rate": 0.00014603481235536775,
      "loss": 0.3104,
      "step": 13414
    },
    {
      "epoch": 26.99195171026157,
      "grad_norm": 0.7843371629714966,
      "learning_rate": 0.00014603078780561426,
      "loss": 0.3086,
      "step": 13415
    },
    {
      "epoch": 26.993963782696177,
      "grad_norm": 0.8037952780723572,
      "learning_rate": 0.00014602676325586075,
      "loss": 0.3014,
      "step": 13416
    },
    {
      "epoch": 26.995975855130784,
      "grad_norm": 0.7913702726364136,
      "learning_rate": 0.00014602273870610726,
      "loss": 0.3217,
      "step": 13417
    },
    {
      "epoch": 26.997987927565394,
      "grad_norm": 0.8316432237625122,
      "learning_rate": 0.00014601871415635377,
      "loss": 0.3279,
      "step": 13418
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.8420460224151611,
      "learning_rate": 0.0001460146896066003,
      "loss": 0.3229,
      "step": 13419
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.1806941032409668,
      "eval_runtime": 49.8651,
      "eval_samples_per_second": 19.894,
      "eval_steps_per_second": 2.487,
      "step": 13419
    },
    {
      "epoch": 27.002012072434606,
      "grad_norm": 0.635467529296875,
      "learning_rate": 0.00014601066505684677,
      "loss": 0.2313,
      "step": 13420
    },
    {
      "epoch": 27.004024144869216,
      "grad_norm": 0.6917542219161987,
      "learning_rate": 0.00014600664050709328,
      "loss": 0.2476,
      "step": 13421
    },
    {
      "epoch": 27.006036217303823,
      "grad_norm": 0.6613900661468506,
      "learning_rate": 0.00014600261595733977,
      "loss": 0.2333,
      "step": 13422
    },
    {
      "epoch": 27.00804828973843,
      "grad_norm": 0.7052215337753296,
      "learning_rate": 0.00014599859140758628,
      "loss": 0.2296,
      "step": 13423
    },
    {
      "epoch": 27.01006036217304,
      "grad_norm": 0.7710582613945007,
      "learning_rate": 0.0001459945668578328,
      "loss": 0.2563,
      "step": 13424
    },
    {
      "epoch": 27.012072434607646,
      "grad_norm": 0.7676764726638794,
      "learning_rate": 0.00014599054230807928,
      "loss": 0.2403,
      "step": 13425
    },
    {
      "epoch": 27.014084507042252,
      "grad_norm": 0.7502952814102173,
      "learning_rate": 0.0001459865177583258,
      "loss": 0.2398,
      "step": 13426
    },
    {
      "epoch": 27.016096579476862,
      "grad_norm": 0.7887134552001953,
      "learning_rate": 0.0001459824932085723,
      "loss": 0.2627,
      "step": 13427
    },
    {
      "epoch": 27.01810865191147,
      "grad_norm": 0.6828569173812866,
      "learning_rate": 0.00014597846865881882,
      "loss": 0.2343,
      "step": 13428
    },
    {
      "epoch": 27.020120724346075,
      "grad_norm": 0.6730322241783142,
      "learning_rate": 0.0001459744441090653,
      "loss": 0.2487,
      "step": 13429
    },
    {
      "epoch": 27.022132796780685,
      "grad_norm": 0.6812807321548462,
      "learning_rate": 0.0001459704195593118,
      "loss": 0.2318,
      "step": 13430
    },
    {
      "epoch": 27.02414486921529,
      "grad_norm": 0.7353934049606323,
      "learning_rate": 0.0001459663950095583,
      "loss": 0.2594,
      "step": 13431
    },
    {
      "epoch": 27.026156941649898,
      "grad_norm": 0.7280320525169373,
      "learning_rate": 0.0001459623704598048,
      "loss": 0.2343,
      "step": 13432
    },
    {
      "epoch": 27.028169014084508,
      "grad_norm": 0.6942689418792725,
      "learning_rate": 0.00014595834591005132,
      "loss": 0.2305,
      "step": 13433
    },
    {
      "epoch": 27.030181086519114,
      "grad_norm": 0.7124126553535461,
      "learning_rate": 0.00014595432136029783,
      "loss": 0.2551,
      "step": 13434
    },
    {
      "epoch": 27.03219315895372,
      "grad_norm": 0.7634553909301758,
      "learning_rate": 0.00014595029681054432,
      "loss": 0.2357,
      "step": 13435
    },
    {
      "epoch": 27.03420523138833,
      "grad_norm": 0.7122233510017395,
      "learning_rate": 0.00014594627226079083,
      "loss": 0.2376,
      "step": 13436
    },
    {
      "epoch": 27.036217303822937,
      "grad_norm": 0.6844787001609802,
      "learning_rate": 0.00014594224771103732,
      "loss": 0.2329,
      "step": 13437
    },
    {
      "epoch": 27.038229376257544,
      "grad_norm": 0.7092581987380981,
      "learning_rate": 0.00014593822316128386,
      "loss": 0.2283,
      "step": 13438
    },
    {
      "epoch": 27.040241448692154,
      "grad_norm": 0.7209005355834961,
      "learning_rate": 0.00014593419861153034,
      "loss": 0.2518,
      "step": 13439
    },
    {
      "epoch": 27.04225352112676,
      "grad_norm": 0.7367451190948486,
      "learning_rate": 0.00014593017406177685,
      "loss": 0.2505,
      "step": 13440
    },
    {
      "epoch": 27.044265593561367,
      "grad_norm": 0.6978110074996948,
      "learning_rate": 0.00014592614951202334,
      "loss": 0.2596,
      "step": 13441
    },
    {
      "epoch": 27.046277665995976,
      "grad_norm": 0.6985238790512085,
      "learning_rate": 0.00014592212496226985,
      "loss": 0.2255,
      "step": 13442
    },
    {
      "epoch": 27.048289738430583,
      "grad_norm": 0.7079429626464844,
      "learning_rate": 0.00014591810041251636,
      "loss": 0.2324,
      "step": 13443
    },
    {
      "epoch": 27.050301810865193,
      "grad_norm": 0.7428597807884216,
      "learning_rate": 0.00014591407586276288,
      "loss": 0.2582,
      "step": 13444
    },
    {
      "epoch": 27.0523138832998,
      "grad_norm": 0.6832690238952637,
      "learning_rate": 0.00014591005131300936,
      "loss": 0.228,
      "step": 13445
    },
    {
      "epoch": 27.054325955734406,
      "grad_norm": 0.6968994736671448,
      "learning_rate": 0.00014590602676325587,
      "loss": 0.2444,
      "step": 13446
    },
    {
      "epoch": 27.056338028169016,
      "grad_norm": 0.7456583976745605,
      "learning_rate": 0.00014590200221350236,
      "loss": 0.2607,
      "step": 13447
    },
    {
      "epoch": 27.058350100603622,
      "grad_norm": 0.7792537212371826,
      "learning_rate": 0.0001458979776637489,
      "loss": 0.2422,
      "step": 13448
    },
    {
      "epoch": 27.06036217303823,
      "grad_norm": 0.7107826471328735,
      "learning_rate": 0.00014589395311399538,
      "loss": 0.2413,
      "step": 13449
    },
    {
      "epoch": 27.06237424547284,
      "grad_norm": 0.7440304756164551,
      "learning_rate": 0.0001458899285642419,
      "loss": 0.2473,
      "step": 13450
    },
    {
      "epoch": 27.064386317907445,
      "grad_norm": 0.675981879234314,
      "learning_rate": 0.00014588590401448838,
      "loss": 0.2364,
      "step": 13451
    },
    {
      "epoch": 27.06639839034205,
      "grad_norm": 0.6656181812286377,
      "learning_rate": 0.0001458818794647349,
      "loss": 0.2221,
      "step": 13452
    },
    {
      "epoch": 27.06841046277666,
      "grad_norm": 0.713928759098053,
      "learning_rate": 0.0001458778549149814,
      "loss": 0.2376,
      "step": 13453
    },
    {
      "epoch": 27.070422535211268,
      "grad_norm": 0.7351924180984497,
      "learning_rate": 0.00014587383036522792,
      "loss": 0.2441,
      "step": 13454
    },
    {
      "epoch": 27.072434607645874,
      "grad_norm": 0.7438234686851501,
      "learning_rate": 0.0001458698058154744,
      "loss": 0.25,
      "step": 13455
    },
    {
      "epoch": 27.074446680080484,
      "grad_norm": 0.7172122597694397,
      "learning_rate": 0.0001458657812657209,
      "loss": 0.2521,
      "step": 13456
    },
    {
      "epoch": 27.07645875251509,
      "grad_norm": 0.7076565623283386,
      "learning_rate": 0.0001458617567159674,
      "loss": 0.2441,
      "step": 13457
    },
    {
      "epoch": 27.078470824949697,
      "grad_norm": 0.7593886852264404,
      "learning_rate": 0.0001458577321662139,
      "loss": 0.2458,
      "step": 13458
    },
    {
      "epoch": 27.080482897384307,
      "grad_norm": 0.6893349289894104,
      "learning_rate": 0.00014585370761646042,
      "loss": 0.228,
      "step": 13459
    },
    {
      "epoch": 27.082494969818914,
      "grad_norm": 0.6775848269462585,
      "learning_rate": 0.0001458496830667069,
      "loss": 0.2213,
      "step": 13460
    },
    {
      "epoch": 27.08450704225352,
      "grad_norm": 0.7300634384155273,
      "learning_rate": 0.00014584565851695342,
      "loss": 0.2516,
      "step": 13461
    },
    {
      "epoch": 27.08651911468813,
      "grad_norm": 0.7353314757347107,
      "learning_rate": 0.00014584163396719993,
      "loss": 0.2591,
      "step": 13462
    },
    {
      "epoch": 27.088531187122737,
      "grad_norm": 0.6903179883956909,
      "learning_rate": 0.00014583760941744642,
      "loss": 0.2527,
      "step": 13463
    },
    {
      "epoch": 27.090543259557343,
      "grad_norm": 0.7529165744781494,
      "learning_rate": 0.00014583358486769293,
      "loss": 0.2499,
      "step": 13464
    },
    {
      "epoch": 27.092555331991953,
      "grad_norm": 0.7415640354156494,
      "learning_rate": 0.00014582956031793944,
      "loss": 0.2918,
      "step": 13465
    },
    {
      "epoch": 27.09456740442656,
      "grad_norm": 0.7017545104026794,
      "learning_rate": 0.00014582553576818593,
      "loss": 0.2462,
      "step": 13466
    },
    {
      "epoch": 27.096579476861166,
      "grad_norm": 0.7242345809936523,
      "learning_rate": 0.00014582151121843244,
      "loss": 0.254,
      "step": 13467
    },
    {
      "epoch": 27.098591549295776,
      "grad_norm": 0.7523054480552673,
      "learning_rate": 0.00014581748666867892,
      "loss": 0.2489,
      "step": 13468
    },
    {
      "epoch": 27.100603621730382,
      "grad_norm": 0.7804407477378845,
      "learning_rate": 0.00014581346211892546,
      "loss": 0.2568,
      "step": 13469
    },
    {
      "epoch": 27.10261569416499,
      "grad_norm": 0.796286404132843,
      "learning_rate": 0.00014580943756917195,
      "loss": 0.2526,
      "step": 13470
    },
    {
      "epoch": 27.1046277665996,
      "grad_norm": 0.7093909978866577,
      "learning_rate": 0.00014580541301941846,
      "loss": 0.2461,
      "step": 13471
    },
    {
      "epoch": 27.106639839034205,
      "grad_norm": 0.7093622088432312,
      "learning_rate": 0.00014580138846966495,
      "loss": 0.24,
      "step": 13472
    },
    {
      "epoch": 27.10865191146881,
      "grad_norm": 0.7374591827392578,
      "learning_rate": 0.00014579736391991146,
      "loss": 0.2631,
      "step": 13473
    },
    {
      "epoch": 27.11066398390342,
      "grad_norm": 0.6840813159942627,
      "learning_rate": 0.00014579333937015797,
      "loss": 0.2384,
      "step": 13474
    },
    {
      "epoch": 27.112676056338028,
      "grad_norm": 0.7804410457611084,
      "learning_rate": 0.00014578931482040448,
      "loss": 0.2402,
      "step": 13475
    },
    {
      "epoch": 27.114688128772634,
      "grad_norm": 0.777888298034668,
      "learning_rate": 0.00014578529027065097,
      "loss": 0.2623,
      "step": 13476
    },
    {
      "epoch": 27.116700201207244,
      "grad_norm": 0.755478024482727,
      "learning_rate": 0.00014578126572089748,
      "loss": 0.2589,
      "step": 13477
    },
    {
      "epoch": 27.11871227364185,
      "grad_norm": 0.7374686002731323,
      "learning_rate": 0.00014577724117114397,
      "loss": 0.2543,
      "step": 13478
    },
    {
      "epoch": 27.120724346076457,
      "grad_norm": 0.7481890916824341,
      "learning_rate": 0.0001457732166213905,
      "loss": 0.2488,
      "step": 13479
    },
    {
      "epoch": 27.122736418511067,
      "grad_norm": 0.7869030237197876,
      "learning_rate": 0.000145769192071637,
      "loss": 0.2836,
      "step": 13480
    },
    {
      "epoch": 27.124748490945674,
      "grad_norm": 0.7453411221504211,
      "learning_rate": 0.0001457651675218835,
      "loss": 0.2473,
      "step": 13481
    },
    {
      "epoch": 27.12676056338028,
      "grad_norm": 0.8014991879463196,
      "learning_rate": 0.00014576114297213,
      "loss": 0.2659,
      "step": 13482
    },
    {
      "epoch": 27.12877263581489,
      "grad_norm": 0.7839587926864624,
      "learning_rate": 0.0001457571184223765,
      "loss": 0.2439,
      "step": 13483
    },
    {
      "epoch": 27.130784708249497,
      "grad_norm": 0.7441588640213013,
      "learning_rate": 0.000145753093872623,
      "loss": 0.2283,
      "step": 13484
    },
    {
      "epoch": 27.132796780684103,
      "grad_norm": 0.7314733266830444,
      "learning_rate": 0.00014574906932286952,
      "loss": 0.2497,
      "step": 13485
    },
    {
      "epoch": 27.134808853118713,
      "grad_norm": 0.7353931069374084,
      "learning_rate": 0.000145745044773116,
      "loss": 0.2763,
      "step": 13486
    },
    {
      "epoch": 27.13682092555332,
      "grad_norm": 0.7510220408439636,
      "learning_rate": 0.00014574102022336252,
      "loss": 0.2457,
      "step": 13487
    },
    {
      "epoch": 27.138832997987926,
      "grad_norm": 0.7774581909179688,
      "learning_rate": 0.000145736995673609,
      "loss": 0.2416,
      "step": 13488
    },
    {
      "epoch": 27.140845070422536,
      "grad_norm": 0.7525772452354431,
      "learning_rate": 0.00014573297112385555,
      "loss": 0.262,
      "step": 13489
    },
    {
      "epoch": 27.142857142857142,
      "grad_norm": 0.7512835264205933,
      "learning_rate": 0.00014572894657410203,
      "loss": 0.2433,
      "step": 13490
    },
    {
      "epoch": 27.14486921529175,
      "grad_norm": 0.8316264748573303,
      "learning_rate": 0.00014572492202434854,
      "loss": 0.2763,
      "step": 13491
    },
    {
      "epoch": 27.14688128772636,
      "grad_norm": 0.7482065558433533,
      "learning_rate": 0.00014572089747459503,
      "loss": 0.2559,
      "step": 13492
    },
    {
      "epoch": 27.148893360160965,
      "grad_norm": 0.793950080871582,
      "learning_rate": 0.00014571687292484154,
      "loss": 0.2636,
      "step": 13493
    },
    {
      "epoch": 27.15090543259557,
      "grad_norm": 0.8006410598754883,
      "learning_rate": 0.00014571284837508805,
      "loss": 0.2445,
      "step": 13494
    },
    {
      "epoch": 27.15291750503018,
      "grad_norm": 0.789794385433197,
      "learning_rate": 0.00014570882382533454,
      "loss": 0.2658,
      "step": 13495
    },
    {
      "epoch": 27.154929577464788,
      "grad_norm": 0.7217269539833069,
      "learning_rate": 0.00014570479927558105,
      "loss": 0.2688,
      "step": 13496
    },
    {
      "epoch": 27.156941649899398,
      "grad_norm": 0.7349231243133545,
      "learning_rate": 0.00014570077472582756,
      "loss": 0.2431,
      "step": 13497
    },
    {
      "epoch": 27.158953722334005,
      "grad_norm": 0.7613375782966614,
      "learning_rate": 0.00014569675017607405,
      "loss": 0.2565,
      "step": 13498
    },
    {
      "epoch": 27.16096579476861,
      "grad_norm": 0.740938663482666,
      "learning_rate": 0.00014569272562632056,
      "loss": 0.2449,
      "step": 13499
    },
    {
      "epoch": 27.16297786720322,
      "grad_norm": 0.848743200302124,
      "learning_rate": 0.00014568870107656707,
      "loss": 0.2719,
      "step": 13500
    },
    {
      "epoch": 27.164989939637827,
      "grad_norm": 0.7897364497184753,
      "learning_rate": 0.00014568467652681356,
      "loss": 0.2683,
      "step": 13501
    },
    {
      "epoch": 27.167002012072434,
      "grad_norm": 0.7625283002853394,
      "learning_rate": 0.00014568065197706007,
      "loss": 0.2744,
      "step": 13502
    },
    {
      "epoch": 27.169014084507044,
      "grad_norm": 0.7296957969665527,
      "learning_rate": 0.00014567662742730655,
      "loss": 0.2435,
      "step": 13503
    },
    {
      "epoch": 27.17102615694165,
      "grad_norm": 0.7715542316436768,
      "learning_rate": 0.0001456726028775531,
      "loss": 0.281,
      "step": 13504
    },
    {
      "epoch": 27.173038229376257,
      "grad_norm": 0.768197774887085,
      "learning_rate": 0.00014566857832779958,
      "loss": 0.2757,
      "step": 13505
    },
    {
      "epoch": 27.175050301810867,
      "grad_norm": 0.7919265031814575,
      "learning_rate": 0.0001456645537780461,
      "loss": 0.2719,
      "step": 13506
    },
    {
      "epoch": 27.177062374245473,
      "grad_norm": 0.7319713234901428,
      "learning_rate": 0.00014566052922829258,
      "loss": 0.2574,
      "step": 13507
    },
    {
      "epoch": 27.17907444668008,
      "grad_norm": 0.7635934352874756,
      "learning_rate": 0.0001456565046785391,
      "loss": 0.2529,
      "step": 13508
    },
    {
      "epoch": 27.18108651911469,
      "grad_norm": 0.7409736514091492,
      "learning_rate": 0.0001456524801287856,
      "loss": 0.2698,
      "step": 13509
    },
    {
      "epoch": 27.183098591549296,
      "grad_norm": 0.7488483190536499,
      "learning_rate": 0.0001456484555790321,
      "loss": 0.2646,
      "step": 13510
    },
    {
      "epoch": 27.185110663983902,
      "grad_norm": 0.7411306500434875,
      "learning_rate": 0.0001456444310292786,
      "loss": 0.237,
      "step": 13511
    },
    {
      "epoch": 27.187122736418512,
      "grad_norm": 0.7652844190597534,
      "learning_rate": 0.0001456404064795251,
      "loss": 0.2568,
      "step": 13512
    },
    {
      "epoch": 27.18913480885312,
      "grad_norm": 0.8154835104942322,
      "learning_rate": 0.0001456363819297716,
      "loss": 0.2671,
      "step": 13513
    },
    {
      "epoch": 27.191146881287725,
      "grad_norm": 0.7932310104370117,
      "learning_rate": 0.00014563235738001813,
      "loss": 0.2485,
      "step": 13514
    },
    {
      "epoch": 27.193158953722335,
      "grad_norm": 0.7513275146484375,
      "learning_rate": 0.00014562833283026462,
      "loss": 0.2753,
      "step": 13515
    },
    {
      "epoch": 27.19517102615694,
      "grad_norm": 0.7750418782234192,
      "learning_rate": 0.00014562430828051113,
      "loss": 0.2474,
      "step": 13516
    },
    {
      "epoch": 27.197183098591548,
      "grad_norm": 0.7398606538772583,
      "learning_rate": 0.00014562028373075762,
      "loss": 0.2621,
      "step": 13517
    },
    {
      "epoch": 27.199195171026158,
      "grad_norm": 0.7574167847633362,
      "learning_rate": 0.00014561625918100413,
      "loss": 0.2396,
      "step": 13518
    },
    {
      "epoch": 27.201207243460765,
      "grad_norm": 0.7701665759086609,
      "learning_rate": 0.00014561223463125064,
      "loss": 0.2517,
      "step": 13519
    },
    {
      "epoch": 27.20321931589537,
      "grad_norm": 0.7330992817878723,
      "learning_rate": 0.00014560821008149715,
      "loss": 0.2516,
      "step": 13520
    },
    {
      "epoch": 27.20523138832998,
      "grad_norm": 0.799474835395813,
      "learning_rate": 0.00014560418553174364,
      "loss": 0.2663,
      "step": 13521
    },
    {
      "epoch": 27.207243460764587,
      "grad_norm": 0.7833111882209778,
      "learning_rate": 0.00014560016098199015,
      "loss": 0.2472,
      "step": 13522
    },
    {
      "epoch": 27.209255533199194,
      "grad_norm": 0.7792481780052185,
      "learning_rate": 0.00014559613643223664,
      "loss": 0.2615,
      "step": 13523
    },
    {
      "epoch": 27.211267605633804,
      "grad_norm": 0.7760464549064636,
      "learning_rate": 0.00014559211188248318,
      "loss": 0.2562,
      "step": 13524
    },
    {
      "epoch": 27.21327967806841,
      "grad_norm": 0.7629110217094421,
      "learning_rate": 0.00014558808733272966,
      "loss": 0.263,
      "step": 13525
    },
    {
      "epoch": 27.215291750503017,
      "grad_norm": 0.801294207572937,
      "learning_rate": 0.00014558406278297617,
      "loss": 0.261,
      "step": 13526
    },
    {
      "epoch": 27.217303822937627,
      "grad_norm": 0.7331221699714661,
      "learning_rate": 0.00014558003823322266,
      "loss": 0.2549,
      "step": 13527
    },
    {
      "epoch": 27.219315895372233,
      "grad_norm": 0.8228785991668701,
      "learning_rate": 0.00014557601368346917,
      "loss": 0.2969,
      "step": 13528
    },
    {
      "epoch": 27.22132796780684,
      "grad_norm": 0.7327119708061218,
      "learning_rate": 0.00014557198913371568,
      "loss": 0.2515,
      "step": 13529
    },
    {
      "epoch": 27.22334004024145,
      "grad_norm": 0.718092143535614,
      "learning_rate": 0.00014556796458396217,
      "loss": 0.2505,
      "step": 13530
    },
    {
      "epoch": 27.225352112676056,
      "grad_norm": 0.8057202100753784,
      "learning_rate": 0.00014556394003420868,
      "loss": 0.2511,
      "step": 13531
    },
    {
      "epoch": 27.227364185110662,
      "grad_norm": 0.7736796736717224,
      "learning_rate": 0.0001455599154844552,
      "loss": 0.2728,
      "step": 13532
    },
    {
      "epoch": 27.229376257545272,
      "grad_norm": 0.7817984223365784,
      "learning_rate": 0.00014555589093470168,
      "loss": 0.2401,
      "step": 13533
    },
    {
      "epoch": 27.23138832997988,
      "grad_norm": 0.7606070041656494,
      "learning_rate": 0.0001455518663849482,
      "loss": 0.2646,
      "step": 13534
    },
    {
      "epoch": 27.233400402414485,
      "grad_norm": 0.8476313352584839,
      "learning_rate": 0.0001455478418351947,
      "loss": 0.2931,
      "step": 13535
    },
    {
      "epoch": 27.235412474849095,
      "grad_norm": 0.733124315738678,
      "learning_rate": 0.00014554381728544119,
      "loss": 0.2637,
      "step": 13536
    },
    {
      "epoch": 27.2374245472837,
      "grad_norm": 0.7566181421279907,
      "learning_rate": 0.0001455397927356877,
      "loss": 0.2705,
      "step": 13537
    },
    {
      "epoch": 27.239436619718308,
      "grad_norm": 0.735460102558136,
      "learning_rate": 0.00014553576818593418,
      "loss": 0.2446,
      "step": 13538
    },
    {
      "epoch": 27.241448692152918,
      "grad_norm": 0.7771515846252441,
      "learning_rate": 0.00014553174363618072,
      "loss": 0.2612,
      "step": 13539
    },
    {
      "epoch": 27.243460764587525,
      "grad_norm": 0.7691581845283508,
      "learning_rate": 0.0001455277190864272,
      "loss": 0.2441,
      "step": 13540
    },
    {
      "epoch": 27.24547283702213,
      "grad_norm": 0.80516517162323,
      "learning_rate": 0.00014552369453667372,
      "loss": 0.2581,
      "step": 13541
    },
    {
      "epoch": 27.24748490945674,
      "grad_norm": 0.803619384765625,
      "learning_rate": 0.0001455196699869202,
      "loss": 0.3048,
      "step": 13542
    },
    {
      "epoch": 27.249496981891348,
      "grad_norm": 0.8021551370620728,
      "learning_rate": 0.00014551564543716672,
      "loss": 0.2631,
      "step": 13543
    },
    {
      "epoch": 27.251509054325957,
      "grad_norm": 0.7888060212135315,
      "learning_rate": 0.00014551162088741323,
      "loss": 0.2532,
      "step": 13544
    },
    {
      "epoch": 27.253521126760564,
      "grad_norm": 0.7716373801231384,
      "learning_rate": 0.00014550759633765974,
      "loss": 0.2804,
      "step": 13545
    },
    {
      "epoch": 27.25553319919517,
      "grad_norm": 0.7728833556175232,
      "learning_rate": 0.00014550357178790623,
      "loss": 0.2553,
      "step": 13546
    },
    {
      "epoch": 27.25754527162978,
      "grad_norm": 0.7902262806892395,
      "learning_rate": 0.00014549954723815274,
      "loss": 0.2536,
      "step": 13547
    },
    {
      "epoch": 27.259557344064387,
      "grad_norm": 0.779672384262085,
      "learning_rate": 0.00014549552268839922,
      "loss": 0.2564,
      "step": 13548
    },
    {
      "epoch": 27.261569416498993,
      "grad_norm": 0.7844740152359009,
      "learning_rate": 0.00014549149813864576,
      "loss": 0.2638,
      "step": 13549
    },
    {
      "epoch": 27.263581488933603,
      "grad_norm": 0.8462986350059509,
      "learning_rate": 0.00014548747358889225,
      "loss": 0.2933,
      "step": 13550
    },
    {
      "epoch": 27.26559356136821,
      "grad_norm": 0.7745380401611328,
      "learning_rate": 0.00014548344903913876,
      "loss": 0.2908,
      "step": 13551
    },
    {
      "epoch": 27.267605633802816,
      "grad_norm": 0.8037104606628418,
      "learning_rate": 0.00014547942448938525,
      "loss": 0.2751,
      "step": 13552
    },
    {
      "epoch": 27.269617706237426,
      "grad_norm": 0.7667537927627563,
      "learning_rate": 0.00014547539993963176,
      "loss": 0.2497,
      "step": 13553
    },
    {
      "epoch": 27.271629778672033,
      "grad_norm": 0.7839632630348206,
      "learning_rate": 0.00014547137538987827,
      "loss": 0.2644,
      "step": 13554
    },
    {
      "epoch": 27.27364185110664,
      "grad_norm": 0.7296701073646545,
      "learning_rate": 0.00014546735084012478,
      "loss": 0.2625,
      "step": 13555
    },
    {
      "epoch": 27.27565392354125,
      "grad_norm": 0.7486222386360168,
      "learning_rate": 0.00014546332629037127,
      "loss": 0.27,
      "step": 13556
    },
    {
      "epoch": 27.277665995975855,
      "grad_norm": 0.7830739617347717,
      "learning_rate": 0.00014545930174061778,
      "loss": 0.2337,
      "step": 13557
    },
    {
      "epoch": 27.279678068410462,
      "grad_norm": 0.7682293057441711,
      "learning_rate": 0.00014545527719086427,
      "loss": 0.2619,
      "step": 13558
    },
    {
      "epoch": 27.281690140845072,
      "grad_norm": 0.7866658568382263,
      "learning_rate": 0.0001454512526411108,
      "loss": 0.263,
      "step": 13559
    },
    {
      "epoch": 27.28370221327968,
      "grad_norm": 0.729550838470459,
      "learning_rate": 0.0001454472280913573,
      "loss": 0.267,
      "step": 13560
    },
    {
      "epoch": 27.285714285714285,
      "grad_norm": 0.7901798486709595,
      "learning_rate": 0.0001454432035416038,
      "loss": 0.2652,
      "step": 13561
    },
    {
      "epoch": 27.287726358148895,
      "grad_norm": 0.7331187129020691,
      "learning_rate": 0.0001454391789918503,
      "loss": 0.267,
      "step": 13562
    },
    {
      "epoch": 27.2897384305835,
      "grad_norm": 0.7678356766700745,
      "learning_rate": 0.0001454351544420968,
      "loss": 0.2493,
      "step": 13563
    },
    {
      "epoch": 27.291750503018108,
      "grad_norm": 0.7737097144126892,
      "learning_rate": 0.0001454311298923433,
      "loss": 0.2554,
      "step": 13564
    },
    {
      "epoch": 27.293762575452718,
      "grad_norm": 0.7858741283416748,
      "learning_rate": 0.0001454271053425898,
      "loss": 0.2621,
      "step": 13565
    },
    {
      "epoch": 27.295774647887324,
      "grad_norm": 0.8053640127182007,
      "learning_rate": 0.0001454230807928363,
      "loss": 0.2755,
      "step": 13566
    },
    {
      "epoch": 27.29778672032193,
      "grad_norm": 0.7838457226753235,
      "learning_rate": 0.00014541905624308282,
      "loss": 0.2774,
      "step": 13567
    },
    {
      "epoch": 27.29979879275654,
      "grad_norm": 0.8055468797683716,
      "learning_rate": 0.0001454150316933293,
      "loss": 0.2542,
      "step": 13568
    },
    {
      "epoch": 27.301810865191147,
      "grad_norm": 0.8649991154670715,
      "learning_rate": 0.00014541100714357582,
      "loss": 0.2838,
      "step": 13569
    },
    {
      "epoch": 27.303822937625753,
      "grad_norm": 0.7379282116889954,
      "learning_rate": 0.00014540698259382233,
      "loss": 0.25,
      "step": 13570
    },
    {
      "epoch": 27.305835010060363,
      "grad_norm": 0.8506722450256348,
      "learning_rate": 0.00014540295804406882,
      "loss": 0.2936,
      "step": 13571
    },
    {
      "epoch": 27.30784708249497,
      "grad_norm": 0.8000175952911377,
      "learning_rate": 0.00014539893349431533,
      "loss": 0.2655,
      "step": 13572
    },
    {
      "epoch": 27.309859154929576,
      "grad_norm": 0.7368122935295105,
      "learning_rate": 0.0001453949089445618,
      "loss": 0.254,
      "step": 13573
    },
    {
      "epoch": 27.311871227364186,
      "grad_norm": 0.7602320313453674,
      "learning_rate": 0.00014539088439480835,
      "loss": 0.2524,
      "step": 13574
    },
    {
      "epoch": 27.313883299798793,
      "grad_norm": 0.7676060795783997,
      "learning_rate": 0.00014538685984505484,
      "loss": 0.2477,
      "step": 13575
    },
    {
      "epoch": 27.3158953722334,
      "grad_norm": 0.803795337677002,
      "learning_rate": 0.00014538283529530135,
      "loss": 0.2681,
      "step": 13576
    },
    {
      "epoch": 27.31790744466801,
      "grad_norm": 0.8032721281051636,
      "learning_rate": 0.00014537881074554783,
      "loss": 0.2797,
      "step": 13577
    },
    {
      "epoch": 27.319919517102615,
      "grad_norm": 0.7956826686859131,
      "learning_rate": 0.00014537478619579435,
      "loss": 0.284,
      "step": 13578
    },
    {
      "epoch": 27.321931589537222,
      "grad_norm": 0.7446459531784058,
      "learning_rate": 0.00014537076164604086,
      "loss": 0.25,
      "step": 13579
    },
    {
      "epoch": 27.323943661971832,
      "grad_norm": 0.7820823192596436,
      "learning_rate": 0.00014536673709628737,
      "loss": 0.268,
      "step": 13580
    },
    {
      "epoch": 27.32595573440644,
      "grad_norm": 0.7958449721336365,
      "learning_rate": 0.00014536271254653386,
      "loss": 0.2689,
      "step": 13581
    },
    {
      "epoch": 27.327967806841045,
      "grad_norm": 0.8096807599067688,
      "learning_rate": 0.00014535868799678037,
      "loss": 0.2558,
      "step": 13582
    },
    {
      "epoch": 27.329979879275655,
      "grad_norm": 0.7871696949005127,
      "learning_rate": 0.00014535466344702685,
      "loss": 0.2609,
      "step": 13583
    },
    {
      "epoch": 27.33199195171026,
      "grad_norm": 0.8044734001159668,
      "learning_rate": 0.0001453506388972734,
      "loss": 0.2759,
      "step": 13584
    },
    {
      "epoch": 27.334004024144868,
      "grad_norm": 0.8006004095077515,
      "learning_rate": 0.00014534661434751988,
      "loss": 0.2547,
      "step": 13585
    },
    {
      "epoch": 27.336016096579478,
      "grad_norm": 0.7501930594444275,
      "learning_rate": 0.0001453425897977664,
      "loss": 0.258,
      "step": 13586
    },
    {
      "epoch": 27.338028169014084,
      "grad_norm": 0.7953486442565918,
      "learning_rate": 0.00014533856524801288,
      "loss": 0.2585,
      "step": 13587
    },
    {
      "epoch": 27.34004024144869,
      "grad_norm": 0.7944614291191101,
      "learning_rate": 0.0001453345406982594,
      "loss": 0.2882,
      "step": 13588
    },
    {
      "epoch": 27.3420523138833,
      "grad_norm": 0.7828247547149658,
      "learning_rate": 0.0001453305161485059,
      "loss": 0.2737,
      "step": 13589
    },
    {
      "epoch": 27.344064386317907,
      "grad_norm": 0.7190844416618347,
      "learning_rate": 0.0001453264915987524,
      "loss": 0.2525,
      "step": 13590
    },
    {
      "epoch": 27.346076458752513,
      "grad_norm": 0.8425770401954651,
      "learning_rate": 0.0001453224670489989,
      "loss": 0.2856,
      "step": 13591
    },
    {
      "epoch": 27.348088531187123,
      "grad_norm": 0.7665982842445374,
      "learning_rate": 0.0001453184424992454,
      "loss": 0.2679,
      "step": 13592
    },
    {
      "epoch": 27.35010060362173,
      "grad_norm": 0.8076183795928955,
      "learning_rate": 0.0001453144179494919,
      "loss": 0.2916,
      "step": 13593
    },
    {
      "epoch": 27.352112676056336,
      "grad_norm": 0.7936151027679443,
      "learning_rate": 0.00014531039339973843,
      "loss": 0.2852,
      "step": 13594
    },
    {
      "epoch": 27.354124748490946,
      "grad_norm": 0.7912189960479736,
      "learning_rate": 0.00014530636884998492,
      "loss": 0.2612,
      "step": 13595
    },
    {
      "epoch": 27.356136820925553,
      "grad_norm": 0.800051748752594,
      "learning_rate": 0.00014530234430023143,
      "loss": 0.2695,
      "step": 13596
    },
    {
      "epoch": 27.358148893360163,
      "grad_norm": 0.8005821108818054,
      "learning_rate": 0.00014529831975047792,
      "loss": 0.281,
      "step": 13597
    },
    {
      "epoch": 27.36016096579477,
      "grad_norm": 0.9150771498680115,
      "learning_rate": 0.00014529429520072443,
      "loss": 0.2854,
      "step": 13598
    },
    {
      "epoch": 27.362173038229376,
      "grad_norm": 0.7783902883529663,
      "learning_rate": 0.00014529027065097094,
      "loss": 0.272,
      "step": 13599
    },
    {
      "epoch": 27.364185110663986,
      "grad_norm": 0.7949551939964294,
      "learning_rate": 0.00014528624610121743,
      "loss": 0.2752,
      "step": 13600
    },
    {
      "epoch": 27.366197183098592,
      "grad_norm": 0.7708297371864319,
      "learning_rate": 0.00014528222155146394,
      "loss": 0.2689,
      "step": 13601
    },
    {
      "epoch": 27.3682092555332,
      "grad_norm": 0.7621726393699646,
      "learning_rate": 0.00014527819700171042,
      "loss": 0.2682,
      "step": 13602
    },
    {
      "epoch": 27.37022132796781,
      "grad_norm": 0.7741009593009949,
      "learning_rate": 0.00014527417245195694,
      "loss": 0.2662,
      "step": 13603
    },
    {
      "epoch": 27.372233400402415,
      "grad_norm": 0.7644980549812317,
      "learning_rate": 0.00014527014790220345,
      "loss": 0.285,
      "step": 13604
    },
    {
      "epoch": 27.37424547283702,
      "grad_norm": 0.7701817750930786,
      "learning_rate": 0.00014526612335244996,
      "loss": 0.2534,
      "step": 13605
    },
    {
      "epoch": 27.37625754527163,
      "grad_norm": 0.7945325374603271,
      "learning_rate": 0.00014526209880269644,
      "loss": 0.2843,
      "step": 13606
    },
    {
      "epoch": 27.378269617706238,
      "grad_norm": 0.8637057542800903,
      "learning_rate": 0.00014525807425294296,
      "loss": 0.3053,
      "step": 13607
    },
    {
      "epoch": 27.380281690140844,
      "grad_norm": 0.8479421734809875,
      "learning_rate": 0.00014525404970318944,
      "loss": 0.2844,
      "step": 13608
    },
    {
      "epoch": 27.382293762575454,
      "grad_norm": 0.7934040427207947,
      "learning_rate": 0.00014525002515343598,
      "loss": 0.2676,
      "step": 13609
    },
    {
      "epoch": 27.38430583501006,
      "grad_norm": 0.7653002142906189,
      "learning_rate": 0.00014524600060368247,
      "loss": 0.2526,
      "step": 13610
    },
    {
      "epoch": 27.386317907444667,
      "grad_norm": 0.8286452293395996,
      "learning_rate": 0.00014524197605392898,
      "loss": 0.2926,
      "step": 13611
    },
    {
      "epoch": 27.388329979879277,
      "grad_norm": 0.7508000135421753,
      "learning_rate": 0.00014523795150417546,
      "loss": 0.2613,
      "step": 13612
    },
    {
      "epoch": 27.390342052313883,
      "grad_norm": 0.7772923111915588,
      "learning_rate": 0.00014523392695442198,
      "loss": 0.2708,
      "step": 13613
    },
    {
      "epoch": 27.39235412474849,
      "grad_norm": 0.7611933350563049,
      "learning_rate": 0.0001452299024046685,
      "loss": 0.2668,
      "step": 13614
    },
    {
      "epoch": 27.3943661971831,
      "grad_norm": 0.7979908585548401,
      "learning_rate": 0.000145225877854915,
      "loss": 0.2719,
      "step": 13615
    },
    {
      "epoch": 27.396378269617706,
      "grad_norm": 0.8178870677947998,
      "learning_rate": 0.00014522185330516149,
      "loss": 0.274,
      "step": 13616
    },
    {
      "epoch": 27.398390342052313,
      "grad_norm": 0.8068494200706482,
      "learning_rate": 0.000145217828755408,
      "loss": 0.2466,
      "step": 13617
    },
    {
      "epoch": 27.400402414486923,
      "grad_norm": 0.8803445100784302,
      "learning_rate": 0.00014521380420565448,
      "loss": 0.2877,
      "step": 13618
    },
    {
      "epoch": 27.40241448692153,
      "grad_norm": 0.7531337141990662,
      "learning_rate": 0.00014520977965590102,
      "loss": 0.2615,
      "step": 13619
    },
    {
      "epoch": 27.404426559356136,
      "grad_norm": 0.8101509809494019,
      "learning_rate": 0.0001452057551061475,
      "loss": 0.2699,
      "step": 13620
    },
    {
      "epoch": 27.406438631790746,
      "grad_norm": 0.7938281297683716,
      "learning_rate": 0.00014520173055639402,
      "loss": 0.2641,
      "step": 13621
    },
    {
      "epoch": 27.408450704225352,
      "grad_norm": 0.8018078804016113,
      "learning_rate": 0.0001451977060066405,
      "loss": 0.2674,
      "step": 13622
    },
    {
      "epoch": 27.41046277665996,
      "grad_norm": 0.7768415808677673,
      "learning_rate": 0.00014519368145688702,
      "loss": 0.2574,
      "step": 13623
    },
    {
      "epoch": 27.41247484909457,
      "grad_norm": 0.7795654535293579,
      "learning_rate": 0.00014518965690713353,
      "loss": 0.2648,
      "step": 13624
    },
    {
      "epoch": 27.414486921529175,
      "grad_norm": 0.8082608580589294,
      "learning_rate": 0.00014518563235738004,
      "loss": 0.2538,
      "step": 13625
    },
    {
      "epoch": 27.41649899396378,
      "grad_norm": 0.8092240691184998,
      "learning_rate": 0.00014518160780762653,
      "loss": 0.2769,
      "step": 13626
    },
    {
      "epoch": 27.41851106639839,
      "grad_norm": 0.7965683937072754,
      "learning_rate": 0.00014517758325787304,
      "loss": 0.2768,
      "step": 13627
    },
    {
      "epoch": 27.420523138832998,
      "grad_norm": 0.8720083236694336,
      "learning_rate": 0.00014517355870811952,
      "loss": 0.2977,
      "step": 13628
    },
    {
      "epoch": 27.422535211267604,
      "grad_norm": 0.7681667804718018,
      "learning_rate": 0.00014516953415836604,
      "loss": 0.2615,
      "step": 13629
    },
    {
      "epoch": 27.424547283702214,
      "grad_norm": 0.7755669355392456,
      "learning_rate": 0.00014516550960861255,
      "loss": 0.2835,
      "step": 13630
    },
    {
      "epoch": 27.42655935613682,
      "grad_norm": 0.7807763814926147,
      "learning_rate": 0.00014516148505885906,
      "loss": 0.2619,
      "step": 13631
    },
    {
      "epoch": 27.428571428571427,
      "grad_norm": 0.8132035136222839,
      "learning_rate": 0.00014515746050910555,
      "loss": 0.2646,
      "step": 13632
    },
    {
      "epoch": 27.430583501006037,
      "grad_norm": 0.7858837246894836,
      "learning_rate": 0.00014515343595935206,
      "loss": 0.2673,
      "step": 13633
    },
    {
      "epoch": 27.432595573440643,
      "grad_norm": 0.8204634785652161,
      "learning_rate": 0.00014514941140959857,
      "loss": 0.2518,
      "step": 13634
    },
    {
      "epoch": 27.43460764587525,
      "grad_norm": 0.8535858392715454,
      "learning_rate": 0.00014514538685984506,
      "loss": 0.2729,
      "step": 13635
    },
    {
      "epoch": 27.43661971830986,
      "grad_norm": 0.781292200088501,
      "learning_rate": 0.00014514136231009157,
      "loss": 0.2865,
      "step": 13636
    },
    {
      "epoch": 27.438631790744466,
      "grad_norm": 0.7996812462806702,
      "learning_rate": 0.00014513733776033805,
      "loss": 0.2804,
      "step": 13637
    },
    {
      "epoch": 27.440643863179073,
      "grad_norm": 0.8074309825897217,
      "learning_rate": 0.00014513331321058456,
      "loss": 0.2803,
      "step": 13638
    },
    {
      "epoch": 27.442655935613683,
      "grad_norm": 0.7967650890350342,
      "learning_rate": 0.00014512928866083108,
      "loss": 0.2752,
      "step": 13639
    },
    {
      "epoch": 27.44466800804829,
      "grad_norm": 0.8051285743713379,
      "learning_rate": 0.0001451252641110776,
      "loss": 0.2608,
      "step": 13640
    },
    {
      "epoch": 27.446680080482896,
      "grad_norm": 0.8388834595680237,
      "learning_rate": 0.00014512123956132407,
      "loss": 0.2829,
      "step": 13641
    },
    {
      "epoch": 27.448692152917506,
      "grad_norm": 0.7910477519035339,
      "learning_rate": 0.0001451172150115706,
      "loss": 0.2586,
      "step": 13642
    },
    {
      "epoch": 27.450704225352112,
      "grad_norm": 0.8086280226707458,
      "learning_rate": 0.00014511319046181707,
      "loss": 0.2751,
      "step": 13643
    },
    {
      "epoch": 27.452716297786722,
      "grad_norm": 0.8576979637145996,
      "learning_rate": 0.0001451091659120636,
      "loss": 0.2864,
      "step": 13644
    },
    {
      "epoch": 27.45472837022133,
      "grad_norm": 0.8036202788352966,
      "learning_rate": 0.0001451051413623101,
      "loss": 0.2795,
      "step": 13645
    },
    {
      "epoch": 27.456740442655935,
      "grad_norm": 0.8072713017463684,
      "learning_rate": 0.0001451011168125566,
      "loss": 0.2599,
      "step": 13646
    },
    {
      "epoch": 27.458752515090545,
      "grad_norm": 0.8281897902488708,
      "learning_rate": 0.0001450970922628031,
      "loss": 0.2846,
      "step": 13647
    },
    {
      "epoch": 27.46076458752515,
      "grad_norm": 0.7840391993522644,
      "learning_rate": 0.0001450930677130496,
      "loss": 0.2649,
      "step": 13648
    },
    {
      "epoch": 27.462776659959758,
      "grad_norm": 0.790688157081604,
      "learning_rate": 0.00014508904316329612,
      "loss": 0.2762,
      "step": 13649
    },
    {
      "epoch": 27.464788732394368,
      "grad_norm": 0.8291399478912354,
      "learning_rate": 0.00014508501861354263,
      "loss": 0.2518,
      "step": 13650
    },
    {
      "epoch": 27.466800804828974,
      "grad_norm": 0.766878604888916,
      "learning_rate": 0.00014508099406378912,
      "loss": 0.293,
      "step": 13651
    },
    {
      "epoch": 27.46881287726358,
      "grad_norm": 0.7915644645690918,
      "learning_rate": 0.00014507696951403563,
      "loss": 0.2733,
      "step": 13652
    },
    {
      "epoch": 27.47082494969819,
      "grad_norm": 0.8212360739707947,
      "learning_rate": 0.0001450729449642821,
      "loss": 0.2586,
      "step": 13653
    },
    {
      "epoch": 27.472837022132797,
      "grad_norm": 0.8759768605232239,
      "learning_rate": 0.00014506892041452865,
      "loss": 0.3292,
      "step": 13654
    },
    {
      "epoch": 27.474849094567404,
      "grad_norm": 0.7584155201911926,
      "learning_rate": 0.00014506489586477514,
      "loss": 0.2561,
      "step": 13655
    },
    {
      "epoch": 27.476861167002014,
      "grad_norm": 0.8159911036491394,
      "learning_rate": 0.00014506087131502165,
      "loss": 0.2827,
      "step": 13656
    },
    {
      "epoch": 27.47887323943662,
      "grad_norm": 0.8030865788459778,
      "learning_rate": 0.00014505684676526813,
      "loss": 0.2754,
      "step": 13657
    },
    {
      "epoch": 27.480885311871226,
      "grad_norm": 0.7923989295959473,
      "learning_rate": 0.00014505282221551465,
      "loss": 0.2765,
      "step": 13658
    },
    {
      "epoch": 27.482897384305836,
      "grad_norm": 0.7900980114936829,
      "learning_rate": 0.00014504879766576116,
      "loss": 0.2647,
      "step": 13659
    },
    {
      "epoch": 27.484909456740443,
      "grad_norm": 0.8114593625068665,
      "learning_rate": 0.00014504477311600767,
      "loss": 0.2816,
      "step": 13660
    },
    {
      "epoch": 27.48692152917505,
      "grad_norm": 0.8494522571563721,
      "learning_rate": 0.00014504074856625416,
      "loss": 0.2808,
      "step": 13661
    },
    {
      "epoch": 27.48893360160966,
      "grad_norm": 0.7966306805610657,
      "learning_rate": 0.00014503672401650067,
      "loss": 0.2773,
      "step": 13662
    },
    {
      "epoch": 27.490945674044266,
      "grad_norm": 0.7813367247581482,
      "learning_rate": 0.00014503269946674715,
      "loss": 0.2873,
      "step": 13663
    },
    {
      "epoch": 27.492957746478872,
      "grad_norm": 0.8027030229568481,
      "learning_rate": 0.00014502867491699367,
      "loss": 0.2613,
      "step": 13664
    },
    {
      "epoch": 27.494969818913482,
      "grad_norm": 0.8671155571937561,
      "learning_rate": 0.00014502465036724018,
      "loss": 0.3089,
      "step": 13665
    },
    {
      "epoch": 27.49698189134809,
      "grad_norm": 0.7921735048294067,
      "learning_rate": 0.0001450206258174867,
      "loss": 0.272,
      "step": 13666
    },
    {
      "epoch": 27.498993963782695,
      "grad_norm": 0.8075029850006104,
      "learning_rate": 0.00014501660126773318,
      "loss": 0.2905,
      "step": 13667
    },
    {
      "epoch": 27.501006036217305,
      "grad_norm": 0.795001745223999,
      "learning_rate": 0.0001450125767179797,
      "loss": 0.2813,
      "step": 13668
    },
    {
      "epoch": 27.50301810865191,
      "grad_norm": 0.7994493246078491,
      "learning_rate": 0.0001450085521682262,
      "loss": 0.2724,
      "step": 13669
    },
    {
      "epoch": 27.505030181086518,
      "grad_norm": 0.8164960741996765,
      "learning_rate": 0.00014500452761847268,
      "loss": 0.2811,
      "step": 13670
    },
    {
      "epoch": 27.507042253521128,
      "grad_norm": 0.8401297926902771,
      "learning_rate": 0.0001450005030687192,
      "loss": 0.2619,
      "step": 13671
    },
    {
      "epoch": 27.509054325955734,
      "grad_norm": 0.91716068983078,
      "learning_rate": 0.00014499647851896568,
      "loss": 0.2908,
      "step": 13672
    },
    {
      "epoch": 27.51106639839034,
      "grad_norm": 0.8812370896339417,
      "learning_rate": 0.0001449924539692122,
      "loss": 0.3149,
      "step": 13673
    },
    {
      "epoch": 27.51307847082495,
      "grad_norm": 0.7958134412765503,
      "learning_rate": 0.0001449884294194587,
      "loss": 0.2778,
      "step": 13674
    },
    {
      "epoch": 27.515090543259557,
      "grad_norm": 0.8067176342010498,
      "learning_rate": 0.00014498440486970522,
      "loss": 0.2889,
      "step": 13675
    },
    {
      "epoch": 27.517102615694164,
      "grad_norm": 0.8022276163101196,
      "learning_rate": 0.0001449803803199517,
      "loss": 0.2927,
      "step": 13676
    },
    {
      "epoch": 27.519114688128774,
      "grad_norm": 0.8009485006332397,
      "learning_rate": 0.00014497635577019822,
      "loss": 0.271,
      "step": 13677
    },
    {
      "epoch": 27.52112676056338,
      "grad_norm": 0.8293830156326294,
      "learning_rate": 0.0001449723312204447,
      "loss": 0.2749,
      "step": 13678
    },
    {
      "epoch": 27.523138832997986,
      "grad_norm": 0.7905867695808411,
      "learning_rate": 0.00014496830667069124,
      "loss": 0.2812,
      "step": 13679
    },
    {
      "epoch": 27.525150905432596,
      "grad_norm": 0.7953153848648071,
      "learning_rate": 0.00014496428212093773,
      "loss": 0.2739,
      "step": 13680
    },
    {
      "epoch": 27.527162977867203,
      "grad_norm": 0.8421898484230042,
      "learning_rate": 0.00014496025757118424,
      "loss": 0.2716,
      "step": 13681
    },
    {
      "epoch": 27.52917505030181,
      "grad_norm": 0.838546872138977,
      "learning_rate": 0.00014495623302143072,
      "loss": 0.303,
      "step": 13682
    },
    {
      "epoch": 27.53118712273642,
      "grad_norm": 0.8739145398139954,
      "learning_rate": 0.00014495220847167724,
      "loss": 0.2888,
      "step": 13683
    },
    {
      "epoch": 27.533199195171026,
      "grad_norm": 0.8297669887542725,
      "learning_rate": 0.00014494818392192375,
      "loss": 0.3001,
      "step": 13684
    },
    {
      "epoch": 27.535211267605632,
      "grad_norm": 0.8396208882331848,
      "learning_rate": 0.00014494415937217026,
      "loss": 0.2899,
      "step": 13685
    },
    {
      "epoch": 27.537223340040242,
      "grad_norm": 0.7755794525146484,
      "learning_rate": 0.00014494013482241674,
      "loss": 0.2724,
      "step": 13686
    },
    {
      "epoch": 27.53923541247485,
      "grad_norm": 0.774411141872406,
      "learning_rate": 0.00014493611027266326,
      "loss": 0.2887,
      "step": 13687
    },
    {
      "epoch": 27.541247484909455,
      "grad_norm": 0.7859936356544495,
      "learning_rate": 0.00014493208572290974,
      "loss": 0.2864,
      "step": 13688
    },
    {
      "epoch": 27.543259557344065,
      "grad_norm": 0.8013809323310852,
      "learning_rate": 0.00014492806117315628,
      "loss": 0.3005,
      "step": 13689
    },
    {
      "epoch": 27.54527162977867,
      "grad_norm": 0.8310509920120239,
      "learning_rate": 0.00014492403662340277,
      "loss": 0.2765,
      "step": 13690
    },
    {
      "epoch": 27.547283702213278,
      "grad_norm": 0.881218433380127,
      "learning_rate": 0.00014492001207364928,
      "loss": 0.2968,
      "step": 13691
    },
    {
      "epoch": 27.549295774647888,
      "grad_norm": 0.7927754521369934,
      "learning_rate": 0.00014491598752389576,
      "loss": 0.2732,
      "step": 13692
    },
    {
      "epoch": 27.551307847082494,
      "grad_norm": 0.8620059490203857,
      "learning_rate": 0.00014491196297414228,
      "loss": 0.2915,
      "step": 13693
    },
    {
      "epoch": 27.5533199195171,
      "grad_norm": 0.8027200698852539,
      "learning_rate": 0.0001449079384243888,
      "loss": 0.2722,
      "step": 13694
    },
    {
      "epoch": 27.55533199195171,
      "grad_norm": 0.8374632596969604,
      "learning_rate": 0.0001449039138746353,
      "loss": 0.283,
      "step": 13695
    },
    {
      "epoch": 27.557344064386317,
      "grad_norm": 0.8139105439186096,
      "learning_rate": 0.00014489988932488179,
      "loss": 0.2773,
      "step": 13696
    },
    {
      "epoch": 27.559356136820927,
      "grad_norm": 0.7816657423973083,
      "learning_rate": 0.0001448958647751283,
      "loss": 0.2939,
      "step": 13697
    },
    {
      "epoch": 27.561368209255534,
      "grad_norm": 0.809458315372467,
      "learning_rate": 0.00014489184022537478,
      "loss": 0.2843,
      "step": 13698
    },
    {
      "epoch": 27.56338028169014,
      "grad_norm": 0.8902403712272644,
      "learning_rate": 0.0001448878156756213,
      "loss": 0.3097,
      "step": 13699
    },
    {
      "epoch": 27.56539235412475,
      "grad_norm": 0.8066923022270203,
      "learning_rate": 0.0001448837911258678,
      "loss": 0.2873,
      "step": 13700
    },
    {
      "epoch": 27.567404426559357,
      "grad_norm": 0.7540032863616943,
      "learning_rate": 0.00014487976657611432,
      "loss": 0.2765,
      "step": 13701
    },
    {
      "epoch": 27.569416498993963,
      "grad_norm": 0.7896744012832642,
      "learning_rate": 0.0001448757420263608,
      "loss": 0.2979,
      "step": 13702
    },
    {
      "epoch": 27.571428571428573,
      "grad_norm": 0.8162328004837036,
      "learning_rate": 0.00014487171747660732,
      "loss": 0.2926,
      "step": 13703
    },
    {
      "epoch": 27.57344064386318,
      "grad_norm": 0.7966801524162292,
      "learning_rate": 0.0001448676929268538,
      "loss": 0.2805,
      "step": 13704
    },
    {
      "epoch": 27.575452716297786,
      "grad_norm": 0.7874460816383362,
      "learning_rate": 0.00014486366837710031,
      "loss": 0.2816,
      "step": 13705
    },
    {
      "epoch": 27.577464788732396,
      "grad_norm": 0.7941781282424927,
      "learning_rate": 0.00014485964382734683,
      "loss": 0.2776,
      "step": 13706
    },
    {
      "epoch": 27.579476861167002,
      "grad_norm": 0.8081645965576172,
      "learning_rate": 0.0001448556192775933,
      "loss": 0.2936,
      "step": 13707
    },
    {
      "epoch": 27.58148893360161,
      "grad_norm": 0.8101111650466919,
      "learning_rate": 0.00014485159472783982,
      "loss": 0.2937,
      "step": 13708
    },
    {
      "epoch": 27.58350100603622,
      "grad_norm": 0.8108460307121277,
      "learning_rate": 0.00014484757017808634,
      "loss": 0.2936,
      "step": 13709
    },
    {
      "epoch": 27.585513078470825,
      "grad_norm": 0.7957895994186401,
      "learning_rate": 0.00014484354562833285,
      "loss": 0.2934,
      "step": 13710
    },
    {
      "epoch": 27.58752515090543,
      "grad_norm": 0.8074647188186646,
      "learning_rate": 0.00014483952107857933,
      "loss": 0.2703,
      "step": 13711
    },
    {
      "epoch": 27.58953722334004,
      "grad_norm": 0.7761335968971252,
      "learning_rate": 0.00014483549652882585,
      "loss": 0.2942,
      "step": 13712
    },
    {
      "epoch": 27.591549295774648,
      "grad_norm": 0.8047485947608948,
      "learning_rate": 0.00014483147197907233,
      "loss": 0.2789,
      "step": 13713
    },
    {
      "epoch": 27.593561368209254,
      "grad_norm": 0.8243881464004517,
      "learning_rate": 0.00014482744742931884,
      "loss": 0.3022,
      "step": 13714
    },
    {
      "epoch": 27.595573440643864,
      "grad_norm": 0.835257351398468,
      "learning_rate": 0.00014482342287956536,
      "loss": 0.2635,
      "step": 13715
    },
    {
      "epoch": 27.59758551307847,
      "grad_norm": 0.8109064698219299,
      "learning_rate": 0.00014481939832981187,
      "loss": 0.2933,
      "step": 13716
    },
    {
      "epoch": 27.599597585513077,
      "grad_norm": 0.7812092304229736,
      "learning_rate": 0.00014481537378005835,
      "loss": 0.2702,
      "step": 13717
    },
    {
      "epoch": 27.601609657947687,
      "grad_norm": 0.8273937702178955,
      "learning_rate": 0.00014481134923030486,
      "loss": 0.2828,
      "step": 13718
    },
    {
      "epoch": 27.603621730382294,
      "grad_norm": 0.789704442024231,
      "learning_rate": 0.00014480732468055135,
      "loss": 0.2682,
      "step": 13719
    },
    {
      "epoch": 27.6056338028169,
      "grad_norm": 0.7912967205047607,
      "learning_rate": 0.0001448033001307979,
      "loss": 0.2753,
      "step": 13720
    },
    {
      "epoch": 27.60764587525151,
      "grad_norm": 0.8222401738166809,
      "learning_rate": 0.00014479927558104437,
      "loss": 0.2989,
      "step": 13721
    },
    {
      "epoch": 27.609657947686117,
      "grad_norm": 0.8041849732398987,
      "learning_rate": 0.00014479525103129089,
      "loss": 0.2834,
      "step": 13722
    },
    {
      "epoch": 27.611670020120723,
      "grad_norm": 0.7938147187232971,
      "learning_rate": 0.00014479122648153737,
      "loss": 0.2917,
      "step": 13723
    },
    {
      "epoch": 27.613682092555333,
      "grad_norm": 0.8265436291694641,
      "learning_rate": 0.00014478720193178388,
      "loss": 0.2904,
      "step": 13724
    },
    {
      "epoch": 27.61569416498994,
      "grad_norm": 0.8634277582168579,
      "learning_rate": 0.0001447831773820304,
      "loss": 0.2786,
      "step": 13725
    },
    {
      "epoch": 27.617706237424546,
      "grad_norm": 0.7882735729217529,
      "learning_rate": 0.0001447791528322769,
      "loss": 0.2735,
      "step": 13726
    },
    {
      "epoch": 27.619718309859156,
      "grad_norm": 0.840070366859436,
      "learning_rate": 0.0001447751282825234,
      "loss": 0.2735,
      "step": 13727
    },
    {
      "epoch": 27.621730382293762,
      "grad_norm": 0.7984204888343811,
      "learning_rate": 0.0001447711037327699,
      "loss": 0.3044,
      "step": 13728
    },
    {
      "epoch": 27.62374245472837,
      "grad_norm": 0.7630361318588257,
      "learning_rate": 0.0001447670791830164,
      "loss": 0.298,
      "step": 13729
    },
    {
      "epoch": 27.62575452716298,
      "grad_norm": 0.8133465647697449,
      "learning_rate": 0.00014476305463326293,
      "loss": 0.3067,
      "step": 13730
    },
    {
      "epoch": 27.627766599597585,
      "grad_norm": 0.84237140417099,
      "learning_rate": 0.00014475903008350942,
      "loss": 0.3281,
      "step": 13731
    },
    {
      "epoch": 27.62977867203219,
      "grad_norm": 0.7841379046440125,
      "learning_rate": 0.00014475500553375593,
      "loss": 0.2948,
      "step": 13732
    },
    {
      "epoch": 27.6317907444668,
      "grad_norm": 0.8043936491012573,
      "learning_rate": 0.0001447509809840024,
      "loss": 0.2776,
      "step": 13733
    },
    {
      "epoch": 27.633802816901408,
      "grad_norm": 0.8071929812431335,
      "learning_rate": 0.00014474695643424892,
      "loss": 0.2888,
      "step": 13734
    },
    {
      "epoch": 27.635814889336014,
      "grad_norm": 0.8314589858055115,
      "learning_rate": 0.00014474293188449544,
      "loss": 0.2867,
      "step": 13735
    },
    {
      "epoch": 27.637826961770624,
      "grad_norm": 0.8270874619483948,
      "learning_rate": 0.00014473890733474195,
      "loss": 0.2848,
      "step": 13736
    },
    {
      "epoch": 27.63983903420523,
      "grad_norm": 0.8320611715316772,
      "learning_rate": 0.00014473488278498843,
      "loss": 0.304,
      "step": 13737
    },
    {
      "epoch": 27.641851106639837,
      "grad_norm": 0.8065530061721802,
      "learning_rate": 0.00014473085823523495,
      "loss": 0.2897,
      "step": 13738
    },
    {
      "epoch": 27.643863179074447,
      "grad_norm": 0.8134946823120117,
      "learning_rate": 0.00014472683368548143,
      "loss": 0.2836,
      "step": 13739
    },
    {
      "epoch": 27.645875251509054,
      "grad_norm": 0.7931272387504578,
      "learning_rate": 0.00014472280913572794,
      "loss": 0.2717,
      "step": 13740
    },
    {
      "epoch": 27.647887323943664,
      "grad_norm": 0.8254125714302063,
      "learning_rate": 0.00014471878458597446,
      "loss": 0.2887,
      "step": 13741
    },
    {
      "epoch": 27.64989939637827,
      "grad_norm": 0.8232835531234741,
      "learning_rate": 0.00014471476003622094,
      "loss": 0.3055,
      "step": 13742
    },
    {
      "epoch": 27.651911468812877,
      "grad_norm": 0.792250394821167,
      "learning_rate": 0.00014471073548646745,
      "loss": 0.3026,
      "step": 13743
    },
    {
      "epoch": 27.653923541247487,
      "grad_norm": 0.7906078696250916,
      "learning_rate": 0.00014470671093671394,
      "loss": 0.2838,
      "step": 13744
    },
    {
      "epoch": 27.655935613682093,
      "grad_norm": 0.7817354798316956,
      "learning_rate": 0.00014470268638696048,
      "loss": 0.2759,
      "step": 13745
    },
    {
      "epoch": 27.6579476861167,
      "grad_norm": 0.7976745367050171,
      "learning_rate": 0.00014469866183720696,
      "loss": 0.2666,
      "step": 13746
    },
    {
      "epoch": 27.65995975855131,
      "grad_norm": 0.9162082076072693,
      "learning_rate": 0.00014469463728745347,
      "loss": 0.3054,
      "step": 13747
    },
    {
      "epoch": 27.661971830985916,
      "grad_norm": 0.8821436166763306,
      "learning_rate": 0.00014469061273769996,
      "loss": 0.282,
      "step": 13748
    },
    {
      "epoch": 27.663983903420522,
      "grad_norm": 0.8543000221252441,
      "learning_rate": 0.00014468658818794647,
      "loss": 0.302,
      "step": 13749
    },
    {
      "epoch": 27.665995975855132,
      "grad_norm": 0.7780048847198486,
      "learning_rate": 0.00014468256363819298,
      "loss": 0.2835,
      "step": 13750
    },
    {
      "epoch": 27.66800804828974,
      "grad_norm": 0.7925797700881958,
      "learning_rate": 0.0001446785390884395,
      "loss": 0.2693,
      "step": 13751
    },
    {
      "epoch": 27.670020120724345,
      "grad_norm": 0.8590739369392395,
      "learning_rate": 0.00014467451453868598,
      "loss": 0.267,
      "step": 13752
    },
    {
      "epoch": 27.672032193158955,
      "grad_norm": 0.7977386116981506,
      "learning_rate": 0.0001446704899889325,
      "loss": 0.2861,
      "step": 13753
    },
    {
      "epoch": 27.67404426559356,
      "grad_norm": 0.833861768245697,
      "learning_rate": 0.00014466646543917898,
      "loss": 0.2943,
      "step": 13754
    },
    {
      "epoch": 27.676056338028168,
      "grad_norm": 0.8300899267196655,
      "learning_rate": 0.00014466244088942552,
      "loss": 0.3014,
      "step": 13755
    },
    {
      "epoch": 27.678068410462778,
      "grad_norm": 0.7989466786384583,
      "learning_rate": 0.000144658416339672,
      "loss": 0.2779,
      "step": 13756
    },
    {
      "epoch": 27.680080482897385,
      "grad_norm": 0.8062613010406494,
      "learning_rate": 0.00014465439178991852,
      "loss": 0.3041,
      "step": 13757
    },
    {
      "epoch": 27.68209255533199,
      "grad_norm": 0.8642515540122986,
      "learning_rate": 0.000144650367240165,
      "loss": 0.2972,
      "step": 13758
    },
    {
      "epoch": 27.6841046277666,
      "grad_norm": 0.8308777213096619,
      "learning_rate": 0.0001446463426904115,
      "loss": 0.2878,
      "step": 13759
    },
    {
      "epoch": 27.686116700201207,
      "grad_norm": 0.8094350695610046,
      "learning_rate": 0.00014464231814065803,
      "loss": 0.3058,
      "step": 13760
    },
    {
      "epoch": 27.688128772635814,
      "grad_norm": 0.8103421926498413,
      "learning_rate": 0.00014463829359090454,
      "loss": 0.2834,
      "step": 13761
    },
    {
      "epoch": 27.690140845070424,
      "grad_norm": 0.8478419780731201,
      "learning_rate": 0.00014463426904115102,
      "loss": 0.3027,
      "step": 13762
    },
    {
      "epoch": 27.69215291750503,
      "grad_norm": 0.861042857170105,
      "learning_rate": 0.00014463024449139753,
      "loss": 0.3124,
      "step": 13763
    },
    {
      "epoch": 27.694164989939637,
      "grad_norm": 0.7754316329956055,
      "learning_rate": 0.00014462621994164402,
      "loss": 0.2936,
      "step": 13764
    },
    {
      "epoch": 27.696177062374247,
      "grad_norm": 0.8199280500411987,
      "learning_rate": 0.00014462219539189056,
      "loss": 0.2922,
      "step": 13765
    },
    {
      "epoch": 27.698189134808853,
      "grad_norm": 0.8108301758766174,
      "learning_rate": 0.00014461817084213704,
      "loss": 0.302,
      "step": 13766
    },
    {
      "epoch": 27.70020120724346,
      "grad_norm": 0.850600004196167,
      "learning_rate": 0.00014461414629238356,
      "loss": 0.3049,
      "step": 13767
    },
    {
      "epoch": 27.70221327967807,
      "grad_norm": 0.850130558013916,
      "learning_rate": 0.00014461012174263004,
      "loss": 0.335,
      "step": 13768
    },
    {
      "epoch": 27.704225352112676,
      "grad_norm": 0.837374746799469,
      "learning_rate": 0.00014460609719287655,
      "loss": 0.2734,
      "step": 13769
    },
    {
      "epoch": 27.706237424547282,
      "grad_norm": 0.8297444581985474,
      "learning_rate": 0.00014460207264312307,
      "loss": 0.3107,
      "step": 13770
    },
    {
      "epoch": 27.708249496981892,
      "grad_norm": 0.7902045249938965,
      "learning_rate": 0.00014459804809336955,
      "loss": 0.2851,
      "step": 13771
    },
    {
      "epoch": 27.7102615694165,
      "grad_norm": 0.8497843146324158,
      "learning_rate": 0.00014459402354361606,
      "loss": 0.3154,
      "step": 13772
    },
    {
      "epoch": 27.712273641851105,
      "grad_norm": 0.8203868269920349,
      "learning_rate": 0.00014458999899386258,
      "loss": 0.2717,
      "step": 13773
    },
    {
      "epoch": 27.714285714285715,
      "grad_norm": 0.8909492492675781,
      "learning_rate": 0.00014458597444410906,
      "loss": 0.3165,
      "step": 13774
    },
    {
      "epoch": 27.71629778672032,
      "grad_norm": 0.7903909683227539,
      "learning_rate": 0.00014458194989435557,
      "loss": 0.291,
      "step": 13775
    },
    {
      "epoch": 27.718309859154928,
      "grad_norm": 0.8207089900970459,
      "learning_rate": 0.00014457792534460209,
      "loss": 0.2893,
      "step": 13776
    },
    {
      "epoch": 27.720321931589538,
      "grad_norm": 0.8240193724632263,
      "learning_rate": 0.00014457390079484857,
      "loss": 0.3147,
      "step": 13777
    },
    {
      "epoch": 27.722334004024145,
      "grad_norm": 0.9173137545585632,
      "learning_rate": 0.00014456987624509508,
      "loss": 0.2806,
      "step": 13778
    },
    {
      "epoch": 27.72434607645875,
      "grad_norm": 0.8131744861602783,
      "learning_rate": 0.00014456585169534157,
      "loss": 0.2924,
      "step": 13779
    },
    {
      "epoch": 27.72635814889336,
      "grad_norm": 0.799447238445282,
      "learning_rate": 0.0001445618271455881,
      "loss": 0.269,
      "step": 13780
    },
    {
      "epoch": 27.728370221327967,
      "grad_norm": 0.8180708885192871,
      "learning_rate": 0.0001445578025958346,
      "loss": 0.2926,
      "step": 13781
    },
    {
      "epoch": 27.730382293762574,
      "grad_norm": 0.9336409568786621,
      "learning_rate": 0.0001445537780460811,
      "loss": 0.3184,
      "step": 13782
    },
    {
      "epoch": 27.732394366197184,
      "grad_norm": 0.8524032235145569,
      "learning_rate": 0.0001445497534963276,
      "loss": 0.3312,
      "step": 13783
    },
    {
      "epoch": 27.73440643863179,
      "grad_norm": 0.8156559467315674,
      "learning_rate": 0.0001445457289465741,
      "loss": 0.2773,
      "step": 13784
    },
    {
      "epoch": 27.736418511066397,
      "grad_norm": 0.8256754875183105,
      "learning_rate": 0.00014454170439682061,
      "loss": 0.2909,
      "step": 13785
    },
    {
      "epoch": 27.738430583501007,
      "grad_norm": 0.7851683497428894,
      "learning_rate": 0.00014453767984706713,
      "loss": 0.3121,
      "step": 13786
    },
    {
      "epoch": 27.740442655935613,
      "grad_norm": 0.8154087662696838,
      "learning_rate": 0.0001445336552973136,
      "loss": 0.28,
      "step": 13787
    },
    {
      "epoch": 27.74245472837022,
      "grad_norm": 0.8525300025939941,
      "learning_rate": 0.00014452963074756012,
      "loss": 0.3077,
      "step": 13788
    },
    {
      "epoch": 27.74446680080483,
      "grad_norm": 0.835159420967102,
      "learning_rate": 0.0001445256061978066,
      "loss": 0.3076,
      "step": 13789
    },
    {
      "epoch": 27.746478873239436,
      "grad_norm": 0.8332032561302185,
      "learning_rate": 0.00014452158164805315,
      "loss": 0.2874,
      "step": 13790
    },
    {
      "epoch": 27.748490945674043,
      "grad_norm": 0.8581051826477051,
      "learning_rate": 0.00014451755709829963,
      "loss": 0.2979,
      "step": 13791
    },
    {
      "epoch": 27.750503018108652,
      "grad_norm": 0.8217785954475403,
      "learning_rate": 0.00014451353254854615,
      "loss": 0.2752,
      "step": 13792
    },
    {
      "epoch": 27.75251509054326,
      "grad_norm": 0.7841001749038696,
      "learning_rate": 0.00014450950799879263,
      "loss": 0.2594,
      "step": 13793
    },
    {
      "epoch": 27.754527162977865,
      "grad_norm": 0.7747488617897034,
      "learning_rate": 0.00014450548344903914,
      "loss": 0.2951,
      "step": 13794
    },
    {
      "epoch": 27.756539235412475,
      "grad_norm": 0.8280455470085144,
      "learning_rate": 0.00014450145889928565,
      "loss": 0.2864,
      "step": 13795
    },
    {
      "epoch": 27.758551307847082,
      "grad_norm": 0.8316911458969116,
      "learning_rate": 0.00014449743434953217,
      "loss": 0.2861,
      "step": 13796
    },
    {
      "epoch": 27.760563380281692,
      "grad_norm": 0.7850828170776367,
      "learning_rate": 0.00014449340979977865,
      "loss": 0.2833,
      "step": 13797
    },
    {
      "epoch": 27.7625754527163,
      "grad_norm": 0.8563140630722046,
      "learning_rate": 0.00014448938525002516,
      "loss": 0.3155,
      "step": 13798
    },
    {
      "epoch": 27.764587525150905,
      "grad_norm": 0.7796343564987183,
      "learning_rate": 0.00014448536070027165,
      "loss": 0.2967,
      "step": 13799
    },
    {
      "epoch": 27.766599597585515,
      "grad_norm": 0.7991251349449158,
      "learning_rate": 0.0001444813361505182,
      "loss": 0.3053,
      "step": 13800
    },
    {
      "epoch": 27.76861167002012,
      "grad_norm": 0.8174277544021606,
      "learning_rate": 0.00014447731160076467,
      "loss": 0.2985,
      "step": 13801
    },
    {
      "epoch": 27.770623742454728,
      "grad_norm": 0.8132980465888977,
      "learning_rate": 0.00014447328705101119,
      "loss": 0.2963,
      "step": 13802
    },
    {
      "epoch": 27.772635814889338,
      "grad_norm": 0.8600842356681824,
      "learning_rate": 0.00014446926250125767,
      "loss": 0.3155,
      "step": 13803
    },
    {
      "epoch": 27.774647887323944,
      "grad_norm": 0.8150426745414734,
      "learning_rate": 0.00014446523795150418,
      "loss": 0.2721,
      "step": 13804
    },
    {
      "epoch": 27.77665995975855,
      "grad_norm": 0.8312621712684631,
      "learning_rate": 0.0001444612134017507,
      "loss": 0.3022,
      "step": 13805
    },
    {
      "epoch": 27.77867203219316,
      "grad_norm": 0.7997766137123108,
      "learning_rate": 0.00014445718885199718,
      "loss": 0.2712,
      "step": 13806
    },
    {
      "epoch": 27.780684104627767,
      "grad_norm": 0.8356156349182129,
      "learning_rate": 0.0001444531643022437,
      "loss": 0.2903,
      "step": 13807
    },
    {
      "epoch": 27.782696177062373,
      "grad_norm": 0.8163408041000366,
      "learning_rate": 0.0001444491397524902,
      "loss": 0.2906,
      "step": 13808
    },
    {
      "epoch": 27.784708249496983,
      "grad_norm": 0.8332036733627319,
      "learning_rate": 0.0001444451152027367,
      "loss": 0.2933,
      "step": 13809
    },
    {
      "epoch": 27.78672032193159,
      "grad_norm": 0.775363028049469,
      "learning_rate": 0.0001444410906529832,
      "loss": 0.2973,
      "step": 13810
    },
    {
      "epoch": 27.788732394366196,
      "grad_norm": 0.7763693928718567,
      "learning_rate": 0.00014443706610322971,
      "loss": 0.2744,
      "step": 13811
    },
    {
      "epoch": 27.790744466800806,
      "grad_norm": 0.7863075733184814,
      "learning_rate": 0.0001444330415534762,
      "loss": 0.3005,
      "step": 13812
    },
    {
      "epoch": 27.792756539235413,
      "grad_norm": 0.8250927925109863,
      "learning_rate": 0.0001444290170037227,
      "loss": 0.307,
      "step": 13813
    },
    {
      "epoch": 27.79476861167002,
      "grad_norm": 0.8154325485229492,
      "learning_rate": 0.0001444249924539692,
      "loss": 0.3045,
      "step": 13814
    },
    {
      "epoch": 27.79678068410463,
      "grad_norm": 0.7758697867393494,
      "learning_rate": 0.00014442096790421574,
      "loss": 0.295,
      "step": 13815
    },
    {
      "epoch": 27.798792756539235,
      "grad_norm": 0.8117054104804993,
      "learning_rate": 0.00014441694335446222,
      "loss": 0.3202,
      "step": 13816
    },
    {
      "epoch": 27.800804828973842,
      "grad_norm": 0.7792965173721313,
      "learning_rate": 0.00014441291880470873,
      "loss": 0.2943,
      "step": 13817
    },
    {
      "epoch": 27.802816901408452,
      "grad_norm": 0.8053746223449707,
      "learning_rate": 0.00014440889425495522,
      "loss": 0.2962,
      "step": 13818
    },
    {
      "epoch": 27.80482897384306,
      "grad_norm": 0.8359693884849548,
      "learning_rate": 0.00014440486970520173,
      "loss": 0.3003,
      "step": 13819
    },
    {
      "epoch": 27.806841046277665,
      "grad_norm": 0.8365604281425476,
      "learning_rate": 0.00014440084515544824,
      "loss": 0.3129,
      "step": 13820
    },
    {
      "epoch": 27.808853118712275,
      "grad_norm": 0.8148526549339294,
      "learning_rate": 0.00014439682060569476,
      "loss": 0.3089,
      "step": 13821
    },
    {
      "epoch": 27.81086519114688,
      "grad_norm": 0.8029958009719849,
      "learning_rate": 0.00014439279605594124,
      "loss": 0.3009,
      "step": 13822
    },
    {
      "epoch": 27.812877263581488,
      "grad_norm": 0.7623211741447449,
      "learning_rate": 0.00014438877150618775,
      "loss": 0.2805,
      "step": 13823
    },
    {
      "epoch": 27.814889336016098,
      "grad_norm": 0.8100267052650452,
      "learning_rate": 0.00014438474695643424,
      "loss": 0.2936,
      "step": 13824
    },
    {
      "epoch": 27.816901408450704,
      "grad_norm": 0.8281301259994507,
      "learning_rate": 0.00014438072240668078,
      "loss": 0.2922,
      "step": 13825
    },
    {
      "epoch": 27.81891348088531,
      "grad_norm": 0.8678866624832153,
      "learning_rate": 0.00014437669785692726,
      "loss": 0.2927,
      "step": 13826
    },
    {
      "epoch": 27.82092555331992,
      "grad_norm": 0.8709585070610046,
      "learning_rate": 0.00014437267330717377,
      "loss": 0.3068,
      "step": 13827
    },
    {
      "epoch": 27.822937625754527,
      "grad_norm": 0.8441687822341919,
      "learning_rate": 0.00014436864875742026,
      "loss": 0.2984,
      "step": 13828
    },
    {
      "epoch": 27.824949698189133,
      "grad_norm": 0.8165860772132874,
      "learning_rate": 0.00014436462420766677,
      "loss": 0.2916,
      "step": 13829
    },
    {
      "epoch": 27.826961770623743,
      "grad_norm": 0.8107860684394836,
      "learning_rate": 0.00014436059965791328,
      "loss": 0.3088,
      "step": 13830
    },
    {
      "epoch": 27.82897384305835,
      "grad_norm": 0.8227680921554565,
      "learning_rate": 0.0001443565751081598,
      "loss": 0.3059,
      "step": 13831
    },
    {
      "epoch": 27.830985915492956,
      "grad_norm": 0.8372220993041992,
      "learning_rate": 0.00014435255055840628,
      "loss": 0.2884,
      "step": 13832
    },
    {
      "epoch": 27.832997987927566,
      "grad_norm": 0.8455427885055542,
      "learning_rate": 0.0001443485260086528,
      "loss": 0.3053,
      "step": 13833
    },
    {
      "epoch": 27.835010060362173,
      "grad_norm": 0.7956783175468445,
      "learning_rate": 0.00014434450145889928,
      "loss": 0.2722,
      "step": 13834
    },
    {
      "epoch": 27.83702213279678,
      "grad_norm": 0.8084475994110107,
      "learning_rate": 0.00014434047690914582,
      "loss": 0.2766,
      "step": 13835
    },
    {
      "epoch": 27.83903420523139,
      "grad_norm": 0.8023751974105835,
      "learning_rate": 0.0001443364523593923,
      "loss": 0.2986,
      "step": 13836
    },
    {
      "epoch": 27.841046277665995,
      "grad_norm": 0.7920693159103394,
      "learning_rate": 0.00014433242780963882,
      "loss": 0.2989,
      "step": 13837
    },
    {
      "epoch": 27.843058350100602,
      "grad_norm": 0.8060933947563171,
      "learning_rate": 0.0001443284032598853,
      "loss": 0.293,
      "step": 13838
    },
    {
      "epoch": 27.845070422535212,
      "grad_norm": 0.8120244145393372,
      "learning_rate": 0.0001443243787101318,
      "loss": 0.2853,
      "step": 13839
    },
    {
      "epoch": 27.84708249496982,
      "grad_norm": 0.8055292367935181,
      "learning_rate": 0.00014432035416037833,
      "loss": 0.3056,
      "step": 13840
    },
    {
      "epoch": 27.84909456740443,
      "grad_norm": 0.8071144223213196,
      "learning_rate": 0.0001443163296106248,
      "loss": 0.2739,
      "step": 13841
    },
    {
      "epoch": 27.851106639839035,
      "grad_norm": 0.8157686591148376,
      "learning_rate": 0.00014431230506087132,
      "loss": 0.3007,
      "step": 13842
    },
    {
      "epoch": 27.85311871227364,
      "grad_norm": 0.7957690954208374,
      "learning_rate": 0.00014430828051111783,
      "loss": 0.2778,
      "step": 13843
    },
    {
      "epoch": 27.85513078470825,
      "grad_norm": 0.7879748940467834,
      "learning_rate": 0.00014430425596136432,
      "loss": 0.2916,
      "step": 13844
    },
    {
      "epoch": 27.857142857142858,
      "grad_norm": 0.7719729542732239,
      "learning_rate": 0.00014430023141161083,
      "loss": 0.2958,
      "step": 13845
    },
    {
      "epoch": 27.859154929577464,
      "grad_norm": 0.7917994856834412,
      "learning_rate": 0.00014429620686185734,
      "loss": 0.303,
      "step": 13846
    },
    {
      "epoch": 27.861167002012074,
      "grad_norm": 0.8170813322067261,
      "learning_rate": 0.00014429218231210383,
      "loss": 0.2921,
      "step": 13847
    },
    {
      "epoch": 27.86317907444668,
      "grad_norm": 0.8164986968040466,
      "learning_rate": 0.00014428815776235034,
      "loss": 0.2952,
      "step": 13848
    },
    {
      "epoch": 27.865191146881287,
      "grad_norm": 0.7921253442764282,
      "learning_rate": 0.00014428413321259683,
      "loss": 0.2811,
      "step": 13849
    },
    {
      "epoch": 27.867203219315897,
      "grad_norm": 0.8177896738052368,
      "learning_rate": 0.00014428010866284337,
      "loss": 0.3318,
      "step": 13850
    },
    {
      "epoch": 27.869215291750503,
      "grad_norm": 0.8232327699661255,
      "learning_rate": 0.00014427608411308985,
      "loss": 0.3054,
      "step": 13851
    },
    {
      "epoch": 27.87122736418511,
      "grad_norm": 0.7863945364952087,
      "learning_rate": 0.00014427205956333636,
      "loss": 0.3,
      "step": 13852
    },
    {
      "epoch": 27.87323943661972,
      "grad_norm": 0.7460017800331116,
      "learning_rate": 0.00014426803501358285,
      "loss": 0.2791,
      "step": 13853
    },
    {
      "epoch": 27.875251509054326,
      "grad_norm": 0.8430429100990295,
      "learning_rate": 0.00014426401046382936,
      "loss": 0.3088,
      "step": 13854
    },
    {
      "epoch": 27.877263581488933,
      "grad_norm": 0.8016794323921204,
      "learning_rate": 0.00014425998591407587,
      "loss": 0.2809,
      "step": 13855
    },
    {
      "epoch": 27.879275653923543,
      "grad_norm": 0.8494243025779724,
      "learning_rate": 0.00014425596136432239,
      "loss": 0.3039,
      "step": 13856
    },
    {
      "epoch": 27.88128772635815,
      "grad_norm": 0.7976228594779968,
      "learning_rate": 0.00014425193681456887,
      "loss": 0.3044,
      "step": 13857
    },
    {
      "epoch": 27.883299798792756,
      "grad_norm": 0.8321148753166199,
      "learning_rate": 0.00014424791226481538,
      "loss": 0.3238,
      "step": 13858
    },
    {
      "epoch": 27.885311871227366,
      "grad_norm": 0.77802973985672,
      "learning_rate": 0.00014424388771506187,
      "loss": 0.2684,
      "step": 13859
    },
    {
      "epoch": 27.887323943661972,
      "grad_norm": 0.7874739766120911,
      "learning_rate": 0.0001442398631653084,
      "loss": 0.2755,
      "step": 13860
    },
    {
      "epoch": 27.88933601609658,
      "grad_norm": 0.788340151309967,
      "learning_rate": 0.0001442358386155549,
      "loss": 0.2931,
      "step": 13861
    },
    {
      "epoch": 27.89134808853119,
      "grad_norm": 0.8235111832618713,
      "learning_rate": 0.0001442318140658014,
      "loss": 0.2999,
      "step": 13862
    },
    {
      "epoch": 27.893360160965795,
      "grad_norm": 0.7846525311470032,
      "learning_rate": 0.0001442277895160479,
      "loss": 0.2746,
      "step": 13863
    },
    {
      "epoch": 27.8953722334004,
      "grad_norm": 0.8440790176391602,
      "learning_rate": 0.0001442237649662944,
      "loss": 0.328,
      "step": 13864
    },
    {
      "epoch": 27.89738430583501,
      "grad_norm": 0.8442246317863464,
      "learning_rate": 0.00014421974041654091,
      "loss": 0.3108,
      "step": 13865
    },
    {
      "epoch": 27.899396378269618,
      "grad_norm": 0.8753852844238281,
      "learning_rate": 0.00014421571586678743,
      "loss": 0.3284,
      "step": 13866
    },
    {
      "epoch": 27.901408450704224,
      "grad_norm": 0.7895985841751099,
      "learning_rate": 0.0001442116913170339,
      "loss": 0.2925,
      "step": 13867
    },
    {
      "epoch": 27.903420523138834,
      "grad_norm": 0.8735191226005554,
      "learning_rate": 0.00014420766676728042,
      "loss": 0.3118,
      "step": 13868
    },
    {
      "epoch": 27.90543259557344,
      "grad_norm": 0.8134374618530273,
      "learning_rate": 0.0001442036422175269,
      "loss": 0.3142,
      "step": 13869
    },
    {
      "epoch": 27.907444668008047,
      "grad_norm": 0.8014823794364929,
      "learning_rate": 0.00014419961766777345,
      "loss": 0.2864,
      "step": 13870
    },
    {
      "epoch": 27.909456740442657,
      "grad_norm": 0.7678934335708618,
      "learning_rate": 0.00014419559311801993,
      "loss": 0.3063,
      "step": 13871
    },
    {
      "epoch": 27.911468812877263,
      "grad_norm": 0.8506094813346863,
      "learning_rate": 0.00014419156856826644,
      "loss": 0.3279,
      "step": 13872
    },
    {
      "epoch": 27.91348088531187,
      "grad_norm": 0.8326452970504761,
      "learning_rate": 0.00014418754401851293,
      "loss": 0.2942,
      "step": 13873
    },
    {
      "epoch": 27.91549295774648,
      "grad_norm": 0.8202739357948303,
      "learning_rate": 0.00014418351946875944,
      "loss": 0.2741,
      "step": 13874
    },
    {
      "epoch": 27.917505030181086,
      "grad_norm": 0.829366147518158,
      "learning_rate": 0.00014417949491900595,
      "loss": 0.2943,
      "step": 13875
    },
    {
      "epoch": 27.919517102615693,
      "grad_norm": 0.7729424238204956,
      "learning_rate": 0.00014417547036925244,
      "loss": 0.3042,
      "step": 13876
    },
    {
      "epoch": 27.921529175050303,
      "grad_norm": 0.8355152010917664,
      "learning_rate": 0.00014417144581949895,
      "loss": 0.3093,
      "step": 13877
    },
    {
      "epoch": 27.92354124748491,
      "grad_norm": 0.8219920992851257,
      "learning_rate": 0.00014416742126974546,
      "loss": 0.3019,
      "step": 13878
    },
    {
      "epoch": 27.925553319919516,
      "grad_norm": 0.8250957131385803,
      "learning_rate": 0.00014416339671999195,
      "loss": 0.3048,
      "step": 13879
    },
    {
      "epoch": 27.927565392354126,
      "grad_norm": 0.850484311580658,
      "learning_rate": 0.00014415937217023846,
      "loss": 0.3,
      "step": 13880
    },
    {
      "epoch": 27.929577464788732,
      "grad_norm": 0.7730538249015808,
      "learning_rate": 0.00014415534762048497,
      "loss": 0.2761,
      "step": 13881
    },
    {
      "epoch": 27.93158953722334,
      "grad_norm": 0.8381786942481995,
      "learning_rate": 0.00014415132307073146,
      "loss": 0.3228,
      "step": 13882
    },
    {
      "epoch": 27.93360160965795,
      "grad_norm": 0.8106959462165833,
      "learning_rate": 0.00014414729852097797,
      "loss": 0.308,
      "step": 13883
    },
    {
      "epoch": 27.935613682092555,
      "grad_norm": 0.8363901376724243,
      "learning_rate": 0.00014414327397122446,
      "loss": 0.2996,
      "step": 13884
    },
    {
      "epoch": 27.93762575452716,
      "grad_norm": 0.8215862512588501,
      "learning_rate": 0.000144139249421471,
      "loss": 0.3233,
      "step": 13885
    },
    {
      "epoch": 27.93963782696177,
      "grad_norm": 0.8065775632858276,
      "learning_rate": 0.00014413522487171748,
      "loss": 0.2865,
      "step": 13886
    },
    {
      "epoch": 27.941649899396378,
      "grad_norm": 0.8097920417785645,
      "learning_rate": 0.000144131200321964,
      "loss": 0.3255,
      "step": 13887
    },
    {
      "epoch": 27.943661971830984,
      "grad_norm": 0.7873779535293579,
      "learning_rate": 0.00014412717577221048,
      "loss": 0.2858,
      "step": 13888
    },
    {
      "epoch": 27.945674044265594,
      "grad_norm": 0.8131843209266663,
      "learning_rate": 0.000144123151222457,
      "loss": 0.316,
      "step": 13889
    },
    {
      "epoch": 27.9476861167002,
      "grad_norm": 0.8680776953697205,
      "learning_rate": 0.0001441191266727035,
      "loss": 0.3195,
      "step": 13890
    },
    {
      "epoch": 27.949698189134807,
      "grad_norm": 0.8253580927848816,
      "learning_rate": 0.00014411510212295001,
      "loss": 0.3103,
      "step": 13891
    },
    {
      "epoch": 27.951710261569417,
      "grad_norm": 0.8129251003265381,
      "learning_rate": 0.0001441110775731965,
      "loss": 0.3,
      "step": 13892
    },
    {
      "epoch": 27.953722334004024,
      "grad_norm": 0.8128021359443665,
      "learning_rate": 0.000144107053023443,
      "loss": 0.2769,
      "step": 13893
    },
    {
      "epoch": 27.955734406438633,
      "grad_norm": 0.8493416905403137,
      "learning_rate": 0.0001441030284736895,
      "loss": 0.3018,
      "step": 13894
    },
    {
      "epoch": 27.95774647887324,
      "grad_norm": 0.8160996437072754,
      "learning_rate": 0.00014409900392393604,
      "loss": 0.295,
      "step": 13895
    },
    {
      "epoch": 27.959758551307846,
      "grad_norm": 0.793428897857666,
      "learning_rate": 0.00014409497937418252,
      "loss": 0.2862,
      "step": 13896
    },
    {
      "epoch": 27.961770623742456,
      "grad_norm": 0.8260968327522278,
      "learning_rate": 0.00014409095482442903,
      "loss": 0.3153,
      "step": 13897
    },
    {
      "epoch": 27.963782696177063,
      "grad_norm": 0.8335617780685425,
      "learning_rate": 0.00014408693027467552,
      "loss": 0.2908,
      "step": 13898
    },
    {
      "epoch": 27.96579476861167,
      "grad_norm": 0.7616159915924072,
      "learning_rate": 0.00014408290572492203,
      "loss": 0.2739,
      "step": 13899
    },
    {
      "epoch": 27.96780684104628,
      "grad_norm": 0.8220507502555847,
      "learning_rate": 0.00014407888117516854,
      "loss": 0.3154,
      "step": 13900
    },
    {
      "epoch": 27.969818913480886,
      "grad_norm": 0.8258208632469177,
      "learning_rate": 0.00014407485662541506,
      "loss": 0.2974,
      "step": 13901
    },
    {
      "epoch": 27.971830985915492,
      "grad_norm": 0.849293053150177,
      "learning_rate": 0.00014407083207566154,
      "loss": 0.3098,
      "step": 13902
    },
    {
      "epoch": 27.973843058350102,
      "grad_norm": 0.8241729140281677,
      "learning_rate": 0.00014406680752590805,
      "loss": 0.2935,
      "step": 13903
    },
    {
      "epoch": 27.97585513078471,
      "grad_norm": 0.7883780002593994,
      "learning_rate": 0.00014406278297615454,
      "loss": 0.2972,
      "step": 13904
    },
    {
      "epoch": 27.977867203219315,
      "grad_norm": 0.8332850933074951,
      "learning_rate": 0.00014405875842640108,
      "loss": 0.3275,
      "step": 13905
    },
    {
      "epoch": 27.979879275653925,
      "grad_norm": 0.8369799256324768,
      "learning_rate": 0.00014405473387664756,
      "loss": 0.3005,
      "step": 13906
    },
    {
      "epoch": 27.98189134808853,
      "grad_norm": 0.8199807405471802,
      "learning_rate": 0.00014405070932689407,
      "loss": 0.3052,
      "step": 13907
    },
    {
      "epoch": 27.983903420523138,
      "grad_norm": 0.8175498247146606,
      "learning_rate": 0.00014404668477714056,
      "loss": 0.3279,
      "step": 13908
    },
    {
      "epoch": 27.985915492957748,
      "grad_norm": 0.8381057977676392,
      "learning_rate": 0.00014404266022738707,
      "loss": 0.2867,
      "step": 13909
    },
    {
      "epoch": 27.987927565392354,
      "grad_norm": 0.9043185710906982,
      "learning_rate": 0.00014403863567763358,
      "loss": 0.2835,
      "step": 13910
    },
    {
      "epoch": 27.98993963782696,
      "grad_norm": 0.8280707001686096,
      "learning_rate": 0.00014403461112788007,
      "loss": 0.2906,
      "step": 13911
    },
    {
      "epoch": 27.99195171026157,
      "grad_norm": 0.8107689023017883,
      "learning_rate": 0.00014403058657812658,
      "loss": 0.2968,
      "step": 13912
    },
    {
      "epoch": 27.993963782696177,
      "grad_norm": 0.8447045087814331,
      "learning_rate": 0.00014402656202837307,
      "loss": 0.3149,
      "step": 13913
    },
    {
      "epoch": 27.995975855130784,
      "grad_norm": 0.8413166999816895,
      "learning_rate": 0.00014402253747861958,
      "loss": 0.3149,
      "step": 13914
    },
    {
      "epoch": 27.997987927565394,
      "grad_norm": 0.8686469793319702,
      "learning_rate": 0.0001440185129288661,
      "loss": 0.3227,
      "step": 13915
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.8470941781997681,
      "learning_rate": 0.0001440144883791126,
      "loss": 0.2956,
      "step": 13916
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.2151905298233032,
      "eval_runtime": 49.8665,
      "eval_samples_per_second": 19.893,
      "eval_steps_per_second": 2.487,
      "step": 13916
    },
    {
      "epoch": 28.002012072434606,
      "grad_norm": 0.7587860822677612,
      "learning_rate": 0.0001440104638293591,
      "loss": 0.2509,
      "step": 13917
    },
    {
      "epoch": 28.004024144869216,
      "grad_norm": 0.6793289184570312,
      "learning_rate": 0.0001440064392796056,
      "loss": 0.2411,
      "step": 13918
    },
    {
      "epoch": 28.006036217303823,
      "grad_norm": 0.7218167185783386,
      "learning_rate": 0.00014400241472985209,
      "loss": 0.243,
      "step": 13919
    },
    {
      "epoch": 28.00804828973843,
      "grad_norm": 0.7117940187454224,
      "learning_rate": 0.00014399839018009862,
      "loss": 0.2142,
      "step": 13920
    },
    {
      "epoch": 28.01006036217304,
      "grad_norm": 0.7860442996025085,
      "learning_rate": 0.0001439943656303451,
      "loss": 0.2437,
      "step": 13921
    },
    {
      "epoch": 28.012072434607646,
      "grad_norm": 0.7861413955688477,
      "learning_rate": 0.00014399034108059162,
      "loss": 0.2372,
      "step": 13922
    },
    {
      "epoch": 28.014084507042252,
      "grad_norm": 0.8314623236656189,
      "learning_rate": 0.0001439863165308381,
      "loss": 0.257,
      "step": 13923
    },
    {
      "epoch": 28.016096579476862,
      "grad_norm": 0.6956418752670288,
      "learning_rate": 0.00014398229198108462,
      "loss": 0.2388,
      "step": 13924
    },
    {
      "epoch": 28.01810865191147,
      "grad_norm": 0.6973669528961182,
      "learning_rate": 0.00014397826743133113,
      "loss": 0.2201,
      "step": 13925
    },
    {
      "epoch": 28.020120724346075,
      "grad_norm": 0.7750295400619507,
      "learning_rate": 0.00014397424288157764,
      "loss": 0.2358,
      "step": 13926
    },
    {
      "epoch": 28.022132796780685,
      "grad_norm": 0.7479721903800964,
      "learning_rate": 0.00014397021833182413,
      "loss": 0.2353,
      "step": 13927
    },
    {
      "epoch": 28.02414486921529,
      "grad_norm": 0.7063255310058594,
      "learning_rate": 0.00014396619378207064,
      "loss": 0.2398,
      "step": 13928
    },
    {
      "epoch": 28.026156941649898,
      "grad_norm": 0.6814321279525757,
      "learning_rate": 0.00014396216923231713,
      "loss": 0.2161,
      "step": 13929
    },
    {
      "epoch": 28.028169014084508,
      "grad_norm": 0.7353249192237854,
      "learning_rate": 0.00014395814468256367,
      "loss": 0.2209,
      "step": 13930
    },
    {
      "epoch": 28.030181086519114,
      "grad_norm": 0.7179041504859924,
      "learning_rate": 0.00014395412013281015,
      "loss": 0.2229,
      "step": 13931
    },
    {
      "epoch": 28.03219315895372,
      "grad_norm": 0.7892978191375732,
      "learning_rate": 0.00014395009558305666,
      "loss": 0.2694,
      "step": 13932
    },
    {
      "epoch": 28.03420523138833,
      "grad_norm": 0.7909760475158691,
      "learning_rate": 0.00014394607103330315,
      "loss": 0.2658,
      "step": 13933
    },
    {
      "epoch": 28.036217303822937,
      "grad_norm": 0.7673904895782471,
      "learning_rate": 0.00014394204648354966,
      "loss": 0.2356,
      "step": 13934
    },
    {
      "epoch": 28.038229376257544,
      "grad_norm": 0.6764724850654602,
      "learning_rate": 0.00014393802193379617,
      "loss": 0.2164,
      "step": 13935
    },
    {
      "epoch": 28.040241448692154,
      "grad_norm": 0.7502451539039612,
      "learning_rate": 0.00014393399738404268,
      "loss": 0.2365,
      "step": 13936
    },
    {
      "epoch": 28.04225352112676,
      "grad_norm": 0.7430627346038818,
      "learning_rate": 0.00014392997283428917,
      "loss": 0.253,
      "step": 13937
    },
    {
      "epoch": 28.044265593561367,
      "grad_norm": 0.7522829174995422,
      "learning_rate": 0.00014392594828453568,
      "loss": 0.2427,
      "step": 13938
    },
    {
      "epoch": 28.046277665995976,
      "grad_norm": 0.7096893787384033,
      "learning_rate": 0.00014392192373478217,
      "loss": 0.2244,
      "step": 13939
    },
    {
      "epoch": 28.048289738430583,
      "grad_norm": 0.729336678981781,
      "learning_rate": 0.00014391789918502868,
      "loss": 0.2238,
      "step": 13940
    },
    {
      "epoch": 28.050301810865193,
      "grad_norm": 0.8009111881256104,
      "learning_rate": 0.0001439138746352752,
      "loss": 0.2509,
      "step": 13941
    },
    {
      "epoch": 28.0523138832998,
      "grad_norm": 0.6880134344100952,
      "learning_rate": 0.0001439098500855217,
      "loss": 0.2311,
      "step": 13942
    },
    {
      "epoch": 28.054325955734406,
      "grad_norm": 0.7658634185791016,
      "learning_rate": 0.0001439058255357682,
      "loss": 0.2427,
      "step": 13943
    },
    {
      "epoch": 28.056338028169016,
      "grad_norm": 0.7301385998725891,
      "learning_rate": 0.0001439018009860147,
      "loss": 0.2213,
      "step": 13944
    },
    {
      "epoch": 28.058350100603622,
      "grad_norm": 0.7392843961715698,
      "learning_rate": 0.00014389777643626119,
      "loss": 0.2427,
      "step": 13945
    },
    {
      "epoch": 28.06036217303823,
      "grad_norm": 0.7824962735176086,
      "learning_rate": 0.0001438937518865077,
      "loss": 0.2359,
      "step": 13946
    },
    {
      "epoch": 28.06237424547284,
      "grad_norm": 0.7617596983909607,
      "learning_rate": 0.0001438897273367542,
      "loss": 0.2273,
      "step": 13947
    },
    {
      "epoch": 28.064386317907445,
      "grad_norm": 0.752395749092102,
      "learning_rate": 0.0001438857027870007,
      "loss": 0.2343,
      "step": 13948
    },
    {
      "epoch": 28.06639839034205,
      "grad_norm": 0.7445898056030273,
      "learning_rate": 0.0001438816782372472,
      "loss": 0.2229,
      "step": 13949
    },
    {
      "epoch": 28.06841046277666,
      "grad_norm": 0.7466303110122681,
      "learning_rate": 0.00014387765368749372,
      "loss": 0.2448,
      "step": 13950
    },
    {
      "epoch": 28.070422535211268,
      "grad_norm": 0.7999193072319031,
      "learning_rate": 0.00014387362913774023,
      "loss": 0.2531,
      "step": 13951
    },
    {
      "epoch": 28.072434607645874,
      "grad_norm": 0.8025931119918823,
      "learning_rate": 0.00014386960458798672,
      "loss": 0.2607,
      "step": 13952
    },
    {
      "epoch": 28.074446680080484,
      "grad_norm": 0.7314179539680481,
      "learning_rate": 0.00014386558003823323,
      "loss": 0.2327,
      "step": 13953
    },
    {
      "epoch": 28.07645875251509,
      "grad_norm": 0.7161502838134766,
      "learning_rate": 0.00014386155548847971,
      "loss": 0.2317,
      "step": 13954
    },
    {
      "epoch": 28.078470824949697,
      "grad_norm": 0.7621152400970459,
      "learning_rate": 0.00014385753093872623,
      "loss": 0.2302,
      "step": 13955
    },
    {
      "epoch": 28.080482897384307,
      "grad_norm": 0.7418241500854492,
      "learning_rate": 0.00014385350638897274,
      "loss": 0.2452,
      "step": 13956
    },
    {
      "epoch": 28.082494969818914,
      "grad_norm": 0.7662647366523743,
      "learning_rate": 0.00014384948183921925,
      "loss": 0.2451,
      "step": 13957
    },
    {
      "epoch": 28.08450704225352,
      "grad_norm": 0.7205809354782104,
      "learning_rate": 0.00014384545728946574,
      "loss": 0.2356,
      "step": 13958
    },
    {
      "epoch": 28.08651911468813,
      "grad_norm": 0.7175899147987366,
      "learning_rate": 0.00014384143273971225,
      "loss": 0.2133,
      "step": 13959
    },
    {
      "epoch": 28.088531187122737,
      "grad_norm": 0.7311840653419495,
      "learning_rate": 0.00014383740818995873,
      "loss": 0.2236,
      "step": 13960
    },
    {
      "epoch": 28.090543259557343,
      "grad_norm": 0.7180078029632568,
      "learning_rate": 0.00014383338364020527,
      "loss": 0.2307,
      "step": 13961
    },
    {
      "epoch": 28.092555331991953,
      "grad_norm": 0.8126228451728821,
      "learning_rate": 0.00014382935909045176,
      "loss": 0.2515,
      "step": 13962
    },
    {
      "epoch": 28.09456740442656,
      "grad_norm": 0.7439733743667603,
      "learning_rate": 0.00014382533454069827,
      "loss": 0.238,
      "step": 13963
    },
    {
      "epoch": 28.096579476861166,
      "grad_norm": 0.7682477235794067,
      "learning_rate": 0.00014382130999094476,
      "loss": 0.2169,
      "step": 13964
    },
    {
      "epoch": 28.098591549295776,
      "grad_norm": 0.7227667570114136,
      "learning_rate": 0.00014381728544119127,
      "loss": 0.2335,
      "step": 13965
    },
    {
      "epoch": 28.100603621730382,
      "grad_norm": 0.7875794768333435,
      "learning_rate": 0.00014381326089143778,
      "loss": 0.2389,
      "step": 13966
    },
    {
      "epoch": 28.10261569416499,
      "grad_norm": 0.7733066082000732,
      "learning_rate": 0.0001438092363416843,
      "loss": 0.2546,
      "step": 13967
    },
    {
      "epoch": 28.1046277665996,
      "grad_norm": 0.7181590795516968,
      "learning_rate": 0.00014380521179193078,
      "loss": 0.2138,
      "step": 13968
    },
    {
      "epoch": 28.106639839034205,
      "grad_norm": 0.7780157923698425,
      "learning_rate": 0.0001438011872421773,
      "loss": 0.2408,
      "step": 13969
    },
    {
      "epoch": 28.10865191146881,
      "grad_norm": 0.8414782285690308,
      "learning_rate": 0.00014379716269242377,
      "loss": 0.2833,
      "step": 13970
    },
    {
      "epoch": 28.11066398390342,
      "grad_norm": 0.7406511902809143,
      "learning_rate": 0.00014379313814267031,
      "loss": 0.2421,
      "step": 13971
    },
    {
      "epoch": 28.112676056338028,
      "grad_norm": 0.7675588130950928,
      "learning_rate": 0.0001437891135929168,
      "loss": 0.2455,
      "step": 13972
    },
    {
      "epoch": 28.114688128772634,
      "grad_norm": 0.7653166651725769,
      "learning_rate": 0.0001437850890431633,
      "loss": 0.2453,
      "step": 13973
    },
    {
      "epoch": 28.116700201207244,
      "grad_norm": 0.7701137065887451,
      "learning_rate": 0.0001437810644934098,
      "loss": 0.2668,
      "step": 13974
    },
    {
      "epoch": 28.11871227364185,
      "grad_norm": 0.7679930329322815,
      "learning_rate": 0.0001437770399436563,
      "loss": 0.2411,
      "step": 13975
    },
    {
      "epoch": 28.120724346076457,
      "grad_norm": 0.7556629180908203,
      "learning_rate": 0.00014377301539390282,
      "loss": 0.2501,
      "step": 13976
    },
    {
      "epoch": 28.122736418511067,
      "grad_norm": 0.7646316885948181,
      "learning_rate": 0.00014376899084414933,
      "loss": 0.2328,
      "step": 13977
    },
    {
      "epoch": 28.124748490945674,
      "grad_norm": 0.7951898574829102,
      "learning_rate": 0.00014376496629439582,
      "loss": 0.2335,
      "step": 13978
    },
    {
      "epoch": 28.12676056338028,
      "grad_norm": 0.7948253750801086,
      "learning_rate": 0.00014376094174464233,
      "loss": 0.2462,
      "step": 13979
    },
    {
      "epoch": 28.12877263581489,
      "grad_norm": 0.7548762559890747,
      "learning_rate": 0.00014375691719488882,
      "loss": 0.2243,
      "step": 13980
    },
    {
      "epoch": 28.130784708249497,
      "grad_norm": 0.7908279895782471,
      "learning_rate": 0.00014375289264513533,
      "loss": 0.2282,
      "step": 13981
    },
    {
      "epoch": 28.132796780684103,
      "grad_norm": 0.7609739303588867,
      "learning_rate": 0.00014374886809538184,
      "loss": 0.2517,
      "step": 13982
    },
    {
      "epoch": 28.134808853118713,
      "grad_norm": 0.7786126136779785,
      "learning_rate": 0.00014374484354562833,
      "loss": 0.2521,
      "step": 13983
    },
    {
      "epoch": 28.13682092555332,
      "grad_norm": 0.754361093044281,
      "learning_rate": 0.00014374081899587484,
      "loss": 0.2453,
      "step": 13984
    },
    {
      "epoch": 28.138832997987926,
      "grad_norm": 0.8327226042747498,
      "learning_rate": 0.00014373679444612135,
      "loss": 0.2653,
      "step": 13985
    },
    {
      "epoch": 28.140845070422536,
      "grad_norm": 0.8035540580749512,
      "learning_rate": 0.00014373276989636786,
      "loss": 0.2281,
      "step": 13986
    },
    {
      "epoch": 28.142857142857142,
      "grad_norm": 0.7407401204109192,
      "learning_rate": 0.00014372874534661435,
      "loss": 0.233,
      "step": 13987
    },
    {
      "epoch": 28.14486921529175,
      "grad_norm": 0.834670901298523,
      "learning_rate": 0.00014372472079686086,
      "loss": 0.2432,
      "step": 13988
    },
    {
      "epoch": 28.14688128772636,
      "grad_norm": 0.807711660861969,
      "learning_rate": 0.00014372069624710734,
      "loss": 0.2563,
      "step": 13989
    },
    {
      "epoch": 28.148893360160965,
      "grad_norm": 0.7940139174461365,
      "learning_rate": 0.00014371667169735386,
      "loss": 0.2595,
      "step": 13990
    },
    {
      "epoch": 28.15090543259557,
      "grad_norm": 0.7243675589561462,
      "learning_rate": 0.00014371264714760037,
      "loss": 0.2376,
      "step": 13991
    },
    {
      "epoch": 28.15291750503018,
      "grad_norm": 0.71541428565979,
      "learning_rate": 0.00014370862259784688,
      "loss": 0.2321,
      "step": 13992
    },
    {
      "epoch": 28.154929577464788,
      "grad_norm": 0.7930836081504822,
      "learning_rate": 0.00014370459804809337,
      "loss": 0.2289,
      "step": 13993
    },
    {
      "epoch": 28.156941649899398,
      "grad_norm": 0.7824115753173828,
      "learning_rate": 0.00014370057349833988,
      "loss": 0.2597,
      "step": 13994
    },
    {
      "epoch": 28.158953722334005,
      "grad_norm": 0.802405595779419,
      "learning_rate": 0.00014369654894858636,
      "loss": 0.2401,
      "step": 13995
    },
    {
      "epoch": 28.16096579476861,
      "grad_norm": 0.7762100100517273,
      "learning_rate": 0.0001436925243988329,
      "loss": 0.2356,
      "step": 13996
    },
    {
      "epoch": 28.16297786720322,
      "grad_norm": 0.8244270086288452,
      "learning_rate": 0.0001436884998490794,
      "loss": 0.2422,
      "step": 13997
    },
    {
      "epoch": 28.164989939637827,
      "grad_norm": 0.7577173113822937,
      "learning_rate": 0.0001436844752993259,
      "loss": 0.2373,
      "step": 13998
    },
    {
      "epoch": 28.167002012072434,
      "grad_norm": 0.8200576901435852,
      "learning_rate": 0.00014368045074957239,
      "loss": 0.2541,
      "step": 13999
    },
    {
      "epoch": 28.169014084507044,
      "grad_norm": 0.782426118850708,
      "learning_rate": 0.0001436764261998189,
      "loss": 0.2633,
      "step": 14000
    },
    {
      "epoch": 28.17102615694165,
      "grad_norm": 0.8066853880882263,
      "learning_rate": 0.0001436724016500654,
      "loss": 0.2277,
      "step": 14001
    },
    {
      "epoch": 28.173038229376257,
      "grad_norm": 0.7443135976791382,
      "learning_rate": 0.00014366837710031192,
      "loss": 0.2508,
      "step": 14002
    },
    {
      "epoch": 28.175050301810867,
      "grad_norm": 0.7531794309616089,
      "learning_rate": 0.0001436643525505584,
      "loss": 0.2602,
      "step": 14003
    },
    {
      "epoch": 28.177062374245473,
      "grad_norm": 0.8136844635009766,
      "learning_rate": 0.00014366032800080492,
      "loss": 0.2747,
      "step": 14004
    },
    {
      "epoch": 28.17907444668008,
      "grad_norm": 0.739870011806488,
      "learning_rate": 0.0001436563034510514,
      "loss": 0.2378,
      "step": 14005
    },
    {
      "epoch": 28.18108651911469,
      "grad_norm": 0.7779455184936523,
      "learning_rate": 0.00014365227890129794,
      "loss": 0.2454,
      "step": 14006
    },
    {
      "epoch": 28.183098591549296,
      "grad_norm": 0.7789104580879211,
      "learning_rate": 0.00014364825435154443,
      "loss": 0.2405,
      "step": 14007
    },
    {
      "epoch": 28.185110663983902,
      "grad_norm": 0.8191255331039429,
      "learning_rate": 0.00014364422980179094,
      "loss": 0.2688,
      "step": 14008
    },
    {
      "epoch": 28.187122736418512,
      "grad_norm": 0.7919420003890991,
      "learning_rate": 0.00014364020525203743,
      "loss": 0.2373,
      "step": 14009
    },
    {
      "epoch": 28.18913480885312,
      "grad_norm": 0.7646768093109131,
      "learning_rate": 0.00014363618070228394,
      "loss": 0.228,
      "step": 14010
    },
    {
      "epoch": 28.191146881287725,
      "grad_norm": 0.7972437739372253,
      "learning_rate": 0.00014363215615253045,
      "loss": 0.2446,
      "step": 14011
    },
    {
      "epoch": 28.193158953722335,
      "grad_norm": 0.7553104162216187,
      "learning_rate": 0.00014362813160277696,
      "loss": 0.231,
      "step": 14012
    },
    {
      "epoch": 28.19517102615694,
      "grad_norm": 0.7450980544090271,
      "learning_rate": 0.00014362410705302345,
      "loss": 0.2464,
      "step": 14013
    },
    {
      "epoch": 28.197183098591548,
      "grad_norm": 0.765927791595459,
      "learning_rate": 0.00014362008250326996,
      "loss": 0.244,
      "step": 14014
    },
    {
      "epoch": 28.199195171026158,
      "grad_norm": 0.8523359298706055,
      "learning_rate": 0.00014361605795351645,
      "loss": 0.2573,
      "step": 14015
    },
    {
      "epoch": 28.201207243460765,
      "grad_norm": 0.771266520023346,
      "learning_rate": 0.00014361203340376296,
      "loss": 0.2523,
      "step": 14016
    },
    {
      "epoch": 28.20321931589537,
      "grad_norm": 0.7850390076637268,
      "learning_rate": 0.00014360800885400947,
      "loss": 0.2256,
      "step": 14017
    },
    {
      "epoch": 28.20523138832998,
      "grad_norm": 0.7936782240867615,
      "learning_rate": 0.00014360398430425595,
      "loss": 0.2581,
      "step": 14018
    },
    {
      "epoch": 28.207243460764587,
      "grad_norm": 0.7993671894073486,
      "learning_rate": 0.00014359995975450247,
      "loss": 0.2424,
      "step": 14019
    },
    {
      "epoch": 28.209255533199194,
      "grad_norm": 0.7641136050224304,
      "learning_rate": 0.00014359593520474898,
      "loss": 0.2309,
      "step": 14020
    },
    {
      "epoch": 28.211267605633804,
      "grad_norm": 0.7547406554222107,
      "learning_rate": 0.0001435919106549955,
      "loss": 0.2316,
      "step": 14021
    },
    {
      "epoch": 28.21327967806841,
      "grad_norm": 0.7643262147903442,
      "learning_rate": 0.00014358788610524198,
      "loss": 0.2277,
      "step": 14022
    },
    {
      "epoch": 28.215291750503017,
      "grad_norm": 0.7804955244064331,
      "learning_rate": 0.0001435838615554885,
      "loss": 0.2405,
      "step": 14023
    },
    {
      "epoch": 28.217303822937627,
      "grad_norm": 0.8042469620704651,
      "learning_rate": 0.00014357983700573497,
      "loss": 0.2581,
      "step": 14024
    },
    {
      "epoch": 28.219315895372233,
      "grad_norm": 0.8011068105697632,
      "learning_rate": 0.00014357581245598149,
      "loss": 0.2505,
      "step": 14025
    },
    {
      "epoch": 28.22132796780684,
      "grad_norm": 0.8019136786460876,
      "learning_rate": 0.000143571787906228,
      "loss": 0.2707,
      "step": 14026
    },
    {
      "epoch": 28.22334004024145,
      "grad_norm": 0.807847261428833,
      "learning_rate": 0.0001435677633564745,
      "loss": 0.2585,
      "step": 14027
    },
    {
      "epoch": 28.225352112676056,
      "grad_norm": 0.7885246276855469,
      "learning_rate": 0.000143563738806721,
      "loss": 0.2602,
      "step": 14028
    },
    {
      "epoch": 28.227364185110662,
      "grad_norm": 0.7425408363342285,
      "learning_rate": 0.0001435597142569675,
      "loss": 0.2356,
      "step": 14029
    },
    {
      "epoch": 28.229376257545272,
      "grad_norm": 0.7907676100730896,
      "learning_rate": 0.000143555689707214,
      "loss": 0.2427,
      "step": 14030
    },
    {
      "epoch": 28.23138832997988,
      "grad_norm": 0.7924006581306458,
      "learning_rate": 0.00014355166515746053,
      "loss": 0.2597,
      "step": 14031
    },
    {
      "epoch": 28.233400402414485,
      "grad_norm": 0.8494141101837158,
      "learning_rate": 0.00014354764060770702,
      "loss": 0.2658,
      "step": 14032
    },
    {
      "epoch": 28.235412474849095,
      "grad_norm": 0.7298098206520081,
      "learning_rate": 0.00014354361605795353,
      "loss": 0.2452,
      "step": 14033
    },
    {
      "epoch": 28.2374245472837,
      "grad_norm": 0.7453085780143738,
      "learning_rate": 0.00014353959150820001,
      "loss": 0.244,
      "step": 14034
    },
    {
      "epoch": 28.239436619718308,
      "grad_norm": 0.7776082158088684,
      "learning_rate": 0.00014353556695844653,
      "loss": 0.2512,
      "step": 14035
    },
    {
      "epoch": 28.241448692152918,
      "grad_norm": 0.7766296863555908,
      "learning_rate": 0.00014353154240869304,
      "loss": 0.2438,
      "step": 14036
    },
    {
      "epoch": 28.243460764587525,
      "grad_norm": 0.766361653804779,
      "learning_rate": 0.00014352751785893955,
      "loss": 0.2583,
      "step": 14037
    },
    {
      "epoch": 28.24547283702213,
      "grad_norm": 0.803503155708313,
      "learning_rate": 0.00014352349330918604,
      "loss": 0.2633,
      "step": 14038
    },
    {
      "epoch": 28.24748490945674,
      "grad_norm": 0.7854212522506714,
      "learning_rate": 0.00014351946875943255,
      "loss": 0.2441,
      "step": 14039
    },
    {
      "epoch": 28.249496981891348,
      "grad_norm": 0.8541795611381531,
      "learning_rate": 0.00014351544420967903,
      "loss": 0.2709,
      "step": 14040
    },
    {
      "epoch": 28.251509054325957,
      "grad_norm": 0.8010191321372986,
      "learning_rate": 0.00014351141965992557,
      "loss": 0.2669,
      "step": 14041
    },
    {
      "epoch": 28.253521126760564,
      "grad_norm": 0.8781204223632812,
      "learning_rate": 0.00014350739511017206,
      "loss": 0.2689,
      "step": 14042
    },
    {
      "epoch": 28.25553319919517,
      "grad_norm": 0.8493524193763733,
      "learning_rate": 0.00014350337056041857,
      "loss": 0.2821,
      "step": 14043
    },
    {
      "epoch": 28.25754527162978,
      "grad_norm": 0.8393985629081726,
      "learning_rate": 0.00014349934601066506,
      "loss": 0.2337,
      "step": 14044
    },
    {
      "epoch": 28.259557344064387,
      "grad_norm": 0.7240688800811768,
      "learning_rate": 0.00014349532146091157,
      "loss": 0.2304,
      "step": 14045
    },
    {
      "epoch": 28.261569416498993,
      "grad_norm": 0.807735800743103,
      "learning_rate": 0.00014349129691115808,
      "loss": 0.2644,
      "step": 14046
    },
    {
      "epoch": 28.263581488933603,
      "grad_norm": 0.7977282404899597,
      "learning_rate": 0.0001434872723614046,
      "loss": 0.2203,
      "step": 14047
    },
    {
      "epoch": 28.26559356136821,
      "grad_norm": 0.7663992047309875,
      "learning_rate": 0.00014348324781165108,
      "loss": 0.2472,
      "step": 14048
    },
    {
      "epoch": 28.267605633802816,
      "grad_norm": 0.8247396349906921,
      "learning_rate": 0.0001434792232618976,
      "loss": 0.2599,
      "step": 14049
    },
    {
      "epoch": 28.269617706237426,
      "grad_norm": 0.8255720734596252,
      "learning_rate": 0.00014347519871214407,
      "loss": 0.2721,
      "step": 14050
    },
    {
      "epoch": 28.271629778672033,
      "grad_norm": 0.8263118267059326,
      "learning_rate": 0.0001434711741623906,
      "loss": 0.2577,
      "step": 14051
    },
    {
      "epoch": 28.27364185110664,
      "grad_norm": 0.8160929679870605,
      "learning_rate": 0.0001434671496126371,
      "loss": 0.2445,
      "step": 14052
    },
    {
      "epoch": 28.27565392354125,
      "grad_norm": 0.8077276349067688,
      "learning_rate": 0.00014346312506288358,
      "loss": 0.2547,
      "step": 14053
    },
    {
      "epoch": 28.277665995975855,
      "grad_norm": 0.8391115665435791,
      "learning_rate": 0.0001434591005131301,
      "loss": 0.2647,
      "step": 14054
    },
    {
      "epoch": 28.279678068410462,
      "grad_norm": 0.8403608798980713,
      "learning_rate": 0.0001434550759633766,
      "loss": 0.2598,
      "step": 14055
    },
    {
      "epoch": 28.281690140845072,
      "grad_norm": 0.8650616407394409,
      "learning_rate": 0.00014345105141362312,
      "loss": 0.267,
      "step": 14056
    },
    {
      "epoch": 28.28370221327968,
      "grad_norm": 0.743932843208313,
      "learning_rate": 0.0001434470268638696,
      "loss": 0.2426,
      "step": 14057
    },
    {
      "epoch": 28.285714285714285,
      "grad_norm": 0.7786108255386353,
      "learning_rate": 0.00014344300231411612,
      "loss": 0.2472,
      "step": 14058
    },
    {
      "epoch": 28.287726358148895,
      "grad_norm": 0.8124315142631531,
      "learning_rate": 0.0001434389777643626,
      "loss": 0.2776,
      "step": 14059
    },
    {
      "epoch": 28.2897384305835,
      "grad_norm": 0.8670357465744019,
      "learning_rate": 0.00014343495321460912,
      "loss": 0.2473,
      "step": 14060
    },
    {
      "epoch": 28.291750503018108,
      "grad_norm": 0.7780382633209229,
      "learning_rate": 0.00014343092866485563,
      "loss": 0.2516,
      "step": 14061
    },
    {
      "epoch": 28.293762575452718,
      "grad_norm": 0.7772423624992371,
      "learning_rate": 0.00014342690411510214,
      "loss": 0.2568,
      "step": 14062
    },
    {
      "epoch": 28.295774647887324,
      "grad_norm": 0.8649764657020569,
      "learning_rate": 0.00014342287956534862,
      "loss": 0.273,
      "step": 14063
    },
    {
      "epoch": 28.29778672032193,
      "grad_norm": 0.8496367335319519,
      "learning_rate": 0.00014341885501559514,
      "loss": 0.2616,
      "step": 14064
    },
    {
      "epoch": 28.29979879275654,
      "grad_norm": 0.7981406450271606,
      "learning_rate": 0.00014341483046584162,
      "loss": 0.2622,
      "step": 14065
    },
    {
      "epoch": 28.301810865191147,
      "grad_norm": 0.8439220786094666,
      "learning_rate": 0.00014341080591608816,
      "loss": 0.2817,
      "step": 14066
    },
    {
      "epoch": 28.303822937625753,
      "grad_norm": 0.8108251690864563,
      "learning_rate": 0.00014340678136633465,
      "loss": 0.222,
      "step": 14067
    },
    {
      "epoch": 28.305835010060363,
      "grad_norm": 0.8266608715057373,
      "learning_rate": 0.00014340275681658116,
      "loss": 0.2887,
      "step": 14068
    },
    {
      "epoch": 28.30784708249497,
      "grad_norm": 0.7951242923736572,
      "learning_rate": 0.00014339873226682764,
      "loss": 0.2476,
      "step": 14069
    },
    {
      "epoch": 28.309859154929576,
      "grad_norm": 0.8191654086112976,
      "learning_rate": 0.00014339470771707416,
      "loss": 0.2489,
      "step": 14070
    },
    {
      "epoch": 28.311871227364186,
      "grad_norm": 0.8007465600967407,
      "learning_rate": 0.00014339068316732067,
      "loss": 0.2548,
      "step": 14071
    },
    {
      "epoch": 28.313883299798793,
      "grad_norm": 0.8132423758506775,
      "learning_rate": 0.00014338665861756718,
      "loss": 0.2549,
      "step": 14072
    },
    {
      "epoch": 28.3158953722334,
      "grad_norm": 0.7846961617469788,
      "learning_rate": 0.00014338263406781367,
      "loss": 0.2466,
      "step": 14073
    },
    {
      "epoch": 28.31790744466801,
      "grad_norm": 0.808986485004425,
      "learning_rate": 0.00014337860951806018,
      "loss": 0.2732,
      "step": 14074
    },
    {
      "epoch": 28.319919517102615,
      "grad_norm": 0.8183405995368958,
      "learning_rate": 0.00014337458496830666,
      "loss": 0.2419,
      "step": 14075
    },
    {
      "epoch": 28.321931589537222,
      "grad_norm": 0.8107686042785645,
      "learning_rate": 0.0001433705604185532,
      "loss": 0.2495,
      "step": 14076
    },
    {
      "epoch": 28.323943661971832,
      "grad_norm": 0.8697158098220825,
      "learning_rate": 0.0001433665358687997,
      "loss": 0.2812,
      "step": 14077
    },
    {
      "epoch": 28.32595573440644,
      "grad_norm": 0.8318988680839539,
      "learning_rate": 0.0001433625113190462,
      "loss": 0.2839,
      "step": 14078
    },
    {
      "epoch": 28.327967806841045,
      "grad_norm": 0.7962914109230042,
      "learning_rate": 0.00014335848676929268,
      "loss": 0.2593,
      "step": 14079
    },
    {
      "epoch": 28.329979879275655,
      "grad_norm": 0.7809178829193115,
      "learning_rate": 0.0001433544622195392,
      "loss": 0.2613,
      "step": 14080
    },
    {
      "epoch": 28.33199195171026,
      "grad_norm": 0.8594837188720703,
      "learning_rate": 0.0001433504376697857,
      "loss": 0.276,
      "step": 14081
    },
    {
      "epoch": 28.334004024144868,
      "grad_norm": 0.8698878288269043,
      "learning_rate": 0.0001433464131200322,
      "loss": 0.2436,
      "step": 14082
    },
    {
      "epoch": 28.336016096579478,
      "grad_norm": 0.8461390733718872,
      "learning_rate": 0.0001433423885702787,
      "loss": 0.2748,
      "step": 14083
    },
    {
      "epoch": 28.338028169014084,
      "grad_norm": 0.8533278703689575,
      "learning_rate": 0.00014333836402052522,
      "loss": 0.2672,
      "step": 14084
    },
    {
      "epoch": 28.34004024144869,
      "grad_norm": 0.8427322506904602,
      "learning_rate": 0.0001433343394707717,
      "loss": 0.2476,
      "step": 14085
    },
    {
      "epoch": 28.3420523138833,
      "grad_norm": 0.7919354438781738,
      "learning_rate": 0.00014333031492101822,
      "loss": 0.2548,
      "step": 14086
    },
    {
      "epoch": 28.344064386317907,
      "grad_norm": 0.7953972816467285,
      "learning_rate": 0.00014332629037126473,
      "loss": 0.2377,
      "step": 14087
    },
    {
      "epoch": 28.346076458752513,
      "grad_norm": 0.8213239908218384,
      "learning_rate": 0.00014332226582151121,
      "loss": 0.2452,
      "step": 14088
    },
    {
      "epoch": 28.348088531187123,
      "grad_norm": 0.839897632598877,
      "learning_rate": 0.00014331824127175773,
      "loss": 0.2545,
      "step": 14089
    },
    {
      "epoch": 28.35010060362173,
      "grad_norm": 0.7886326909065247,
      "learning_rate": 0.0001433142167220042,
      "loss": 0.2816,
      "step": 14090
    },
    {
      "epoch": 28.352112676056336,
      "grad_norm": 0.7805805206298828,
      "learning_rate": 0.00014331019217225075,
      "loss": 0.2494,
      "step": 14091
    },
    {
      "epoch": 28.354124748490946,
      "grad_norm": 0.8456043004989624,
      "learning_rate": 0.00014330616762249724,
      "loss": 0.271,
      "step": 14092
    },
    {
      "epoch": 28.356136820925553,
      "grad_norm": 0.8078425526618958,
      "learning_rate": 0.00014330214307274375,
      "loss": 0.2693,
      "step": 14093
    },
    {
      "epoch": 28.358148893360163,
      "grad_norm": 0.8052250146865845,
      "learning_rate": 0.00014329811852299023,
      "loss": 0.285,
      "step": 14094
    },
    {
      "epoch": 28.36016096579477,
      "grad_norm": 0.8029214143753052,
      "learning_rate": 0.00014329409397323674,
      "loss": 0.2546,
      "step": 14095
    },
    {
      "epoch": 28.362173038229376,
      "grad_norm": 0.845272421836853,
      "learning_rate": 0.00014329006942348326,
      "loss": 0.2693,
      "step": 14096
    },
    {
      "epoch": 28.364185110663986,
      "grad_norm": 0.8734286427497864,
      "learning_rate": 0.00014328604487372977,
      "loss": 0.2693,
      "step": 14097
    },
    {
      "epoch": 28.366197183098592,
      "grad_norm": 0.8206833600997925,
      "learning_rate": 0.00014328202032397625,
      "loss": 0.2643,
      "step": 14098
    },
    {
      "epoch": 28.3682092555332,
      "grad_norm": 0.8169181942939758,
      "learning_rate": 0.00014327799577422277,
      "loss": 0.2546,
      "step": 14099
    },
    {
      "epoch": 28.37022132796781,
      "grad_norm": 0.8423251509666443,
      "learning_rate": 0.00014327397122446925,
      "loss": 0.2694,
      "step": 14100
    },
    {
      "epoch": 28.372233400402415,
      "grad_norm": 0.8208847641944885,
      "learning_rate": 0.0001432699466747158,
      "loss": 0.2621,
      "step": 14101
    },
    {
      "epoch": 28.37424547283702,
      "grad_norm": 0.8133832812309265,
      "learning_rate": 0.00014326592212496228,
      "loss": 0.2784,
      "step": 14102
    },
    {
      "epoch": 28.37625754527163,
      "grad_norm": 0.857670247554779,
      "learning_rate": 0.0001432618975752088,
      "loss": 0.2423,
      "step": 14103
    },
    {
      "epoch": 28.378269617706238,
      "grad_norm": 0.8125706911087036,
      "learning_rate": 0.00014325787302545527,
      "loss": 0.2627,
      "step": 14104
    },
    {
      "epoch": 28.380281690140844,
      "grad_norm": 0.8890683054924011,
      "learning_rate": 0.00014325384847570179,
      "loss": 0.2485,
      "step": 14105
    },
    {
      "epoch": 28.382293762575454,
      "grad_norm": 0.7767321467399597,
      "learning_rate": 0.0001432498239259483,
      "loss": 0.2402,
      "step": 14106
    },
    {
      "epoch": 28.38430583501006,
      "grad_norm": 0.8003105521202087,
      "learning_rate": 0.0001432457993761948,
      "loss": 0.2646,
      "step": 14107
    },
    {
      "epoch": 28.386317907444667,
      "grad_norm": 0.8458088636398315,
      "learning_rate": 0.0001432417748264413,
      "loss": 0.2739,
      "step": 14108
    },
    {
      "epoch": 28.388329979879277,
      "grad_norm": 0.847642183303833,
      "learning_rate": 0.0001432377502766878,
      "loss": 0.2581,
      "step": 14109
    },
    {
      "epoch": 28.390342052313883,
      "grad_norm": 0.7943468689918518,
      "learning_rate": 0.0001432337257269343,
      "loss": 0.2818,
      "step": 14110
    },
    {
      "epoch": 28.39235412474849,
      "grad_norm": 0.8275811672210693,
      "learning_rate": 0.00014322970117718083,
      "loss": 0.2614,
      "step": 14111
    },
    {
      "epoch": 28.3943661971831,
      "grad_norm": 0.8317729830741882,
      "learning_rate": 0.00014322567662742732,
      "loss": 0.2669,
      "step": 14112
    },
    {
      "epoch": 28.396378269617706,
      "grad_norm": 0.8161378502845764,
      "learning_rate": 0.00014322165207767383,
      "loss": 0.2845,
      "step": 14113
    },
    {
      "epoch": 28.398390342052313,
      "grad_norm": 0.7750470638275146,
      "learning_rate": 0.00014321762752792031,
      "loss": 0.2554,
      "step": 14114
    },
    {
      "epoch": 28.400402414486923,
      "grad_norm": 0.8859254121780396,
      "learning_rate": 0.00014321360297816683,
      "loss": 0.2942,
      "step": 14115
    },
    {
      "epoch": 28.40241448692153,
      "grad_norm": 0.8088310360908508,
      "learning_rate": 0.00014320957842841334,
      "loss": 0.2694,
      "step": 14116
    },
    {
      "epoch": 28.404426559356136,
      "grad_norm": 0.8191734552383423,
      "learning_rate": 0.00014320555387865982,
      "loss": 0.2832,
      "step": 14117
    },
    {
      "epoch": 28.406438631790746,
      "grad_norm": 0.8122125864028931,
      "learning_rate": 0.00014320152932890634,
      "loss": 0.2784,
      "step": 14118
    },
    {
      "epoch": 28.408450704225352,
      "grad_norm": 0.7750797271728516,
      "learning_rate": 0.00014319750477915285,
      "loss": 0.2626,
      "step": 14119
    },
    {
      "epoch": 28.41046277665996,
      "grad_norm": 0.788460910320282,
      "learning_rate": 0.00014319348022939933,
      "loss": 0.2649,
      "step": 14120
    },
    {
      "epoch": 28.41247484909457,
      "grad_norm": 0.7917011976242065,
      "learning_rate": 0.00014318945567964585,
      "loss": 0.2515,
      "step": 14121
    },
    {
      "epoch": 28.414486921529175,
      "grad_norm": 0.8132568001747131,
      "learning_rate": 0.00014318543112989236,
      "loss": 0.2667,
      "step": 14122
    },
    {
      "epoch": 28.41649899396378,
      "grad_norm": 0.8463783264160156,
      "learning_rate": 0.00014318140658013884,
      "loss": 0.2727,
      "step": 14123
    },
    {
      "epoch": 28.41851106639839,
      "grad_norm": 0.8124575018882751,
      "learning_rate": 0.00014317738203038536,
      "loss": 0.2701,
      "step": 14124
    },
    {
      "epoch": 28.420523138832998,
      "grad_norm": 0.8551589846611023,
      "learning_rate": 0.00014317335748063184,
      "loss": 0.2881,
      "step": 14125
    },
    {
      "epoch": 28.422535211267604,
      "grad_norm": 0.7917295098304749,
      "learning_rate": 0.00014316933293087838,
      "loss": 0.2407,
      "step": 14126
    },
    {
      "epoch": 28.424547283702214,
      "grad_norm": 0.8234844207763672,
      "learning_rate": 0.00014316530838112486,
      "loss": 0.2528,
      "step": 14127
    },
    {
      "epoch": 28.42655935613682,
      "grad_norm": 0.8457015156745911,
      "learning_rate": 0.00014316128383137138,
      "loss": 0.2656,
      "step": 14128
    },
    {
      "epoch": 28.428571428571427,
      "grad_norm": 0.8386327028274536,
      "learning_rate": 0.00014315725928161786,
      "loss": 0.2781,
      "step": 14129
    },
    {
      "epoch": 28.430583501006037,
      "grad_norm": 0.8609511256217957,
      "learning_rate": 0.00014315323473186437,
      "loss": 0.268,
      "step": 14130
    },
    {
      "epoch": 28.432595573440643,
      "grad_norm": 0.7907289862632751,
      "learning_rate": 0.0001431492101821109,
      "loss": 0.255,
      "step": 14131
    },
    {
      "epoch": 28.43460764587525,
      "grad_norm": 0.7806375622749329,
      "learning_rate": 0.0001431451856323574,
      "loss": 0.2396,
      "step": 14132
    },
    {
      "epoch": 28.43661971830986,
      "grad_norm": 0.8266255855560303,
      "learning_rate": 0.00014314116108260388,
      "loss": 0.2763,
      "step": 14133
    },
    {
      "epoch": 28.438631790744466,
      "grad_norm": 0.8345390558242798,
      "learning_rate": 0.0001431371365328504,
      "loss": 0.278,
      "step": 14134
    },
    {
      "epoch": 28.440643863179073,
      "grad_norm": 0.8448925018310547,
      "learning_rate": 0.00014313311198309688,
      "loss": 0.2655,
      "step": 14135
    },
    {
      "epoch": 28.442655935613683,
      "grad_norm": 0.8455674648284912,
      "learning_rate": 0.00014312908743334342,
      "loss": 0.266,
      "step": 14136
    },
    {
      "epoch": 28.44466800804829,
      "grad_norm": 0.9130999445915222,
      "learning_rate": 0.0001431250628835899,
      "loss": 0.2725,
      "step": 14137
    },
    {
      "epoch": 28.446680080482896,
      "grad_norm": 0.8348110914230347,
      "learning_rate": 0.00014312103833383642,
      "loss": 0.2901,
      "step": 14138
    },
    {
      "epoch": 28.448692152917506,
      "grad_norm": 0.8074715733528137,
      "learning_rate": 0.0001431170137840829,
      "loss": 0.2683,
      "step": 14139
    },
    {
      "epoch": 28.450704225352112,
      "grad_norm": 0.8267754316329956,
      "learning_rate": 0.00014311298923432942,
      "loss": 0.245,
      "step": 14140
    },
    {
      "epoch": 28.452716297786722,
      "grad_norm": 0.8313595652580261,
      "learning_rate": 0.00014310896468457593,
      "loss": 0.2853,
      "step": 14141
    },
    {
      "epoch": 28.45472837022133,
      "grad_norm": 0.8367205858230591,
      "learning_rate": 0.00014310494013482244,
      "loss": 0.2674,
      "step": 14142
    },
    {
      "epoch": 28.456740442655935,
      "grad_norm": 0.8283962607383728,
      "learning_rate": 0.00014310091558506892,
      "loss": 0.2747,
      "step": 14143
    },
    {
      "epoch": 28.458752515090545,
      "grad_norm": 0.8507528901100159,
      "learning_rate": 0.00014309689103531544,
      "loss": 0.2648,
      "step": 14144
    },
    {
      "epoch": 28.46076458752515,
      "grad_norm": 0.7699345946311951,
      "learning_rate": 0.00014309286648556192,
      "loss": 0.258,
      "step": 14145
    },
    {
      "epoch": 28.462776659959758,
      "grad_norm": 0.8049615025520325,
      "learning_rate": 0.00014308884193580846,
      "loss": 0.2726,
      "step": 14146
    },
    {
      "epoch": 28.464788732394368,
      "grad_norm": 0.8385976552963257,
      "learning_rate": 0.00014308481738605495,
      "loss": 0.2705,
      "step": 14147
    },
    {
      "epoch": 28.466800804828974,
      "grad_norm": 0.7875432372093201,
      "learning_rate": 0.00014308079283630146,
      "loss": 0.2534,
      "step": 14148
    },
    {
      "epoch": 28.46881287726358,
      "grad_norm": 0.7742582559585571,
      "learning_rate": 0.00014307676828654794,
      "loss": 0.2553,
      "step": 14149
    },
    {
      "epoch": 28.47082494969819,
      "grad_norm": 0.8359620571136475,
      "learning_rate": 0.00014307274373679446,
      "loss": 0.2608,
      "step": 14150
    },
    {
      "epoch": 28.472837022132797,
      "grad_norm": 0.8808003067970276,
      "learning_rate": 0.00014306871918704097,
      "loss": 0.2645,
      "step": 14151
    },
    {
      "epoch": 28.474849094567404,
      "grad_norm": 0.8924473524093628,
      "learning_rate": 0.00014306469463728745,
      "loss": 0.2933,
      "step": 14152
    },
    {
      "epoch": 28.476861167002014,
      "grad_norm": 0.7999585270881653,
      "learning_rate": 0.00014306067008753397,
      "loss": 0.2763,
      "step": 14153
    },
    {
      "epoch": 28.47887323943662,
      "grad_norm": 0.8359424471855164,
      "learning_rate": 0.00014305664553778048,
      "loss": 0.2851,
      "step": 14154
    },
    {
      "epoch": 28.480885311871226,
      "grad_norm": 0.8362511396408081,
      "learning_rate": 0.00014305262098802696,
      "loss": 0.2615,
      "step": 14155
    },
    {
      "epoch": 28.482897384305836,
      "grad_norm": 0.9104903340339661,
      "learning_rate": 0.00014304859643827348,
      "loss": 0.2723,
      "step": 14156
    },
    {
      "epoch": 28.484909456740443,
      "grad_norm": 0.8462619185447693,
      "learning_rate": 0.00014304457188852,
      "loss": 0.2859,
      "step": 14157
    },
    {
      "epoch": 28.48692152917505,
      "grad_norm": 0.7916669845581055,
      "learning_rate": 0.00014304054733876647,
      "loss": 0.2688,
      "step": 14158
    },
    {
      "epoch": 28.48893360160966,
      "grad_norm": 0.827888011932373,
      "learning_rate": 0.00014303652278901298,
      "loss": 0.2775,
      "step": 14159
    },
    {
      "epoch": 28.490945674044266,
      "grad_norm": 0.8193063139915466,
      "learning_rate": 0.00014303249823925947,
      "loss": 0.2455,
      "step": 14160
    },
    {
      "epoch": 28.492957746478872,
      "grad_norm": 0.8487287759780884,
      "learning_rate": 0.000143028473689506,
      "loss": 0.2923,
      "step": 14161
    },
    {
      "epoch": 28.494969818913482,
      "grad_norm": 0.8141837120056152,
      "learning_rate": 0.0001430244491397525,
      "loss": 0.2573,
      "step": 14162
    },
    {
      "epoch": 28.49698189134809,
      "grad_norm": 0.83329838514328,
      "learning_rate": 0.000143020424589999,
      "loss": 0.2614,
      "step": 14163
    },
    {
      "epoch": 28.498993963782695,
      "grad_norm": 0.8493459820747375,
      "learning_rate": 0.0001430164000402455,
      "loss": 0.2672,
      "step": 14164
    },
    {
      "epoch": 28.501006036217305,
      "grad_norm": 0.8018661737442017,
      "learning_rate": 0.000143012375490492,
      "loss": 0.2662,
      "step": 14165
    },
    {
      "epoch": 28.50301810865191,
      "grad_norm": 0.8177817463874817,
      "learning_rate": 0.00014300835094073852,
      "loss": 0.265,
      "step": 14166
    },
    {
      "epoch": 28.505030181086518,
      "grad_norm": 0.8376792073249817,
      "learning_rate": 0.00014300432639098503,
      "loss": 0.2651,
      "step": 14167
    },
    {
      "epoch": 28.507042253521128,
      "grad_norm": 0.8135420083999634,
      "learning_rate": 0.0001430003018412315,
      "loss": 0.2614,
      "step": 14168
    },
    {
      "epoch": 28.509054325955734,
      "grad_norm": 0.8151350021362305,
      "learning_rate": 0.00014299627729147803,
      "loss": 0.2664,
      "step": 14169
    },
    {
      "epoch": 28.51106639839034,
      "grad_norm": 0.856099009513855,
      "learning_rate": 0.0001429922527417245,
      "loss": 0.2838,
      "step": 14170
    },
    {
      "epoch": 28.51307847082495,
      "grad_norm": 0.81730717420578,
      "learning_rate": 0.00014298822819197105,
      "loss": 0.2609,
      "step": 14171
    },
    {
      "epoch": 28.515090543259557,
      "grad_norm": 0.8910678625106812,
      "learning_rate": 0.00014298420364221754,
      "loss": 0.2611,
      "step": 14172
    },
    {
      "epoch": 28.517102615694164,
      "grad_norm": 0.8707258105278015,
      "learning_rate": 0.00014298017909246405,
      "loss": 0.2892,
      "step": 14173
    },
    {
      "epoch": 28.519114688128774,
      "grad_norm": 0.9078187942504883,
      "learning_rate": 0.00014297615454271053,
      "loss": 0.2716,
      "step": 14174
    },
    {
      "epoch": 28.52112676056338,
      "grad_norm": 0.899055540561676,
      "learning_rate": 0.00014297212999295704,
      "loss": 0.2554,
      "step": 14175
    },
    {
      "epoch": 28.523138832997986,
      "grad_norm": 0.8703068494796753,
      "learning_rate": 0.00014296810544320356,
      "loss": 0.278,
      "step": 14176
    },
    {
      "epoch": 28.525150905432596,
      "grad_norm": 0.8728393912315369,
      "learning_rate": 0.00014296408089345007,
      "loss": 0.2884,
      "step": 14177
    },
    {
      "epoch": 28.527162977867203,
      "grad_norm": 0.8146734833717346,
      "learning_rate": 0.00014296005634369655,
      "loss": 0.2697,
      "step": 14178
    },
    {
      "epoch": 28.52917505030181,
      "grad_norm": 0.8530182838439941,
      "learning_rate": 0.00014295603179394307,
      "loss": 0.293,
      "step": 14179
    },
    {
      "epoch": 28.53118712273642,
      "grad_norm": 0.8555422425270081,
      "learning_rate": 0.00014295200724418955,
      "loss": 0.279,
      "step": 14180
    },
    {
      "epoch": 28.533199195171026,
      "grad_norm": 0.7814145088195801,
      "learning_rate": 0.0001429479826944361,
      "loss": 0.2499,
      "step": 14181
    },
    {
      "epoch": 28.535211267605632,
      "grad_norm": 0.8556250929832458,
      "learning_rate": 0.00014294395814468258,
      "loss": 0.2738,
      "step": 14182
    },
    {
      "epoch": 28.537223340040242,
      "grad_norm": 0.8202619552612305,
      "learning_rate": 0.0001429399335949291,
      "loss": 0.264,
      "step": 14183
    },
    {
      "epoch": 28.53923541247485,
      "grad_norm": 0.856543779373169,
      "learning_rate": 0.00014293590904517557,
      "loss": 0.2802,
      "step": 14184
    },
    {
      "epoch": 28.541247484909455,
      "grad_norm": 0.88807213306427,
      "learning_rate": 0.00014293188449542209,
      "loss": 0.2786,
      "step": 14185
    },
    {
      "epoch": 28.543259557344065,
      "grad_norm": 0.8319766521453857,
      "learning_rate": 0.0001429278599456686,
      "loss": 0.2837,
      "step": 14186
    },
    {
      "epoch": 28.54527162977867,
      "grad_norm": 0.8137751817703247,
      "learning_rate": 0.00014292383539591508,
      "loss": 0.2612,
      "step": 14187
    },
    {
      "epoch": 28.547283702213278,
      "grad_norm": 0.810760498046875,
      "learning_rate": 0.0001429198108461616,
      "loss": 0.2655,
      "step": 14188
    },
    {
      "epoch": 28.549295774647888,
      "grad_norm": 0.8538414239883423,
      "learning_rate": 0.0001429157862964081,
      "loss": 0.2735,
      "step": 14189
    },
    {
      "epoch": 28.551307847082494,
      "grad_norm": 0.8306266665458679,
      "learning_rate": 0.0001429117617466546,
      "loss": 0.2777,
      "step": 14190
    },
    {
      "epoch": 28.5533199195171,
      "grad_norm": 0.8351368308067322,
      "learning_rate": 0.0001429077371969011,
      "loss": 0.2661,
      "step": 14191
    },
    {
      "epoch": 28.55533199195171,
      "grad_norm": 0.7992642521858215,
      "learning_rate": 0.00014290371264714762,
      "loss": 0.265,
      "step": 14192
    },
    {
      "epoch": 28.557344064386317,
      "grad_norm": 0.8301090598106384,
      "learning_rate": 0.0001428996880973941,
      "loss": 0.2594,
      "step": 14193
    },
    {
      "epoch": 28.559356136820927,
      "grad_norm": 0.7868616580963135,
      "learning_rate": 0.00014289566354764061,
      "loss": 0.2623,
      "step": 14194
    },
    {
      "epoch": 28.561368209255534,
      "grad_norm": 0.8567237854003906,
      "learning_rate": 0.0001428916389978871,
      "loss": 0.2561,
      "step": 14195
    },
    {
      "epoch": 28.56338028169014,
      "grad_norm": 0.8044700026512146,
      "learning_rate": 0.0001428876144481336,
      "loss": 0.2731,
      "step": 14196
    },
    {
      "epoch": 28.56539235412475,
      "grad_norm": 0.8017409443855286,
      "learning_rate": 0.00014288358989838012,
      "loss": 0.2641,
      "step": 14197
    },
    {
      "epoch": 28.567404426559357,
      "grad_norm": 0.8033395409584045,
      "learning_rate": 0.00014287956534862664,
      "loss": 0.2863,
      "step": 14198
    },
    {
      "epoch": 28.569416498993963,
      "grad_norm": 0.8010913729667664,
      "learning_rate": 0.00014287554079887312,
      "loss": 0.2571,
      "step": 14199
    },
    {
      "epoch": 28.571428571428573,
      "grad_norm": 0.8368162512779236,
      "learning_rate": 0.00014287151624911963,
      "loss": 0.2831,
      "step": 14200
    },
    {
      "epoch": 28.57344064386318,
      "grad_norm": 0.8669058680534363,
      "learning_rate": 0.00014286749169936612,
      "loss": 0.2869,
      "step": 14201
    },
    {
      "epoch": 28.575452716297786,
      "grad_norm": 0.8006537556648254,
      "learning_rate": 0.00014286346714961266,
      "loss": 0.2846,
      "step": 14202
    },
    {
      "epoch": 28.577464788732396,
      "grad_norm": 0.8029081225395203,
      "learning_rate": 0.00014285944259985914,
      "loss": 0.2771,
      "step": 14203
    },
    {
      "epoch": 28.579476861167002,
      "grad_norm": 0.8453283905982971,
      "learning_rate": 0.00014285541805010565,
      "loss": 0.2789,
      "step": 14204
    },
    {
      "epoch": 28.58148893360161,
      "grad_norm": 0.8816806674003601,
      "learning_rate": 0.00014285139350035214,
      "loss": 0.2637,
      "step": 14205
    },
    {
      "epoch": 28.58350100603622,
      "grad_norm": 0.8036819696426392,
      "learning_rate": 0.00014284736895059865,
      "loss": 0.2515,
      "step": 14206
    },
    {
      "epoch": 28.585513078470825,
      "grad_norm": 0.8492409586906433,
      "learning_rate": 0.00014284334440084516,
      "loss": 0.2633,
      "step": 14207
    },
    {
      "epoch": 28.58752515090543,
      "grad_norm": 0.8972705602645874,
      "learning_rate": 0.00014283931985109168,
      "loss": 0.3127,
      "step": 14208
    },
    {
      "epoch": 28.58953722334004,
      "grad_norm": 0.8177000880241394,
      "learning_rate": 0.00014283529530133816,
      "loss": 0.2766,
      "step": 14209
    },
    {
      "epoch": 28.591549295774648,
      "grad_norm": 0.8428341150283813,
      "learning_rate": 0.00014283127075158467,
      "loss": 0.2964,
      "step": 14210
    },
    {
      "epoch": 28.593561368209254,
      "grad_norm": 0.8327856659889221,
      "learning_rate": 0.00014282724620183116,
      "loss": 0.2864,
      "step": 14211
    },
    {
      "epoch": 28.595573440643864,
      "grad_norm": 0.8831402063369751,
      "learning_rate": 0.0001428232216520777,
      "loss": 0.2655,
      "step": 14212
    },
    {
      "epoch": 28.59758551307847,
      "grad_norm": 0.8421638607978821,
      "learning_rate": 0.00014281919710232418,
      "loss": 0.2655,
      "step": 14213
    },
    {
      "epoch": 28.599597585513077,
      "grad_norm": 0.8686479926109314,
      "learning_rate": 0.0001428151725525707,
      "loss": 0.2834,
      "step": 14214
    },
    {
      "epoch": 28.601609657947687,
      "grad_norm": 0.7971205115318298,
      "learning_rate": 0.00014281114800281718,
      "loss": 0.2705,
      "step": 14215
    },
    {
      "epoch": 28.603621730382294,
      "grad_norm": 0.8513196706771851,
      "learning_rate": 0.0001428071234530637,
      "loss": 0.2721,
      "step": 14216
    },
    {
      "epoch": 28.6056338028169,
      "grad_norm": 0.8177825808525085,
      "learning_rate": 0.0001428030989033102,
      "loss": 0.2585,
      "step": 14217
    },
    {
      "epoch": 28.60764587525151,
      "grad_norm": 0.8004145622253418,
      "learning_rate": 0.00014279907435355672,
      "loss": 0.2544,
      "step": 14218
    },
    {
      "epoch": 28.609657947686117,
      "grad_norm": 0.8156328201293945,
      "learning_rate": 0.0001427950498038032,
      "loss": 0.2878,
      "step": 14219
    },
    {
      "epoch": 28.611670020120723,
      "grad_norm": 0.8015463948249817,
      "learning_rate": 0.00014279102525404971,
      "loss": 0.2795,
      "step": 14220
    },
    {
      "epoch": 28.613682092555333,
      "grad_norm": 0.820199728012085,
      "learning_rate": 0.0001427870007042962,
      "loss": 0.2709,
      "step": 14221
    },
    {
      "epoch": 28.61569416498994,
      "grad_norm": 0.8256057500839233,
      "learning_rate": 0.0001427829761545427,
      "loss": 0.2961,
      "step": 14222
    },
    {
      "epoch": 28.617706237424546,
      "grad_norm": 0.8076759576797485,
      "learning_rate": 0.00014277895160478922,
      "loss": 0.2729,
      "step": 14223
    },
    {
      "epoch": 28.619718309859156,
      "grad_norm": 0.8315242528915405,
      "learning_rate": 0.00014277492705503574,
      "loss": 0.272,
      "step": 14224
    },
    {
      "epoch": 28.621730382293762,
      "grad_norm": 0.8233897089958191,
      "learning_rate": 0.00014277090250528222,
      "loss": 0.2935,
      "step": 14225
    },
    {
      "epoch": 28.62374245472837,
      "grad_norm": 0.8553898930549622,
      "learning_rate": 0.00014276687795552873,
      "loss": 0.2877,
      "step": 14226
    },
    {
      "epoch": 28.62575452716298,
      "grad_norm": 0.8122934103012085,
      "learning_rate": 0.00014276285340577525,
      "loss": 0.268,
      "step": 14227
    },
    {
      "epoch": 28.627766599597585,
      "grad_norm": 0.8713796734809875,
      "learning_rate": 0.00014275882885602173,
      "loss": 0.2821,
      "step": 14228
    },
    {
      "epoch": 28.62977867203219,
      "grad_norm": 0.8108362555503845,
      "learning_rate": 0.00014275480430626824,
      "loss": 0.2702,
      "step": 14229
    },
    {
      "epoch": 28.6317907444668,
      "grad_norm": 0.7771120071411133,
      "learning_rate": 0.00014275077975651473,
      "loss": 0.2804,
      "step": 14230
    },
    {
      "epoch": 28.633802816901408,
      "grad_norm": 0.8171408772468567,
      "learning_rate": 0.00014274675520676124,
      "loss": 0.2749,
      "step": 14231
    },
    {
      "epoch": 28.635814889336014,
      "grad_norm": 0.812218189239502,
      "learning_rate": 0.00014274273065700775,
      "loss": 0.2688,
      "step": 14232
    },
    {
      "epoch": 28.637826961770624,
      "grad_norm": 0.8559247255325317,
      "learning_rate": 0.00014273870610725427,
      "loss": 0.3042,
      "step": 14233
    },
    {
      "epoch": 28.63983903420523,
      "grad_norm": 0.8086247444152832,
      "learning_rate": 0.00014273468155750075,
      "loss": 0.2794,
      "step": 14234
    },
    {
      "epoch": 28.641851106639837,
      "grad_norm": 0.8576151132583618,
      "learning_rate": 0.00014273065700774726,
      "loss": 0.2998,
      "step": 14235
    },
    {
      "epoch": 28.643863179074447,
      "grad_norm": 0.8386883735656738,
      "learning_rate": 0.00014272663245799375,
      "loss": 0.2922,
      "step": 14236
    },
    {
      "epoch": 28.645875251509054,
      "grad_norm": 0.8589850068092346,
      "learning_rate": 0.0001427226079082403,
      "loss": 0.2973,
      "step": 14237
    },
    {
      "epoch": 28.647887323943664,
      "grad_norm": 0.8515059351921082,
      "learning_rate": 0.00014271858335848677,
      "loss": 0.2962,
      "step": 14238
    },
    {
      "epoch": 28.64989939637827,
      "grad_norm": 0.7979241609573364,
      "learning_rate": 0.00014271455880873328,
      "loss": 0.27,
      "step": 14239
    },
    {
      "epoch": 28.651911468812877,
      "grad_norm": 0.8092750310897827,
      "learning_rate": 0.00014271053425897977,
      "loss": 0.2738,
      "step": 14240
    },
    {
      "epoch": 28.653923541247487,
      "grad_norm": 0.8792903423309326,
      "learning_rate": 0.00014270650970922628,
      "loss": 0.2421,
      "step": 14241
    },
    {
      "epoch": 28.655935613682093,
      "grad_norm": 0.806316077709198,
      "learning_rate": 0.0001427024851594728,
      "loss": 0.2731,
      "step": 14242
    },
    {
      "epoch": 28.6579476861167,
      "grad_norm": 0.8377469778060913,
      "learning_rate": 0.0001426984606097193,
      "loss": 0.2673,
      "step": 14243
    },
    {
      "epoch": 28.65995975855131,
      "grad_norm": 0.8580787181854248,
      "learning_rate": 0.0001426944360599658,
      "loss": 0.2839,
      "step": 14244
    },
    {
      "epoch": 28.661971830985916,
      "grad_norm": 0.8162717223167419,
      "learning_rate": 0.0001426904115102123,
      "loss": 0.2751,
      "step": 14245
    },
    {
      "epoch": 28.663983903420522,
      "grad_norm": 0.8663384914398193,
      "learning_rate": 0.0001426863869604588,
      "loss": 0.2941,
      "step": 14246
    },
    {
      "epoch": 28.665995975855132,
      "grad_norm": 0.7895713448524475,
      "learning_rate": 0.00014268236241070533,
      "loss": 0.2765,
      "step": 14247
    },
    {
      "epoch": 28.66800804828974,
      "grad_norm": 0.8401223421096802,
      "learning_rate": 0.0001426783378609518,
      "loss": 0.2559,
      "step": 14248
    },
    {
      "epoch": 28.670020120724345,
      "grad_norm": 0.8035988211631775,
      "learning_rate": 0.00014267431331119833,
      "loss": 0.2581,
      "step": 14249
    },
    {
      "epoch": 28.672032193158955,
      "grad_norm": 0.82688307762146,
      "learning_rate": 0.0001426702887614448,
      "loss": 0.2729,
      "step": 14250
    },
    {
      "epoch": 28.67404426559356,
      "grad_norm": 0.8393074870109558,
      "learning_rate": 0.00014266626421169132,
      "loss": 0.3011,
      "step": 14251
    },
    {
      "epoch": 28.676056338028168,
      "grad_norm": 0.8356239795684814,
      "learning_rate": 0.00014266223966193783,
      "loss": 0.2891,
      "step": 14252
    },
    {
      "epoch": 28.678068410462778,
      "grad_norm": 0.8140264749526978,
      "learning_rate": 0.00014265821511218435,
      "loss": 0.2622,
      "step": 14253
    },
    {
      "epoch": 28.680080482897385,
      "grad_norm": 0.8141516447067261,
      "learning_rate": 0.00014265419056243083,
      "loss": 0.2821,
      "step": 14254
    },
    {
      "epoch": 28.68209255533199,
      "grad_norm": 0.8287993669509888,
      "learning_rate": 0.00014265016601267734,
      "loss": 0.277,
      "step": 14255
    },
    {
      "epoch": 28.6841046277666,
      "grad_norm": 0.8070634603500366,
      "learning_rate": 0.00014264614146292383,
      "loss": 0.2636,
      "step": 14256
    },
    {
      "epoch": 28.686116700201207,
      "grad_norm": 0.8524597883224487,
      "learning_rate": 0.00014264211691317034,
      "loss": 0.2751,
      "step": 14257
    },
    {
      "epoch": 28.688128772635814,
      "grad_norm": 0.7731164693832397,
      "learning_rate": 0.00014263809236341685,
      "loss": 0.2463,
      "step": 14258
    },
    {
      "epoch": 28.690140845070424,
      "grad_norm": 0.8217890858650208,
      "learning_rate": 0.00014263406781366334,
      "loss": 0.277,
      "step": 14259
    },
    {
      "epoch": 28.69215291750503,
      "grad_norm": 0.7812027931213379,
      "learning_rate": 0.00014263004326390985,
      "loss": 0.2916,
      "step": 14260
    },
    {
      "epoch": 28.694164989939637,
      "grad_norm": 0.8586209416389465,
      "learning_rate": 0.00014262601871415636,
      "loss": 0.2871,
      "step": 14261
    },
    {
      "epoch": 28.696177062374247,
      "grad_norm": 0.9198853373527527,
      "learning_rate": 0.00014262199416440288,
      "loss": 0.2742,
      "step": 14262
    },
    {
      "epoch": 28.698189134808853,
      "grad_norm": 0.8197181820869446,
      "learning_rate": 0.00014261796961464936,
      "loss": 0.2613,
      "step": 14263
    },
    {
      "epoch": 28.70020120724346,
      "grad_norm": 0.7590827941894531,
      "learning_rate": 0.00014261394506489587,
      "loss": 0.2437,
      "step": 14264
    },
    {
      "epoch": 28.70221327967807,
      "grad_norm": 0.8234840035438538,
      "learning_rate": 0.00014260992051514236,
      "loss": 0.2682,
      "step": 14265
    },
    {
      "epoch": 28.704225352112676,
      "grad_norm": 0.8404760956764221,
      "learning_rate": 0.00014260589596538887,
      "loss": 0.2869,
      "step": 14266
    },
    {
      "epoch": 28.706237424547282,
      "grad_norm": 0.8093054890632629,
      "learning_rate": 0.00014260187141563538,
      "loss": 0.2987,
      "step": 14267
    },
    {
      "epoch": 28.708249496981892,
      "grad_norm": 0.8694412112236023,
      "learning_rate": 0.0001425978468658819,
      "loss": 0.2729,
      "step": 14268
    },
    {
      "epoch": 28.7102615694165,
      "grad_norm": 0.8506776690483093,
      "learning_rate": 0.00014259382231612838,
      "loss": 0.274,
      "step": 14269
    },
    {
      "epoch": 28.712273641851105,
      "grad_norm": 0.8275520205497742,
      "learning_rate": 0.0001425897977663749,
      "loss": 0.2779,
      "step": 14270
    },
    {
      "epoch": 28.714285714285715,
      "grad_norm": 0.8394514322280884,
      "learning_rate": 0.00014258577321662138,
      "loss": 0.2859,
      "step": 14271
    },
    {
      "epoch": 28.71629778672032,
      "grad_norm": 0.932924747467041,
      "learning_rate": 0.00014258174866686792,
      "loss": 0.2981,
      "step": 14272
    },
    {
      "epoch": 28.718309859154928,
      "grad_norm": 0.7996684312820435,
      "learning_rate": 0.0001425777241171144,
      "loss": 0.287,
      "step": 14273
    },
    {
      "epoch": 28.720321931589538,
      "grad_norm": 0.8820791840553284,
      "learning_rate": 0.00014257369956736091,
      "loss": 0.2978,
      "step": 14274
    },
    {
      "epoch": 28.722334004024145,
      "grad_norm": 0.7964577674865723,
      "learning_rate": 0.0001425696750176074,
      "loss": 0.2727,
      "step": 14275
    },
    {
      "epoch": 28.72434607645875,
      "grad_norm": 0.834404706954956,
      "learning_rate": 0.0001425656504678539,
      "loss": 0.3061,
      "step": 14276
    },
    {
      "epoch": 28.72635814889336,
      "grad_norm": 0.8266156911849976,
      "learning_rate": 0.00014256162591810042,
      "loss": 0.2493,
      "step": 14277
    },
    {
      "epoch": 28.728370221327967,
      "grad_norm": 0.8734566569328308,
      "learning_rate": 0.00014255760136834694,
      "loss": 0.2698,
      "step": 14278
    },
    {
      "epoch": 28.730382293762574,
      "grad_norm": 0.8243893384933472,
      "learning_rate": 0.00014255357681859342,
      "loss": 0.2697,
      "step": 14279
    },
    {
      "epoch": 28.732394366197184,
      "grad_norm": 0.8194550275802612,
      "learning_rate": 0.00014254955226883993,
      "loss": 0.2666,
      "step": 14280
    },
    {
      "epoch": 28.73440643863179,
      "grad_norm": 0.8117753863334656,
      "learning_rate": 0.00014254552771908642,
      "loss": 0.2687,
      "step": 14281
    },
    {
      "epoch": 28.736418511066397,
      "grad_norm": 0.837844967842102,
      "learning_rate": 0.00014254150316933296,
      "loss": 0.2759,
      "step": 14282
    },
    {
      "epoch": 28.738430583501007,
      "grad_norm": 0.8022066354751587,
      "learning_rate": 0.00014253747861957944,
      "loss": 0.2615,
      "step": 14283
    },
    {
      "epoch": 28.740442655935613,
      "grad_norm": 0.8112566471099854,
      "learning_rate": 0.00014253345406982595,
      "loss": 0.2602,
      "step": 14284
    },
    {
      "epoch": 28.74245472837022,
      "grad_norm": 0.8698475956916809,
      "learning_rate": 0.00014252942952007244,
      "loss": 0.2867,
      "step": 14285
    },
    {
      "epoch": 28.74446680080483,
      "grad_norm": 0.812489926815033,
      "learning_rate": 0.00014252540497031895,
      "loss": 0.2792,
      "step": 14286
    },
    {
      "epoch": 28.746478873239436,
      "grad_norm": 0.7841649651527405,
      "learning_rate": 0.00014252138042056546,
      "loss": 0.2794,
      "step": 14287
    },
    {
      "epoch": 28.748490945674043,
      "grad_norm": 0.8378332257270813,
      "learning_rate": 0.00014251735587081198,
      "loss": 0.2834,
      "step": 14288
    },
    {
      "epoch": 28.750503018108652,
      "grad_norm": 0.8637892603874207,
      "learning_rate": 0.00014251333132105846,
      "loss": 0.2818,
      "step": 14289
    },
    {
      "epoch": 28.75251509054326,
      "grad_norm": 0.8488144874572754,
      "learning_rate": 0.00014250930677130497,
      "loss": 0.2847,
      "step": 14290
    },
    {
      "epoch": 28.754527162977865,
      "grad_norm": 0.8196126818656921,
      "learning_rate": 0.00014250528222155146,
      "loss": 0.2838,
      "step": 14291
    },
    {
      "epoch": 28.756539235412475,
      "grad_norm": 0.8393483757972717,
      "learning_rate": 0.00014250125767179797,
      "loss": 0.2584,
      "step": 14292
    },
    {
      "epoch": 28.758551307847082,
      "grad_norm": 0.8579365611076355,
      "learning_rate": 0.00014249723312204448,
      "loss": 0.2606,
      "step": 14293
    },
    {
      "epoch": 28.760563380281692,
      "grad_norm": 0.8068324327468872,
      "learning_rate": 0.00014249320857229097,
      "loss": 0.2919,
      "step": 14294
    },
    {
      "epoch": 28.7625754527163,
      "grad_norm": 0.8431517481803894,
      "learning_rate": 0.00014248918402253748,
      "loss": 0.2825,
      "step": 14295
    },
    {
      "epoch": 28.764587525150905,
      "grad_norm": 0.8448169231414795,
      "learning_rate": 0.000142485159472784,
      "loss": 0.2646,
      "step": 14296
    },
    {
      "epoch": 28.766599597585515,
      "grad_norm": 0.8227527141571045,
      "learning_rate": 0.0001424811349230305,
      "loss": 0.2783,
      "step": 14297
    },
    {
      "epoch": 28.76861167002012,
      "grad_norm": 0.8286462426185608,
      "learning_rate": 0.000142477110373277,
      "loss": 0.2914,
      "step": 14298
    },
    {
      "epoch": 28.770623742454728,
      "grad_norm": 0.8164089918136597,
      "learning_rate": 0.0001424730858235235,
      "loss": 0.2803,
      "step": 14299
    },
    {
      "epoch": 28.772635814889338,
      "grad_norm": 0.8145703673362732,
      "learning_rate": 0.00014246906127377,
      "loss": 0.289,
      "step": 14300
    },
    {
      "epoch": 28.774647887323944,
      "grad_norm": 0.8339268565177917,
      "learning_rate": 0.0001424650367240165,
      "loss": 0.2755,
      "step": 14301
    },
    {
      "epoch": 28.77665995975855,
      "grad_norm": 0.8496865630149841,
      "learning_rate": 0.000142461012174263,
      "loss": 0.2925,
      "step": 14302
    },
    {
      "epoch": 28.77867203219316,
      "grad_norm": 0.8251456022262573,
      "learning_rate": 0.00014245698762450952,
      "loss": 0.2789,
      "step": 14303
    },
    {
      "epoch": 28.780684104627767,
      "grad_norm": 0.893556535243988,
      "learning_rate": 0.000142452963074756,
      "loss": 0.2848,
      "step": 14304
    },
    {
      "epoch": 28.782696177062373,
      "grad_norm": 0.8246044516563416,
      "learning_rate": 0.00014244893852500252,
      "loss": 0.2936,
      "step": 14305
    },
    {
      "epoch": 28.784708249496983,
      "grad_norm": 0.8359683156013489,
      "learning_rate": 0.000142444913975249,
      "loss": 0.2693,
      "step": 14306
    },
    {
      "epoch": 28.78672032193159,
      "grad_norm": 0.8173997402191162,
      "learning_rate": 0.00014244088942549555,
      "loss": 0.2597,
      "step": 14307
    },
    {
      "epoch": 28.788732394366196,
      "grad_norm": 0.8281726837158203,
      "learning_rate": 0.00014243686487574203,
      "loss": 0.2799,
      "step": 14308
    },
    {
      "epoch": 28.790744466800806,
      "grad_norm": 0.8655234575271606,
      "learning_rate": 0.00014243284032598854,
      "loss": 0.2944,
      "step": 14309
    },
    {
      "epoch": 28.792756539235413,
      "grad_norm": 0.8056590557098389,
      "learning_rate": 0.00014242881577623503,
      "loss": 0.2615,
      "step": 14310
    },
    {
      "epoch": 28.79476861167002,
      "grad_norm": 0.8531521558761597,
      "learning_rate": 0.00014242479122648154,
      "loss": 0.2926,
      "step": 14311
    },
    {
      "epoch": 28.79678068410463,
      "grad_norm": 0.8428457379341125,
      "learning_rate": 0.00014242076667672805,
      "loss": 0.3015,
      "step": 14312
    },
    {
      "epoch": 28.798792756539235,
      "grad_norm": 0.874502420425415,
      "learning_rate": 0.00014241674212697457,
      "loss": 0.2628,
      "step": 14313
    },
    {
      "epoch": 28.800804828973842,
      "grad_norm": 0.89361572265625,
      "learning_rate": 0.00014241271757722105,
      "loss": 0.2873,
      "step": 14314
    },
    {
      "epoch": 28.802816901408452,
      "grad_norm": 0.809399425983429,
      "learning_rate": 0.00014240869302746756,
      "loss": 0.2696,
      "step": 14315
    },
    {
      "epoch": 28.80482897384306,
      "grad_norm": 0.8346030116081238,
      "learning_rate": 0.00014240466847771405,
      "loss": 0.3019,
      "step": 14316
    },
    {
      "epoch": 28.806841046277665,
      "grad_norm": 0.796235203742981,
      "learning_rate": 0.0001424006439279606,
      "loss": 0.2823,
      "step": 14317
    },
    {
      "epoch": 28.808853118712275,
      "grad_norm": 0.7972003221511841,
      "learning_rate": 0.00014239661937820707,
      "loss": 0.2809,
      "step": 14318
    },
    {
      "epoch": 28.81086519114688,
      "grad_norm": 0.8902040123939514,
      "learning_rate": 0.00014239259482845358,
      "loss": 0.3046,
      "step": 14319
    },
    {
      "epoch": 28.812877263581488,
      "grad_norm": 0.8228824138641357,
      "learning_rate": 0.00014238857027870007,
      "loss": 0.2669,
      "step": 14320
    },
    {
      "epoch": 28.814889336016098,
      "grad_norm": 0.8698612451553345,
      "learning_rate": 0.00014238454572894658,
      "loss": 0.3009,
      "step": 14321
    },
    {
      "epoch": 28.816901408450704,
      "grad_norm": 0.8496220707893372,
      "learning_rate": 0.0001423805211791931,
      "loss": 0.2943,
      "step": 14322
    },
    {
      "epoch": 28.81891348088531,
      "grad_norm": 0.8213128447532654,
      "learning_rate": 0.0001423764966294396,
      "loss": 0.2757,
      "step": 14323
    },
    {
      "epoch": 28.82092555331992,
      "grad_norm": 0.8539489507675171,
      "learning_rate": 0.0001423724720796861,
      "loss": 0.2954,
      "step": 14324
    },
    {
      "epoch": 28.822937625754527,
      "grad_norm": 0.9102267026901245,
      "learning_rate": 0.0001423684475299326,
      "loss": 0.3113,
      "step": 14325
    },
    {
      "epoch": 28.824949698189133,
      "grad_norm": 0.9053558707237244,
      "learning_rate": 0.0001423644229801791,
      "loss": 0.2935,
      "step": 14326
    },
    {
      "epoch": 28.826961770623743,
      "grad_norm": 0.8107975125312805,
      "learning_rate": 0.0001423603984304256,
      "loss": 0.2907,
      "step": 14327
    },
    {
      "epoch": 28.82897384305835,
      "grad_norm": 0.8324617743492126,
      "learning_rate": 0.0001423563738806721,
      "loss": 0.2692,
      "step": 14328
    },
    {
      "epoch": 28.830985915492956,
      "grad_norm": 0.8290502429008484,
      "learning_rate": 0.0001423523493309186,
      "loss": 0.2728,
      "step": 14329
    },
    {
      "epoch": 28.832997987927566,
      "grad_norm": 0.8914905190467834,
      "learning_rate": 0.0001423483247811651,
      "loss": 0.2941,
      "step": 14330
    },
    {
      "epoch": 28.835010060362173,
      "grad_norm": 0.8833290934562683,
      "learning_rate": 0.00014234430023141162,
      "loss": 0.3069,
      "step": 14331
    },
    {
      "epoch": 28.83702213279678,
      "grad_norm": 0.8627190589904785,
      "learning_rate": 0.00014234027568165813,
      "loss": 0.3187,
      "step": 14332
    },
    {
      "epoch": 28.83903420523139,
      "grad_norm": 0.7989897727966309,
      "learning_rate": 0.00014233625113190462,
      "loss": 0.2919,
      "step": 14333
    },
    {
      "epoch": 28.841046277665995,
      "grad_norm": 0.8109256625175476,
      "learning_rate": 0.00014233222658215113,
      "loss": 0.2795,
      "step": 14334
    },
    {
      "epoch": 28.843058350100602,
      "grad_norm": 0.8168609142303467,
      "learning_rate": 0.00014232820203239762,
      "loss": 0.3059,
      "step": 14335
    },
    {
      "epoch": 28.845070422535212,
      "grad_norm": 0.8174217939376831,
      "learning_rate": 0.00014232417748264413,
      "loss": 0.2844,
      "step": 14336
    },
    {
      "epoch": 28.84708249496982,
      "grad_norm": 0.8108137845993042,
      "learning_rate": 0.00014232015293289064,
      "loss": 0.2684,
      "step": 14337
    },
    {
      "epoch": 28.84909456740443,
      "grad_norm": 0.9038894176483154,
      "learning_rate": 0.00014231612838313715,
      "loss": 0.2923,
      "step": 14338
    },
    {
      "epoch": 28.851106639839035,
      "grad_norm": 0.8980525732040405,
      "learning_rate": 0.00014231210383338364,
      "loss": 0.286,
      "step": 14339
    },
    {
      "epoch": 28.85311871227364,
      "grad_norm": 0.8083288669586182,
      "learning_rate": 0.00014230807928363015,
      "loss": 0.2645,
      "step": 14340
    },
    {
      "epoch": 28.85513078470825,
      "grad_norm": 0.8926802277565002,
      "learning_rate": 0.00014230405473387664,
      "loss": 0.3052,
      "step": 14341
    },
    {
      "epoch": 28.857142857142858,
      "grad_norm": 0.8969147205352783,
      "learning_rate": 0.00014230003018412318,
      "loss": 0.3075,
      "step": 14342
    },
    {
      "epoch": 28.859154929577464,
      "grad_norm": 0.8953953385353088,
      "learning_rate": 0.00014229600563436966,
      "loss": 0.2682,
      "step": 14343
    },
    {
      "epoch": 28.861167002012074,
      "grad_norm": 0.8428427577018738,
      "learning_rate": 0.00014229198108461617,
      "loss": 0.2971,
      "step": 14344
    },
    {
      "epoch": 28.86317907444668,
      "grad_norm": 0.8382193446159363,
      "learning_rate": 0.00014228795653486266,
      "loss": 0.2855,
      "step": 14345
    },
    {
      "epoch": 28.865191146881287,
      "grad_norm": 0.8376733660697937,
      "learning_rate": 0.00014228393198510917,
      "loss": 0.282,
      "step": 14346
    },
    {
      "epoch": 28.867203219315897,
      "grad_norm": 0.8544546365737915,
      "learning_rate": 0.00014227990743535568,
      "loss": 0.2732,
      "step": 14347
    },
    {
      "epoch": 28.869215291750503,
      "grad_norm": 0.8672119379043579,
      "learning_rate": 0.0001422758828856022,
      "loss": 0.2813,
      "step": 14348
    },
    {
      "epoch": 28.87122736418511,
      "grad_norm": 0.8589946627616882,
      "learning_rate": 0.00014227185833584868,
      "loss": 0.3033,
      "step": 14349
    },
    {
      "epoch": 28.87323943661972,
      "grad_norm": 0.8605431318283081,
      "learning_rate": 0.0001422678337860952,
      "loss": 0.2832,
      "step": 14350
    },
    {
      "epoch": 28.875251509054326,
      "grad_norm": 0.8427898287773132,
      "learning_rate": 0.00014226380923634168,
      "loss": 0.3079,
      "step": 14351
    },
    {
      "epoch": 28.877263581488933,
      "grad_norm": 0.8692612051963806,
      "learning_rate": 0.00014225978468658822,
      "loss": 0.3285,
      "step": 14352
    },
    {
      "epoch": 28.879275653923543,
      "grad_norm": 0.8169295787811279,
      "learning_rate": 0.0001422557601368347,
      "loss": 0.2941,
      "step": 14353
    },
    {
      "epoch": 28.88128772635815,
      "grad_norm": 0.8712218403816223,
      "learning_rate": 0.0001422517355870812,
      "loss": 0.3034,
      "step": 14354
    },
    {
      "epoch": 28.883299798792756,
      "grad_norm": 0.8281391263008118,
      "learning_rate": 0.0001422477110373277,
      "loss": 0.3199,
      "step": 14355
    },
    {
      "epoch": 28.885311871227366,
      "grad_norm": 0.7930867671966553,
      "learning_rate": 0.0001422436864875742,
      "loss": 0.288,
      "step": 14356
    },
    {
      "epoch": 28.887323943661972,
      "grad_norm": 0.8061619997024536,
      "learning_rate": 0.00014223966193782072,
      "loss": 0.2793,
      "step": 14357
    },
    {
      "epoch": 28.88933601609658,
      "grad_norm": 0.8135395646095276,
      "learning_rate": 0.00014223563738806724,
      "loss": 0.293,
      "step": 14358
    },
    {
      "epoch": 28.89134808853119,
      "grad_norm": 0.8736454248428345,
      "learning_rate": 0.00014223161283831372,
      "loss": 0.2947,
      "step": 14359
    },
    {
      "epoch": 28.893360160965795,
      "grad_norm": 0.8968122005462646,
      "learning_rate": 0.00014222758828856023,
      "loss": 0.3079,
      "step": 14360
    },
    {
      "epoch": 28.8953722334004,
      "grad_norm": 0.8088013529777527,
      "learning_rate": 0.00014222356373880672,
      "loss": 0.2846,
      "step": 14361
    },
    {
      "epoch": 28.89738430583501,
      "grad_norm": 0.8203174471855164,
      "learning_rate": 0.00014221953918905323,
      "loss": 0.2841,
      "step": 14362
    },
    {
      "epoch": 28.899396378269618,
      "grad_norm": 0.8164810538291931,
      "learning_rate": 0.00014221551463929974,
      "loss": 0.3025,
      "step": 14363
    },
    {
      "epoch": 28.901408450704224,
      "grad_norm": 0.8112584948539734,
      "learning_rate": 0.00014221149008954623,
      "loss": 0.2636,
      "step": 14364
    },
    {
      "epoch": 28.903420523138834,
      "grad_norm": 0.8196499347686768,
      "learning_rate": 0.00014220746553979274,
      "loss": 0.2858,
      "step": 14365
    },
    {
      "epoch": 28.90543259557344,
      "grad_norm": 0.8527626991271973,
      "learning_rate": 0.00014220344099003925,
      "loss": 0.2928,
      "step": 14366
    },
    {
      "epoch": 28.907444668008047,
      "grad_norm": 0.8084765076637268,
      "learning_rate": 0.00014219941644028576,
      "loss": 0.2835,
      "step": 14367
    },
    {
      "epoch": 28.909456740442657,
      "grad_norm": 0.8410788178443909,
      "learning_rate": 0.00014219539189053225,
      "loss": 0.281,
      "step": 14368
    },
    {
      "epoch": 28.911468812877263,
      "grad_norm": 0.8500283360481262,
      "learning_rate": 0.00014219136734077876,
      "loss": 0.2892,
      "step": 14369
    },
    {
      "epoch": 28.91348088531187,
      "grad_norm": 0.8386800289154053,
      "learning_rate": 0.00014218734279102525,
      "loss": 0.2927,
      "step": 14370
    },
    {
      "epoch": 28.91549295774648,
      "grad_norm": 0.8623353838920593,
      "learning_rate": 0.00014218331824127176,
      "loss": 0.2998,
      "step": 14371
    },
    {
      "epoch": 28.917505030181086,
      "grad_norm": 0.8672869801521301,
      "learning_rate": 0.00014217929369151827,
      "loss": 0.2826,
      "step": 14372
    },
    {
      "epoch": 28.919517102615693,
      "grad_norm": 0.871378481388092,
      "learning_rate": 0.00014217526914176478,
      "loss": 0.2909,
      "step": 14373
    },
    {
      "epoch": 28.921529175050303,
      "grad_norm": 0.8190911412239075,
      "learning_rate": 0.00014217124459201127,
      "loss": 0.2824,
      "step": 14374
    },
    {
      "epoch": 28.92354124748491,
      "grad_norm": 0.8156266808509827,
      "learning_rate": 0.00014216722004225778,
      "loss": 0.3004,
      "step": 14375
    },
    {
      "epoch": 28.925553319919516,
      "grad_norm": 0.8146084547042847,
      "learning_rate": 0.00014216319549250427,
      "loss": 0.2691,
      "step": 14376
    },
    {
      "epoch": 28.927565392354126,
      "grad_norm": 0.8426242470741272,
      "learning_rate": 0.0001421591709427508,
      "loss": 0.2959,
      "step": 14377
    },
    {
      "epoch": 28.929577464788732,
      "grad_norm": 0.821479320526123,
      "learning_rate": 0.0001421551463929973,
      "loss": 0.2822,
      "step": 14378
    },
    {
      "epoch": 28.93158953722334,
      "grad_norm": 0.8209400773048401,
      "learning_rate": 0.0001421511218432438,
      "loss": 0.3018,
      "step": 14379
    },
    {
      "epoch": 28.93360160965795,
      "grad_norm": 0.9154871702194214,
      "learning_rate": 0.0001421470972934903,
      "loss": 0.3034,
      "step": 14380
    },
    {
      "epoch": 28.935613682092555,
      "grad_norm": 0.910950779914856,
      "learning_rate": 0.0001421430727437368,
      "loss": 0.3165,
      "step": 14381
    },
    {
      "epoch": 28.93762575452716,
      "grad_norm": 0.8068962693214417,
      "learning_rate": 0.0001421390481939833,
      "loss": 0.2851,
      "step": 14382
    },
    {
      "epoch": 28.93963782696177,
      "grad_norm": 0.830168604850769,
      "learning_rate": 0.00014213502364422982,
      "loss": 0.3002,
      "step": 14383
    },
    {
      "epoch": 28.941649899396378,
      "grad_norm": 0.7738298773765564,
      "learning_rate": 0.0001421309990944763,
      "loss": 0.2682,
      "step": 14384
    },
    {
      "epoch": 28.943661971830984,
      "grad_norm": 0.84464430809021,
      "learning_rate": 0.00014212697454472282,
      "loss": 0.2869,
      "step": 14385
    },
    {
      "epoch": 28.945674044265594,
      "grad_norm": 0.8868423104286194,
      "learning_rate": 0.0001421229499949693,
      "loss": 0.3099,
      "step": 14386
    },
    {
      "epoch": 28.9476861167002,
      "grad_norm": 0.8216031789779663,
      "learning_rate": 0.00014211892544521585,
      "loss": 0.3087,
      "step": 14387
    },
    {
      "epoch": 28.949698189134807,
      "grad_norm": 0.7935535311698914,
      "learning_rate": 0.00014211490089546233,
      "loss": 0.283,
      "step": 14388
    },
    {
      "epoch": 28.951710261569417,
      "grad_norm": 0.8951951861381531,
      "learning_rate": 0.00014211087634570884,
      "loss": 0.3284,
      "step": 14389
    },
    {
      "epoch": 28.953722334004024,
      "grad_norm": 0.8333196043968201,
      "learning_rate": 0.00014210685179595533,
      "loss": 0.2965,
      "step": 14390
    },
    {
      "epoch": 28.955734406438633,
      "grad_norm": 0.849265992641449,
      "learning_rate": 0.00014210282724620184,
      "loss": 0.2803,
      "step": 14391
    },
    {
      "epoch": 28.95774647887324,
      "grad_norm": 0.8180453181266785,
      "learning_rate": 0.00014209880269644835,
      "loss": 0.2868,
      "step": 14392
    },
    {
      "epoch": 28.959758551307846,
      "grad_norm": 0.8489932417869568,
      "learning_rate": 0.00014209477814669486,
      "loss": 0.3183,
      "step": 14393
    },
    {
      "epoch": 28.961770623742456,
      "grad_norm": 0.8542335629463196,
      "learning_rate": 0.00014209075359694135,
      "loss": 0.2887,
      "step": 14394
    },
    {
      "epoch": 28.963782696177063,
      "grad_norm": 0.8136263489723206,
      "learning_rate": 0.00014208672904718786,
      "loss": 0.2772,
      "step": 14395
    },
    {
      "epoch": 28.96579476861167,
      "grad_norm": 0.898979663848877,
      "learning_rate": 0.00014208270449743435,
      "loss": 0.3127,
      "step": 14396
    },
    {
      "epoch": 28.96780684104628,
      "grad_norm": 0.8562383055686951,
      "learning_rate": 0.00014207867994768086,
      "loss": 0.285,
      "step": 14397
    },
    {
      "epoch": 28.969818913480886,
      "grad_norm": 0.8356423377990723,
      "learning_rate": 0.00014207465539792737,
      "loss": 0.3082,
      "step": 14398
    },
    {
      "epoch": 28.971830985915492,
      "grad_norm": 0.795448362827301,
      "learning_rate": 0.00014207063084817386,
      "loss": 0.2817,
      "step": 14399
    },
    {
      "epoch": 28.973843058350102,
      "grad_norm": 0.8215768337249756,
      "learning_rate": 0.00014206660629842037,
      "loss": 0.2924,
      "step": 14400
    },
    {
      "epoch": 28.97585513078471,
      "grad_norm": 0.8506009578704834,
      "learning_rate": 0.00014206258174866685,
      "loss": 0.2896,
      "step": 14401
    },
    {
      "epoch": 28.977867203219315,
      "grad_norm": 0.7975337505340576,
      "learning_rate": 0.0001420585571989134,
      "loss": 0.3015,
      "step": 14402
    },
    {
      "epoch": 28.979879275653925,
      "grad_norm": 0.787083089351654,
      "learning_rate": 0.00014205453264915988,
      "loss": 0.2987,
      "step": 14403
    },
    {
      "epoch": 28.98189134808853,
      "grad_norm": 0.8399941921234131,
      "learning_rate": 0.0001420505080994064,
      "loss": 0.2921,
      "step": 14404
    },
    {
      "epoch": 28.983903420523138,
      "grad_norm": 0.8117421269416809,
      "learning_rate": 0.00014204648354965288,
      "loss": 0.285,
      "step": 14405
    },
    {
      "epoch": 28.985915492957748,
      "grad_norm": 0.7985119819641113,
      "learning_rate": 0.0001420424589998994,
      "loss": 0.2915,
      "step": 14406
    },
    {
      "epoch": 28.987927565392354,
      "grad_norm": 0.826756477355957,
      "learning_rate": 0.0001420384344501459,
      "loss": 0.294,
      "step": 14407
    },
    {
      "epoch": 28.98993963782696,
      "grad_norm": 0.8646136522293091,
      "learning_rate": 0.0001420344099003924,
      "loss": 0.2653,
      "step": 14408
    },
    {
      "epoch": 28.99195171026157,
      "grad_norm": 0.8276276588439941,
      "learning_rate": 0.0001420303853506389,
      "loss": 0.3118,
      "step": 14409
    },
    {
      "epoch": 28.993963782696177,
      "grad_norm": 0.8063302040100098,
      "learning_rate": 0.0001420263608008854,
      "loss": 0.2866,
      "step": 14410
    },
    {
      "epoch": 28.995975855130784,
      "grad_norm": 0.8253330588340759,
      "learning_rate": 0.0001420223362511319,
      "loss": 0.274,
      "step": 14411
    },
    {
      "epoch": 28.997987927565394,
      "grad_norm": 0.8319035768508911,
      "learning_rate": 0.00014201831170137843,
      "loss": 0.2916,
      "step": 14412
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.8131168484687805,
      "learning_rate": 0.00014201428715162492,
      "loss": 0.2617,
      "step": 14413
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.2484239339828491,
      "eval_runtime": 49.8331,
      "eval_samples_per_second": 19.906,
      "eval_steps_per_second": 2.488,
      "step": 14413
    },
    {
      "epoch": 29.002012072434606,
      "grad_norm": 0.7306622862815857,
      "learning_rate": 0.00014201026260187143,
      "loss": 0.2365,
      "step": 14414
    },
    {
      "epoch": 29.004024144869216,
      "grad_norm": 0.6819815635681152,
      "learning_rate": 0.00014200623805211792,
      "loss": 0.2314,
      "step": 14415
    },
    {
      "epoch": 29.006036217303823,
      "grad_norm": 0.7504948973655701,
      "learning_rate": 0.00014200221350236443,
      "loss": 0.2522,
      "step": 14416
    },
    {
      "epoch": 29.00804828973843,
      "grad_norm": 0.7892667055130005,
      "learning_rate": 0.00014199818895261094,
      "loss": 0.2331,
      "step": 14417
    },
    {
      "epoch": 29.01006036217304,
      "grad_norm": 0.761275053024292,
      "learning_rate": 0.00014199416440285745,
      "loss": 0.2124,
      "step": 14418
    },
    {
      "epoch": 29.012072434607646,
      "grad_norm": 0.8034414052963257,
      "learning_rate": 0.00014199013985310394,
      "loss": 0.2227,
      "step": 14419
    },
    {
      "epoch": 29.014084507042252,
      "grad_norm": 0.7604200839996338,
      "learning_rate": 0.00014198611530335045,
      "loss": 0.2242,
      "step": 14420
    },
    {
      "epoch": 29.016096579476862,
      "grad_norm": 0.785578191280365,
      "learning_rate": 0.00014198209075359694,
      "loss": 0.2402,
      "step": 14421
    },
    {
      "epoch": 29.01810865191147,
      "grad_norm": 0.7079845666885376,
      "learning_rate": 0.00014197806620384348,
      "loss": 0.2192,
      "step": 14422
    },
    {
      "epoch": 29.020120724346075,
      "grad_norm": 0.7327902913093567,
      "learning_rate": 0.00014197404165408996,
      "loss": 0.239,
      "step": 14423
    },
    {
      "epoch": 29.022132796780685,
      "grad_norm": 0.6940625309944153,
      "learning_rate": 0.00014197001710433647,
      "loss": 0.2108,
      "step": 14424
    },
    {
      "epoch": 29.02414486921529,
      "grad_norm": 0.7805233597755432,
      "learning_rate": 0.00014196599255458296,
      "loss": 0.2442,
      "step": 14425
    },
    {
      "epoch": 29.026156941649898,
      "grad_norm": 0.7530798316001892,
      "learning_rate": 0.00014196196800482947,
      "loss": 0.2333,
      "step": 14426
    },
    {
      "epoch": 29.028169014084508,
      "grad_norm": 0.7600207924842834,
      "learning_rate": 0.00014195794345507598,
      "loss": 0.2356,
      "step": 14427
    },
    {
      "epoch": 29.030181086519114,
      "grad_norm": 0.779719889163971,
      "learning_rate": 0.00014195391890532247,
      "loss": 0.2456,
      "step": 14428
    },
    {
      "epoch": 29.03219315895372,
      "grad_norm": 0.7623526453971863,
      "learning_rate": 0.00014194989435556898,
      "loss": 0.2092,
      "step": 14429
    },
    {
      "epoch": 29.03420523138833,
      "grad_norm": 0.7578974962234497,
      "learning_rate": 0.0001419458698058155,
      "loss": 0.2001,
      "step": 14430
    },
    {
      "epoch": 29.036217303822937,
      "grad_norm": 0.8100025057792664,
      "learning_rate": 0.00014194184525606198,
      "loss": 0.2383,
      "step": 14431
    },
    {
      "epoch": 29.038229376257544,
      "grad_norm": 0.7204296588897705,
      "learning_rate": 0.0001419378207063085,
      "loss": 0.2236,
      "step": 14432
    },
    {
      "epoch": 29.040241448692154,
      "grad_norm": 0.7605918049812317,
      "learning_rate": 0.000141933796156555,
      "loss": 0.2296,
      "step": 14433
    },
    {
      "epoch": 29.04225352112676,
      "grad_norm": 0.7452715039253235,
      "learning_rate": 0.00014192977160680149,
      "loss": 0.2187,
      "step": 14434
    },
    {
      "epoch": 29.044265593561367,
      "grad_norm": 0.7599251866340637,
      "learning_rate": 0.000141925747057048,
      "loss": 0.2336,
      "step": 14435
    },
    {
      "epoch": 29.046277665995976,
      "grad_norm": 0.7496304512023926,
      "learning_rate": 0.00014192172250729448,
      "loss": 0.2269,
      "step": 14436
    },
    {
      "epoch": 29.048289738430583,
      "grad_norm": 0.7790625095367432,
      "learning_rate": 0.000141917697957541,
      "loss": 0.2344,
      "step": 14437
    },
    {
      "epoch": 29.050301810865193,
      "grad_norm": 0.7341296076774597,
      "learning_rate": 0.0001419136734077875,
      "loss": 0.2259,
      "step": 14438
    },
    {
      "epoch": 29.0523138832998,
      "grad_norm": 0.696122944355011,
      "learning_rate": 0.00014190964885803402,
      "loss": 0.2049,
      "step": 14439
    },
    {
      "epoch": 29.054325955734406,
      "grad_norm": 0.7551306486129761,
      "learning_rate": 0.0001419056243082805,
      "loss": 0.2307,
      "step": 14440
    },
    {
      "epoch": 29.056338028169016,
      "grad_norm": 0.7136484980583191,
      "learning_rate": 0.00014190159975852702,
      "loss": 0.2091,
      "step": 14441
    },
    {
      "epoch": 29.058350100603622,
      "grad_norm": 0.7619640827178955,
      "learning_rate": 0.0001418975752087735,
      "loss": 0.2171,
      "step": 14442
    },
    {
      "epoch": 29.06036217303823,
      "grad_norm": 0.7469576001167297,
      "learning_rate": 0.00014189355065902004,
      "loss": 0.219,
      "step": 14443
    },
    {
      "epoch": 29.06237424547284,
      "grad_norm": 0.8101944327354431,
      "learning_rate": 0.00014188952610926653,
      "loss": 0.2349,
      "step": 14444
    },
    {
      "epoch": 29.064386317907445,
      "grad_norm": 0.7837004065513611,
      "learning_rate": 0.00014188550155951304,
      "loss": 0.222,
      "step": 14445
    },
    {
      "epoch": 29.06639839034205,
      "grad_norm": 0.7175511121749878,
      "learning_rate": 0.00014188147700975952,
      "loss": 0.2118,
      "step": 14446
    },
    {
      "epoch": 29.06841046277666,
      "grad_norm": 0.7718584537506104,
      "learning_rate": 0.00014187745246000604,
      "loss": 0.2173,
      "step": 14447
    },
    {
      "epoch": 29.070422535211268,
      "grad_norm": 0.8090631365776062,
      "learning_rate": 0.00014187342791025255,
      "loss": 0.2502,
      "step": 14448
    },
    {
      "epoch": 29.072434607645874,
      "grad_norm": 0.7598713040351868,
      "learning_rate": 0.00014186940336049906,
      "loss": 0.2227,
      "step": 14449
    },
    {
      "epoch": 29.074446680080484,
      "grad_norm": 0.7645447850227356,
      "learning_rate": 0.00014186537881074555,
      "loss": 0.2209,
      "step": 14450
    },
    {
      "epoch": 29.07645875251509,
      "grad_norm": 0.7552334070205688,
      "learning_rate": 0.00014186135426099206,
      "loss": 0.2234,
      "step": 14451
    },
    {
      "epoch": 29.078470824949697,
      "grad_norm": 0.7686693668365479,
      "learning_rate": 0.00014185732971123854,
      "loss": 0.2401,
      "step": 14452
    },
    {
      "epoch": 29.080482897384307,
      "grad_norm": 0.7345223426818848,
      "learning_rate": 0.00014185330516148508,
      "loss": 0.2241,
      "step": 14453
    },
    {
      "epoch": 29.082494969818914,
      "grad_norm": 0.7307919263839722,
      "learning_rate": 0.00014184928061173157,
      "loss": 0.2228,
      "step": 14454
    },
    {
      "epoch": 29.08450704225352,
      "grad_norm": 0.8161749243736267,
      "learning_rate": 0.00014184525606197808,
      "loss": 0.2441,
      "step": 14455
    },
    {
      "epoch": 29.08651911468813,
      "grad_norm": 0.7638425230979919,
      "learning_rate": 0.00014184123151222457,
      "loss": 0.2314,
      "step": 14456
    },
    {
      "epoch": 29.088531187122737,
      "grad_norm": 0.8048124313354492,
      "learning_rate": 0.00014183720696247108,
      "loss": 0.2227,
      "step": 14457
    },
    {
      "epoch": 29.090543259557343,
      "grad_norm": 0.7676243185997009,
      "learning_rate": 0.0001418331824127176,
      "loss": 0.2258,
      "step": 14458
    },
    {
      "epoch": 29.092555331991953,
      "grad_norm": 0.8107849359512329,
      "learning_rate": 0.0001418291578629641,
      "loss": 0.2452,
      "step": 14459
    },
    {
      "epoch": 29.09456740442656,
      "grad_norm": 0.8131837844848633,
      "learning_rate": 0.0001418251333132106,
      "loss": 0.2526,
      "step": 14460
    },
    {
      "epoch": 29.096579476861166,
      "grad_norm": 0.7375468611717224,
      "learning_rate": 0.0001418211087634571,
      "loss": 0.2316,
      "step": 14461
    },
    {
      "epoch": 29.098591549295776,
      "grad_norm": 0.7599979639053345,
      "learning_rate": 0.00014181708421370358,
      "loss": 0.2399,
      "step": 14462
    },
    {
      "epoch": 29.100603621730382,
      "grad_norm": 0.7178705334663391,
      "learning_rate": 0.0001418130596639501,
      "loss": 0.2292,
      "step": 14463
    },
    {
      "epoch": 29.10261569416499,
      "grad_norm": 0.7315605878829956,
      "learning_rate": 0.0001418090351141966,
      "loss": 0.2241,
      "step": 14464
    },
    {
      "epoch": 29.1046277665996,
      "grad_norm": 0.7955278754234314,
      "learning_rate": 0.00014180501056444312,
      "loss": 0.2237,
      "step": 14465
    },
    {
      "epoch": 29.106639839034205,
      "grad_norm": 0.8051336407661438,
      "learning_rate": 0.0001418009860146896,
      "loss": 0.2433,
      "step": 14466
    },
    {
      "epoch": 29.10865191146881,
      "grad_norm": 0.8284440040588379,
      "learning_rate": 0.00014179696146493612,
      "loss": 0.2523,
      "step": 14467
    },
    {
      "epoch": 29.11066398390342,
      "grad_norm": 0.7885008454322815,
      "learning_rate": 0.00014179293691518263,
      "loss": 0.2344,
      "step": 14468
    },
    {
      "epoch": 29.112676056338028,
      "grad_norm": 0.8142132759094238,
      "learning_rate": 0.00014178891236542912,
      "loss": 0.235,
      "step": 14469
    },
    {
      "epoch": 29.114688128772634,
      "grad_norm": 0.7558096051216125,
      "learning_rate": 0.00014178488781567563,
      "loss": 0.2251,
      "step": 14470
    },
    {
      "epoch": 29.116700201207244,
      "grad_norm": 0.7611958384513855,
      "learning_rate": 0.0001417808632659221,
      "loss": 0.2301,
      "step": 14471
    },
    {
      "epoch": 29.11871227364185,
      "grad_norm": 0.8514437675476074,
      "learning_rate": 0.00014177683871616863,
      "loss": 0.2209,
      "step": 14472
    },
    {
      "epoch": 29.120724346076457,
      "grad_norm": 0.7640625238418579,
      "learning_rate": 0.00014177281416641514,
      "loss": 0.231,
      "step": 14473
    },
    {
      "epoch": 29.122736418511067,
      "grad_norm": 0.7572223544120789,
      "learning_rate": 0.00014176878961666165,
      "loss": 0.2303,
      "step": 14474
    },
    {
      "epoch": 29.124748490945674,
      "grad_norm": 0.7240080833435059,
      "learning_rate": 0.00014176476506690813,
      "loss": 0.2078,
      "step": 14475
    },
    {
      "epoch": 29.12676056338028,
      "grad_norm": 0.8361571431159973,
      "learning_rate": 0.00014176074051715465,
      "loss": 0.2313,
      "step": 14476
    },
    {
      "epoch": 29.12877263581489,
      "grad_norm": 0.800974428653717,
      "learning_rate": 0.00014175671596740113,
      "loss": 0.2229,
      "step": 14477
    },
    {
      "epoch": 29.130784708249497,
      "grad_norm": 0.7583799958229065,
      "learning_rate": 0.00014175269141764767,
      "loss": 0.219,
      "step": 14478
    },
    {
      "epoch": 29.132796780684103,
      "grad_norm": 0.7771662473678589,
      "learning_rate": 0.00014174866686789416,
      "loss": 0.2203,
      "step": 14479
    },
    {
      "epoch": 29.134808853118713,
      "grad_norm": 0.7855383157730103,
      "learning_rate": 0.00014174464231814067,
      "loss": 0.2401,
      "step": 14480
    },
    {
      "epoch": 29.13682092555332,
      "grad_norm": 0.8041967153549194,
      "learning_rate": 0.00014174061776838715,
      "loss": 0.2358,
      "step": 14481
    },
    {
      "epoch": 29.138832997987926,
      "grad_norm": 0.7985458970069885,
      "learning_rate": 0.00014173659321863367,
      "loss": 0.2537,
      "step": 14482
    },
    {
      "epoch": 29.140845070422536,
      "grad_norm": 0.7687416672706604,
      "learning_rate": 0.00014173256866888018,
      "loss": 0.2431,
      "step": 14483
    },
    {
      "epoch": 29.142857142857142,
      "grad_norm": 0.7992222905158997,
      "learning_rate": 0.0001417285441191267,
      "loss": 0.248,
      "step": 14484
    },
    {
      "epoch": 29.14486921529175,
      "grad_norm": 0.8544418811798096,
      "learning_rate": 0.00014172451956937318,
      "loss": 0.2594,
      "step": 14485
    },
    {
      "epoch": 29.14688128772636,
      "grad_norm": 0.8110424876213074,
      "learning_rate": 0.0001417204950196197,
      "loss": 0.2386,
      "step": 14486
    },
    {
      "epoch": 29.148893360160965,
      "grad_norm": 0.7790142893791199,
      "learning_rate": 0.00014171647046986617,
      "loss": 0.2313,
      "step": 14487
    },
    {
      "epoch": 29.15090543259557,
      "grad_norm": 0.728516697883606,
      "learning_rate": 0.0001417124459201127,
      "loss": 0.223,
      "step": 14488
    },
    {
      "epoch": 29.15291750503018,
      "grad_norm": 0.7631428837776184,
      "learning_rate": 0.0001417084213703592,
      "loss": 0.2286,
      "step": 14489
    },
    {
      "epoch": 29.154929577464788,
      "grad_norm": 0.87738436460495,
      "learning_rate": 0.0001417043968206057,
      "loss": 0.2498,
      "step": 14490
    },
    {
      "epoch": 29.156941649899398,
      "grad_norm": 0.7584992051124573,
      "learning_rate": 0.0001417003722708522,
      "loss": 0.2335,
      "step": 14491
    },
    {
      "epoch": 29.158953722334005,
      "grad_norm": 0.8061431050300598,
      "learning_rate": 0.0001416963477210987,
      "loss": 0.2311,
      "step": 14492
    },
    {
      "epoch": 29.16096579476861,
      "grad_norm": 0.8250923156738281,
      "learning_rate": 0.00014169232317134522,
      "loss": 0.2294,
      "step": 14493
    },
    {
      "epoch": 29.16297786720322,
      "grad_norm": 0.8208860158920288,
      "learning_rate": 0.00014168829862159173,
      "loss": 0.2573,
      "step": 14494
    },
    {
      "epoch": 29.164989939637827,
      "grad_norm": 0.8496801853179932,
      "learning_rate": 0.00014168427407183822,
      "loss": 0.2424,
      "step": 14495
    },
    {
      "epoch": 29.167002012072434,
      "grad_norm": 0.7850407361984253,
      "learning_rate": 0.00014168024952208473,
      "loss": 0.2375,
      "step": 14496
    },
    {
      "epoch": 29.169014084507044,
      "grad_norm": 0.7987897992134094,
      "learning_rate": 0.00014167622497233121,
      "loss": 0.2453,
      "step": 14497
    },
    {
      "epoch": 29.17102615694165,
      "grad_norm": 0.7977416515350342,
      "learning_rate": 0.00014167220042257773,
      "loss": 0.257,
      "step": 14498
    },
    {
      "epoch": 29.173038229376257,
      "grad_norm": 0.8648441433906555,
      "learning_rate": 0.00014166817587282424,
      "loss": 0.2235,
      "step": 14499
    },
    {
      "epoch": 29.175050301810867,
      "grad_norm": 0.7991726398468018,
      "learning_rate": 0.00014166415132307075,
      "loss": 0.2443,
      "step": 14500
    },
    {
      "epoch": 29.177062374245473,
      "grad_norm": 0.7942979335784912,
      "learning_rate": 0.00014166012677331724,
      "loss": 0.2274,
      "step": 14501
    },
    {
      "epoch": 29.17907444668008,
      "grad_norm": 0.776305079460144,
      "learning_rate": 0.00014165610222356375,
      "loss": 0.249,
      "step": 14502
    },
    {
      "epoch": 29.18108651911469,
      "grad_norm": 0.8112636804580688,
      "learning_rate": 0.00014165207767381026,
      "loss": 0.2333,
      "step": 14503
    },
    {
      "epoch": 29.183098591549296,
      "grad_norm": 0.8288986682891846,
      "learning_rate": 0.00014164805312405675,
      "loss": 0.2386,
      "step": 14504
    },
    {
      "epoch": 29.185110663983902,
      "grad_norm": 0.7798833250999451,
      "learning_rate": 0.00014164402857430326,
      "loss": 0.2123,
      "step": 14505
    },
    {
      "epoch": 29.187122736418512,
      "grad_norm": 0.9181838035583496,
      "learning_rate": 0.00014164000402454974,
      "loss": 0.2519,
      "step": 14506
    },
    {
      "epoch": 29.18913480885312,
      "grad_norm": 0.7950835824012756,
      "learning_rate": 0.00014163597947479625,
      "loss": 0.2462,
      "step": 14507
    },
    {
      "epoch": 29.191146881287725,
      "grad_norm": 0.8222152590751648,
      "learning_rate": 0.00014163195492504277,
      "loss": 0.2508,
      "step": 14508
    },
    {
      "epoch": 29.193158953722335,
      "grad_norm": 0.7830302119255066,
      "learning_rate": 0.00014162793037528928,
      "loss": 0.2326,
      "step": 14509
    },
    {
      "epoch": 29.19517102615694,
      "grad_norm": 0.7914854288101196,
      "learning_rate": 0.00014162390582553576,
      "loss": 0.2257,
      "step": 14510
    },
    {
      "epoch": 29.197183098591548,
      "grad_norm": 0.8443990349769592,
      "learning_rate": 0.00014161988127578228,
      "loss": 0.2474,
      "step": 14511
    },
    {
      "epoch": 29.199195171026158,
      "grad_norm": 0.8694729208946228,
      "learning_rate": 0.00014161585672602876,
      "loss": 0.2518,
      "step": 14512
    },
    {
      "epoch": 29.201207243460765,
      "grad_norm": 0.8159420490264893,
      "learning_rate": 0.0001416118321762753,
      "loss": 0.2224,
      "step": 14513
    },
    {
      "epoch": 29.20321931589537,
      "grad_norm": 0.8103013634681702,
      "learning_rate": 0.00014160780762652179,
      "loss": 0.2515,
      "step": 14514
    },
    {
      "epoch": 29.20523138832998,
      "grad_norm": 0.8576521277427673,
      "learning_rate": 0.0001416037830767683,
      "loss": 0.2488,
      "step": 14515
    },
    {
      "epoch": 29.207243460764587,
      "grad_norm": 0.7968403100967407,
      "learning_rate": 0.00014159975852701478,
      "loss": 0.2262,
      "step": 14516
    },
    {
      "epoch": 29.209255533199194,
      "grad_norm": 0.7818397879600525,
      "learning_rate": 0.0001415957339772613,
      "loss": 0.24,
      "step": 14517
    },
    {
      "epoch": 29.211267605633804,
      "grad_norm": 0.816291093826294,
      "learning_rate": 0.0001415917094275078,
      "loss": 0.2527,
      "step": 14518
    },
    {
      "epoch": 29.21327967806841,
      "grad_norm": 0.8142408728599548,
      "learning_rate": 0.00014158768487775432,
      "loss": 0.235,
      "step": 14519
    },
    {
      "epoch": 29.215291750503017,
      "grad_norm": 0.8253024816513062,
      "learning_rate": 0.0001415836603280008,
      "loss": 0.236,
      "step": 14520
    },
    {
      "epoch": 29.217303822937627,
      "grad_norm": 0.8463361263275146,
      "learning_rate": 0.00014157963577824732,
      "loss": 0.241,
      "step": 14521
    },
    {
      "epoch": 29.219315895372233,
      "grad_norm": 0.8415970206260681,
      "learning_rate": 0.0001415756112284938,
      "loss": 0.253,
      "step": 14522
    },
    {
      "epoch": 29.22132796780684,
      "grad_norm": 0.8153095841407776,
      "learning_rate": 0.00014157158667874034,
      "loss": 0.2465,
      "step": 14523
    },
    {
      "epoch": 29.22334004024145,
      "grad_norm": 0.8073958158493042,
      "learning_rate": 0.00014156756212898683,
      "loss": 0.2517,
      "step": 14524
    },
    {
      "epoch": 29.225352112676056,
      "grad_norm": 0.8532907366752625,
      "learning_rate": 0.00014156353757923334,
      "loss": 0.2528,
      "step": 14525
    },
    {
      "epoch": 29.227364185110662,
      "grad_norm": 0.7768384218215942,
      "learning_rate": 0.00014155951302947982,
      "loss": 0.2358,
      "step": 14526
    },
    {
      "epoch": 29.229376257545272,
      "grad_norm": 0.8162475228309631,
      "learning_rate": 0.00014155548847972634,
      "loss": 0.2371,
      "step": 14527
    },
    {
      "epoch": 29.23138832997988,
      "grad_norm": 0.8256481289863586,
      "learning_rate": 0.00014155146392997285,
      "loss": 0.2371,
      "step": 14528
    },
    {
      "epoch": 29.233400402414485,
      "grad_norm": 0.8374273180961609,
      "learning_rate": 0.00014154743938021936,
      "loss": 0.2701,
      "step": 14529
    },
    {
      "epoch": 29.235412474849095,
      "grad_norm": 0.8419851064682007,
      "learning_rate": 0.00014154341483046585,
      "loss": 0.2637,
      "step": 14530
    },
    {
      "epoch": 29.2374245472837,
      "grad_norm": 0.8520470261573792,
      "learning_rate": 0.00014153939028071236,
      "loss": 0.2299,
      "step": 14531
    },
    {
      "epoch": 29.239436619718308,
      "grad_norm": 0.7742085456848145,
      "learning_rate": 0.00014153536573095884,
      "loss": 0.2541,
      "step": 14532
    },
    {
      "epoch": 29.241448692152918,
      "grad_norm": 0.8516123294830322,
      "learning_rate": 0.00014153134118120536,
      "loss": 0.2428,
      "step": 14533
    },
    {
      "epoch": 29.243460764587525,
      "grad_norm": 0.8032832145690918,
      "learning_rate": 0.00014152731663145187,
      "loss": 0.2691,
      "step": 14534
    },
    {
      "epoch": 29.24547283702213,
      "grad_norm": 0.8106683492660522,
      "learning_rate": 0.00014152329208169838,
      "loss": 0.245,
      "step": 14535
    },
    {
      "epoch": 29.24748490945674,
      "grad_norm": 0.7904976606369019,
      "learning_rate": 0.00014151926753194486,
      "loss": 0.2425,
      "step": 14536
    },
    {
      "epoch": 29.249496981891348,
      "grad_norm": 0.8088733553886414,
      "learning_rate": 0.00014151524298219138,
      "loss": 0.2304,
      "step": 14537
    },
    {
      "epoch": 29.251509054325957,
      "grad_norm": 0.7814403772354126,
      "learning_rate": 0.0001415112184324379,
      "loss": 0.2403,
      "step": 14538
    },
    {
      "epoch": 29.253521126760564,
      "grad_norm": 0.8552762269973755,
      "learning_rate": 0.00014150719388268437,
      "loss": 0.2427,
      "step": 14539
    },
    {
      "epoch": 29.25553319919517,
      "grad_norm": 0.8487927317619324,
      "learning_rate": 0.0001415031693329309,
      "loss": 0.2529,
      "step": 14540
    },
    {
      "epoch": 29.25754527162978,
      "grad_norm": 0.808867871761322,
      "learning_rate": 0.00014149914478317737,
      "loss": 0.2237,
      "step": 14541
    },
    {
      "epoch": 29.259557344064387,
      "grad_norm": 0.7823817729949951,
      "learning_rate": 0.00014149512023342388,
      "loss": 0.2262,
      "step": 14542
    },
    {
      "epoch": 29.261569416498993,
      "grad_norm": 0.8073505759239197,
      "learning_rate": 0.0001414910956836704,
      "loss": 0.2492,
      "step": 14543
    },
    {
      "epoch": 29.263581488933603,
      "grad_norm": 0.7641788125038147,
      "learning_rate": 0.0001414870711339169,
      "loss": 0.2268,
      "step": 14544
    },
    {
      "epoch": 29.26559356136821,
      "grad_norm": 0.8618635535240173,
      "learning_rate": 0.0001414830465841634,
      "loss": 0.2586,
      "step": 14545
    },
    {
      "epoch": 29.267605633802816,
      "grad_norm": 0.8319565653800964,
      "learning_rate": 0.0001414790220344099,
      "loss": 0.2305,
      "step": 14546
    },
    {
      "epoch": 29.269617706237426,
      "grad_norm": 0.8831154704093933,
      "learning_rate": 0.0001414749974846564,
      "loss": 0.2284,
      "step": 14547
    },
    {
      "epoch": 29.271629778672033,
      "grad_norm": 0.7859747409820557,
      "learning_rate": 0.00014147097293490293,
      "loss": 0.2276,
      "step": 14548
    },
    {
      "epoch": 29.27364185110664,
      "grad_norm": 0.8584246039390564,
      "learning_rate": 0.00014146694838514942,
      "loss": 0.2502,
      "step": 14549
    },
    {
      "epoch": 29.27565392354125,
      "grad_norm": 0.8458436727523804,
      "learning_rate": 0.00014146292383539593,
      "loss": 0.2297,
      "step": 14550
    },
    {
      "epoch": 29.277665995975855,
      "grad_norm": 0.8139868974685669,
      "learning_rate": 0.0001414588992856424,
      "loss": 0.2596,
      "step": 14551
    },
    {
      "epoch": 29.279678068410462,
      "grad_norm": 0.8292933702468872,
      "learning_rate": 0.00014145487473588892,
      "loss": 0.2293,
      "step": 14552
    },
    {
      "epoch": 29.281690140845072,
      "grad_norm": 0.7613927721977234,
      "learning_rate": 0.00014145085018613544,
      "loss": 0.2371,
      "step": 14553
    },
    {
      "epoch": 29.28370221327968,
      "grad_norm": 0.9112048745155334,
      "learning_rate": 0.00014144682563638195,
      "loss": 0.2317,
      "step": 14554
    },
    {
      "epoch": 29.285714285714285,
      "grad_norm": 0.8378166556358337,
      "learning_rate": 0.00014144280108662843,
      "loss": 0.257,
      "step": 14555
    },
    {
      "epoch": 29.287726358148895,
      "grad_norm": 0.8279803395271301,
      "learning_rate": 0.00014143877653687495,
      "loss": 0.2426,
      "step": 14556
    },
    {
      "epoch": 29.2897384305835,
      "grad_norm": 0.8486900329589844,
      "learning_rate": 0.00014143475198712143,
      "loss": 0.2502,
      "step": 14557
    },
    {
      "epoch": 29.291750503018108,
      "grad_norm": 0.8529283404350281,
      "learning_rate": 0.00014143072743736797,
      "loss": 0.2651,
      "step": 14558
    },
    {
      "epoch": 29.293762575452718,
      "grad_norm": 0.7739863991737366,
      "learning_rate": 0.00014142670288761446,
      "loss": 0.2323,
      "step": 14559
    },
    {
      "epoch": 29.295774647887324,
      "grad_norm": 0.8123324513435364,
      "learning_rate": 0.00014142267833786097,
      "loss": 0.2246,
      "step": 14560
    },
    {
      "epoch": 29.29778672032193,
      "grad_norm": 0.803550660610199,
      "learning_rate": 0.00014141865378810745,
      "loss": 0.2396,
      "step": 14561
    },
    {
      "epoch": 29.29979879275654,
      "grad_norm": 0.7996088266372681,
      "learning_rate": 0.00014141462923835397,
      "loss": 0.2257,
      "step": 14562
    },
    {
      "epoch": 29.301810865191147,
      "grad_norm": 0.8558862209320068,
      "learning_rate": 0.00014141060468860048,
      "loss": 0.2561,
      "step": 14563
    },
    {
      "epoch": 29.303822937625753,
      "grad_norm": 0.8207147717475891,
      "learning_rate": 0.000141406580138847,
      "loss": 0.2306,
      "step": 14564
    },
    {
      "epoch": 29.305835010060363,
      "grad_norm": 0.8198786973953247,
      "learning_rate": 0.00014140255558909348,
      "loss": 0.2335,
      "step": 14565
    },
    {
      "epoch": 29.30784708249497,
      "grad_norm": 0.7880398631095886,
      "learning_rate": 0.00014139853103934,
      "loss": 0.2129,
      "step": 14566
    },
    {
      "epoch": 29.309859154929576,
      "grad_norm": 0.8371657729148865,
      "learning_rate": 0.00014139450648958647,
      "loss": 0.2546,
      "step": 14567
    },
    {
      "epoch": 29.311871227364186,
      "grad_norm": 0.8730876445770264,
      "learning_rate": 0.00014139048193983298,
      "loss": 0.2286,
      "step": 14568
    },
    {
      "epoch": 29.313883299798793,
      "grad_norm": 0.8014441132545471,
      "learning_rate": 0.0001413864573900795,
      "loss": 0.2293,
      "step": 14569
    },
    {
      "epoch": 29.3158953722334,
      "grad_norm": 0.842234194278717,
      "learning_rate": 0.00014138243284032598,
      "loss": 0.275,
      "step": 14570
    },
    {
      "epoch": 29.31790744466801,
      "grad_norm": 0.8064677715301514,
      "learning_rate": 0.0001413784082905725,
      "loss": 0.2503,
      "step": 14571
    },
    {
      "epoch": 29.319919517102615,
      "grad_norm": 0.8098334074020386,
      "learning_rate": 0.000141374383740819,
      "loss": 0.2523,
      "step": 14572
    },
    {
      "epoch": 29.321931589537222,
      "grad_norm": 0.814940869808197,
      "learning_rate": 0.00014137035919106552,
      "loss": 0.2568,
      "step": 14573
    },
    {
      "epoch": 29.323943661971832,
      "grad_norm": 0.7990788221359253,
      "learning_rate": 0.000141366334641312,
      "loss": 0.2521,
      "step": 14574
    },
    {
      "epoch": 29.32595573440644,
      "grad_norm": 0.8797382116317749,
      "learning_rate": 0.00014136231009155852,
      "loss": 0.269,
      "step": 14575
    },
    {
      "epoch": 29.327967806841045,
      "grad_norm": 0.8230870366096497,
      "learning_rate": 0.000141358285541805,
      "loss": 0.2561,
      "step": 14576
    },
    {
      "epoch": 29.329979879275655,
      "grad_norm": 0.82582688331604,
      "learning_rate": 0.0001413542609920515,
      "loss": 0.2612,
      "step": 14577
    },
    {
      "epoch": 29.33199195171026,
      "grad_norm": 0.8191226124763489,
      "learning_rate": 0.00014135023644229803,
      "loss": 0.2517,
      "step": 14578
    },
    {
      "epoch": 29.334004024144868,
      "grad_norm": 0.8891233801841736,
      "learning_rate": 0.00014134621189254454,
      "loss": 0.2431,
      "step": 14579
    },
    {
      "epoch": 29.336016096579478,
      "grad_norm": 0.842296838760376,
      "learning_rate": 0.00014134218734279102,
      "loss": 0.2467,
      "step": 14580
    },
    {
      "epoch": 29.338028169014084,
      "grad_norm": 0.8456013202667236,
      "learning_rate": 0.00014133816279303754,
      "loss": 0.2379,
      "step": 14581
    },
    {
      "epoch": 29.34004024144869,
      "grad_norm": 0.8135915994644165,
      "learning_rate": 0.00014133413824328402,
      "loss": 0.2548,
      "step": 14582
    },
    {
      "epoch": 29.3420523138833,
      "grad_norm": 0.8633344769477844,
      "learning_rate": 0.00014133011369353056,
      "loss": 0.2536,
      "step": 14583
    },
    {
      "epoch": 29.344064386317907,
      "grad_norm": 0.7692466974258423,
      "learning_rate": 0.00014132608914377704,
      "loss": 0.2136,
      "step": 14584
    },
    {
      "epoch": 29.346076458752513,
      "grad_norm": 0.8013625741004944,
      "learning_rate": 0.00014132206459402356,
      "loss": 0.2532,
      "step": 14585
    },
    {
      "epoch": 29.348088531187123,
      "grad_norm": 0.8820867538452148,
      "learning_rate": 0.00014131804004427004,
      "loss": 0.2487,
      "step": 14586
    },
    {
      "epoch": 29.35010060362173,
      "grad_norm": 0.8397467732429504,
      "learning_rate": 0.00014131401549451655,
      "loss": 0.2353,
      "step": 14587
    },
    {
      "epoch": 29.352112676056336,
      "grad_norm": 0.8861340880393982,
      "learning_rate": 0.00014130999094476307,
      "loss": 0.2608,
      "step": 14588
    },
    {
      "epoch": 29.354124748490946,
      "grad_norm": 0.8721787333488464,
      "learning_rate": 0.00014130596639500958,
      "loss": 0.2373,
      "step": 14589
    },
    {
      "epoch": 29.356136820925553,
      "grad_norm": 0.8382940292358398,
      "learning_rate": 0.00014130194184525606,
      "loss": 0.2618,
      "step": 14590
    },
    {
      "epoch": 29.358148893360163,
      "grad_norm": 0.8467596173286438,
      "learning_rate": 0.00014129791729550258,
      "loss": 0.2646,
      "step": 14591
    },
    {
      "epoch": 29.36016096579477,
      "grad_norm": 0.8140667080879211,
      "learning_rate": 0.00014129389274574906,
      "loss": 0.2495,
      "step": 14592
    },
    {
      "epoch": 29.362173038229376,
      "grad_norm": 0.8231886625289917,
      "learning_rate": 0.0001412898681959956,
      "loss": 0.2524,
      "step": 14593
    },
    {
      "epoch": 29.364185110663986,
      "grad_norm": 0.9479340314865112,
      "learning_rate": 0.00014128584364624209,
      "loss": 0.2652,
      "step": 14594
    },
    {
      "epoch": 29.366197183098592,
      "grad_norm": 0.7989441752433777,
      "learning_rate": 0.0001412818190964886,
      "loss": 0.2389,
      "step": 14595
    },
    {
      "epoch": 29.3682092555332,
      "grad_norm": 0.7993038296699524,
      "learning_rate": 0.00014127779454673508,
      "loss": 0.2563,
      "step": 14596
    },
    {
      "epoch": 29.37022132796781,
      "grad_norm": 0.9068105816841125,
      "learning_rate": 0.0001412737699969816,
      "loss": 0.2589,
      "step": 14597
    },
    {
      "epoch": 29.372233400402415,
      "grad_norm": 0.8125926852226257,
      "learning_rate": 0.0001412697454472281,
      "loss": 0.2444,
      "step": 14598
    },
    {
      "epoch": 29.37424547283702,
      "grad_norm": 0.8583069443702698,
      "learning_rate": 0.00014126572089747462,
      "loss": 0.2655,
      "step": 14599
    },
    {
      "epoch": 29.37625754527163,
      "grad_norm": 0.8828263282775879,
      "learning_rate": 0.0001412616963477211,
      "loss": 0.2725,
      "step": 14600
    },
    {
      "epoch": 29.378269617706238,
      "grad_norm": 0.794222891330719,
      "learning_rate": 0.00014125767179796762,
      "loss": 0.2449,
      "step": 14601
    },
    {
      "epoch": 29.380281690140844,
      "grad_norm": 0.8386780023574829,
      "learning_rate": 0.0001412536472482141,
      "loss": 0.2605,
      "step": 14602
    },
    {
      "epoch": 29.382293762575454,
      "grad_norm": 0.820508599281311,
      "learning_rate": 0.00014124962269846061,
      "loss": 0.253,
      "step": 14603
    },
    {
      "epoch": 29.38430583501006,
      "grad_norm": 0.821262776851654,
      "learning_rate": 0.00014124559814870713,
      "loss": 0.2448,
      "step": 14604
    },
    {
      "epoch": 29.386317907444667,
      "grad_norm": 0.8264985084533691,
      "learning_rate": 0.0001412415735989536,
      "loss": 0.2396,
      "step": 14605
    },
    {
      "epoch": 29.388329979879277,
      "grad_norm": 0.828574001789093,
      "learning_rate": 0.00014123754904920012,
      "loss": 0.2405,
      "step": 14606
    },
    {
      "epoch": 29.390342052313883,
      "grad_norm": 0.7849679589271545,
      "learning_rate": 0.00014123352449944664,
      "loss": 0.2391,
      "step": 14607
    },
    {
      "epoch": 29.39235412474849,
      "grad_norm": 0.8321583271026611,
      "learning_rate": 0.00014122949994969315,
      "loss": 0.243,
      "step": 14608
    },
    {
      "epoch": 29.3943661971831,
      "grad_norm": 0.8127031326293945,
      "learning_rate": 0.00014122547539993963,
      "loss": 0.2505,
      "step": 14609
    },
    {
      "epoch": 29.396378269617706,
      "grad_norm": 0.8514244556427002,
      "learning_rate": 0.00014122145085018615,
      "loss": 0.2744,
      "step": 14610
    },
    {
      "epoch": 29.398390342052313,
      "grad_norm": 0.8211829662322998,
      "learning_rate": 0.00014121742630043263,
      "loss": 0.2513,
      "step": 14611
    },
    {
      "epoch": 29.400402414486923,
      "grad_norm": 0.8256931304931641,
      "learning_rate": 0.00014121340175067914,
      "loss": 0.2485,
      "step": 14612
    },
    {
      "epoch": 29.40241448692153,
      "grad_norm": 0.8301150798797607,
      "learning_rate": 0.00014120937720092566,
      "loss": 0.2508,
      "step": 14613
    },
    {
      "epoch": 29.404426559356136,
      "grad_norm": 0.8306719660758972,
      "learning_rate": 0.00014120535265117217,
      "loss": 0.2555,
      "step": 14614
    },
    {
      "epoch": 29.406438631790746,
      "grad_norm": 0.851608157157898,
      "learning_rate": 0.00014120132810141865,
      "loss": 0.2775,
      "step": 14615
    },
    {
      "epoch": 29.408450704225352,
      "grad_norm": 0.83184415102005,
      "learning_rate": 0.00014119730355166516,
      "loss": 0.2633,
      "step": 14616
    },
    {
      "epoch": 29.41046277665996,
      "grad_norm": 0.8147653937339783,
      "learning_rate": 0.00014119327900191165,
      "loss": 0.2489,
      "step": 14617
    },
    {
      "epoch": 29.41247484909457,
      "grad_norm": 0.8513490557670593,
      "learning_rate": 0.0001411892544521582,
      "loss": 0.2627,
      "step": 14618
    },
    {
      "epoch": 29.414486921529175,
      "grad_norm": 0.8572989702224731,
      "learning_rate": 0.00014118522990240467,
      "loss": 0.2601,
      "step": 14619
    },
    {
      "epoch": 29.41649899396378,
      "grad_norm": 0.7935940623283386,
      "learning_rate": 0.00014118120535265119,
      "loss": 0.2292,
      "step": 14620
    },
    {
      "epoch": 29.41851106639839,
      "grad_norm": 0.8422014713287354,
      "learning_rate": 0.00014117718080289767,
      "loss": 0.2516,
      "step": 14621
    },
    {
      "epoch": 29.420523138832998,
      "grad_norm": 0.8361640572547913,
      "learning_rate": 0.00014117315625314418,
      "loss": 0.2598,
      "step": 14622
    },
    {
      "epoch": 29.422535211267604,
      "grad_norm": 0.8118926286697388,
      "learning_rate": 0.0001411691317033907,
      "loss": 0.2386,
      "step": 14623
    },
    {
      "epoch": 29.424547283702214,
      "grad_norm": 0.8014401793479919,
      "learning_rate": 0.0001411651071536372,
      "loss": 0.238,
      "step": 14624
    },
    {
      "epoch": 29.42655935613682,
      "grad_norm": 0.8749712109565735,
      "learning_rate": 0.0001411610826038837,
      "loss": 0.2803,
      "step": 14625
    },
    {
      "epoch": 29.428571428571427,
      "grad_norm": 0.8269875645637512,
      "learning_rate": 0.0001411570580541302,
      "loss": 0.2536,
      "step": 14626
    },
    {
      "epoch": 29.430583501006037,
      "grad_norm": 0.7982841730117798,
      "learning_rate": 0.0001411530335043767,
      "loss": 0.2493,
      "step": 14627
    },
    {
      "epoch": 29.432595573440643,
      "grad_norm": 0.8119916319847107,
      "learning_rate": 0.00014114900895462323,
      "loss": 0.2372,
      "step": 14628
    },
    {
      "epoch": 29.43460764587525,
      "grad_norm": 0.9000664949417114,
      "learning_rate": 0.00014114498440486972,
      "loss": 0.2949,
      "step": 14629
    },
    {
      "epoch": 29.43661971830986,
      "grad_norm": 0.8248348236083984,
      "learning_rate": 0.00014114095985511623,
      "loss": 0.2613,
      "step": 14630
    },
    {
      "epoch": 29.438631790744466,
      "grad_norm": 0.8770782351493835,
      "learning_rate": 0.0001411369353053627,
      "loss": 0.2809,
      "step": 14631
    },
    {
      "epoch": 29.440643863179073,
      "grad_norm": 0.8538477420806885,
      "learning_rate": 0.00014113291075560922,
      "loss": 0.267,
      "step": 14632
    },
    {
      "epoch": 29.442655935613683,
      "grad_norm": 0.8296898603439331,
      "learning_rate": 0.00014112888620585574,
      "loss": 0.2532,
      "step": 14633
    },
    {
      "epoch": 29.44466800804829,
      "grad_norm": 0.8423649668693542,
      "learning_rate": 0.00014112486165610225,
      "loss": 0.2619,
      "step": 14634
    },
    {
      "epoch": 29.446680080482896,
      "grad_norm": 0.8378604650497437,
      "learning_rate": 0.00014112083710634873,
      "loss": 0.2511,
      "step": 14635
    },
    {
      "epoch": 29.448692152917506,
      "grad_norm": 0.8613253831863403,
      "learning_rate": 0.00014111681255659525,
      "loss": 0.26,
      "step": 14636
    },
    {
      "epoch": 29.450704225352112,
      "grad_norm": 0.844027578830719,
      "learning_rate": 0.00014111278800684173,
      "loss": 0.268,
      "step": 14637
    },
    {
      "epoch": 29.452716297786722,
      "grad_norm": 0.8153764009475708,
      "learning_rate": 0.00014110876345708824,
      "loss": 0.2619,
      "step": 14638
    },
    {
      "epoch": 29.45472837022133,
      "grad_norm": 0.8815524578094482,
      "learning_rate": 0.00014110473890733476,
      "loss": 0.2602,
      "step": 14639
    },
    {
      "epoch": 29.456740442655935,
      "grad_norm": 0.8601438999176025,
      "learning_rate": 0.00014110071435758124,
      "loss": 0.2708,
      "step": 14640
    },
    {
      "epoch": 29.458752515090545,
      "grad_norm": 0.8878605961799622,
      "learning_rate": 0.00014109668980782775,
      "loss": 0.2734,
      "step": 14641
    },
    {
      "epoch": 29.46076458752515,
      "grad_norm": 0.8470481634140015,
      "learning_rate": 0.00014109266525807427,
      "loss": 0.2712,
      "step": 14642
    },
    {
      "epoch": 29.462776659959758,
      "grad_norm": 0.7719308137893677,
      "learning_rate": 0.00014108864070832078,
      "loss": 0.2371,
      "step": 14643
    },
    {
      "epoch": 29.464788732394368,
      "grad_norm": 0.8146101832389832,
      "learning_rate": 0.00014108461615856726,
      "loss": 0.2449,
      "step": 14644
    },
    {
      "epoch": 29.466800804828974,
      "grad_norm": 0.9493109583854675,
      "learning_rate": 0.00014108059160881377,
      "loss": 0.2589,
      "step": 14645
    },
    {
      "epoch": 29.46881287726358,
      "grad_norm": 0.8829402327537537,
      "learning_rate": 0.00014107656705906026,
      "loss": 0.2586,
      "step": 14646
    },
    {
      "epoch": 29.47082494969819,
      "grad_norm": 0.8814971446990967,
      "learning_rate": 0.00014107254250930677,
      "loss": 0.2588,
      "step": 14647
    },
    {
      "epoch": 29.472837022132797,
      "grad_norm": 0.8531123995780945,
      "learning_rate": 0.00014106851795955328,
      "loss": 0.2531,
      "step": 14648
    },
    {
      "epoch": 29.474849094567404,
      "grad_norm": 0.7857639193534851,
      "learning_rate": 0.0001410644934097998,
      "loss": 0.2408,
      "step": 14649
    },
    {
      "epoch": 29.476861167002014,
      "grad_norm": 0.8449098467826843,
      "learning_rate": 0.00014106046886004628,
      "loss": 0.2461,
      "step": 14650
    },
    {
      "epoch": 29.47887323943662,
      "grad_norm": 0.860404372215271,
      "learning_rate": 0.0001410564443102928,
      "loss": 0.2326,
      "step": 14651
    },
    {
      "epoch": 29.480885311871226,
      "grad_norm": 0.8763137459754944,
      "learning_rate": 0.00014105241976053928,
      "loss": 0.2795,
      "step": 14652
    },
    {
      "epoch": 29.482897384305836,
      "grad_norm": 0.8606947660446167,
      "learning_rate": 0.00014104839521078582,
      "loss": 0.2729,
      "step": 14653
    },
    {
      "epoch": 29.484909456740443,
      "grad_norm": 0.8086593747138977,
      "learning_rate": 0.0001410443706610323,
      "loss": 0.2396,
      "step": 14654
    },
    {
      "epoch": 29.48692152917505,
      "grad_norm": 0.8610376119613647,
      "learning_rate": 0.00014104034611127882,
      "loss": 0.2567,
      "step": 14655
    },
    {
      "epoch": 29.48893360160966,
      "grad_norm": 0.9053038954734802,
      "learning_rate": 0.0001410363215615253,
      "loss": 0.2925,
      "step": 14656
    },
    {
      "epoch": 29.490945674044266,
      "grad_norm": 0.826175332069397,
      "learning_rate": 0.0001410322970117718,
      "loss": 0.2372,
      "step": 14657
    },
    {
      "epoch": 29.492957746478872,
      "grad_norm": 0.8576977252960205,
      "learning_rate": 0.00014102827246201833,
      "loss": 0.2688,
      "step": 14658
    },
    {
      "epoch": 29.494969818913482,
      "grad_norm": 0.8424230813980103,
      "learning_rate": 0.00014102424791226484,
      "loss": 0.2504,
      "step": 14659
    },
    {
      "epoch": 29.49698189134809,
      "grad_norm": 0.8909123539924622,
      "learning_rate": 0.00014102022336251132,
      "loss": 0.2594,
      "step": 14660
    },
    {
      "epoch": 29.498993963782695,
      "grad_norm": 0.8711214661598206,
      "learning_rate": 0.00014101619881275783,
      "loss": 0.2821,
      "step": 14661
    },
    {
      "epoch": 29.501006036217305,
      "grad_norm": 0.9389494061470032,
      "learning_rate": 0.00014101217426300432,
      "loss": 0.287,
      "step": 14662
    },
    {
      "epoch": 29.50301810865191,
      "grad_norm": 0.8030335903167725,
      "learning_rate": 0.00014100814971325086,
      "loss": 0.258,
      "step": 14663
    },
    {
      "epoch": 29.505030181086518,
      "grad_norm": 0.8667756915092468,
      "learning_rate": 0.00014100412516349734,
      "loss": 0.2697,
      "step": 14664
    },
    {
      "epoch": 29.507042253521128,
      "grad_norm": 0.9197157621383667,
      "learning_rate": 0.00014100010061374386,
      "loss": 0.2511,
      "step": 14665
    },
    {
      "epoch": 29.509054325955734,
      "grad_norm": 0.8994073867797852,
      "learning_rate": 0.00014099607606399034,
      "loss": 0.2769,
      "step": 14666
    },
    {
      "epoch": 29.51106639839034,
      "grad_norm": 0.8195826411247253,
      "learning_rate": 0.00014099205151423685,
      "loss": 0.2498,
      "step": 14667
    },
    {
      "epoch": 29.51307847082495,
      "grad_norm": 0.8196073770523071,
      "learning_rate": 0.00014098802696448337,
      "loss": 0.2546,
      "step": 14668
    },
    {
      "epoch": 29.515090543259557,
      "grad_norm": 0.88247150182724,
      "learning_rate": 0.00014098400241472988,
      "loss": 0.2779,
      "step": 14669
    },
    {
      "epoch": 29.517102615694164,
      "grad_norm": 0.8945110440254211,
      "learning_rate": 0.00014097997786497636,
      "loss": 0.2678,
      "step": 14670
    },
    {
      "epoch": 29.519114688128774,
      "grad_norm": 0.8876241445541382,
      "learning_rate": 0.00014097595331522288,
      "loss": 0.2645,
      "step": 14671
    },
    {
      "epoch": 29.52112676056338,
      "grad_norm": 0.8569868206977844,
      "learning_rate": 0.00014097192876546936,
      "loss": 0.2531,
      "step": 14672
    },
    {
      "epoch": 29.523138832997986,
      "grad_norm": 0.8904827833175659,
      "learning_rate": 0.00014096790421571587,
      "loss": 0.2684,
      "step": 14673
    },
    {
      "epoch": 29.525150905432596,
      "grad_norm": 0.8361833095550537,
      "learning_rate": 0.00014096387966596239,
      "loss": 0.27,
      "step": 14674
    },
    {
      "epoch": 29.527162977867203,
      "grad_norm": 0.8862919211387634,
      "learning_rate": 0.00014095985511620887,
      "loss": 0.2587,
      "step": 14675
    },
    {
      "epoch": 29.52917505030181,
      "grad_norm": 0.7770684957504272,
      "learning_rate": 0.00014095583056645538,
      "loss": 0.2449,
      "step": 14676
    },
    {
      "epoch": 29.53118712273642,
      "grad_norm": 0.8727614879608154,
      "learning_rate": 0.0001409518060167019,
      "loss": 0.2684,
      "step": 14677
    },
    {
      "epoch": 29.533199195171026,
      "grad_norm": 0.8328308463096619,
      "learning_rate": 0.00014094778146694838,
      "loss": 0.2682,
      "step": 14678
    },
    {
      "epoch": 29.535211267605632,
      "grad_norm": 0.887212872505188,
      "learning_rate": 0.0001409437569171949,
      "loss": 0.2616,
      "step": 14679
    },
    {
      "epoch": 29.537223340040242,
      "grad_norm": 0.8715649843215942,
      "learning_rate": 0.0001409397323674414,
      "loss": 0.2741,
      "step": 14680
    },
    {
      "epoch": 29.53923541247485,
      "grad_norm": 0.7855929732322693,
      "learning_rate": 0.0001409357078176879,
      "loss": 0.2518,
      "step": 14681
    },
    {
      "epoch": 29.541247484909455,
      "grad_norm": 0.8505197763442993,
      "learning_rate": 0.0001409316832679344,
      "loss": 0.2671,
      "step": 14682
    },
    {
      "epoch": 29.543259557344065,
      "grad_norm": 0.8275030851364136,
      "learning_rate": 0.0001409276587181809,
      "loss": 0.2665,
      "step": 14683
    },
    {
      "epoch": 29.54527162977867,
      "grad_norm": 0.8128353953361511,
      "learning_rate": 0.00014092363416842743,
      "loss": 0.2749,
      "step": 14684
    },
    {
      "epoch": 29.547283702213278,
      "grad_norm": 0.7894430756568909,
      "learning_rate": 0.0001409196096186739,
      "loss": 0.2685,
      "step": 14685
    },
    {
      "epoch": 29.549295774647888,
      "grad_norm": 0.8617122173309326,
      "learning_rate": 0.00014091558506892042,
      "loss": 0.2599,
      "step": 14686
    },
    {
      "epoch": 29.551307847082494,
      "grad_norm": 0.8782392740249634,
      "learning_rate": 0.0001409115605191669,
      "loss": 0.2733,
      "step": 14687
    },
    {
      "epoch": 29.5533199195171,
      "grad_norm": 0.8635876774787903,
      "learning_rate": 0.00014090753596941342,
      "loss": 0.2697,
      "step": 14688
    },
    {
      "epoch": 29.55533199195171,
      "grad_norm": 0.8956014513969421,
      "learning_rate": 0.00014090351141965993,
      "loss": 0.2814,
      "step": 14689
    },
    {
      "epoch": 29.557344064386317,
      "grad_norm": 0.8098606467247009,
      "learning_rate": 0.00014089948686990645,
      "loss": 0.2498,
      "step": 14690
    },
    {
      "epoch": 29.559356136820927,
      "grad_norm": 0.8632982969284058,
      "learning_rate": 0.00014089546232015293,
      "loss": 0.2626,
      "step": 14691
    },
    {
      "epoch": 29.561368209255534,
      "grad_norm": 0.8020268082618713,
      "learning_rate": 0.00014089143777039944,
      "loss": 0.2691,
      "step": 14692
    },
    {
      "epoch": 29.56338028169014,
      "grad_norm": 0.8279693722724915,
      "learning_rate": 0.00014088741322064593,
      "loss": 0.2621,
      "step": 14693
    },
    {
      "epoch": 29.56539235412475,
      "grad_norm": 0.8544771075248718,
      "learning_rate": 0.00014088338867089247,
      "loss": 0.2555,
      "step": 14694
    },
    {
      "epoch": 29.567404426559357,
      "grad_norm": 0.8587117791175842,
      "learning_rate": 0.00014087936412113895,
      "loss": 0.2353,
      "step": 14695
    },
    {
      "epoch": 29.569416498993963,
      "grad_norm": 0.8393533229827881,
      "learning_rate": 0.00014087533957138546,
      "loss": 0.2611,
      "step": 14696
    },
    {
      "epoch": 29.571428571428573,
      "grad_norm": 0.831817090511322,
      "learning_rate": 0.00014087131502163195,
      "loss": 0.2614,
      "step": 14697
    },
    {
      "epoch": 29.57344064386318,
      "grad_norm": 0.8424140810966492,
      "learning_rate": 0.00014086729047187846,
      "loss": 0.2715,
      "step": 14698
    },
    {
      "epoch": 29.575452716297786,
      "grad_norm": 0.8319795727729797,
      "learning_rate": 0.00014086326592212497,
      "loss": 0.2629,
      "step": 14699
    },
    {
      "epoch": 29.577464788732396,
      "grad_norm": 0.7944474220275879,
      "learning_rate": 0.00014085924137237149,
      "loss": 0.2499,
      "step": 14700
    },
    {
      "epoch": 29.579476861167002,
      "grad_norm": 0.8286362290382385,
      "learning_rate": 0.00014085521682261797,
      "loss": 0.2651,
      "step": 14701
    },
    {
      "epoch": 29.58148893360161,
      "grad_norm": 0.8310215473175049,
      "learning_rate": 0.00014085119227286448,
      "loss": 0.2708,
      "step": 14702
    },
    {
      "epoch": 29.58350100603622,
      "grad_norm": 0.8943758010864258,
      "learning_rate": 0.00014084716772311097,
      "loss": 0.2556,
      "step": 14703
    },
    {
      "epoch": 29.585513078470825,
      "grad_norm": 0.8429585099220276,
      "learning_rate": 0.0001408431431733575,
      "loss": 0.2689,
      "step": 14704
    },
    {
      "epoch": 29.58752515090543,
      "grad_norm": 0.8646457195281982,
      "learning_rate": 0.000140839118623604,
      "loss": 0.2524,
      "step": 14705
    },
    {
      "epoch": 29.58953722334004,
      "grad_norm": 0.8423929214477539,
      "learning_rate": 0.0001408350940738505,
      "loss": 0.2386,
      "step": 14706
    },
    {
      "epoch": 29.591549295774648,
      "grad_norm": 0.87236088514328,
      "learning_rate": 0.000140831069524097,
      "loss": 0.2638,
      "step": 14707
    },
    {
      "epoch": 29.593561368209254,
      "grad_norm": 0.8361575603485107,
      "learning_rate": 0.0001408270449743435,
      "loss": 0.2346,
      "step": 14708
    },
    {
      "epoch": 29.595573440643864,
      "grad_norm": 0.836748480796814,
      "learning_rate": 0.00014082302042459001,
      "loss": 0.2487,
      "step": 14709
    },
    {
      "epoch": 29.59758551307847,
      "grad_norm": 0.8842620849609375,
      "learning_rate": 0.0001408189958748365,
      "loss": 0.259,
      "step": 14710
    },
    {
      "epoch": 29.599597585513077,
      "grad_norm": 0.857751190662384,
      "learning_rate": 0.000140814971325083,
      "loss": 0.2766,
      "step": 14711
    },
    {
      "epoch": 29.601609657947687,
      "grad_norm": 0.8812964558601379,
      "learning_rate": 0.00014081094677532952,
      "loss": 0.2511,
      "step": 14712
    },
    {
      "epoch": 29.603621730382294,
      "grad_norm": 0.8195757269859314,
      "learning_rate": 0.000140806922225576,
      "loss": 0.2658,
      "step": 14713
    },
    {
      "epoch": 29.6056338028169,
      "grad_norm": 0.8397015333175659,
      "learning_rate": 0.00014080289767582252,
      "loss": 0.2723,
      "step": 14714
    },
    {
      "epoch": 29.60764587525151,
      "grad_norm": 0.8652260303497314,
      "learning_rate": 0.00014079887312606903,
      "loss": 0.2656,
      "step": 14715
    },
    {
      "epoch": 29.609657947686117,
      "grad_norm": 0.8904139995574951,
      "learning_rate": 0.00014079484857631552,
      "loss": 0.2808,
      "step": 14716
    },
    {
      "epoch": 29.611670020120723,
      "grad_norm": 0.8369053602218628,
      "learning_rate": 0.00014079082402656203,
      "loss": 0.2736,
      "step": 14717
    },
    {
      "epoch": 29.613682092555333,
      "grad_norm": 0.8744050860404968,
      "learning_rate": 0.00014078679947680852,
      "loss": 0.259,
      "step": 14718
    },
    {
      "epoch": 29.61569416498994,
      "grad_norm": 0.8434670567512512,
      "learning_rate": 0.00014078277492705506,
      "loss": 0.2854,
      "step": 14719
    },
    {
      "epoch": 29.617706237424546,
      "grad_norm": 0.870074987411499,
      "learning_rate": 0.00014077875037730154,
      "loss": 0.2684,
      "step": 14720
    },
    {
      "epoch": 29.619718309859156,
      "grad_norm": 0.8595392107963562,
      "learning_rate": 0.00014077472582754805,
      "loss": 0.2539,
      "step": 14721
    },
    {
      "epoch": 29.621730382293762,
      "grad_norm": 0.8553277850151062,
      "learning_rate": 0.00014077070127779454,
      "loss": 0.2922,
      "step": 14722
    },
    {
      "epoch": 29.62374245472837,
      "grad_norm": 0.8753719329833984,
      "learning_rate": 0.00014076667672804105,
      "loss": 0.2638,
      "step": 14723
    },
    {
      "epoch": 29.62575452716298,
      "grad_norm": 0.8104599714279175,
      "learning_rate": 0.00014076265217828756,
      "loss": 0.2688,
      "step": 14724
    },
    {
      "epoch": 29.627766599597585,
      "grad_norm": 0.8546199798583984,
      "learning_rate": 0.00014075862762853407,
      "loss": 0.2669,
      "step": 14725
    },
    {
      "epoch": 29.62977867203219,
      "grad_norm": 0.8166238069534302,
      "learning_rate": 0.00014075460307878056,
      "loss": 0.2854,
      "step": 14726
    },
    {
      "epoch": 29.6317907444668,
      "grad_norm": 0.8380587697029114,
      "learning_rate": 0.00014075057852902707,
      "loss": 0.2687,
      "step": 14727
    },
    {
      "epoch": 29.633802816901408,
      "grad_norm": 0.8368755578994751,
      "learning_rate": 0.00014074655397927356,
      "loss": 0.2402,
      "step": 14728
    },
    {
      "epoch": 29.635814889336014,
      "grad_norm": 0.894082248210907,
      "learning_rate": 0.0001407425294295201,
      "loss": 0.2595,
      "step": 14729
    },
    {
      "epoch": 29.637826961770624,
      "grad_norm": 0.9219014048576355,
      "learning_rate": 0.00014073850487976658,
      "loss": 0.2738,
      "step": 14730
    },
    {
      "epoch": 29.63983903420523,
      "grad_norm": 0.8251115679740906,
      "learning_rate": 0.0001407344803300131,
      "loss": 0.2401,
      "step": 14731
    },
    {
      "epoch": 29.641851106639837,
      "grad_norm": 0.8727725744247437,
      "learning_rate": 0.00014073045578025958,
      "loss": 0.2754,
      "step": 14732
    },
    {
      "epoch": 29.643863179074447,
      "grad_norm": 0.8790705800056458,
      "learning_rate": 0.0001407264312305061,
      "loss": 0.2691,
      "step": 14733
    },
    {
      "epoch": 29.645875251509054,
      "grad_norm": 0.8972576260566711,
      "learning_rate": 0.0001407224066807526,
      "loss": 0.2828,
      "step": 14734
    },
    {
      "epoch": 29.647887323943664,
      "grad_norm": 0.8620893359184265,
      "learning_rate": 0.00014071838213099912,
      "loss": 0.2807,
      "step": 14735
    },
    {
      "epoch": 29.64989939637827,
      "grad_norm": 0.9179394245147705,
      "learning_rate": 0.0001407143575812456,
      "loss": 0.2865,
      "step": 14736
    },
    {
      "epoch": 29.651911468812877,
      "grad_norm": 0.8092600703239441,
      "learning_rate": 0.0001407103330314921,
      "loss": 0.2637,
      "step": 14737
    },
    {
      "epoch": 29.653923541247487,
      "grad_norm": 0.8197773694992065,
      "learning_rate": 0.0001407063084817386,
      "loss": 0.2761,
      "step": 14738
    },
    {
      "epoch": 29.655935613682093,
      "grad_norm": 0.7971811890602112,
      "learning_rate": 0.00014070228393198514,
      "loss": 0.2504,
      "step": 14739
    },
    {
      "epoch": 29.6579476861167,
      "grad_norm": 0.8501836657524109,
      "learning_rate": 0.00014069825938223162,
      "loss": 0.2677,
      "step": 14740
    },
    {
      "epoch": 29.65995975855131,
      "grad_norm": 0.8385975956916809,
      "learning_rate": 0.00014069423483247813,
      "loss": 0.2745,
      "step": 14741
    },
    {
      "epoch": 29.661971830985916,
      "grad_norm": 0.8111276626586914,
      "learning_rate": 0.00014069021028272462,
      "loss": 0.2566,
      "step": 14742
    },
    {
      "epoch": 29.663983903420522,
      "grad_norm": 0.8967136144638062,
      "learning_rate": 0.00014068618573297113,
      "loss": 0.2721,
      "step": 14743
    },
    {
      "epoch": 29.665995975855132,
      "grad_norm": 0.8935210108757019,
      "learning_rate": 0.00014068216118321764,
      "loss": 0.2849,
      "step": 14744
    },
    {
      "epoch": 29.66800804828974,
      "grad_norm": 0.8305362462997437,
      "learning_rate": 0.00014067813663346413,
      "loss": 0.2556,
      "step": 14745
    },
    {
      "epoch": 29.670020120724345,
      "grad_norm": 0.8187344670295715,
      "learning_rate": 0.00014067411208371064,
      "loss": 0.2651,
      "step": 14746
    },
    {
      "epoch": 29.672032193158955,
      "grad_norm": 0.8447856307029724,
      "learning_rate": 0.00014067008753395713,
      "loss": 0.2768,
      "step": 14747
    },
    {
      "epoch": 29.67404426559356,
      "grad_norm": 0.7961488366127014,
      "learning_rate": 0.00014066606298420364,
      "loss": 0.2692,
      "step": 14748
    },
    {
      "epoch": 29.676056338028168,
      "grad_norm": 0.8630984425544739,
      "learning_rate": 0.00014066203843445015,
      "loss": 0.2726,
      "step": 14749
    },
    {
      "epoch": 29.678068410462778,
      "grad_norm": 0.8729594945907593,
      "learning_rate": 0.00014065801388469666,
      "loss": 0.284,
      "step": 14750
    },
    {
      "epoch": 29.680080482897385,
      "grad_norm": 0.8939769864082336,
      "learning_rate": 0.00014065398933494315,
      "loss": 0.3036,
      "step": 14751
    },
    {
      "epoch": 29.68209255533199,
      "grad_norm": 0.8905017375946045,
      "learning_rate": 0.00014064996478518966,
      "loss": 0.2792,
      "step": 14752
    },
    {
      "epoch": 29.6841046277666,
      "grad_norm": 0.9108257293701172,
      "learning_rate": 0.00014064594023543615,
      "loss": 0.2702,
      "step": 14753
    },
    {
      "epoch": 29.686116700201207,
      "grad_norm": 0.8807234168052673,
      "learning_rate": 0.00014064191568568269,
      "loss": 0.2725,
      "step": 14754
    },
    {
      "epoch": 29.688128772635814,
      "grad_norm": 0.8647993803024292,
      "learning_rate": 0.00014063789113592917,
      "loss": 0.2709,
      "step": 14755
    },
    {
      "epoch": 29.690140845070424,
      "grad_norm": 0.7954453825950623,
      "learning_rate": 0.00014063386658617568,
      "loss": 0.2502,
      "step": 14756
    },
    {
      "epoch": 29.69215291750503,
      "grad_norm": 0.852907657623291,
      "learning_rate": 0.00014062984203642217,
      "loss": 0.2592,
      "step": 14757
    },
    {
      "epoch": 29.694164989939637,
      "grad_norm": 0.8909332752227783,
      "learning_rate": 0.00014062581748666868,
      "loss": 0.2714,
      "step": 14758
    },
    {
      "epoch": 29.696177062374247,
      "grad_norm": 0.8530853390693665,
      "learning_rate": 0.0001406217929369152,
      "loss": 0.2734,
      "step": 14759
    },
    {
      "epoch": 29.698189134808853,
      "grad_norm": 0.912006139755249,
      "learning_rate": 0.0001406177683871617,
      "loss": 0.2998,
      "step": 14760
    },
    {
      "epoch": 29.70020120724346,
      "grad_norm": 0.8733096718788147,
      "learning_rate": 0.0001406137438374082,
      "loss": 0.2649,
      "step": 14761
    },
    {
      "epoch": 29.70221327967807,
      "grad_norm": 0.880294919013977,
      "learning_rate": 0.0001406097192876547,
      "loss": 0.265,
      "step": 14762
    },
    {
      "epoch": 29.704225352112676,
      "grad_norm": 0.9012339115142822,
      "learning_rate": 0.0001406056947379012,
      "loss": 0.2691,
      "step": 14763
    },
    {
      "epoch": 29.706237424547282,
      "grad_norm": 0.8881610631942749,
      "learning_rate": 0.00014060167018814773,
      "loss": 0.2713,
      "step": 14764
    },
    {
      "epoch": 29.708249496981892,
      "grad_norm": 0.8609813451766968,
      "learning_rate": 0.0001405976456383942,
      "loss": 0.2812,
      "step": 14765
    },
    {
      "epoch": 29.7102615694165,
      "grad_norm": 0.8669432401657104,
      "learning_rate": 0.00014059362108864072,
      "loss": 0.2556,
      "step": 14766
    },
    {
      "epoch": 29.712273641851105,
      "grad_norm": 0.8613907098770142,
      "learning_rate": 0.0001405895965388872,
      "loss": 0.2649,
      "step": 14767
    },
    {
      "epoch": 29.714285714285715,
      "grad_norm": 0.8433294296264648,
      "learning_rate": 0.00014058557198913372,
      "loss": 0.2782,
      "step": 14768
    },
    {
      "epoch": 29.71629778672032,
      "grad_norm": 0.8128905296325684,
      "learning_rate": 0.00014058154743938023,
      "loss": 0.2569,
      "step": 14769
    },
    {
      "epoch": 29.718309859154928,
      "grad_norm": 0.8448299765586853,
      "learning_rate": 0.00014057752288962674,
      "loss": 0.2527,
      "step": 14770
    },
    {
      "epoch": 29.720321931589538,
      "grad_norm": 0.8801876902580261,
      "learning_rate": 0.00014057349833987323,
      "loss": 0.2734,
      "step": 14771
    },
    {
      "epoch": 29.722334004024145,
      "grad_norm": 0.8382781744003296,
      "learning_rate": 0.00014056947379011974,
      "loss": 0.2702,
      "step": 14772
    },
    {
      "epoch": 29.72434607645875,
      "grad_norm": 0.8788579702377319,
      "learning_rate": 0.00014056544924036623,
      "loss": 0.2865,
      "step": 14773
    },
    {
      "epoch": 29.72635814889336,
      "grad_norm": 0.8312315344810486,
      "learning_rate": 0.00014056142469061274,
      "loss": 0.2782,
      "step": 14774
    },
    {
      "epoch": 29.728370221327967,
      "grad_norm": 0.8908981680870056,
      "learning_rate": 0.00014055740014085925,
      "loss": 0.2779,
      "step": 14775
    },
    {
      "epoch": 29.730382293762574,
      "grad_norm": 0.8258574604988098,
      "learning_rate": 0.00014055337559110576,
      "loss": 0.2615,
      "step": 14776
    },
    {
      "epoch": 29.732394366197184,
      "grad_norm": 0.8329684138298035,
      "learning_rate": 0.00014054935104135225,
      "loss": 0.2582,
      "step": 14777
    },
    {
      "epoch": 29.73440643863179,
      "grad_norm": 0.8603456020355225,
      "learning_rate": 0.00014054532649159876,
      "loss": 0.2767,
      "step": 14778
    },
    {
      "epoch": 29.736418511066397,
      "grad_norm": 0.8645063638687134,
      "learning_rate": 0.00014054130194184527,
      "loss": 0.2798,
      "step": 14779
    },
    {
      "epoch": 29.738430583501007,
      "grad_norm": 0.8339186906814575,
      "learning_rate": 0.00014053727739209176,
      "loss": 0.2747,
      "step": 14780
    },
    {
      "epoch": 29.740442655935613,
      "grad_norm": 0.8353267312049866,
      "learning_rate": 0.00014053325284233827,
      "loss": 0.2752,
      "step": 14781
    },
    {
      "epoch": 29.74245472837022,
      "grad_norm": 0.8297129273414612,
      "learning_rate": 0.00014052922829258476,
      "loss": 0.2487,
      "step": 14782
    },
    {
      "epoch": 29.74446680080483,
      "grad_norm": 0.8928718566894531,
      "learning_rate": 0.00014052520374283127,
      "loss": 0.2727,
      "step": 14783
    },
    {
      "epoch": 29.746478873239436,
      "grad_norm": 0.9021010398864746,
      "learning_rate": 0.00014052117919307778,
      "loss": 0.2649,
      "step": 14784
    },
    {
      "epoch": 29.748490945674043,
      "grad_norm": 0.8494195938110352,
      "learning_rate": 0.0001405171546433243,
      "loss": 0.2694,
      "step": 14785
    },
    {
      "epoch": 29.750503018108652,
      "grad_norm": 0.8671942353248596,
      "learning_rate": 0.00014051313009357078,
      "loss": 0.2759,
      "step": 14786
    },
    {
      "epoch": 29.75251509054326,
      "grad_norm": 0.9155465364456177,
      "learning_rate": 0.0001405091055438173,
      "loss": 0.2826,
      "step": 14787
    },
    {
      "epoch": 29.754527162977865,
      "grad_norm": 0.8607343435287476,
      "learning_rate": 0.00014050508099406378,
      "loss": 0.2666,
      "step": 14788
    },
    {
      "epoch": 29.756539235412475,
      "grad_norm": 0.8123425841331482,
      "learning_rate": 0.00014050105644431031,
      "loss": 0.2653,
      "step": 14789
    },
    {
      "epoch": 29.758551307847082,
      "grad_norm": 0.8619193434715271,
      "learning_rate": 0.0001404970318945568,
      "loss": 0.2824,
      "step": 14790
    },
    {
      "epoch": 29.760563380281692,
      "grad_norm": 0.8496956825256348,
      "learning_rate": 0.0001404930073448033,
      "loss": 0.2773,
      "step": 14791
    },
    {
      "epoch": 29.7625754527163,
      "grad_norm": 0.853654146194458,
      "learning_rate": 0.0001404889827950498,
      "loss": 0.2881,
      "step": 14792
    },
    {
      "epoch": 29.764587525150905,
      "grad_norm": 0.8726080656051636,
      "learning_rate": 0.0001404849582452963,
      "loss": 0.2766,
      "step": 14793
    },
    {
      "epoch": 29.766599597585515,
      "grad_norm": 0.8685818910598755,
      "learning_rate": 0.00014048093369554282,
      "loss": 0.2874,
      "step": 14794
    },
    {
      "epoch": 29.76861167002012,
      "grad_norm": 0.8301870822906494,
      "learning_rate": 0.00014047690914578933,
      "loss": 0.2763,
      "step": 14795
    },
    {
      "epoch": 29.770623742454728,
      "grad_norm": 0.8677110075950623,
      "learning_rate": 0.00014047288459603582,
      "loss": 0.2586,
      "step": 14796
    },
    {
      "epoch": 29.772635814889338,
      "grad_norm": 0.824508786201477,
      "learning_rate": 0.00014046886004628233,
      "loss": 0.28,
      "step": 14797
    },
    {
      "epoch": 29.774647887323944,
      "grad_norm": 0.852928102016449,
      "learning_rate": 0.00014046483549652882,
      "loss": 0.2754,
      "step": 14798
    },
    {
      "epoch": 29.77665995975855,
      "grad_norm": 0.9069353342056274,
      "learning_rate": 0.00014046081094677536,
      "loss": 0.2511,
      "step": 14799
    },
    {
      "epoch": 29.77867203219316,
      "grad_norm": 0.8112273812294006,
      "learning_rate": 0.00014045678639702184,
      "loss": 0.2624,
      "step": 14800
    },
    {
      "epoch": 29.780684104627767,
      "grad_norm": 0.877381443977356,
      "learning_rate": 0.00014045276184726835,
      "loss": 0.2651,
      "step": 14801
    },
    {
      "epoch": 29.782696177062373,
      "grad_norm": 0.893500566482544,
      "learning_rate": 0.00014044873729751484,
      "loss": 0.2883,
      "step": 14802
    },
    {
      "epoch": 29.784708249496983,
      "grad_norm": 0.9539921283721924,
      "learning_rate": 0.00014044471274776135,
      "loss": 0.2848,
      "step": 14803
    },
    {
      "epoch": 29.78672032193159,
      "grad_norm": 0.8169414401054382,
      "learning_rate": 0.00014044068819800786,
      "loss": 0.2681,
      "step": 14804
    },
    {
      "epoch": 29.788732394366196,
      "grad_norm": 0.8477129340171814,
      "learning_rate": 0.00014043666364825437,
      "loss": 0.2649,
      "step": 14805
    },
    {
      "epoch": 29.790744466800806,
      "grad_norm": 0.8764018416404724,
      "learning_rate": 0.00014043263909850086,
      "loss": 0.2652,
      "step": 14806
    },
    {
      "epoch": 29.792756539235413,
      "grad_norm": 0.9459212422370911,
      "learning_rate": 0.00014042861454874737,
      "loss": 0.2988,
      "step": 14807
    },
    {
      "epoch": 29.79476861167002,
      "grad_norm": 0.8861428499221802,
      "learning_rate": 0.00014042458999899386,
      "loss": 0.278,
      "step": 14808
    },
    {
      "epoch": 29.79678068410463,
      "grad_norm": 0.9144459962844849,
      "learning_rate": 0.00014042056544924037,
      "loss": 0.2712,
      "step": 14809
    },
    {
      "epoch": 29.798792756539235,
      "grad_norm": 0.8715929985046387,
      "learning_rate": 0.00014041654089948688,
      "loss": 0.2798,
      "step": 14810
    },
    {
      "epoch": 29.800804828973842,
      "grad_norm": 0.8808457255363464,
      "learning_rate": 0.0001404125163497334,
      "loss": 0.3014,
      "step": 14811
    },
    {
      "epoch": 29.802816901408452,
      "grad_norm": 0.8648571372032166,
      "learning_rate": 0.00014040849179997988,
      "loss": 0.2933,
      "step": 14812
    },
    {
      "epoch": 29.80482897384306,
      "grad_norm": 0.895404577255249,
      "learning_rate": 0.0001404044672502264,
      "loss": 0.2693,
      "step": 14813
    },
    {
      "epoch": 29.806841046277665,
      "grad_norm": 0.7997094988822937,
      "learning_rate": 0.0001404004427004729,
      "loss": 0.2647,
      "step": 14814
    },
    {
      "epoch": 29.808853118712275,
      "grad_norm": 0.869179368019104,
      "learning_rate": 0.0001403964181507194,
      "loss": 0.2876,
      "step": 14815
    },
    {
      "epoch": 29.81086519114688,
      "grad_norm": 0.9737088084220886,
      "learning_rate": 0.0001403923936009659,
      "loss": 0.2792,
      "step": 14816
    },
    {
      "epoch": 29.812877263581488,
      "grad_norm": 0.8617150783538818,
      "learning_rate": 0.00014038836905121239,
      "loss": 0.2689,
      "step": 14817
    },
    {
      "epoch": 29.814889336016098,
      "grad_norm": 0.8701072931289673,
      "learning_rate": 0.0001403843445014589,
      "loss": 0.2815,
      "step": 14818
    },
    {
      "epoch": 29.816901408450704,
      "grad_norm": 0.8691920042037964,
      "learning_rate": 0.0001403803199517054,
      "loss": 0.288,
      "step": 14819
    },
    {
      "epoch": 29.81891348088531,
      "grad_norm": 0.8590965270996094,
      "learning_rate": 0.00014037629540195192,
      "loss": 0.2676,
      "step": 14820
    },
    {
      "epoch": 29.82092555331992,
      "grad_norm": 0.8328593373298645,
      "learning_rate": 0.0001403722708521984,
      "loss": 0.2669,
      "step": 14821
    },
    {
      "epoch": 29.822937625754527,
      "grad_norm": 0.8903605937957764,
      "learning_rate": 0.00014036824630244492,
      "loss": 0.2812,
      "step": 14822
    },
    {
      "epoch": 29.824949698189133,
      "grad_norm": 0.8252047896385193,
      "learning_rate": 0.0001403642217526914,
      "loss": 0.2712,
      "step": 14823
    },
    {
      "epoch": 29.826961770623743,
      "grad_norm": 0.9317743182182312,
      "learning_rate": 0.00014036019720293794,
      "loss": 0.2866,
      "step": 14824
    },
    {
      "epoch": 29.82897384305835,
      "grad_norm": 0.8718292713165283,
      "learning_rate": 0.00014035617265318443,
      "loss": 0.2827,
      "step": 14825
    },
    {
      "epoch": 29.830985915492956,
      "grad_norm": 0.9042980074882507,
      "learning_rate": 0.00014035214810343094,
      "loss": 0.2818,
      "step": 14826
    },
    {
      "epoch": 29.832997987927566,
      "grad_norm": 0.9172772169113159,
      "learning_rate": 0.00014034812355367743,
      "loss": 0.2709,
      "step": 14827
    },
    {
      "epoch": 29.835010060362173,
      "grad_norm": 0.8991601467132568,
      "learning_rate": 0.00014034409900392394,
      "loss": 0.2907,
      "step": 14828
    },
    {
      "epoch": 29.83702213279678,
      "grad_norm": 0.8602042198181152,
      "learning_rate": 0.00014034007445417045,
      "loss": 0.2711,
      "step": 14829
    },
    {
      "epoch": 29.83903420523139,
      "grad_norm": 0.859747588634491,
      "learning_rate": 0.00014033604990441696,
      "loss": 0.2735,
      "step": 14830
    },
    {
      "epoch": 29.841046277665995,
      "grad_norm": 0.8339147567749023,
      "learning_rate": 0.00014033202535466345,
      "loss": 0.2697,
      "step": 14831
    },
    {
      "epoch": 29.843058350100602,
      "grad_norm": 0.8707596659660339,
      "learning_rate": 0.00014032800080490996,
      "loss": 0.2746,
      "step": 14832
    },
    {
      "epoch": 29.845070422535212,
      "grad_norm": 0.8644594550132751,
      "learning_rate": 0.00014032397625515645,
      "loss": 0.2756,
      "step": 14833
    },
    {
      "epoch": 29.84708249496982,
      "grad_norm": 0.9190695285797119,
      "learning_rate": 0.00014031995170540298,
      "loss": 0.2989,
      "step": 14834
    },
    {
      "epoch": 29.84909456740443,
      "grad_norm": 0.8883953094482422,
      "learning_rate": 0.00014031592715564947,
      "loss": 0.2711,
      "step": 14835
    },
    {
      "epoch": 29.851106639839035,
      "grad_norm": 0.8519483804702759,
      "learning_rate": 0.00014031190260589598,
      "loss": 0.2507,
      "step": 14836
    },
    {
      "epoch": 29.85311871227364,
      "grad_norm": 0.8849740624427795,
      "learning_rate": 0.00014030787805614247,
      "loss": 0.2705,
      "step": 14837
    },
    {
      "epoch": 29.85513078470825,
      "grad_norm": 0.9185163974761963,
      "learning_rate": 0.00014030385350638898,
      "loss": 0.2776,
      "step": 14838
    },
    {
      "epoch": 29.857142857142858,
      "grad_norm": 0.8666282892227173,
      "learning_rate": 0.0001402998289566355,
      "loss": 0.2717,
      "step": 14839
    },
    {
      "epoch": 29.859154929577464,
      "grad_norm": 0.8644740581512451,
      "learning_rate": 0.000140295804406882,
      "loss": 0.2754,
      "step": 14840
    },
    {
      "epoch": 29.861167002012074,
      "grad_norm": 0.8788278698921204,
      "learning_rate": 0.0001402917798571285,
      "loss": 0.2872,
      "step": 14841
    },
    {
      "epoch": 29.86317907444668,
      "grad_norm": 0.8493941426277161,
      "learning_rate": 0.000140287755307375,
      "loss": 0.2784,
      "step": 14842
    },
    {
      "epoch": 29.865191146881287,
      "grad_norm": 0.8265724778175354,
      "learning_rate": 0.00014028373075762149,
      "loss": 0.262,
      "step": 14843
    },
    {
      "epoch": 29.867203219315897,
      "grad_norm": 0.9177117943763733,
      "learning_rate": 0.000140279706207868,
      "loss": 0.2929,
      "step": 14844
    },
    {
      "epoch": 29.869215291750503,
      "grad_norm": 0.8913309574127197,
      "learning_rate": 0.0001402756816581145,
      "loss": 0.2825,
      "step": 14845
    },
    {
      "epoch": 29.87122736418511,
      "grad_norm": 0.8428804278373718,
      "learning_rate": 0.00014027165710836102,
      "loss": 0.2592,
      "step": 14846
    },
    {
      "epoch": 29.87323943661972,
      "grad_norm": 0.8201872110366821,
      "learning_rate": 0.0001402676325586075,
      "loss": 0.2437,
      "step": 14847
    },
    {
      "epoch": 29.875251509054326,
      "grad_norm": 0.8698937296867371,
      "learning_rate": 0.00014026360800885402,
      "loss": 0.2546,
      "step": 14848
    },
    {
      "epoch": 29.877263581488933,
      "grad_norm": 0.8552132844924927,
      "learning_rate": 0.00014025958345910053,
      "loss": 0.2896,
      "step": 14849
    },
    {
      "epoch": 29.879275653923543,
      "grad_norm": 0.8657070398330688,
      "learning_rate": 0.00014025555890934702,
      "loss": 0.2624,
      "step": 14850
    },
    {
      "epoch": 29.88128772635815,
      "grad_norm": 0.8461324572563171,
      "learning_rate": 0.00014025153435959353,
      "loss": 0.2973,
      "step": 14851
    },
    {
      "epoch": 29.883299798792756,
      "grad_norm": 0.8490885496139526,
      "learning_rate": 0.00014024750980984001,
      "loss": 0.2838,
      "step": 14852
    },
    {
      "epoch": 29.885311871227366,
      "grad_norm": 0.8803766965866089,
      "learning_rate": 0.00014024348526008653,
      "loss": 0.2843,
      "step": 14853
    },
    {
      "epoch": 29.887323943661972,
      "grad_norm": 0.8786994218826294,
      "learning_rate": 0.00014023946071033304,
      "loss": 0.279,
      "step": 14854
    },
    {
      "epoch": 29.88933601609658,
      "grad_norm": 0.9175035953521729,
      "learning_rate": 0.00014023543616057955,
      "loss": 0.2653,
      "step": 14855
    },
    {
      "epoch": 29.89134808853119,
      "grad_norm": 0.8949468731880188,
      "learning_rate": 0.00014023141161082604,
      "loss": 0.2668,
      "step": 14856
    },
    {
      "epoch": 29.893360160965795,
      "grad_norm": 0.872570812702179,
      "learning_rate": 0.00014022738706107255,
      "loss": 0.2781,
      "step": 14857
    },
    {
      "epoch": 29.8953722334004,
      "grad_norm": 0.8587127923965454,
      "learning_rate": 0.00014022336251131903,
      "loss": 0.2533,
      "step": 14858
    },
    {
      "epoch": 29.89738430583501,
      "grad_norm": 0.9060816168785095,
      "learning_rate": 0.00014021933796156557,
      "loss": 0.2868,
      "step": 14859
    },
    {
      "epoch": 29.899396378269618,
      "grad_norm": 0.8343018293380737,
      "learning_rate": 0.00014021531341181206,
      "loss": 0.2567,
      "step": 14860
    },
    {
      "epoch": 29.901408450704224,
      "grad_norm": 0.8529779314994812,
      "learning_rate": 0.00014021128886205857,
      "loss": 0.2818,
      "step": 14861
    },
    {
      "epoch": 29.903420523138834,
      "grad_norm": 0.9002594947814941,
      "learning_rate": 0.00014020726431230506,
      "loss": 0.298,
      "step": 14862
    },
    {
      "epoch": 29.90543259557344,
      "grad_norm": 0.8682353496551514,
      "learning_rate": 0.00014020323976255157,
      "loss": 0.2784,
      "step": 14863
    },
    {
      "epoch": 29.907444668008047,
      "grad_norm": 0.8727009892463684,
      "learning_rate": 0.00014019921521279808,
      "loss": 0.2785,
      "step": 14864
    },
    {
      "epoch": 29.909456740442657,
      "grad_norm": 0.8144893050193787,
      "learning_rate": 0.0001401951906630446,
      "loss": 0.2751,
      "step": 14865
    },
    {
      "epoch": 29.911468812877263,
      "grad_norm": 0.8426839709281921,
      "learning_rate": 0.00014019116611329108,
      "loss": 0.2854,
      "step": 14866
    },
    {
      "epoch": 29.91348088531187,
      "grad_norm": 0.8272255659103394,
      "learning_rate": 0.0001401871415635376,
      "loss": 0.2752,
      "step": 14867
    },
    {
      "epoch": 29.91549295774648,
      "grad_norm": 0.8498010635375977,
      "learning_rate": 0.00014018311701378407,
      "loss": 0.2852,
      "step": 14868
    },
    {
      "epoch": 29.917505030181086,
      "grad_norm": 0.927800714969635,
      "learning_rate": 0.00014017909246403061,
      "loss": 0.2693,
      "step": 14869
    },
    {
      "epoch": 29.919517102615693,
      "grad_norm": 0.9495332837104797,
      "learning_rate": 0.0001401750679142771,
      "loss": 0.2923,
      "step": 14870
    },
    {
      "epoch": 29.921529175050303,
      "grad_norm": 0.8885304927825928,
      "learning_rate": 0.0001401710433645236,
      "loss": 0.2778,
      "step": 14871
    },
    {
      "epoch": 29.92354124748491,
      "grad_norm": 0.8689802289009094,
      "learning_rate": 0.0001401670188147701,
      "loss": 0.2872,
      "step": 14872
    },
    {
      "epoch": 29.925553319919516,
      "grad_norm": 0.8850389719009399,
      "learning_rate": 0.0001401629942650166,
      "loss": 0.3067,
      "step": 14873
    },
    {
      "epoch": 29.927565392354126,
      "grad_norm": 0.8604443669319153,
      "learning_rate": 0.00014015896971526312,
      "loss": 0.2918,
      "step": 14874
    },
    {
      "epoch": 29.929577464788732,
      "grad_norm": 0.8527631759643555,
      "learning_rate": 0.00014015494516550963,
      "loss": 0.2801,
      "step": 14875
    },
    {
      "epoch": 29.93158953722334,
      "grad_norm": 0.8680406212806702,
      "learning_rate": 0.00014015092061575612,
      "loss": 0.2927,
      "step": 14876
    },
    {
      "epoch": 29.93360160965795,
      "grad_norm": 0.8513140082359314,
      "learning_rate": 0.00014014689606600263,
      "loss": 0.2759,
      "step": 14877
    },
    {
      "epoch": 29.935613682092555,
      "grad_norm": 0.821980893611908,
      "learning_rate": 0.00014014287151624912,
      "loss": 0.27,
      "step": 14878
    },
    {
      "epoch": 29.93762575452716,
      "grad_norm": 0.8698682188987732,
      "learning_rate": 0.00014013884696649563,
      "loss": 0.2816,
      "step": 14879
    },
    {
      "epoch": 29.93963782696177,
      "grad_norm": 0.8464955687522888,
      "learning_rate": 0.00014013482241674214,
      "loss": 0.2686,
      "step": 14880
    },
    {
      "epoch": 29.941649899396378,
      "grad_norm": 0.8847566843032837,
      "learning_rate": 0.00014013079786698865,
      "loss": 0.291,
      "step": 14881
    },
    {
      "epoch": 29.943661971830984,
      "grad_norm": 0.8662945628166199,
      "learning_rate": 0.00014012677331723514,
      "loss": 0.2734,
      "step": 14882
    },
    {
      "epoch": 29.945674044265594,
      "grad_norm": 0.8692779541015625,
      "learning_rate": 0.00014012274876748165,
      "loss": 0.2734,
      "step": 14883
    },
    {
      "epoch": 29.9476861167002,
      "grad_norm": 0.8203120827674866,
      "learning_rate": 0.00014011872421772816,
      "loss": 0.2449,
      "step": 14884
    },
    {
      "epoch": 29.949698189134807,
      "grad_norm": 0.8954872488975525,
      "learning_rate": 0.00014011469966797465,
      "loss": 0.3244,
      "step": 14885
    },
    {
      "epoch": 29.951710261569417,
      "grad_norm": 0.8199930787086487,
      "learning_rate": 0.00014011067511822116,
      "loss": 0.2743,
      "step": 14886
    },
    {
      "epoch": 29.953722334004024,
      "grad_norm": 0.830177366733551,
      "learning_rate": 0.00014010665056846764,
      "loss": 0.2455,
      "step": 14887
    },
    {
      "epoch": 29.955734406438633,
      "grad_norm": 0.8641648888587952,
      "learning_rate": 0.00014010262601871416,
      "loss": 0.272,
      "step": 14888
    },
    {
      "epoch": 29.95774647887324,
      "grad_norm": 0.8535212278366089,
      "learning_rate": 0.00014009860146896067,
      "loss": 0.282,
      "step": 14889
    },
    {
      "epoch": 29.959758551307846,
      "grad_norm": 0.8983683586120605,
      "learning_rate": 0.00014009457691920718,
      "loss": 0.2795,
      "step": 14890
    },
    {
      "epoch": 29.961770623742456,
      "grad_norm": 0.9078501462936401,
      "learning_rate": 0.00014009055236945367,
      "loss": 0.2971,
      "step": 14891
    },
    {
      "epoch": 29.963782696177063,
      "grad_norm": 0.8577735424041748,
      "learning_rate": 0.00014008652781970018,
      "loss": 0.2889,
      "step": 14892
    },
    {
      "epoch": 29.96579476861167,
      "grad_norm": 0.8435924053192139,
      "learning_rate": 0.00014008250326994666,
      "loss": 0.2893,
      "step": 14893
    },
    {
      "epoch": 29.96780684104628,
      "grad_norm": 0.9369690418243408,
      "learning_rate": 0.0001400784787201932,
      "loss": 0.2798,
      "step": 14894
    },
    {
      "epoch": 29.969818913480886,
      "grad_norm": 0.9060451984405518,
      "learning_rate": 0.0001400744541704397,
      "loss": 0.3176,
      "step": 14895
    },
    {
      "epoch": 29.971830985915492,
      "grad_norm": 0.8789569139480591,
      "learning_rate": 0.0001400704296206862,
      "loss": 0.2998,
      "step": 14896
    },
    {
      "epoch": 29.973843058350102,
      "grad_norm": 0.8402811884880066,
      "learning_rate": 0.00014006640507093269,
      "loss": 0.2602,
      "step": 14897
    },
    {
      "epoch": 29.97585513078471,
      "grad_norm": 0.8745757341384888,
      "learning_rate": 0.0001400623805211792,
      "loss": 0.273,
      "step": 14898
    },
    {
      "epoch": 29.977867203219315,
      "grad_norm": 0.8320832848548889,
      "learning_rate": 0.0001400583559714257,
      "loss": 0.2707,
      "step": 14899
    },
    {
      "epoch": 29.979879275653925,
      "grad_norm": 0.8657708168029785,
      "learning_rate": 0.00014005433142167222,
      "loss": 0.2902,
      "step": 14900
    },
    {
      "epoch": 29.98189134808853,
      "grad_norm": 0.9022613763809204,
      "learning_rate": 0.0001400503068719187,
      "loss": 0.2756,
      "step": 14901
    },
    {
      "epoch": 29.983903420523138,
      "grad_norm": 0.8436570763587952,
      "learning_rate": 0.00014004628232216522,
      "loss": 0.2882,
      "step": 14902
    },
    {
      "epoch": 29.985915492957748,
      "grad_norm": 0.8291561007499695,
      "learning_rate": 0.0001400422577724117,
      "loss": 0.2708,
      "step": 14903
    },
    {
      "epoch": 29.987927565392354,
      "grad_norm": 0.793157160282135,
      "learning_rate": 0.00014003823322265824,
      "loss": 0.2605,
      "step": 14904
    },
    {
      "epoch": 29.98993963782696,
      "grad_norm": 0.8213186264038086,
      "learning_rate": 0.00014003420867290473,
      "loss": 0.2873,
      "step": 14905
    },
    {
      "epoch": 29.99195171026157,
      "grad_norm": 0.8983868956565857,
      "learning_rate": 0.00014003018412315124,
      "loss": 0.2788,
      "step": 14906
    },
    {
      "epoch": 29.993963782696177,
      "grad_norm": 0.9000378847122192,
      "learning_rate": 0.00014002615957339773,
      "loss": 0.2741,
      "step": 14907
    },
    {
      "epoch": 29.995975855130784,
      "grad_norm": 0.8857243061065674,
      "learning_rate": 0.00014002213502364424,
      "loss": 0.2878,
      "step": 14908
    },
    {
      "epoch": 29.997987927565394,
      "grad_norm": 0.826000452041626,
      "learning_rate": 0.00014001811047389075,
      "loss": 0.2672,
      "step": 14909
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.8762754797935486,
      "learning_rate": 0.00014001408592413726,
      "loss": 0.2682,
      "step": 14910
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.2909533977508545,
      "eval_runtime": 49.8198,
      "eval_samples_per_second": 19.912,
      "eval_steps_per_second": 2.489,
      "step": 14910
    },
    {
      "epoch": 30.002012072434606,
      "grad_norm": 0.8023247718811035,
      "learning_rate": 0.00014001006137438375,
      "loss": 0.2326,
      "step": 14911
    },
    {
      "epoch": 30.004024144869216,
      "grad_norm": 0.7241729497909546,
      "learning_rate": 0.00014000603682463026,
      "loss": 0.215,
      "step": 14912
    },
    {
      "epoch": 30.006036217303823,
      "grad_norm": 0.8084142208099365,
      "learning_rate": 0.00014000201227487675,
      "loss": 0.2381,
      "step": 14913
    },
    {
      "epoch": 30.00804828973843,
      "grad_norm": 0.8352400660514832,
      "learning_rate": 0.00013999798772512326,
      "loss": 0.24,
      "step": 14914
    },
    {
      "epoch": 30.01006036217304,
      "grad_norm": 0.8143430352210999,
      "learning_rate": 0.00013999396317536977,
      "loss": 0.2178,
      "step": 14915
    },
    {
      "epoch": 30.012072434607646,
      "grad_norm": 0.7668663263320923,
      "learning_rate": 0.00013998993862561625,
      "loss": 0.2058,
      "step": 14916
    },
    {
      "epoch": 30.014084507042252,
      "grad_norm": 0.8653715252876282,
      "learning_rate": 0.00013998591407586277,
      "loss": 0.2395,
      "step": 14917
    },
    {
      "epoch": 30.016096579476862,
      "grad_norm": 0.8117532134056091,
      "learning_rate": 0.00013998188952610928,
      "loss": 0.218,
      "step": 14918
    },
    {
      "epoch": 30.01810865191147,
      "grad_norm": 0.7706761360168457,
      "learning_rate": 0.00013997786497635576,
      "loss": 0.208,
      "step": 14919
    },
    {
      "epoch": 30.020120724346075,
      "grad_norm": 0.8173351883888245,
      "learning_rate": 0.00013997384042660228,
      "loss": 0.2309,
      "step": 14920
    },
    {
      "epoch": 30.022132796780685,
      "grad_norm": 0.7686948776245117,
      "learning_rate": 0.0001399698158768488,
      "loss": 0.2002,
      "step": 14921
    },
    {
      "epoch": 30.02414486921529,
      "grad_norm": 0.8085407018661499,
      "learning_rate": 0.00013996579132709527,
      "loss": 0.2388,
      "step": 14922
    },
    {
      "epoch": 30.026156941649898,
      "grad_norm": 0.9358965754508972,
      "learning_rate": 0.00013996176677734179,
      "loss": 0.2493,
      "step": 14923
    },
    {
      "epoch": 30.028169014084508,
      "grad_norm": 0.8083407878875732,
      "learning_rate": 0.00013995774222758827,
      "loss": 0.2224,
      "step": 14924
    },
    {
      "epoch": 30.030181086519114,
      "grad_norm": 0.7982849478721619,
      "learning_rate": 0.0001399537176778348,
      "loss": 0.2355,
      "step": 14925
    },
    {
      "epoch": 30.03219315895372,
      "grad_norm": 0.8079350590705872,
      "learning_rate": 0.0001399496931280813,
      "loss": 0.2178,
      "step": 14926
    },
    {
      "epoch": 30.03420523138833,
      "grad_norm": 0.7914530038833618,
      "learning_rate": 0.0001399456685783278,
      "loss": 0.2221,
      "step": 14927
    },
    {
      "epoch": 30.036217303822937,
      "grad_norm": 0.8384020924568176,
      "learning_rate": 0.0001399416440285743,
      "loss": 0.2432,
      "step": 14928
    },
    {
      "epoch": 30.038229376257544,
      "grad_norm": 0.7510201334953308,
      "learning_rate": 0.0001399376194788208,
      "loss": 0.2169,
      "step": 14929
    },
    {
      "epoch": 30.040241448692154,
      "grad_norm": 0.7811651825904846,
      "learning_rate": 0.00013993359492906732,
      "loss": 0.247,
      "step": 14930
    },
    {
      "epoch": 30.04225352112676,
      "grad_norm": 0.7207983136177063,
      "learning_rate": 0.00013992957037931383,
      "loss": 0.2168,
      "step": 14931
    },
    {
      "epoch": 30.044265593561367,
      "grad_norm": 0.7075991034507751,
      "learning_rate": 0.00013992554582956031,
      "loss": 0.2102,
      "step": 14932
    },
    {
      "epoch": 30.046277665995976,
      "grad_norm": 0.7227444052696228,
      "learning_rate": 0.00013992152127980683,
      "loss": 0.2072,
      "step": 14933
    },
    {
      "epoch": 30.048289738430583,
      "grad_norm": 0.8089320659637451,
      "learning_rate": 0.0001399174967300533,
      "loss": 0.2196,
      "step": 14934
    },
    {
      "epoch": 30.050301810865193,
      "grad_norm": 0.8170385360717773,
      "learning_rate": 0.00013991347218029985,
      "loss": 0.2381,
      "step": 14935
    },
    {
      "epoch": 30.0523138832998,
      "grad_norm": 0.787202000617981,
      "learning_rate": 0.00013990944763054634,
      "loss": 0.2134,
      "step": 14936
    },
    {
      "epoch": 30.054325955734406,
      "grad_norm": 0.7741368412971497,
      "learning_rate": 0.00013990542308079285,
      "loss": 0.1928,
      "step": 14937
    },
    {
      "epoch": 30.056338028169016,
      "grad_norm": 0.7729910612106323,
      "learning_rate": 0.00013990139853103933,
      "loss": 0.2176,
      "step": 14938
    },
    {
      "epoch": 30.058350100603622,
      "grad_norm": 0.777759850025177,
      "learning_rate": 0.00013989737398128585,
      "loss": 0.2152,
      "step": 14939
    },
    {
      "epoch": 30.06036217303823,
      "grad_norm": 0.7818411588668823,
      "learning_rate": 0.00013989334943153236,
      "loss": 0.2287,
      "step": 14940
    },
    {
      "epoch": 30.06237424547284,
      "grad_norm": 0.7464687824249268,
      "learning_rate": 0.00013988932488177887,
      "loss": 0.2019,
      "step": 14941
    },
    {
      "epoch": 30.064386317907445,
      "grad_norm": 0.762890100479126,
      "learning_rate": 0.00013988530033202536,
      "loss": 0.2169,
      "step": 14942
    },
    {
      "epoch": 30.06639839034205,
      "grad_norm": 0.8016976714134216,
      "learning_rate": 0.00013988127578227187,
      "loss": 0.2095,
      "step": 14943
    },
    {
      "epoch": 30.06841046277666,
      "grad_norm": 0.7612920999526978,
      "learning_rate": 0.00013987725123251835,
      "loss": 0.2273,
      "step": 14944
    },
    {
      "epoch": 30.070422535211268,
      "grad_norm": 0.7734920382499695,
      "learning_rate": 0.0001398732266827649,
      "loss": 0.2051,
      "step": 14945
    },
    {
      "epoch": 30.072434607645874,
      "grad_norm": 0.792279839515686,
      "learning_rate": 0.00013986920213301138,
      "loss": 0.2147,
      "step": 14946
    },
    {
      "epoch": 30.074446680080484,
      "grad_norm": 0.7770116329193115,
      "learning_rate": 0.0001398651775832579,
      "loss": 0.2142,
      "step": 14947
    },
    {
      "epoch": 30.07645875251509,
      "grad_norm": 0.7704424262046814,
      "learning_rate": 0.00013986115303350437,
      "loss": 0.2334,
      "step": 14948
    },
    {
      "epoch": 30.078470824949697,
      "grad_norm": 0.7714694738388062,
      "learning_rate": 0.0001398571284837509,
      "loss": 0.2036,
      "step": 14949
    },
    {
      "epoch": 30.080482897384307,
      "grad_norm": 0.7632524967193604,
      "learning_rate": 0.0001398531039339974,
      "loss": 0.2245,
      "step": 14950
    },
    {
      "epoch": 30.082494969818914,
      "grad_norm": 0.8671344518661499,
      "learning_rate": 0.00013984907938424388,
      "loss": 0.2277,
      "step": 14951
    },
    {
      "epoch": 30.08450704225352,
      "grad_norm": 0.8321678042411804,
      "learning_rate": 0.0001398450548344904,
      "loss": 0.2194,
      "step": 14952
    },
    {
      "epoch": 30.08651911468813,
      "grad_norm": 0.8153348565101624,
      "learning_rate": 0.0001398410302847369,
      "loss": 0.2267,
      "step": 14953
    },
    {
      "epoch": 30.088531187122737,
      "grad_norm": 0.7236051559448242,
      "learning_rate": 0.0001398370057349834,
      "loss": 0.1929,
      "step": 14954
    },
    {
      "epoch": 30.090543259557343,
      "grad_norm": 0.7561371922492981,
      "learning_rate": 0.0001398329811852299,
      "loss": 0.2348,
      "step": 14955
    },
    {
      "epoch": 30.092555331991953,
      "grad_norm": 0.7957860231399536,
      "learning_rate": 0.00013982895663547642,
      "loss": 0.2308,
      "step": 14956
    },
    {
      "epoch": 30.09456740442656,
      "grad_norm": 0.7986541986465454,
      "learning_rate": 0.0001398249320857229,
      "loss": 0.2149,
      "step": 14957
    },
    {
      "epoch": 30.096579476861166,
      "grad_norm": 0.754309892654419,
      "learning_rate": 0.00013982090753596942,
      "loss": 0.2255,
      "step": 14958
    },
    {
      "epoch": 30.098591549295776,
      "grad_norm": 0.7746975421905518,
      "learning_rate": 0.0001398168829862159,
      "loss": 0.2254,
      "step": 14959
    },
    {
      "epoch": 30.100603621730382,
      "grad_norm": 0.804052472114563,
      "learning_rate": 0.00013981285843646244,
      "loss": 0.2065,
      "step": 14960
    },
    {
      "epoch": 30.10261569416499,
      "grad_norm": 0.7727026343345642,
      "learning_rate": 0.00013980883388670893,
      "loss": 0.2209,
      "step": 14961
    },
    {
      "epoch": 30.1046277665996,
      "grad_norm": 0.8061894774436951,
      "learning_rate": 0.00013980480933695544,
      "loss": 0.2249,
      "step": 14962
    },
    {
      "epoch": 30.106639839034205,
      "grad_norm": 0.783903181552887,
      "learning_rate": 0.00013980078478720192,
      "loss": 0.2142,
      "step": 14963
    },
    {
      "epoch": 30.10865191146881,
      "grad_norm": 0.7665119767189026,
      "learning_rate": 0.00013979676023744843,
      "loss": 0.2157,
      "step": 14964
    },
    {
      "epoch": 30.11066398390342,
      "grad_norm": 0.7643443942070007,
      "learning_rate": 0.00013979273568769495,
      "loss": 0.2068,
      "step": 14965
    },
    {
      "epoch": 30.112676056338028,
      "grad_norm": 0.8111039996147156,
      "learning_rate": 0.00013978871113794146,
      "loss": 0.2237,
      "step": 14966
    },
    {
      "epoch": 30.114688128772634,
      "grad_norm": 0.7086449265480042,
      "learning_rate": 0.00013978468658818794,
      "loss": 0.1931,
      "step": 14967
    },
    {
      "epoch": 30.116700201207244,
      "grad_norm": 0.8145108819007874,
      "learning_rate": 0.00013978066203843446,
      "loss": 0.2191,
      "step": 14968
    },
    {
      "epoch": 30.11871227364185,
      "grad_norm": 0.7974298596382141,
      "learning_rate": 0.00013977663748868094,
      "loss": 0.226,
      "step": 14969
    },
    {
      "epoch": 30.120724346076457,
      "grad_norm": 0.7846224904060364,
      "learning_rate": 0.00013977261293892748,
      "loss": 0.2316,
      "step": 14970
    },
    {
      "epoch": 30.122736418511067,
      "grad_norm": 0.7934728860855103,
      "learning_rate": 0.00013976858838917397,
      "loss": 0.2203,
      "step": 14971
    },
    {
      "epoch": 30.124748490945674,
      "grad_norm": 0.7809755802154541,
      "learning_rate": 0.00013976456383942048,
      "loss": 0.223,
      "step": 14972
    },
    {
      "epoch": 30.12676056338028,
      "grad_norm": 0.7620492577552795,
      "learning_rate": 0.00013976053928966696,
      "loss": 0.2142,
      "step": 14973
    },
    {
      "epoch": 30.12877263581489,
      "grad_norm": 0.7816911935806274,
      "learning_rate": 0.00013975651473991348,
      "loss": 0.2245,
      "step": 14974
    },
    {
      "epoch": 30.130784708249497,
      "grad_norm": 0.7877248525619507,
      "learning_rate": 0.00013975249019016,
      "loss": 0.2183,
      "step": 14975
    },
    {
      "epoch": 30.132796780684103,
      "grad_norm": 0.8127319812774658,
      "learning_rate": 0.0001397484656404065,
      "loss": 0.2447,
      "step": 14976
    },
    {
      "epoch": 30.134808853118713,
      "grad_norm": 0.8038638234138489,
      "learning_rate": 0.00013974444109065298,
      "loss": 0.2259,
      "step": 14977
    },
    {
      "epoch": 30.13682092555332,
      "grad_norm": 0.8404989242553711,
      "learning_rate": 0.0001397404165408995,
      "loss": 0.243,
      "step": 14978
    },
    {
      "epoch": 30.138832997987926,
      "grad_norm": 0.8339924216270447,
      "learning_rate": 0.00013973639199114598,
      "loss": 0.2086,
      "step": 14979
    },
    {
      "epoch": 30.140845070422536,
      "grad_norm": 0.8524388074874878,
      "learning_rate": 0.00013973236744139252,
      "loss": 0.2251,
      "step": 14980
    },
    {
      "epoch": 30.142857142857142,
      "grad_norm": 0.8006795644760132,
      "learning_rate": 0.000139728342891639,
      "loss": 0.2158,
      "step": 14981
    },
    {
      "epoch": 30.14486921529175,
      "grad_norm": 0.7888465523719788,
      "learning_rate": 0.00013972431834188552,
      "loss": 0.2253,
      "step": 14982
    },
    {
      "epoch": 30.14688128772636,
      "grad_norm": 0.8385117650032043,
      "learning_rate": 0.000139720293792132,
      "loss": 0.2301,
      "step": 14983
    },
    {
      "epoch": 30.148893360160965,
      "grad_norm": 0.783174991607666,
      "learning_rate": 0.00013971626924237852,
      "loss": 0.2246,
      "step": 14984
    },
    {
      "epoch": 30.15090543259557,
      "grad_norm": 0.7840462923049927,
      "learning_rate": 0.00013971224469262503,
      "loss": 0.212,
      "step": 14985
    },
    {
      "epoch": 30.15291750503018,
      "grad_norm": 0.8484194874763489,
      "learning_rate": 0.00013970822014287151,
      "loss": 0.2248,
      "step": 14986
    },
    {
      "epoch": 30.154929577464788,
      "grad_norm": 0.7754706144332886,
      "learning_rate": 0.00013970419559311803,
      "loss": 0.2194,
      "step": 14987
    },
    {
      "epoch": 30.156941649899398,
      "grad_norm": 0.7833287715911865,
      "learning_rate": 0.00013970017104336454,
      "loss": 0.2261,
      "step": 14988
    },
    {
      "epoch": 30.158953722334005,
      "grad_norm": 0.8076835870742798,
      "learning_rate": 0.00013969614649361102,
      "loss": 0.2147,
      "step": 14989
    },
    {
      "epoch": 30.16096579476861,
      "grad_norm": 0.8041014075279236,
      "learning_rate": 0.00013969212194385754,
      "loss": 0.2253,
      "step": 14990
    },
    {
      "epoch": 30.16297786720322,
      "grad_norm": 0.8352798223495483,
      "learning_rate": 0.00013968809739410405,
      "loss": 0.2433,
      "step": 14991
    },
    {
      "epoch": 30.164989939637827,
      "grad_norm": 0.8011649250984192,
      "learning_rate": 0.00013968407284435053,
      "loss": 0.2267,
      "step": 14992
    },
    {
      "epoch": 30.167002012072434,
      "grad_norm": 0.7648217678070068,
      "learning_rate": 0.00013968004829459704,
      "loss": 0.2255,
      "step": 14993
    },
    {
      "epoch": 30.169014084507044,
      "grad_norm": 0.7829076647758484,
      "learning_rate": 0.00013967602374484353,
      "loss": 0.2167,
      "step": 14994
    },
    {
      "epoch": 30.17102615694165,
      "grad_norm": 0.8126546144485474,
      "learning_rate": 0.00013967199919509007,
      "loss": 0.2543,
      "step": 14995
    },
    {
      "epoch": 30.173038229376257,
      "grad_norm": 0.8176954984664917,
      "learning_rate": 0.00013966797464533655,
      "loss": 0.2274,
      "step": 14996
    },
    {
      "epoch": 30.175050301810867,
      "grad_norm": 0.8456249237060547,
      "learning_rate": 0.00013966395009558307,
      "loss": 0.2463,
      "step": 14997
    },
    {
      "epoch": 30.177062374245473,
      "grad_norm": 0.8268572092056274,
      "learning_rate": 0.00013965992554582955,
      "loss": 0.2209,
      "step": 14998
    },
    {
      "epoch": 30.17907444668008,
      "grad_norm": 0.822644829750061,
      "learning_rate": 0.00013965590099607606,
      "loss": 0.2393,
      "step": 14999
    },
    {
      "epoch": 30.18108651911469,
      "grad_norm": 0.8190850019454956,
      "learning_rate": 0.00013965187644632258,
      "loss": 0.2349,
      "step": 15000
    },
    {
      "epoch": 30.183098591549296,
      "grad_norm": 0.8007157444953918,
      "learning_rate": 0.0001396478518965691,
      "loss": 0.2264,
      "step": 15001
    },
    {
      "epoch": 30.185110663983902,
      "grad_norm": 0.8651202917098999,
      "learning_rate": 0.00013964382734681557,
      "loss": 0.2432,
      "step": 15002
    },
    {
      "epoch": 30.187122736418512,
      "grad_norm": 0.8018503189086914,
      "learning_rate": 0.00013963980279706209,
      "loss": 0.2327,
      "step": 15003
    },
    {
      "epoch": 30.18913480885312,
      "grad_norm": 0.7893446087837219,
      "learning_rate": 0.00013963577824730857,
      "loss": 0.2242,
      "step": 15004
    },
    {
      "epoch": 30.191146881287725,
      "grad_norm": 0.8060505986213684,
      "learning_rate": 0.0001396317536975551,
      "loss": 0.2167,
      "step": 15005
    },
    {
      "epoch": 30.193158953722335,
      "grad_norm": 0.7678210735321045,
      "learning_rate": 0.0001396277291478016,
      "loss": 0.2271,
      "step": 15006
    },
    {
      "epoch": 30.19517102615694,
      "grad_norm": 0.8324878811836243,
      "learning_rate": 0.0001396237045980481,
      "loss": 0.2456,
      "step": 15007
    },
    {
      "epoch": 30.197183098591548,
      "grad_norm": 0.7615331411361694,
      "learning_rate": 0.0001396196800482946,
      "loss": 0.2246,
      "step": 15008
    },
    {
      "epoch": 30.199195171026158,
      "grad_norm": 0.8941498398780823,
      "learning_rate": 0.0001396156554985411,
      "loss": 0.2132,
      "step": 15009
    },
    {
      "epoch": 30.201207243460765,
      "grad_norm": 0.8736677169799805,
      "learning_rate": 0.00013961163094878762,
      "loss": 0.2504,
      "step": 15010
    },
    {
      "epoch": 30.20321931589537,
      "grad_norm": 0.8632407784461975,
      "learning_rate": 0.00013960760639903413,
      "loss": 0.232,
      "step": 15011
    },
    {
      "epoch": 30.20523138832998,
      "grad_norm": 0.8226110935211182,
      "learning_rate": 0.00013960358184928061,
      "loss": 0.243,
      "step": 15012
    },
    {
      "epoch": 30.207243460764587,
      "grad_norm": 0.8224267363548279,
      "learning_rate": 0.00013959955729952713,
      "loss": 0.2228,
      "step": 15013
    },
    {
      "epoch": 30.209255533199194,
      "grad_norm": 0.8122011423110962,
      "learning_rate": 0.0001395955327497736,
      "loss": 0.2346,
      "step": 15014
    },
    {
      "epoch": 30.211267605633804,
      "grad_norm": 0.8414570093154907,
      "learning_rate": 0.00013959150820002015,
      "loss": 0.2246,
      "step": 15015
    },
    {
      "epoch": 30.21327967806841,
      "grad_norm": 0.7964183688163757,
      "learning_rate": 0.00013958748365026664,
      "loss": 0.2142,
      "step": 15016
    },
    {
      "epoch": 30.215291750503017,
      "grad_norm": 0.8214002847671509,
      "learning_rate": 0.00013958345910051315,
      "loss": 0.2416,
      "step": 15017
    },
    {
      "epoch": 30.217303822937627,
      "grad_norm": 0.7986194491386414,
      "learning_rate": 0.00013957943455075963,
      "loss": 0.2282,
      "step": 15018
    },
    {
      "epoch": 30.219315895372233,
      "grad_norm": 0.8563823103904724,
      "learning_rate": 0.00013957541000100615,
      "loss": 0.2421,
      "step": 15019
    },
    {
      "epoch": 30.22132796780684,
      "grad_norm": 0.7934738993644714,
      "learning_rate": 0.00013957138545125266,
      "loss": 0.2194,
      "step": 15020
    },
    {
      "epoch": 30.22334004024145,
      "grad_norm": 0.8978814482688904,
      "learning_rate": 0.00013956736090149914,
      "loss": 0.2409,
      "step": 15021
    },
    {
      "epoch": 30.225352112676056,
      "grad_norm": 0.9283680319786072,
      "learning_rate": 0.00013956333635174566,
      "loss": 0.2529,
      "step": 15022
    },
    {
      "epoch": 30.227364185110662,
      "grad_norm": 0.8469935059547424,
      "learning_rate": 0.00013955931180199217,
      "loss": 0.2251,
      "step": 15023
    },
    {
      "epoch": 30.229376257545272,
      "grad_norm": 0.881660521030426,
      "learning_rate": 0.00013955528725223865,
      "loss": 0.2448,
      "step": 15024
    },
    {
      "epoch": 30.23138832997988,
      "grad_norm": 0.768204391002655,
      "learning_rate": 0.00013955126270248516,
      "loss": 0.2173,
      "step": 15025
    },
    {
      "epoch": 30.233400402414485,
      "grad_norm": 0.8437636494636536,
      "learning_rate": 0.00013954723815273168,
      "loss": 0.2442,
      "step": 15026
    },
    {
      "epoch": 30.235412474849095,
      "grad_norm": 0.9082536697387695,
      "learning_rate": 0.00013954321360297816,
      "loss": 0.2358,
      "step": 15027
    },
    {
      "epoch": 30.2374245472837,
      "grad_norm": 0.7909454703330994,
      "learning_rate": 0.00013953918905322467,
      "loss": 0.2245,
      "step": 15028
    },
    {
      "epoch": 30.239436619718308,
      "grad_norm": 0.8810176849365234,
      "learning_rate": 0.00013953516450347116,
      "loss": 0.2388,
      "step": 15029
    },
    {
      "epoch": 30.241448692152918,
      "grad_norm": 0.7531331777572632,
      "learning_rate": 0.0001395311399537177,
      "loss": 0.2145,
      "step": 15030
    },
    {
      "epoch": 30.243460764587525,
      "grad_norm": 0.8109076619148254,
      "learning_rate": 0.00013952711540396418,
      "loss": 0.2426,
      "step": 15031
    },
    {
      "epoch": 30.24547283702213,
      "grad_norm": 0.8501294851303101,
      "learning_rate": 0.0001395230908542107,
      "loss": 0.2405,
      "step": 15032
    },
    {
      "epoch": 30.24748490945674,
      "grad_norm": 0.8328281044960022,
      "learning_rate": 0.00013951906630445718,
      "loss": 0.2293,
      "step": 15033
    },
    {
      "epoch": 30.249496981891348,
      "grad_norm": 0.8530357480049133,
      "learning_rate": 0.0001395150417547037,
      "loss": 0.2422,
      "step": 15034
    },
    {
      "epoch": 30.251509054325957,
      "grad_norm": 0.8194112777709961,
      "learning_rate": 0.0001395110172049502,
      "loss": 0.2519,
      "step": 15035
    },
    {
      "epoch": 30.253521126760564,
      "grad_norm": 0.8261650800704956,
      "learning_rate": 0.00013950699265519672,
      "loss": 0.233,
      "step": 15036
    },
    {
      "epoch": 30.25553319919517,
      "grad_norm": 0.8624595999717712,
      "learning_rate": 0.0001395029681054432,
      "loss": 0.2631,
      "step": 15037
    },
    {
      "epoch": 30.25754527162978,
      "grad_norm": 0.7856851816177368,
      "learning_rate": 0.00013949894355568972,
      "loss": 0.236,
      "step": 15038
    },
    {
      "epoch": 30.259557344064387,
      "grad_norm": 0.8022642135620117,
      "learning_rate": 0.0001394949190059362,
      "loss": 0.2383,
      "step": 15039
    },
    {
      "epoch": 30.261569416498993,
      "grad_norm": 0.8738016486167908,
      "learning_rate": 0.00013949089445618274,
      "loss": 0.2371,
      "step": 15040
    },
    {
      "epoch": 30.263581488933603,
      "grad_norm": 0.8347159624099731,
      "learning_rate": 0.00013948686990642922,
      "loss": 0.2223,
      "step": 15041
    },
    {
      "epoch": 30.26559356136821,
      "grad_norm": 0.8623753190040588,
      "learning_rate": 0.00013948284535667574,
      "loss": 0.2276,
      "step": 15042
    },
    {
      "epoch": 30.267605633802816,
      "grad_norm": 0.8661081790924072,
      "learning_rate": 0.00013947882080692222,
      "loss": 0.2412,
      "step": 15043
    },
    {
      "epoch": 30.269617706237426,
      "grad_norm": 0.8598690629005432,
      "learning_rate": 0.00013947479625716873,
      "loss": 0.238,
      "step": 15044
    },
    {
      "epoch": 30.271629778672033,
      "grad_norm": 0.8310718536376953,
      "learning_rate": 0.00013947077170741525,
      "loss": 0.2385,
      "step": 15045
    },
    {
      "epoch": 30.27364185110664,
      "grad_norm": 0.8581821322441101,
      "learning_rate": 0.00013946674715766176,
      "loss": 0.2239,
      "step": 15046
    },
    {
      "epoch": 30.27565392354125,
      "grad_norm": 0.8526563048362732,
      "learning_rate": 0.00013946272260790824,
      "loss": 0.233,
      "step": 15047
    },
    {
      "epoch": 30.277665995975855,
      "grad_norm": 0.8430448770523071,
      "learning_rate": 0.00013945869805815476,
      "loss": 0.2596,
      "step": 15048
    },
    {
      "epoch": 30.279678068410462,
      "grad_norm": 0.8200827836990356,
      "learning_rate": 0.00013945467350840124,
      "loss": 0.2499,
      "step": 15049
    },
    {
      "epoch": 30.281690140845072,
      "grad_norm": 0.8266736268997192,
      "learning_rate": 0.00013945064895864778,
      "loss": 0.2281,
      "step": 15050
    },
    {
      "epoch": 30.28370221327968,
      "grad_norm": 0.832103967666626,
      "learning_rate": 0.00013944662440889427,
      "loss": 0.2285,
      "step": 15051
    },
    {
      "epoch": 30.285714285714285,
      "grad_norm": 0.8517701029777527,
      "learning_rate": 0.00013944259985914078,
      "loss": 0.2491,
      "step": 15052
    },
    {
      "epoch": 30.287726358148895,
      "grad_norm": 0.8571256399154663,
      "learning_rate": 0.00013943857530938726,
      "loss": 0.2412,
      "step": 15053
    },
    {
      "epoch": 30.2897384305835,
      "grad_norm": 0.8357716798782349,
      "learning_rate": 0.00013943455075963378,
      "loss": 0.2327,
      "step": 15054
    },
    {
      "epoch": 30.291750503018108,
      "grad_norm": 0.8381335139274597,
      "learning_rate": 0.0001394305262098803,
      "loss": 0.2268,
      "step": 15055
    },
    {
      "epoch": 30.293762575452718,
      "grad_norm": 0.8326700329780579,
      "learning_rate": 0.00013942650166012677,
      "loss": 0.2505,
      "step": 15056
    },
    {
      "epoch": 30.295774647887324,
      "grad_norm": 0.8256263732910156,
      "learning_rate": 0.00013942247711037328,
      "loss": 0.245,
      "step": 15057
    },
    {
      "epoch": 30.29778672032193,
      "grad_norm": 0.8831014633178711,
      "learning_rate": 0.00013941845256061977,
      "loss": 0.2416,
      "step": 15058
    },
    {
      "epoch": 30.29979879275654,
      "grad_norm": 0.8676580786705017,
      "learning_rate": 0.00013941442801086628,
      "loss": 0.2471,
      "step": 15059
    },
    {
      "epoch": 30.301810865191147,
      "grad_norm": 0.8529226183891296,
      "learning_rate": 0.0001394104034611128,
      "loss": 0.2492,
      "step": 15060
    },
    {
      "epoch": 30.303822937625753,
      "grad_norm": 0.8493870496749878,
      "learning_rate": 0.0001394063789113593,
      "loss": 0.2425,
      "step": 15061
    },
    {
      "epoch": 30.305835010060363,
      "grad_norm": 0.7730135917663574,
      "learning_rate": 0.0001394023543616058,
      "loss": 0.2321,
      "step": 15062
    },
    {
      "epoch": 30.30784708249497,
      "grad_norm": 0.8642377257347107,
      "learning_rate": 0.0001393983298118523,
      "loss": 0.2289,
      "step": 15063
    },
    {
      "epoch": 30.309859154929576,
      "grad_norm": 0.8122088313102722,
      "learning_rate": 0.0001393943052620988,
      "loss": 0.2363,
      "step": 15064
    },
    {
      "epoch": 30.311871227364186,
      "grad_norm": 0.8894031047821045,
      "learning_rate": 0.00013939028071234533,
      "loss": 0.257,
      "step": 15065
    },
    {
      "epoch": 30.313883299798793,
      "grad_norm": 0.8266621828079224,
      "learning_rate": 0.0001393862561625918,
      "loss": 0.2517,
      "step": 15066
    },
    {
      "epoch": 30.3158953722334,
      "grad_norm": 0.8376973867416382,
      "learning_rate": 0.00013938223161283833,
      "loss": 0.2184,
      "step": 15067
    },
    {
      "epoch": 30.31790744466801,
      "grad_norm": 0.8361218571662903,
      "learning_rate": 0.0001393782070630848,
      "loss": 0.2248,
      "step": 15068
    },
    {
      "epoch": 30.319919517102615,
      "grad_norm": 0.8278533220291138,
      "learning_rate": 0.00013937418251333132,
      "loss": 0.2401,
      "step": 15069
    },
    {
      "epoch": 30.321931589537222,
      "grad_norm": 0.8201502561569214,
      "learning_rate": 0.00013937015796357784,
      "loss": 0.2433,
      "step": 15070
    },
    {
      "epoch": 30.323943661971832,
      "grad_norm": 0.8062523603439331,
      "learning_rate": 0.00013936613341382435,
      "loss": 0.2432,
      "step": 15071
    },
    {
      "epoch": 30.32595573440644,
      "grad_norm": 0.7993463277816772,
      "learning_rate": 0.00013936210886407083,
      "loss": 0.2164,
      "step": 15072
    },
    {
      "epoch": 30.327967806841045,
      "grad_norm": 0.8504349589347839,
      "learning_rate": 0.00013935808431431734,
      "loss": 0.2367,
      "step": 15073
    },
    {
      "epoch": 30.329979879275655,
      "grad_norm": 0.872622013092041,
      "learning_rate": 0.00013935405976456383,
      "loss": 0.2395,
      "step": 15074
    },
    {
      "epoch": 30.33199195171026,
      "grad_norm": 0.8582693934440613,
      "learning_rate": 0.00013935003521481037,
      "loss": 0.263,
      "step": 15075
    },
    {
      "epoch": 30.334004024144868,
      "grad_norm": 0.9036448001861572,
      "learning_rate": 0.00013934601066505685,
      "loss": 0.2478,
      "step": 15076
    },
    {
      "epoch": 30.336016096579478,
      "grad_norm": 0.8529244661331177,
      "learning_rate": 0.00013934198611530337,
      "loss": 0.247,
      "step": 15077
    },
    {
      "epoch": 30.338028169014084,
      "grad_norm": 0.8466749787330627,
      "learning_rate": 0.00013933796156554985,
      "loss": 0.2072,
      "step": 15078
    },
    {
      "epoch": 30.34004024144869,
      "grad_norm": 0.827564001083374,
      "learning_rate": 0.00013933393701579636,
      "loss": 0.2401,
      "step": 15079
    },
    {
      "epoch": 30.3420523138833,
      "grad_norm": 0.8246797323226929,
      "learning_rate": 0.00013932991246604288,
      "loss": 0.2278,
      "step": 15080
    },
    {
      "epoch": 30.344064386317907,
      "grad_norm": 0.8305550217628479,
      "learning_rate": 0.0001393258879162894,
      "loss": 0.2371,
      "step": 15081
    },
    {
      "epoch": 30.346076458752513,
      "grad_norm": 0.8436625599861145,
      "learning_rate": 0.00013932186336653587,
      "loss": 0.2436,
      "step": 15082
    },
    {
      "epoch": 30.348088531187123,
      "grad_norm": 0.8399848341941833,
      "learning_rate": 0.00013931783881678239,
      "loss": 0.2561,
      "step": 15083
    },
    {
      "epoch": 30.35010060362173,
      "grad_norm": 0.8543189167976379,
      "learning_rate": 0.00013931381426702887,
      "loss": 0.2391,
      "step": 15084
    },
    {
      "epoch": 30.352112676056336,
      "grad_norm": 0.8442783355712891,
      "learning_rate": 0.00013930978971727538,
      "loss": 0.2236,
      "step": 15085
    },
    {
      "epoch": 30.354124748490946,
      "grad_norm": 0.8823053240776062,
      "learning_rate": 0.0001393057651675219,
      "loss": 0.2491,
      "step": 15086
    },
    {
      "epoch": 30.356136820925553,
      "grad_norm": 0.8765469193458557,
      "learning_rate": 0.0001393017406177684,
      "loss": 0.2404,
      "step": 15087
    },
    {
      "epoch": 30.358148893360163,
      "grad_norm": 0.8149089217185974,
      "learning_rate": 0.0001392977160680149,
      "loss": 0.2391,
      "step": 15088
    },
    {
      "epoch": 30.36016096579477,
      "grad_norm": 0.8453466892242432,
      "learning_rate": 0.0001392936915182614,
      "loss": 0.232,
      "step": 15089
    },
    {
      "epoch": 30.362173038229376,
      "grad_norm": 0.8428980708122253,
      "learning_rate": 0.00013928966696850792,
      "loss": 0.2417,
      "step": 15090
    },
    {
      "epoch": 30.364185110663986,
      "grad_norm": 0.8283656239509583,
      "learning_rate": 0.0001392856424187544,
      "loss": 0.2443,
      "step": 15091
    },
    {
      "epoch": 30.366197183098592,
      "grad_norm": 0.9187449216842651,
      "learning_rate": 0.00013928161786900091,
      "loss": 0.2349,
      "step": 15092
    },
    {
      "epoch": 30.3682092555332,
      "grad_norm": 0.8478832840919495,
      "learning_rate": 0.0001392775933192474,
      "loss": 0.2342,
      "step": 15093
    },
    {
      "epoch": 30.37022132796781,
      "grad_norm": 0.7977643609046936,
      "learning_rate": 0.0001392735687694939,
      "loss": 0.2392,
      "step": 15094
    },
    {
      "epoch": 30.372233400402415,
      "grad_norm": 0.8855370879173279,
      "learning_rate": 0.00013926954421974042,
      "loss": 0.2487,
      "step": 15095
    },
    {
      "epoch": 30.37424547283702,
      "grad_norm": 0.8910651803016663,
      "learning_rate": 0.00013926551966998694,
      "loss": 0.249,
      "step": 15096
    },
    {
      "epoch": 30.37625754527163,
      "grad_norm": 0.8750113844871521,
      "learning_rate": 0.00013926149512023342,
      "loss": 0.2491,
      "step": 15097
    },
    {
      "epoch": 30.378269617706238,
      "grad_norm": 0.8117910623550415,
      "learning_rate": 0.00013925747057047993,
      "loss": 0.2401,
      "step": 15098
    },
    {
      "epoch": 30.380281690140844,
      "grad_norm": 0.853955864906311,
      "learning_rate": 0.00013925344602072642,
      "loss": 0.247,
      "step": 15099
    },
    {
      "epoch": 30.382293762575454,
      "grad_norm": 0.8644211888313293,
      "learning_rate": 0.00013924942147097296,
      "loss": 0.2318,
      "step": 15100
    },
    {
      "epoch": 30.38430583501006,
      "grad_norm": 0.7975677847862244,
      "learning_rate": 0.00013924539692121944,
      "loss": 0.2241,
      "step": 15101
    },
    {
      "epoch": 30.386317907444667,
      "grad_norm": 0.8277243971824646,
      "learning_rate": 0.00013924137237146595,
      "loss": 0.2298,
      "step": 15102
    },
    {
      "epoch": 30.388329979879277,
      "grad_norm": 0.8807799220085144,
      "learning_rate": 0.00013923734782171244,
      "loss": 0.2463,
      "step": 15103
    },
    {
      "epoch": 30.390342052313883,
      "grad_norm": 0.8302869200706482,
      "learning_rate": 0.00013923332327195895,
      "loss": 0.2429,
      "step": 15104
    },
    {
      "epoch": 30.39235412474849,
      "grad_norm": 0.8542106747627258,
      "learning_rate": 0.00013922929872220546,
      "loss": 0.2317,
      "step": 15105
    },
    {
      "epoch": 30.3943661971831,
      "grad_norm": 0.809414267539978,
      "learning_rate": 0.00013922527417245198,
      "loss": 0.2335,
      "step": 15106
    },
    {
      "epoch": 30.396378269617706,
      "grad_norm": 0.9319563508033752,
      "learning_rate": 0.00013922124962269846,
      "loss": 0.2484,
      "step": 15107
    },
    {
      "epoch": 30.398390342052313,
      "grad_norm": 0.8717673420906067,
      "learning_rate": 0.00013921722507294497,
      "loss": 0.2431,
      "step": 15108
    },
    {
      "epoch": 30.400402414486923,
      "grad_norm": 0.8347278237342834,
      "learning_rate": 0.00013921320052319146,
      "loss": 0.2412,
      "step": 15109
    },
    {
      "epoch": 30.40241448692153,
      "grad_norm": 0.8252801299095154,
      "learning_rate": 0.000139209175973438,
      "loss": 0.2123,
      "step": 15110
    },
    {
      "epoch": 30.404426559356136,
      "grad_norm": 0.8646487593650818,
      "learning_rate": 0.00013920515142368448,
      "loss": 0.229,
      "step": 15111
    },
    {
      "epoch": 30.406438631790746,
      "grad_norm": 0.9486965537071228,
      "learning_rate": 0.000139201126873931,
      "loss": 0.2648,
      "step": 15112
    },
    {
      "epoch": 30.408450704225352,
      "grad_norm": 0.8924963474273682,
      "learning_rate": 0.00013919710232417748,
      "loss": 0.2403,
      "step": 15113
    },
    {
      "epoch": 30.41046277665996,
      "grad_norm": 0.8611428737640381,
      "learning_rate": 0.000139193077774424,
      "loss": 0.2527,
      "step": 15114
    },
    {
      "epoch": 30.41247484909457,
      "grad_norm": 0.9303199052810669,
      "learning_rate": 0.0001391890532246705,
      "loss": 0.2522,
      "step": 15115
    },
    {
      "epoch": 30.414486921529175,
      "grad_norm": 0.8426361680030823,
      "learning_rate": 0.00013918502867491702,
      "loss": 0.2205,
      "step": 15116
    },
    {
      "epoch": 30.41649899396378,
      "grad_norm": 0.878689169883728,
      "learning_rate": 0.0001391810041251635,
      "loss": 0.2422,
      "step": 15117
    },
    {
      "epoch": 30.41851106639839,
      "grad_norm": 0.8856695890426636,
      "learning_rate": 0.00013917697957541001,
      "loss": 0.2453,
      "step": 15118
    },
    {
      "epoch": 30.420523138832998,
      "grad_norm": 0.8352988958358765,
      "learning_rate": 0.0001391729550256565,
      "loss": 0.2306,
      "step": 15119
    },
    {
      "epoch": 30.422535211267604,
      "grad_norm": 0.843724250793457,
      "learning_rate": 0.000139168930475903,
      "loss": 0.2542,
      "step": 15120
    },
    {
      "epoch": 30.424547283702214,
      "grad_norm": 0.8055031895637512,
      "learning_rate": 0.00013916490592614952,
      "loss": 0.2475,
      "step": 15121
    },
    {
      "epoch": 30.42655935613682,
      "grad_norm": 0.8644869327545166,
      "learning_rate": 0.00013916088137639604,
      "loss": 0.2385,
      "step": 15122
    },
    {
      "epoch": 30.428571428571427,
      "grad_norm": 0.8530833125114441,
      "learning_rate": 0.00013915685682664252,
      "loss": 0.239,
      "step": 15123
    },
    {
      "epoch": 30.430583501006037,
      "grad_norm": 0.8540896773338318,
      "learning_rate": 0.00013915283227688903,
      "loss": 0.2437,
      "step": 15124
    },
    {
      "epoch": 30.432595573440643,
      "grad_norm": 0.9228827357292175,
      "learning_rate": 0.00013914880772713555,
      "loss": 0.2518,
      "step": 15125
    },
    {
      "epoch": 30.43460764587525,
      "grad_norm": 0.8416258096694946,
      "learning_rate": 0.00013914478317738203,
      "loss": 0.2357,
      "step": 15126
    },
    {
      "epoch": 30.43661971830986,
      "grad_norm": 0.890547513961792,
      "learning_rate": 0.00013914075862762854,
      "loss": 0.2493,
      "step": 15127
    },
    {
      "epoch": 30.438631790744466,
      "grad_norm": 0.7995548248291016,
      "learning_rate": 0.00013913673407787503,
      "loss": 0.2225,
      "step": 15128
    },
    {
      "epoch": 30.440643863179073,
      "grad_norm": 0.9280117154121399,
      "learning_rate": 0.00013913270952812154,
      "loss": 0.2564,
      "step": 15129
    },
    {
      "epoch": 30.442655935613683,
      "grad_norm": 0.8590514659881592,
      "learning_rate": 0.00013912868497836805,
      "loss": 0.2511,
      "step": 15130
    },
    {
      "epoch": 30.44466800804829,
      "grad_norm": 0.8937737345695496,
      "learning_rate": 0.00013912466042861457,
      "loss": 0.2371,
      "step": 15131
    },
    {
      "epoch": 30.446680080482896,
      "grad_norm": 0.9131980538368225,
      "learning_rate": 0.00013912063587886105,
      "loss": 0.2724,
      "step": 15132
    },
    {
      "epoch": 30.448692152917506,
      "grad_norm": 0.8866675496101379,
      "learning_rate": 0.00013911661132910756,
      "loss": 0.2473,
      "step": 15133
    },
    {
      "epoch": 30.450704225352112,
      "grad_norm": 0.8128195405006409,
      "learning_rate": 0.00013911258677935405,
      "loss": 0.2369,
      "step": 15134
    },
    {
      "epoch": 30.452716297786722,
      "grad_norm": 0.8674376010894775,
      "learning_rate": 0.0001391085622296006,
      "loss": 0.2473,
      "step": 15135
    },
    {
      "epoch": 30.45472837022133,
      "grad_norm": 0.8766764402389526,
      "learning_rate": 0.00013910453767984707,
      "loss": 0.2316,
      "step": 15136
    },
    {
      "epoch": 30.456740442655935,
      "grad_norm": 0.8262248039245605,
      "learning_rate": 0.00013910051313009358,
      "loss": 0.2555,
      "step": 15137
    },
    {
      "epoch": 30.458752515090545,
      "grad_norm": 0.8504267334938049,
      "learning_rate": 0.00013909648858034007,
      "loss": 0.243,
      "step": 15138
    },
    {
      "epoch": 30.46076458752515,
      "grad_norm": 0.8202023506164551,
      "learning_rate": 0.00013909246403058658,
      "loss": 0.2529,
      "step": 15139
    },
    {
      "epoch": 30.462776659959758,
      "grad_norm": 0.8642662167549133,
      "learning_rate": 0.0001390884394808331,
      "loss": 0.2518,
      "step": 15140
    },
    {
      "epoch": 30.464788732394368,
      "grad_norm": 0.8499830961227417,
      "learning_rate": 0.0001390844149310796,
      "loss": 0.2353,
      "step": 15141
    },
    {
      "epoch": 30.466800804828974,
      "grad_norm": 0.8994375467300415,
      "learning_rate": 0.0001390803903813261,
      "loss": 0.2558,
      "step": 15142
    },
    {
      "epoch": 30.46881287726358,
      "grad_norm": 0.8543961048126221,
      "learning_rate": 0.0001390763658315726,
      "loss": 0.2564,
      "step": 15143
    },
    {
      "epoch": 30.47082494969819,
      "grad_norm": 0.8584889769554138,
      "learning_rate": 0.0001390723412818191,
      "loss": 0.2318,
      "step": 15144
    },
    {
      "epoch": 30.472837022132797,
      "grad_norm": 0.9337508678436279,
      "learning_rate": 0.00013906831673206563,
      "loss": 0.2359,
      "step": 15145
    },
    {
      "epoch": 30.474849094567404,
      "grad_norm": 0.8359694480895996,
      "learning_rate": 0.0001390642921823121,
      "loss": 0.2425,
      "step": 15146
    },
    {
      "epoch": 30.476861167002014,
      "grad_norm": 0.9233167767524719,
      "learning_rate": 0.00013906026763255863,
      "loss": 0.2593,
      "step": 15147
    },
    {
      "epoch": 30.47887323943662,
      "grad_norm": 0.8276655077934265,
      "learning_rate": 0.0001390562430828051,
      "loss": 0.2449,
      "step": 15148
    },
    {
      "epoch": 30.480885311871226,
      "grad_norm": 0.8799784183502197,
      "learning_rate": 0.00013905221853305162,
      "loss": 0.2481,
      "step": 15149
    },
    {
      "epoch": 30.482897384305836,
      "grad_norm": 0.8366209268569946,
      "learning_rate": 0.00013904819398329813,
      "loss": 0.2464,
      "step": 15150
    },
    {
      "epoch": 30.484909456740443,
      "grad_norm": 0.8799043297767639,
      "learning_rate": 0.00013904416943354465,
      "loss": 0.2569,
      "step": 15151
    },
    {
      "epoch": 30.48692152917505,
      "grad_norm": 0.7988827228546143,
      "learning_rate": 0.00013904014488379113,
      "loss": 0.246,
      "step": 15152
    },
    {
      "epoch": 30.48893360160966,
      "grad_norm": 0.861625075340271,
      "learning_rate": 0.00013903612033403764,
      "loss": 0.248,
      "step": 15153
    },
    {
      "epoch": 30.490945674044266,
      "grad_norm": 0.8456945419311523,
      "learning_rate": 0.00013903209578428413,
      "loss": 0.2361,
      "step": 15154
    },
    {
      "epoch": 30.492957746478872,
      "grad_norm": 0.8246459364891052,
      "learning_rate": 0.00013902807123453064,
      "loss": 0.2314,
      "step": 15155
    },
    {
      "epoch": 30.494969818913482,
      "grad_norm": 0.8656799793243408,
      "learning_rate": 0.00013902404668477715,
      "loss": 0.2322,
      "step": 15156
    },
    {
      "epoch": 30.49698189134809,
      "grad_norm": 0.8526598215103149,
      "learning_rate": 0.00013902002213502367,
      "loss": 0.2459,
      "step": 15157
    },
    {
      "epoch": 30.498993963782695,
      "grad_norm": 0.8835206627845764,
      "learning_rate": 0.00013901599758527015,
      "loss": 0.2666,
      "step": 15158
    },
    {
      "epoch": 30.501006036217305,
      "grad_norm": 0.8495978713035583,
      "learning_rate": 0.00013901197303551666,
      "loss": 0.25,
      "step": 15159
    },
    {
      "epoch": 30.50301810865191,
      "grad_norm": 0.8806801438331604,
      "learning_rate": 0.00013900794848576315,
      "loss": 0.25,
      "step": 15160
    },
    {
      "epoch": 30.505030181086518,
      "grad_norm": 0.8632898926734924,
      "learning_rate": 0.00013900392393600966,
      "loss": 0.2233,
      "step": 15161
    },
    {
      "epoch": 30.507042253521128,
      "grad_norm": 0.8862243890762329,
      "learning_rate": 0.00013899989938625617,
      "loss": 0.2518,
      "step": 15162
    },
    {
      "epoch": 30.509054325955734,
      "grad_norm": 0.8597020506858826,
      "learning_rate": 0.00013899587483650266,
      "loss": 0.2497,
      "step": 15163
    },
    {
      "epoch": 30.51106639839034,
      "grad_norm": 0.8875630497932434,
      "learning_rate": 0.00013899185028674917,
      "loss": 0.2562,
      "step": 15164
    },
    {
      "epoch": 30.51307847082495,
      "grad_norm": 0.886516273021698,
      "learning_rate": 0.00013898782573699568,
      "loss": 0.249,
      "step": 15165
    },
    {
      "epoch": 30.515090543259557,
      "grad_norm": 0.8858243227005005,
      "learning_rate": 0.0001389838011872422,
      "loss": 0.2758,
      "step": 15166
    },
    {
      "epoch": 30.517102615694164,
      "grad_norm": 0.846316933631897,
      "learning_rate": 0.00013897977663748868,
      "loss": 0.2495,
      "step": 15167
    },
    {
      "epoch": 30.519114688128774,
      "grad_norm": 0.8936691880226135,
      "learning_rate": 0.0001389757520877352,
      "loss": 0.2561,
      "step": 15168
    },
    {
      "epoch": 30.52112676056338,
      "grad_norm": 0.8502718806266785,
      "learning_rate": 0.00013897172753798168,
      "loss": 0.2362,
      "step": 15169
    },
    {
      "epoch": 30.523138832997986,
      "grad_norm": 0.9952223896980286,
      "learning_rate": 0.0001389677029882282,
      "loss": 0.2599,
      "step": 15170
    },
    {
      "epoch": 30.525150905432596,
      "grad_norm": 0.9138308763504028,
      "learning_rate": 0.0001389636784384747,
      "loss": 0.2747,
      "step": 15171
    },
    {
      "epoch": 30.527162977867203,
      "grad_norm": 0.8805283308029175,
      "learning_rate": 0.00013895965388872121,
      "loss": 0.261,
      "step": 15172
    },
    {
      "epoch": 30.52917505030181,
      "grad_norm": 0.8878339529037476,
      "learning_rate": 0.0001389556293389677,
      "loss": 0.243,
      "step": 15173
    },
    {
      "epoch": 30.53118712273642,
      "grad_norm": 0.8261198997497559,
      "learning_rate": 0.0001389516047892142,
      "loss": 0.2505,
      "step": 15174
    },
    {
      "epoch": 30.533199195171026,
      "grad_norm": 0.9546054601669312,
      "learning_rate": 0.0001389475802394607,
      "loss": 0.25,
      "step": 15175
    },
    {
      "epoch": 30.535211267605632,
      "grad_norm": 0.8944340348243713,
      "learning_rate": 0.00013894355568970724,
      "loss": 0.2407,
      "step": 15176
    },
    {
      "epoch": 30.537223340040242,
      "grad_norm": 0.9506548643112183,
      "learning_rate": 0.00013893953113995372,
      "loss": 0.263,
      "step": 15177
    },
    {
      "epoch": 30.53923541247485,
      "grad_norm": 0.877497673034668,
      "learning_rate": 0.00013893550659020023,
      "loss": 0.2857,
      "step": 15178
    },
    {
      "epoch": 30.541247484909455,
      "grad_norm": 0.8486773371696472,
      "learning_rate": 0.00013893148204044672,
      "loss": 0.2427,
      "step": 15179
    },
    {
      "epoch": 30.543259557344065,
      "grad_norm": 0.8969256281852722,
      "learning_rate": 0.00013892745749069323,
      "loss": 0.2506,
      "step": 15180
    },
    {
      "epoch": 30.54527162977867,
      "grad_norm": 0.8447155356407166,
      "learning_rate": 0.00013892343294093974,
      "loss": 0.237,
      "step": 15181
    },
    {
      "epoch": 30.547283702213278,
      "grad_norm": 0.8210151791572571,
      "learning_rate": 0.00013891940839118625,
      "loss": 0.2279,
      "step": 15182
    },
    {
      "epoch": 30.549295774647888,
      "grad_norm": 0.9110803604125977,
      "learning_rate": 0.00013891538384143274,
      "loss": 0.2866,
      "step": 15183
    },
    {
      "epoch": 30.551307847082494,
      "grad_norm": 0.9202659726142883,
      "learning_rate": 0.00013891135929167925,
      "loss": 0.2666,
      "step": 15184
    },
    {
      "epoch": 30.5533199195171,
      "grad_norm": 0.8665937781333923,
      "learning_rate": 0.00013890733474192574,
      "loss": 0.2536,
      "step": 15185
    },
    {
      "epoch": 30.55533199195171,
      "grad_norm": 0.9305711388587952,
      "learning_rate": 0.00013890331019217228,
      "loss": 0.2539,
      "step": 15186
    },
    {
      "epoch": 30.557344064386317,
      "grad_norm": 0.8908573985099792,
      "learning_rate": 0.00013889928564241876,
      "loss": 0.246,
      "step": 15187
    },
    {
      "epoch": 30.559356136820927,
      "grad_norm": 0.9081565737724304,
      "learning_rate": 0.00013889526109266527,
      "loss": 0.2704,
      "step": 15188
    },
    {
      "epoch": 30.561368209255534,
      "grad_norm": 0.8646923899650574,
      "learning_rate": 0.00013889123654291176,
      "loss": 0.2554,
      "step": 15189
    },
    {
      "epoch": 30.56338028169014,
      "grad_norm": 0.8877758383750916,
      "learning_rate": 0.00013888721199315827,
      "loss": 0.2688,
      "step": 15190
    },
    {
      "epoch": 30.56539235412475,
      "grad_norm": 0.8628255128860474,
      "learning_rate": 0.00013888318744340478,
      "loss": 0.2655,
      "step": 15191
    },
    {
      "epoch": 30.567404426559357,
      "grad_norm": 0.8665025234222412,
      "learning_rate": 0.0001388791628936513,
      "loss": 0.2395,
      "step": 15192
    },
    {
      "epoch": 30.569416498993963,
      "grad_norm": 0.8731898665428162,
      "learning_rate": 0.00013887513834389778,
      "loss": 0.2513,
      "step": 15193
    },
    {
      "epoch": 30.571428571428573,
      "grad_norm": 0.9031705856323242,
      "learning_rate": 0.0001388711137941443,
      "loss": 0.2615,
      "step": 15194
    },
    {
      "epoch": 30.57344064386318,
      "grad_norm": 0.8401113152503967,
      "learning_rate": 0.00013886708924439078,
      "loss": 0.2291,
      "step": 15195
    },
    {
      "epoch": 30.575452716297786,
      "grad_norm": 0.9018795490264893,
      "learning_rate": 0.0001388630646946373,
      "loss": 0.2419,
      "step": 15196
    },
    {
      "epoch": 30.577464788732396,
      "grad_norm": 0.959553062915802,
      "learning_rate": 0.0001388590401448838,
      "loss": 0.2618,
      "step": 15197
    },
    {
      "epoch": 30.579476861167002,
      "grad_norm": 0.895446240901947,
      "learning_rate": 0.0001388550155951303,
      "loss": 0.2531,
      "step": 15198
    },
    {
      "epoch": 30.58148893360161,
      "grad_norm": 0.8829119801521301,
      "learning_rate": 0.0001388509910453768,
      "loss": 0.2556,
      "step": 15199
    },
    {
      "epoch": 30.58350100603622,
      "grad_norm": 0.8289411067962646,
      "learning_rate": 0.0001388469664956233,
      "loss": 0.225,
      "step": 15200
    },
    {
      "epoch": 30.585513078470825,
      "grad_norm": 0.9079847931861877,
      "learning_rate": 0.00013884294194586982,
      "loss": 0.263,
      "step": 15201
    },
    {
      "epoch": 30.58752515090543,
      "grad_norm": 0.8995522856712341,
      "learning_rate": 0.0001388389173961163,
      "loss": 0.2423,
      "step": 15202
    },
    {
      "epoch": 30.58953722334004,
      "grad_norm": 0.8367605805397034,
      "learning_rate": 0.00013883489284636282,
      "loss": 0.2562,
      "step": 15203
    },
    {
      "epoch": 30.591549295774648,
      "grad_norm": 0.8379745483398438,
      "learning_rate": 0.0001388308682966093,
      "loss": 0.244,
      "step": 15204
    },
    {
      "epoch": 30.593561368209254,
      "grad_norm": 0.8827806711196899,
      "learning_rate": 0.00013882684374685582,
      "loss": 0.2593,
      "step": 15205
    },
    {
      "epoch": 30.595573440643864,
      "grad_norm": 0.9044387340545654,
      "learning_rate": 0.00013882281919710233,
      "loss": 0.2591,
      "step": 15206
    },
    {
      "epoch": 30.59758551307847,
      "grad_norm": 0.845726490020752,
      "learning_rate": 0.00013881879464734884,
      "loss": 0.2541,
      "step": 15207
    },
    {
      "epoch": 30.599597585513077,
      "grad_norm": 0.8210976719856262,
      "learning_rate": 0.00013881477009759533,
      "loss": 0.2418,
      "step": 15208
    },
    {
      "epoch": 30.601609657947687,
      "grad_norm": 0.8106493949890137,
      "learning_rate": 0.00013881074554784184,
      "loss": 0.2362,
      "step": 15209
    },
    {
      "epoch": 30.603621730382294,
      "grad_norm": 0.9123826026916504,
      "learning_rate": 0.00013880672099808833,
      "loss": 0.2821,
      "step": 15210
    },
    {
      "epoch": 30.6056338028169,
      "grad_norm": 0.8853826522827148,
      "learning_rate": 0.00013880269644833487,
      "loss": 0.2738,
      "step": 15211
    },
    {
      "epoch": 30.60764587525151,
      "grad_norm": 0.8821125030517578,
      "learning_rate": 0.00013879867189858135,
      "loss": 0.2633,
      "step": 15212
    },
    {
      "epoch": 30.609657947686117,
      "grad_norm": 0.8474448919296265,
      "learning_rate": 0.00013879464734882786,
      "loss": 0.2449,
      "step": 15213
    },
    {
      "epoch": 30.611670020120723,
      "grad_norm": 0.8720279932022095,
      "learning_rate": 0.00013879062279907435,
      "loss": 0.2481,
      "step": 15214
    },
    {
      "epoch": 30.613682092555333,
      "grad_norm": 0.815210223197937,
      "learning_rate": 0.00013878659824932086,
      "loss": 0.236,
      "step": 15215
    },
    {
      "epoch": 30.61569416498994,
      "grad_norm": 0.9129481315612793,
      "learning_rate": 0.00013878257369956737,
      "loss": 0.2572,
      "step": 15216
    },
    {
      "epoch": 30.617706237424546,
      "grad_norm": 0.8333563208580017,
      "learning_rate": 0.00013877854914981388,
      "loss": 0.2763,
      "step": 15217
    },
    {
      "epoch": 30.619718309859156,
      "grad_norm": 0.8894381523132324,
      "learning_rate": 0.00013877452460006037,
      "loss": 0.2691,
      "step": 15218
    },
    {
      "epoch": 30.621730382293762,
      "grad_norm": 0.8933583498001099,
      "learning_rate": 0.00013877050005030688,
      "loss": 0.2605,
      "step": 15219
    },
    {
      "epoch": 30.62374245472837,
      "grad_norm": 0.8959454894065857,
      "learning_rate": 0.00013876647550055337,
      "loss": 0.2514,
      "step": 15220
    },
    {
      "epoch": 30.62575452716298,
      "grad_norm": 0.8738548755645752,
      "learning_rate": 0.0001387624509507999,
      "loss": 0.2506,
      "step": 15221
    },
    {
      "epoch": 30.627766599597585,
      "grad_norm": 0.906114399433136,
      "learning_rate": 0.0001387584264010464,
      "loss": 0.2751,
      "step": 15222
    },
    {
      "epoch": 30.62977867203219,
      "grad_norm": 0.8622868061065674,
      "learning_rate": 0.0001387544018512929,
      "loss": 0.2555,
      "step": 15223
    },
    {
      "epoch": 30.6317907444668,
      "grad_norm": 0.8809648156166077,
      "learning_rate": 0.0001387503773015394,
      "loss": 0.2717,
      "step": 15224
    },
    {
      "epoch": 30.633802816901408,
      "grad_norm": 0.8713378310203552,
      "learning_rate": 0.0001387463527517859,
      "loss": 0.2598,
      "step": 15225
    },
    {
      "epoch": 30.635814889336014,
      "grad_norm": 0.8077401518821716,
      "learning_rate": 0.0001387423282020324,
      "loss": 0.2503,
      "step": 15226
    },
    {
      "epoch": 30.637826961770624,
      "grad_norm": 0.8649102449417114,
      "learning_rate": 0.00013873830365227892,
      "loss": 0.2518,
      "step": 15227
    },
    {
      "epoch": 30.63983903420523,
      "grad_norm": 0.8350627422332764,
      "learning_rate": 0.0001387342791025254,
      "loss": 0.246,
      "step": 15228
    },
    {
      "epoch": 30.641851106639837,
      "grad_norm": 0.8557307720184326,
      "learning_rate": 0.00013873025455277192,
      "loss": 0.2501,
      "step": 15229
    },
    {
      "epoch": 30.643863179074447,
      "grad_norm": 0.8876841068267822,
      "learning_rate": 0.0001387262300030184,
      "loss": 0.2627,
      "step": 15230
    },
    {
      "epoch": 30.645875251509054,
      "grad_norm": 0.8912922143936157,
      "learning_rate": 0.00013872220545326492,
      "loss": 0.2495,
      "step": 15231
    },
    {
      "epoch": 30.647887323943664,
      "grad_norm": 0.9641183018684387,
      "learning_rate": 0.00013871818090351143,
      "loss": 0.2744,
      "step": 15232
    },
    {
      "epoch": 30.64989939637827,
      "grad_norm": 0.8491641283035278,
      "learning_rate": 0.00013871415635375792,
      "loss": 0.2442,
      "step": 15233
    },
    {
      "epoch": 30.651911468812877,
      "grad_norm": 0.9009102582931519,
      "learning_rate": 0.00013871013180400443,
      "loss": 0.2819,
      "step": 15234
    },
    {
      "epoch": 30.653923541247487,
      "grad_norm": 0.8697724342346191,
      "learning_rate": 0.00013870610725425091,
      "loss": 0.2427,
      "step": 15235
    },
    {
      "epoch": 30.655935613682093,
      "grad_norm": 0.8734996914863586,
      "learning_rate": 0.00013870208270449745,
      "loss": 0.2602,
      "step": 15236
    },
    {
      "epoch": 30.6579476861167,
      "grad_norm": 0.9121494889259338,
      "learning_rate": 0.00013869805815474394,
      "loss": 0.2532,
      "step": 15237
    },
    {
      "epoch": 30.65995975855131,
      "grad_norm": 0.8674041032791138,
      "learning_rate": 0.00013869403360499045,
      "loss": 0.2515,
      "step": 15238
    },
    {
      "epoch": 30.661971830985916,
      "grad_norm": 0.8537733554840088,
      "learning_rate": 0.00013869000905523694,
      "loss": 0.2505,
      "step": 15239
    },
    {
      "epoch": 30.663983903420522,
      "grad_norm": 0.8406361937522888,
      "learning_rate": 0.00013868598450548345,
      "loss": 0.2478,
      "step": 15240
    },
    {
      "epoch": 30.665995975855132,
      "grad_norm": 0.8295047283172607,
      "learning_rate": 0.00013868195995572996,
      "loss": 0.2573,
      "step": 15241
    },
    {
      "epoch": 30.66800804828974,
      "grad_norm": 0.8811659812927246,
      "learning_rate": 0.00013867793540597647,
      "loss": 0.2483,
      "step": 15242
    },
    {
      "epoch": 30.670020120724345,
      "grad_norm": 0.8982928991317749,
      "learning_rate": 0.00013867391085622296,
      "loss": 0.2929,
      "step": 15243
    },
    {
      "epoch": 30.672032193158955,
      "grad_norm": 0.8541519045829773,
      "learning_rate": 0.00013866988630646947,
      "loss": 0.2475,
      "step": 15244
    },
    {
      "epoch": 30.67404426559356,
      "grad_norm": 0.9858192801475525,
      "learning_rate": 0.00013866586175671596,
      "loss": 0.2693,
      "step": 15245
    },
    {
      "epoch": 30.676056338028168,
      "grad_norm": 0.8187062740325928,
      "learning_rate": 0.0001386618372069625,
      "loss": 0.247,
      "step": 15246
    },
    {
      "epoch": 30.678068410462778,
      "grad_norm": 0.8384768962860107,
      "learning_rate": 0.00013865781265720898,
      "loss": 0.2515,
      "step": 15247
    },
    {
      "epoch": 30.680080482897385,
      "grad_norm": 0.8687986731529236,
      "learning_rate": 0.0001386537881074555,
      "loss": 0.2658,
      "step": 15248
    },
    {
      "epoch": 30.68209255533199,
      "grad_norm": 0.8739796280860901,
      "learning_rate": 0.00013864976355770198,
      "loss": 0.2652,
      "step": 15249
    },
    {
      "epoch": 30.6841046277666,
      "grad_norm": 0.9277433156967163,
      "learning_rate": 0.0001386457390079485,
      "loss": 0.2611,
      "step": 15250
    },
    {
      "epoch": 30.686116700201207,
      "grad_norm": 0.8425954580307007,
      "learning_rate": 0.000138641714458195,
      "loss": 0.2416,
      "step": 15251
    },
    {
      "epoch": 30.688128772635814,
      "grad_norm": 0.8901947736740112,
      "learning_rate": 0.00013863768990844151,
      "loss": 0.2744,
      "step": 15252
    },
    {
      "epoch": 30.690140845070424,
      "grad_norm": 0.9133275151252747,
      "learning_rate": 0.000138633665358688,
      "loss": 0.2771,
      "step": 15253
    },
    {
      "epoch": 30.69215291750503,
      "grad_norm": 0.9262428879737854,
      "learning_rate": 0.0001386296408089345,
      "loss": 0.2518,
      "step": 15254
    },
    {
      "epoch": 30.694164989939637,
      "grad_norm": 0.9623990058898926,
      "learning_rate": 0.000138625616259181,
      "loss": 0.2686,
      "step": 15255
    },
    {
      "epoch": 30.696177062374247,
      "grad_norm": 0.905800461769104,
      "learning_rate": 0.00013862159170942754,
      "loss": 0.2612,
      "step": 15256
    },
    {
      "epoch": 30.698189134808853,
      "grad_norm": 0.9070735573768616,
      "learning_rate": 0.00013861756715967402,
      "loss": 0.265,
      "step": 15257
    },
    {
      "epoch": 30.70020120724346,
      "grad_norm": 0.8901686072349548,
      "learning_rate": 0.00013861354260992053,
      "loss": 0.2718,
      "step": 15258
    },
    {
      "epoch": 30.70221327967807,
      "grad_norm": 0.9282575845718384,
      "learning_rate": 0.00013860951806016702,
      "loss": 0.2575,
      "step": 15259
    },
    {
      "epoch": 30.704225352112676,
      "grad_norm": 0.8710236549377441,
      "learning_rate": 0.00013860549351041353,
      "loss": 0.2514,
      "step": 15260
    },
    {
      "epoch": 30.706237424547282,
      "grad_norm": 0.8880956172943115,
      "learning_rate": 0.00013860146896066004,
      "loss": 0.254,
      "step": 15261
    },
    {
      "epoch": 30.708249496981892,
      "grad_norm": 0.8577941656112671,
      "learning_rate": 0.00013859744441090653,
      "loss": 0.2609,
      "step": 15262
    },
    {
      "epoch": 30.7102615694165,
      "grad_norm": 0.8694925904273987,
      "learning_rate": 0.00013859341986115304,
      "loss": 0.2619,
      "step": 15263
    },
    {
      "epoch": 30.712273641851105,
      "grad_norm": 0.9435506463050842,
      "learning_rate": 0.00013858939531139955,
      "loss": 0.2684,
      "step": 15264
    },
    {
      "epoch": 30.714285714285715,
      "grad_norm": 0.8351605534553528,
      "learning_rate": 0.00013858537076164604,
      "loss": 0.2428,
      "step": 15265
    },
    {
      "epoch": 30.71629778672032,
      "grad_norm": 0.8596640229225159,
      "learning_rate": 0.00013858134621189255,
      "loss": 0.2508,
      "step": 15266
    },
    {
      "epoch": 30.718309859154928,
      "grad_norm": 0.939323902130127,
      "learning_rate": 0.00013857732166213906,
      "loss": 0.2563,
      "step": 15267
    },
    {
      "epoch": 30.720321931589538,
      "grad_norm": 0.8893744349479675,
      "learning_rate": 0.00013857329711238555,
      "loss": 0.2739,
      "step": 15268
    },
    {
      "epoch": 30.722334004024145,
      "grad_norm": 0.8267436027526855,
      "learning_rate": 0.00013856927256263206,
      "loss": 0.2464,
      "step": 15269
    },
    {
      "epoch": 30.72434607645875,
      "grad_norm": 0.8369890451431274,
      "learning_rate": 0.00013856524801287854,
      "loss": 0.2678,
      "step": 15270
    },
    {
      "epoch": 30.72635814889336,
      "grad_norm": 0.8904236555099487,
      "learning_rate": 0.00013856122346312508,
      "loss": 0.2774,
      "step": 15271
    },
    {
      "epoch": 30.728370221327967,
      "grad_norm": 0.9106718897819519,
      "learning_rate": 0.00013855719891337157,
      "loss": 0.2637,
      "step": 15272
    },
    {
      "epoch": 30.730382293762574,
      "grad_norm": 0.9017991423606873,
      "learning_rate": 0.00013855317436361808,
      "loss": 0.2637,
      "step": 15273
    },
    {
      "epoch": 30.732394366197184,
      "grad_norm": 0.8550087213516235,
      "learning_rate": 0.00013854914981386457,
      "loss": 0.2471,
      "step": 15274
    },
    {
      "epoch": 30.73440643863179,
      "grad_norm": 0.8573572635650635,
      "learning_rate": 0.00013854512526411108,
      "loss": 0.2515,
      "step": 15275
    },
    {
      "epoch": 30.736418511066397,
      "grad_norm": 0.8775286674499512,
      "learning_rate": 0.0001385411007143576,
      "loss": 0.2609,
      "step": 15276
    },
    {
      "epoch": 30.738430583501007,
      "grad_norm": 0.9237819910049438,
      "learning_rate": 0.0001385370761646041,
      "loss": 0.2746,
      "step": 15277
    },
    {
      "epoch": 30.740442655935613,
      "grad_norm": 0.8911978602409363,
      "learning_rate": 0.0001385330516148506,
      "loss": 0.2653,
      "step": 15278
    },
    {
      "epoch": 30.74245472837022,
      "grad_norm": 0.9428390860557556,
      "learning_rate": 0.0001385290270650971,
      "loss": 0.2607,
      "step": 15279
    },
    {
      "epoch": 30.74446680080483,
      "grad_norm": 0.8856145143508911,
      "learning_rate": 0.00013852500251534358,
      "loss": 0.2764,
      "step": 15280
    },
    {
      "epoch": 30.746478873239436,
      "grad_norm": 0.8919804096221924,
      "learning_rate": 0.00013852097796559012,
      "loss": 0.2452,
      "step": 15281
    },
    {
      "epoch": 30.748490945674043,
      "grad_norm": 0.9228193759918213,
      "learning_rate": 0.0001385169534158366,
      "loss": 0.2751,
      "step": 15282
    },
    {
      "epoch": 30.750503018108652,
      "grad_norm": 0.9537957310676575,
      "learning_rate": 0.00013851292886608312,
      "loss": 0.2547,
      "step": 15283
    },
    {
      "epoch": 30.75251509054326,
      "grad_norm": 0.8913161158561707,
      "learning_rate": 0.0001385089043163296,
      "loss": 0.2617,
      "step": 15284
    },
    {
      "epoch": 30.754527162977865,
      "grad_norm": 0.9124733805656433,
      "learning_rate": 0.00013850487976657612,
      "loss": 0.2711,
      "step": 15285
    },
    {
      "epoch": 30.756539235412475,
      "grad_norm": 0.8597308993339539,
      "learning_rate": 0.00013850085521682263,
      "loss": 0.2425,
      "step": 15286
    },
    {
      "epoch": 30.758551307847082,
      "grad_norm": 0.8439890146255493,
      "learning_rate": 0.00013849683066706914,
      "loss": 0.2528,
      "step": 15287
    },
    {
      "epoch": 30.760563380281692,
      "grad_norm": 0.8850492238998413,
      "learning_rate": 0.00013849280611731563,
      "loss": 0.2551,
      "step": 15288
    },
    {
      "epoch": 30.7625754527163,
      "grad_norm": 0.9153159856796265,
      "learning_rate": 0.00013848878156756214,
      "loss": 0.2713,
      "step": 15289
    },
    {
      "epoch": 30.764587525150905,
      "grad_norm": 0.8479959964752197,
      "learning_rate": 0.00013848475701780863,
      "loss": 0.261,
      "step": 15290
    },
    {
      "epoch": 30.766599597585515,
      "grad_norm": 0.9104225635528564,
      "learning_rate": 0.00013848073246805516,
      "loss": 0.2914,
      "step": 15291
    },
    {
      "epoch": 30.76861167002012,
      "grad_norm": 0.8173155784606934,
      "learning_rate": 0.00013847670791830165,
      "loss": 0.2429,
      "step": 15292
    },
    {
      "epoch": 30.770623742454728,
      "grad_norm": 0.9036691784858704,
      "learning_rate": 0.00013847268336854816,
      "loss": 0.2805,
      "step": 15293
    },
    {
      "epoch": 30.772635814889338,
      "grad_norm": 0.9612415432929993,
      "learning_rate": 0.00013846865881879465,
      "loss": 0.286,
      "step": 15294
    },
    {
      "epoch": 30.774647887323944,
      "grad_norm": 0.8852028846740723,
      "learning_rate": 0.00013846463426904116,
      "loss": 0.2731,
      "step": 15295
    },
    {
      "epoch": 30.77665995975855,
      "grad_norm": 0.8871769309043884,
      "learning_rate": 0.00013846060971928767,
      "loss": 0.2916,
      "step": 15296
    },
    {
      "epoch": 30.77867203219316,
      "grad_norm": 0.884480893611908,
      "learning_rate": 0.00013845658516953416,
      "loss": 0.2651,
      "step": 15297
    },
    {
      "epoch": 30.780684104627767,
      "grad_norm": 0.8954480886459351,
      "learning_rate": 0.00013845256061978067,
      "loss": 0.2912,
      "step": 15298
    },
    {
      "epoch": 30.782696177062373,
      "grad_norm": 0.9250061511993408,
      "learning_rate": 0.00013844853607002718,
      "loss": 0.2893,
      "step": 15299
    },
    {
      "epoch": 30.784708249496983,
      "grad_norm": 0.8849324584007263,
      "learning_rate": 0.00013844451152027367,
      "loss": 0.2671,
      "step": 15300
    },
    {
      "epoch": 30.78672032193159,
      "grad_norm": 0.8352320790290833,
      "learning_rate": 0.00013844048697052018,
      "loss": 0.2606,
      "step": 15301
    },
    {
      "epoch": 30.788732394366196,
      "grad_norm": 0.93073970079422,
      "learning_rate": 0.0001384364624207667,
      "loss": 0.2849,
      "step": 15302
    },
    {
      "epoch": 30.790744466800806,
      "grad_norm": 0.9220161437988281,
      "learning_rate": 0.00013843243787101318,
      "loss": 0.2655,
      "step": 15303
    },
    {
      "epoch": 30.792756539235413,
      "grad_norm": 0.9163044691085815,
      "learning_rate": 0.0001384284133212597,
      "loss": 0.2661,
      "step": 15304
    },
    {
      "epoch": 30.79476861167002,
      "grad_norm": 0.8532264828681946,
      "learning_rate": 0.00013842438877150617,
      "loss": 0.2717,
      "step": 15305
    },
    {
      "epoch": 30.79678068410463,
      "grad_norm": 0.8409522771835327,
      "learning_rate": 0.0001384203642217527,
      "loss": 0.2742,
      "step": 15306
    },
    {
      "epoch": 30.798792756539235,
      "grad_norm": 0.8695988059043884,
      "learning_rate": 0.0001384163396719992,
      "loss": 0.2496,
      "step": 15307
    },
    {
      "epoch": 30.800804828973842,
      "grad_norm": 0.8950244784355164,
      "learning_rate": 0.0001384123151222457,
      "loss": 0.2786,
      "step": 15308
    },
    {
      "epoch": 30.802816901408452,
      "grad_norm": 0.8963193297386169,
      "learning_rate": 0.0001384082905724922,
      "loss": 0.2654,
      "step": 15309
    },
    {
      "epoch": 30.80482897384306,
      "grad_norm": 0.918635368347168,
      "learning_rate": 0.0001384042660227387,
      "loss": 0.2746,
      "step": 15310
    },
    {
      "epoch": 30.806841046277665,
      "grad_norm": 0.9354287981987,
      "learning_rate": 0.00013840024147298522,
      "loss": 0.2674,
      "step": 15311
    },
    {
      "epoch": 30.808853118712275,
      "grad_norm": 0.8810487389564514,
      "learning_rate": 0.00013839621692323173,
      "loss": 0.2613,
      "step": 15312
    },
    {
      "epoch": 30.81086519114688,
      "grad_norm": 0.8917641043663025,
      "learning_rate": 0.00013839219237347822,
      "loss": 0.2664,
      "step": 15313
    },
    {
      "epoch": 30.812877263581488,
      "grad_norm": 0.8793121576309204,
      "learning_rate": 0.00013838816782372473,
      "loss": 0.2606,
      "step": 15314
    },
    {
      "epoch": 30.814889336016098,
      "grad_norm": 0.8780310153961182,
      "learning_rate": 0.00013838414327397121,
      "loss": 0.2722,
      "step": 15315
    },
    {
      "epoch": 30.816901408450704,
      "grad_norm": 0.8703408241271973,
      "learning_rate": 0.00013838011872421775,
      "loss": 0.27,
      "step": 15316
    },
    {
      "epoch": 30.81891348088531,
      "grad_norm": 0.8976954221725464,
      "learning_rate": 0.00013837609417446424,
      "loss": 0.2844,
      "step": 15317
    },
    {
      "epoch": 30.82092555331992,
      "grad_norm": 0.893000066280365,
      "learning_rate": 0.00013837206962471075,
      "loss": 0.2733,
      "step": 15318
    },
    {
      "epoch": 30.822937625754527,
      "grad_norm": 0.8582830429077148,
      "learning_rate": 0.00013836804507495724,
      "loss": 0.2647,
      "step": 15319
    },
    {
      "epoch": 30.824949698189133,
      "grad_norm": 0.916539192199707,
      "learning_rate": 0.00013836402052520375,
      "loss": 0.2506,
      "step": 15320
    },
    {
      "epoch": 30.826961770623743,
      "grad_norm": 0.9174315929412842,
      "learning_rate": 0.00013835999597545026,
      "loss": 0.2761,
      "step": 15321
    },
    {
      "epoch": 30.82897384305835,
      "grad_norm": 0.8406030535697937,
      "learning_rate": 0.00013835597142569677,
      "loss": 0.2609,
      "step": 15322
    },
    {
      "epoch": 30.830985915492956,
      "grad_norm": 0.8853709101676941,
      "learning_rate": 0.00013835194687594326,
      "loss": 0.263,
      "step": 15323
    },
    {
      "epoch": 30.832997987927566,
      "grad_norm": 0.8417174816131592,
      "learning_rate": 0.00013834792232618977,
      "loss": 0.2717,
      "step": 15324
    },
    {
      "epoch": 30.835010060362173,
      "grad_norm": 0.8442723751068115,
      "learning_rate": 0.00013834389777643625,
      "loss": 0.2847,
      "step": 15325
    },
    {
      "epoch": 30.83702213279678,
      "grad_norm": 0.9172852635383606,
      "learning_rate": 0.0001383398732266828,
      "loss": 0.2567,
      "step": 15326
    },
    {
      "epoch": 30.83903420523139,
      "grad_norm": 0.9157546162605286,
      "learning_rate": 0.00013833584867692928,
      "loss": 0.2967,
      "step": 15327
    },
    {
      "epoch": 30.841046277665995,
      "grad_norm": 0.8188475370407104,
      "learning_rate": 0.0001383318241271758,
      "loss": 0.2651,
      "step": 15328
    },
    {
      "epoch": 30.843058350100602,
      "grad_norm": 0.9496520757675171,
      "learning_rate": 0.00013832779957742228,
      "loss": 0.2774,
      "step": 15329
    },
    {
      "epoch": 30.845070422535212,
      "grad_norm": 0.9052912592887878,
      "learning_rate": 0.0001383237750276688,
      "loss": 0.2639,
      "step": 15330
    },
    {
      "epoch": 30.84708249496982,
      "grad_norm": 0.9558360576629639,
      "learning_rate": 0.0001383197504779153,
      "loss": 0.2876,
      "step": 15331
    },
    {
      "epoch": 30.84909456740443,
      "grad_norm": 0.8943631649017334,
      "learning_rate": 0.00013831572592816179,
      "loss": 0.2814,
      "step": 15332
    },
    {
      "epoch": 30.851106639839035,
      "grad_norm": 0.9410324692726135,
      "learning_rate": 0.0001383117013784083,
      "loss": 0.2572,
      "step": 15333
    },
    {
      "epoch": 30.85311871227364,
      "grad_norm": 0.9052966833114624,
      "learning_rate": 0.0001383076768286548,
      "loss": 0.2624,
      "step": 15334
    },
    {
      "epoch": 30.85513078470825,
      "grad_norm": 0.868314266204834,
      "learning_rate": 0.0001383036522789013,
      "loss": 0.2736,
      "step": 15335
    },
    {
      "epoch": 30.857142857142858,
      "grad_norm": 0.9306198358535767,
      "learning_rate": 0.0001382996277291478,
      "loss": 0.2733,
      "step": 15336
    },
    {
      "epoch": 30.859154929577464,
      "grad_norm": 0.8679391741752625,
      "learning_rate": 0.00013829560317939432,
      "loss": 0.2629,
      "step": 15337
    },
    {
      "epoch": 30.861167002012074,
      "grad_norm": 0.9334732890129089,
      "learning_rate": 0.0001382915786296408,
      "loss": 0.2707,
      "step": 15338
    },
    {
      "epoch": 30.86317907444668,
      "grad_norm": 0.9255499839782715,
      "learning_rate": 0.00013828755407988732,
      "loss": 0.2527,
      "step": 15339
    },
    {
      "epoch": 30.865191146881287,
      "grad_norm": 0.8814858794212341,
      "learning_rate": 0.0001382835295301338,
      "loss": 0.2469,
      "step": 15340
    },
    {
      "epoch": 30.867203219315897,
      "grad_norm": 0.8895805478096008,
      "learning_rate": 0.00013827950498038034,
      "loss": 0.2707,
      "step": 15341
    },
    {
      "epoch": 30.869215291750503,
      "grad_norm": 0.8715097904205322,
      "learning_rate": 0.00013827548043062683,
      "loss": 0.2523,
      "step": 15342
    },
    {
      "epoch": 30.87122736418511,
      "grad_norm": 0.9439982175827026,
      "learning_rate": 0.00013827145588087334,
      "loss": 0.2738,
      "step": 15343
    },
    {
      "epoch": 30.87323943661972,
      "grad_norm": 0.8953227400779724,
      "learning_rate": 0.00013826743133111982,
      "loss": 0.2661,
      "step": 15344
    },
    {
      "epoch": 30.875251509054326,
      "grad_norm": 0.9716477394104004,
      "learning_rate": 0.00013826340678136634,
      "loss": 0.3067,
      "step": 15345
    },
    {
      "epoch": 30.877263581488933,
      "grad_norm": 0.8491317629814148,
      "learning_rate": 0.00013825938223161285,
      "loss": 0.2463,
      "step": 15346
    },
    {
      "epoch": 30.879275653923543,
      "grad_norm": 0.9187295436859131,
      "learning_rate": 0.00013825535768185936,
      "loss": 0.2728,
      "step": 15347
    },
    {
      "epoch": 30.88128772635815,
      "grad_norm": 0.8837389945983887,
      "learning_rate": 0.00013825133313210585,
      "loss": 0.2609,
      "step": 15348
    },
    {
      "epoch": 30.883299798792756,
      "grad_norm": 0.9168703556060791,
      "learning_rate": 0.00013824730858235236,
      "loss": 0.2671,
      "step": 15349
    },
    {
      "epoch": 30.885311871227366,
      "grad_norm": 0.8508613109588623,
      "learning_rate": 0.00013824328403259884,
      "loss": 0.2573,
      "step": 15350
    },
    {
      "epoch": 30.887323943661972,
      "grad_norm": 0.9164894819259644,
      "learning_rate": 0.00013823925948284538,
      "loss": 0.2855,
      "step": 15351
    },
    {
      "epoch": 30.88933601609658,
      "grad_norm": 0.8841699361801147,
      "learning_rate": 0.00013823523493309187,
      "loss": 0.2735,
      "step": 15352
    },
    {
      "epoch": 30.89134808853119,
      "grad_norm": 0.9523028135299683,
      "learning_rate": 0.00013823121038333838,
      "loss": 0.2887,
      "step": 15353
    },
    {
      "epoch": 30.893360160965795,
      "grad_norm": 0.8871446251869202,
      "learning_rate": 0.00013822718583358487,
      "loss": 0.275,
      "step": 15354
    },
    {
      "epoch": 30.8953722334004,
      "grad_norm": 0.8576982021331787,
      "learning_rate": 0.00013822316128383138,
      "loss": 0.2282,
      "step": 15355
    },
    {
      "epoch": 30.89738430583501,
      "grad_norm": 0.9273785948753357,
      "learning_rate": 0.0001382191367340779,
      "loss": 0.2612,
      "step": 15356
    },
    {
      "epoch": 30.899396378269618,
      "grad_norm": 0.8878223896026611,
      "learning_rate": 0.0001382151121843244,
      "loss": 0.271,
      "step": 15357
    },
    {
      "epoch": 30.901408450704224,
      "grad_norm": 0.9213072657585144,
      "learning_rate": 0.0001382110876345709,
      "loss": 0.2615,
      "step": 15358
    },
    {
      "epoch": 30.903420523138834,
      "grad_norm": 0.858981192111969,
      "learning_rate": 0.0001382070630848174,
      "loss": 0.2793,
      "step": 15359
    },
    {
      "epoch": 30.90543259557344,
      "grad_norm": 0.889704167842865,
      "learning_rate": 0.00013820303853506388,
      "loss": 0.263,
      "step": 15360
    },
    {
      "epoch": 30.907444668008047,
      "grad_norm": 0.9196889400482178,
      "learning_rate": 0.00013819901398531042,
      "loss": 0.262,
      "step": 15361
    },
    {
      "epoch": 30.909456740442657,
      "grad_norm": 0.9175558090209961,
      "learning_rate": 0.0001381949894355569,
      "loss": 0.2889,
      "step": 15362
    },
    {
      "epoch": 30.911468812877263,
      "grad_norm": 0.9404804706573486,
      "learning_rate": 0.00013819096488580342,
      "loss": 0.2702,
      "step": 15363
    },
    {
      "epoch": 30.91348088531187,
      "grad_norm": 0.8852161765098572,
      "learning_rate": 0.0001381869403360499,
      "loss": 0.2841,
      "step": 15364
    },
    {
      "epoch": 30.91549295774648,
      "grad_norm": 0.8716940879821777,
      "learning_rate": 0.00013818291578629642,
      "loss": 0.2495,
      "step": 15365
    },
    {
      "epoch": 30.917505030181086,
      "grad_norm": 0.8797226548194885,
      "learning_rate": 0.00013817889123654293,
      "loss": 0.2562,
      "step": 15366
    },
    {
      "epoch": 30.919517102615693,
      "grad_norm": 0.8899399042129517,
      "learning_rate": 0.00013817486668678942,
      "loss": 0.2823,
      "step": 15367
    },
    {
      "epoch": 30.921529175050303,
      "grad_norm": 0.844493567943573,
      "learning_rate": 0.00013817084213703593,
      "loss": 0.276,
      "step": 15368
    },
    {
      "epoch": 30.92354124748491,
      "grad_norm": 0.892326831817627,
      "learning_rate": 0.00013816681758728244,
      "loss": 0.2775,
      "step": 15369
    },
    {
      "epoch": 30.925553319919516,
      "grad_norm": 0.8854213953018188,
      "learning_rate": 0.00013816279303752893,
      "loss": 0.2608,
      "step": 15370
    },
    {
      "epoch": 30.927565392354126,
      "grad_norm": 0.893725574016571,
      "learning_rate": 0.00013815876848777544,
      "loss": 0.269,
      "step": 15371
    },
    {
      "epoch": 30.929577464788732,
      "grad_norm": 0.8433688879013062,
      "learning_rate": 0.00013815474393802195,
      "loss": 0.2629,
      "step": 15372
    },
    {
      "epoch": 30.93158953722334,
      "grad_norm": 0.9050604104995728,
      "learning_rate": 0.00013815071938826843,
      "loss": 0.2764,
      "step": 15373
    },
    {
      "epoch": 30.93360160965795,
      "grad_norm": 0.8849073052406311,
      "learning_rate": 0.00013814669483851495,
      "loss": 0.276,
      "step": 15374
    },
    {
      "epoch": 30.935613682092555,
      "grad_norm": 0.9104459285736084,
      "learning_rate": 0.00013814267028876143,
      "loss": 0.2753,
      "step": 15375
    },
    {
      "epoch": 30.93762575452716,
      "grad_norm": 0.8690068125724792,
      "learning_rate": 0.00013813864573900797,
      "loss": 0.2524,
      "step": 15376
    },
    {
      "epoch": 30.93963782696177,
      "grad_norm": 0.8829755187034607,
      "learning_rate": 0.00013813462118925446,
      "loss": 0.2825,
      "step": 15377
    },
    {
      "epoch": 30.941649899396378,
      "grad_norm": 0.8578284978866577,
      "learning_rate": 0.00013813059663950097,
      "loss": 0.2741,
      "step": 15378
    },
    {
      "epoch": 30.943661971830984,
      "grad_norm": 0.8912954926490784,
      "learning_rate": 0.00013812657208974745,
      "loss": 0.26,
      "step": 15379
    },
    {
      "epoch": 30.945674044265594,
      "grad_norm": 0.8688027262687683,
      "learning_rate": 0.00013812254753999397,
      "loss": 0.2652,
      "step": 15380
    },
    {
      "epoch": 30.9476861167002,
      "grad_norm": 0.8798485994338989,
      "learning_rate": 0.00013811852299024048,
      "loss": 0.2601,
      "step": 15381
    },
    {
      "epoch": 30.949698189134807,
      "grad_norm": 0.8960699439048767,
      "learning_rate": 0.000138114498440487,
      "loss": 0.2659,
      "step": 15382
    },
    {
      "epoch": 30.951710261569417,
      "grad_norm": 0.8268945813179016,
      "learning_rate": 0.00013811047389073348,
      "loss": 0.2408,
      "step": 15383
    },
    {
      "epoch": 30.953722334004024,
      "grad_norm": 0.9347895383834839,
      "learning_rate": 0.00013810644934098,
      "loss": 0.2831,
      "step": 15384
    },
    {
      "epoch": 30.955734406438633,
      "grad_norm": 0.9629115462303162,
      "learning_rate": 0.00013810242479122647,
      "loss": 0.278,
      "step": 15385
    },
    {
      "epoch": 30.95774647887324,
      "grad_norm": 0.8176223039627075,
      "learning_rate": 0.000138098400241473,
      "loss": 0.2591,
      "step": 15386
    },
    {
      "epoch": 30.959758551307846,
      "grad_norm": 0.8948440551757812,
      "learning_rate": 0.0001380943756917195,
      "loss": 0.2483,
      "step": 15387
    },
    {
      "epoch": 30.961770623742456,
      "grad_norm": 0.8520621657371521,
      "learning_rate": 0.000138090351141966,
      "loss": 0.2676,
      "step": 15388
    },
    {
      "epoch": 30.963782696177063,
      "grad_norm": 0.8972865343093872,
      "learning_rate": 0.0001380863265922125,
      "loss": 0.285,
      "step": 15389
    },
    {
      "epoch": 30.96579476861167,
      "grad_norm": 0.8812605738639832,
      "learning_rate": 0.000138082302042459,
      "loss": 0.2821,
      "step": 15390
    },
    {
      "epoch": 30.96780684104628,
      "grad_norm": 0.8667811751365662,
      "learning_rate": 0.00013807827749270552,
      "loss": 0.2707,
      "step": 15391
    },
    {
      "epoch": 30.969818913480886,
      "grad_norm": 0.8808433413505554,
      "learning_rate": 0.00013807425294295203,
      "loss": 0.2831,
      "step": 15392
    },
    {
      "epoch": 30.971830985915492,
      "grad_norm": 0.867606520652771,
      "learning_rate": 0.00013807022839319852,
      "loss": 0.2439,
      "step": 15393
    },
    {
      "epoch": 30.973843058350102,
      "grad_norm": 0.9136475324630737,
      "learning_rate": 0.00013806620384344503,
      "loss": 0.2675,
      "step": 15394
    },
    {
      "epoch": 30.97585513078471,
      "grad_norm": 0.921697199344635,
      "learning_rate": 0.00013806217929369151,
      "loss": 0.2881,
      "step": 15395
    },
    {
      "epoch": 30.977867203219315,
      "grad_norm": 0.9178805351257324,
      "learning_rate": 0.00013805815474393805,
      "loss": 0.2896,
      "step": 15396
    },
    {
      "epoch": 30.979879275653925,
      "grad_norm": 0.8678057193756104,
      "learning_rate": 0.00013805413019418454,
      "loss": 0.2445,
      "step": 15397
    },
    {
      "epoch": 30.98189134808853,
      "grad_norm": 0.8713877201080322,
      "learning_rate": 0.00013805010564443105,
      "loss": 0.2819,
      "step": 15398
    },
    {
      "epoch": 30.983903420523138,
      "grad_norm": 0.851630687713623,
      "learning_rate": 0.00013804608109467754,
      "loss": 0.2742,
      "step": 15399
    },
    {
      "epoch": 30.985915492957748,
      "grad_norm": 0.86686772108078,
      "learning_rate": 0.00013804205654492405,
      "loss": 0.2643,
      "step": 15400
    },
    {
      "epoch": 30.987927565392354,
      "grad_norm": 0.8713510036468506,
      "learning_rate": 0.00013803803199517053,
      "loss": 0.2534,
      "step": 15401
    },
    {
      "epoch": 30.98993963782696,
      "grad_norm": 0.8949283957481384,
      "learning_rate": 0.00013803400744541705,
      "loss": 0.2546,
      "step": 15402
    },
    {
      "epoch": 30.99195171026157,
      "grad_norm": 0.9092631936073303,
      "learning_rate": 0.00013802998289566356,
      "loss": 0.2603,
      "step": 15403
    },
    {
      "epoch": 30.993963782696177,
      "grad_norm": 0.8767805099487305,
      "learning_rate": 0.00013802595834591004,
      "loss": 0.255,
      "step": 15404
    },
    {
      "epoch": 30.995975855130784,
      "grad_norm": 0.9002564549446106,
      "learning_rate": 0.00013802193379615655,
      "loss": 0.2806,
      "step": 15405
    },
    {
      "epoch": 30.997987927565394,
      "grad_norm": 0.8593063354492188,
      "learning_rate": 0.00013801790924640307,
      "loss": 0.2546,
      "step": 15406
    },
    {
      "epoch": 31.0,
      "grad_norm": 0.9073387980461121,
      "learning_rate": 0.00013801388469664958,
      "loss": 0.2821,
      "step": 15407
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.3186758756637573,
      "eval_runtime": 49.8161,
      "eval_samples_per_second": 19.913,
      "eval_steps_per_second": 2.489,
      "step": 15407
    },
    {
      "epoch": 31.002012072434606,
      "grad_norm": 0.7808115482330322,
      "learning_rate": 0.00013800986014689606,
      "loss": 0.2105,
      "step": 15408
    },
    {
      "epoch": 31.004024144869216,
      "grad_norm": 0.7629406452178955,
      "learning_rate": 0.00013800583559714258,
      "loss": 0.1957,
      "step": 15409
    },
    {
      "epoch": 31.006036217303823,
      "grad_norm": 0.8668080568313599,
      "learning_rate": 0.00013800181104738906,
      "loss": 0.2306,
      "step": 15410
    },
    {
      "epoch": 31.00804828973843,
      "grad_norm": 0.7577470541000366,
      "learning_rate": 0.00013799778649763557,
      "loss": 0.1932,
      "step": 15411
    },
    {
      "epoch": 31.01006036217304,
      "grad_norm": 0.8426815867424011,
      "learning_rate": 0.00013799376194788209,
      "loss": 0.2061,
      "step": 15412
    },
    {
      "epoch": 31.012072434607646,
      "grad_norm": 0.7964673042297363,
      "learning_rate": 0.0001379897373981286,
      "loss": 0.2052,
      "step": 15413
    },
    {
      "epoch": 31.014084507042252,
      "grad_norm": 0.791667103767395,
      "learning_rate": 0.00013798571284837508,
      "loss": 0.2027,
      "step": 15414
    },
    {
      "epoch": 31.016096579476862,
      "grad_norm": 0.7541084289550781,
      "learning_rate": 0.0001379816882986216,
      "loss": 0.1951,
      "step": 15415
    },
    {
      "epoch": 31.01810865191147,
      "grad_norm": 0.7511793375015259,
      "learning_rate": 0.00013797766374886808,
      "loss": 0.2071,
      "step": 15416
    },
    {
      "epoch": 31.020120724346075,
      "grad_norm": 0.7871776819229126,
      "learning_rate": 0.00013797363919911462,
      "loss": 0.198,
      "step": 15417
    },
    {
      "epoch": 31.022132796780685,
      "grad_norm": 0.826961100101471,
      "learning_rate": 0.0001379696146493611,
      "loss": 0.2277,
      "step": 15418
    },
    {
      "epoch": 31.02414486921529,
      "grad_norm": 0.8373450040817261,
      "learning_rate": 0.00013796559009960762,
      "loss": 0.2224,
      "step": 15419
    },
    {
      "epoch": 31.026156941649898,
      "grad_norm": 0.7188190817832947,
      "learning_rate": 0.0001379615655498541,
      "loss": 0.2151,
      "step": 15420
    },
    {
      "epoch": 31.028169014084508,
      "grad_norm": 0.8055288791656494,
      "learning_rate": 0.00013795754100010061,
      "loss": 0.2094,
      "step": 15421
    },
    {
      "epoch": 31.030181086519114,
      "grad_norm": 0.7914354801177979,
      "learning_rate": 0.00013795351645034713,
      "loss": 0.202,
      "step": 15422
    },
    {
      "epoch": 31.03219315895372,
      "grad_norm": 0.8649851083755493,
      "learning_rate": 0.00013794949190059364,
      "loss": 0.2228,
      "step": 15423
    },
    {
      "epoch": 31.03420523138833,
      "grad_norm": 0.890779435634613,
      "learning_rate": 0.00013794546735084012,
      "loss": 0.2131,
      "step": 15424
    },
    {
      "epoch": 31.036217303822937,
      "grad_norm": 0.8040229678153992,
      "learning_rate": 0.00013794144280108664,
      "loss": 0.1924,
      "step": 15425
    },
    {
      "epoch": 31.038229376257544,
      "grad_norm": 0.7434244751930237,
      "learning_rate": 0.00013793741825133312,
      "loss": 0.193,
      "step": 15426
    },
    {
      "epoch": 31.040241448692154,
      "grad_norm": 0.7738841772079468,
      "learning_rate": 0.00013793339370157966,
      "loss": 0.2009,
      "step": 15427
    },
    {
      "epoch": 31.04225352112676,
      "grad_norm": 0.8029769062995911,
      "learning_rate": 0.00013792936915182615,
      "loss": 0.2298,
      "step": 15428
    },
    {
      "epoch": 31.044265593561367,
      "grad_norm": 0.7602084279060364,
      "learning_rate": 0.00013792534460207266,
      "loss": 0.2116,
      "step": 15429
    },
    {
      "epoch": 31.046277665995976,
      "grad_norm": 0.7667027115821838,
      "learning_rate": 0.00013792132005231914,
      "loss": 0.2105,
      "step": 15430
    },
    {
      "epoch": 31.048289738430583,
      "grad_norm": 0.8190681338310242,
      "learning_rate": 0.00013791729550256566,
      "loss": 0.2183,
      "step": 15431
    },
    {
      "epoch": 31.050301810865193,
      "grad_norm": 0.7993459701538086,
      "learning_rate": 0.00013791327095281217,
      "loss": 0.2231,
      "step": 15432
    },
    {
      "epoch": 31.0523138832998,
      "grad_norm": 0.7975867986679077,
      "learning_rate": 0.00013790924640305868,
      "loss": 0.1939,
      "step": 15433
    },
    {
      "epoch": 31.054325955734406,
      "grad_norm": 0.8156154751777649,
      "learning_rate": 0.00013790522185330516,
      "loss": 0.2178,
      "step": 15434
    },
    {
      "epoch": 31.056338028169016,
      "grad_norm": 0.7794713973999023,
      "learning_rate": 0.00013790119730355168,
      "loss": 0.2077,
      "step": 15435
    },
    {
      "epoch": 31.058350100603622,
      "grad_norm": 0.7592688202857971,
      "learning_rate": 0.00013789717275379816,
      "loss": 0.1968,
      "step": 15436
    },
    {
      "epoch": 31.06036217303823,
      "grad_norm": 0.7584618330001831,
      "learning_rate": 0.00013789314820404467,
      "loss": 0.2065,
      "step": 15437
    },
    {
      "epoch": 31.06237424547284,
      "grad_norm": 0.856571614742279,
      "learning_rate": 0.0001378891236542912,
      "loss": 0.2153,
      "step": 15438
    },
    {
      "epoch": 31.064386317907445,
      "grad_norm": 0.8096413612365723,
      "learning_rate": 0.00013788509910453767,
      "loss": 0.217,
      "step": 15439
    },
    {
      "epoch": 31.06639839034205,
      "grad_norm": 0.7935089468955994,
      "learning_rate": 0.00013788107455478418,
      "loss": 0.2028,
      "step": 15440
    },
    {
      "epoch": 31.06841046277666,
      "grad_norm": 0.7428464293479919,
      "learning_rate": 0.0001378770500050307,
      "loss": 0.204,
      "step": 15441
    },
    {
      "epoch": 31.070422535211268,
      "grad_norm": 0.7211765050888062,
      "learning_rate": 0.0001378730254552772,
      "loss": 0.1912,
      "step": 15442
    },
    {
      "epoch": 31.072434607645874,
      "grad_norm": 0.8347832560539246,
      "learning_rate": 0.0001378690009055237,
      "loss": 0.2309,
      "step": 15443
    },
    {
      "epoch": 31.074446680080484,
      "grad_norm": 0.7776051163673401,
      "learning_rate": 0.0001378649763557702,
      "loss": 0.1827,
      "step": 15444
    },
    {
      "epoch": 31.07645875251509,
      "grad_norm": 0.8307095170021057,
      "learning_rate": 0.0001378609518060167,
      "loss": 0.2088,
      "step": 15445
    },
    {
      "epoch": 31.078470824949697,
      "grad_norm": 0.7924419045448303,
      "learning_rate": 0.0001378569272562632,
      "loss": 0.2051,
      "step": 15446
    },
    {
      "epoch": 31.080482897384307,
      "grad_norm": 0.8402667045593262,
      "learning_rate": 0.00013785290270650972,
      "loss": 0.225,
      "step": 15447
    },
    {
      "epoch": 31.082494969818914,
      "grad_norm": 0.7863413095474243,
      "learning_rate": 0.00013784887815675623,
      "loss": 0.2112,
      "step": 15448
    },
    {
      "epoch": 31.08450704225352,
      "grad_norm": 0.7612704634666443,
      "learning_rate": 0.0001378448536070027,
      "loss": 0.2023,
      "step": 15449
    },
    {
      "epoch": 31.08651911468813,
      "grad_norm": 0.7568399310112,
      "learning_rate": 0.00013784082905724922,
      "loss": 0.1819,
      "step": 15450
    },
    {
      "epoch": 31.088531187122737,
      "grad_norm": 0.8135926127433777,
      "learning_rate": 0.0001378368045074957,
      "loss": 0.2204,
      "step": 15451
    },
    {
      "epoch": 31.090543259557343,
      "grad_norm": 0.7484936714172363,
      "learning_rate": 0.00013783277995774225,
      "loss": 0.1937,
      "step": 15452
    },
    {
      "epoch": 31.092555331991953,
      "grad_norm": 0.8002670407295227,
      "learning_rate": 0.00013782875540798873,
      "loss": 0.2135,
      "step": 15453
    },
    {
      "epoch": 31.09456740442656,
      "grad_norm": 0.8302013874053955,
      "learning_rate": 0.00013782473085823525,
      "loss": 0.2234,
      "step": 15454
    },
    {
      "epoch": 31.096579476861166,
      "grad_norm": 0.79267418384552,
      "learning_rate": 0.00013782070630848173,
      "loss": 0.1982,
      "step": 15455
    },
    {
      "epoch": 31.098591549295776,
      "grad_norm": 0.8715755939483643,
      "learning_rate": 0.00013781668175872824,
      "loss": 0.2087,
      "step": 15456
    },
    {
      "epoch": 31.100603621730382,
      "grad_norm": 0.807149350643158,
      "learning_rate": 0.00013781265720897476,
      "loss": 0.2245,
      "step": 15457
    },
    {
      "epoch": 31.10261569416499,
      "grad_norm": 0.775255024433136,
      "learning_rate": 0.00013780863265922127,
      "loss": 0.2098,
      "step": 15458
    },
    {
      "epoch": 31.1046277665996,
      "grad_norm": 0.8006113171577454,
      "learning_rate": 0.00013780460810946775,
      "loss": 0.2038,
      "step": 15459
    },
    {
      "epoch": 31.106639839034205,
      "grad_norm": 0.8809162974357605,
      "learning_rate": 0.00013780058355971427,
      "loss": 0.221,
      "step": 15460
    },
    {
      "epoch": 31.10865191146881,
      "grad_norm": 0.8296597003936768,
      "learning_rate": 0.00013779655900996075,
      "loss": 0.2139,
      "step": 15461
    },
    {
      "epoch": 31.11066398390342,
      "grad_norm": 0.8189137578010559,
      "learning_rate": 0.0001377925344602073,
      "loss": 0.2246,
      "step": 15462
    },
    {
      "epoch": 31.112676056338028,
      "grad_norm": 0.8342427611351013,
      "learning_rate": 0.00013778850991045378,
      "loss": 0.2129,
      "step": 15463
    },
    {
      "epoch": 31.114688128772634,
      "grad_norm": 0.784505307674408,
      "learning_rate": 0.0001377844853607003,
      "loss": 0.2103,
      "step": 15464
    },
    {
      "epoch": 31.116700201207244,
      "grad_norm": 0.784658670425415,
      "learning_rate": 0.00013778046081094677,
      "loss": 0.2052,
      "step": 15465
    },
    {
      "epoch": 31.11871227364185,
      "grad_norm": 0.8546441197395325,
      "learning_rate": 0.00013777643626119328,
      "loss": 0.2241,
      "step": 15466
    },
    {
      "epoch": 31.120724346076457,
      "grad_norm": 0.8832308650016785,
      "learning_rate": 0.0001377724117114398,
      "loss": 0.2198,
      "step": 15467
    },
    {
      "epoch": 31.122736418511067,
      "grad_norm": 0.8109705448150635,
      "learning_rate": 0.0001377683871616863,
      "loss": 0.215,
      "step": 15468
    },
    {
      "epoch": 31.124748490945674,
      "grad_norm": 0.8328320980072021,
      "learning_rate": 0.0001377643626119328,
      "loss": 0.2142,
      "step": 15469
    },
    {
      "epoch": 31.12676056338028,
      "grad_norm": 0.7556789517402649,
      "learning_rate": 0.0001377603380621793,
      "loss": 0.1919,
      "step": 15470
    },
    {
      "epoch": 31.12877263581489,
      "grad_norm": 0.8381881713867188,
      "learning_rate": 0.0001377563135124258,
      "loss": 0.2119,
      "step": 15471
    },
    {
      "epoch": 31.130784708249497,
      "grad_norm": 0.8424152731895447,
      "learning_rate": 0.0001377522889626723,
      "loss": 0.2237,
      "step": 15472
    },
    {
      "epoch": 31.132796780684103,
      "grad_norm": 0.8415305018424988,
      "learning_rate": 0.00013774826441291882,
      "loss": 0.2321,
      "step": 15473
    },
    {
      "epoch": 31.134808853118713,
      "grad_norm": 0.7787444591522217,
      "learning_rate": 0.0001377442398631653,
      "loss": 0.1913,
      "step": 15474
    },
    {
      "epoch": 31.13682092555332,
      "grad_norm": 0.8429976105690002,
      "learning_rate": 0.0001377402153134118,
      "loss": 0.2188,
      "step": 15475
    },
    {
      "epoch": 31.138832997987926,
      "grad_norm": 0.8804298043251038,
      "learning_rate": 0.00013773619076365833,
      "loss": 0.232,
      "step": 15476
    },
    {
      "epoch": 31.140845070422536,
      "grad_norm": 0.8285244703292847,
      "learning_rate": 0.00013773216621390484,
      "loss": 0.2301,
      "step": 15477
    },
    {
      "epoch": 31.142857142857142,
      "grad_norm": 0.8266221284866333,
      "learning_rate": 0.00013772814166415132,
      "loss": 0.2144,
      "step": 15478
    },
    {
      "epoch": 31.14486921529175,
      "grad_norm": 0.8259917497634888,
      "learning_rate": 0.00013772411711439784,
      "loss": 0.2318,
      "step": 15479
    },
    {
      "epoch": 31.14688128772636,
      "grad_norm": 0.8184750080108643,
      "learning_rate": 0.00013772009256464432,
      "loss": 0.2235,
      "step": 15480
    },
    {
      "epoch": 31.148893360160965,
      "grad_norm": 0.7786434888839722,
      "learning_rate": 0.00013771606801489083,
      "loss": 0.2087,
      "step": 15481
    },
    {
      "epoch": 31.15090543259557,
      "grad_norm": 0.888140082359314,
      "learning_rate": 0.00013771204346513734,
      "loss": 0.2126,
      "step": 15482
    },
    {
      "epoch": 31.15291750503018,
      "grad_norm": 0.8310655355453491,
      "learning_rate": 0.00013770801891538386,
      "loss": 0.2012,
      "step": 15483
    },
    {
      "epoch": 31.154929577464788,
      "grad_norm": 0.8022864460945129,
      "learning_rate": 0.00013770399436563034,
      "loss": 0.2095,
      "step": 15484
    },
    {
      "epoch": 31.156941649899398,
      "grad_norm": 0.8746486306190491,
      "learning_rate": 0.00013769996981587685,
      "loss": 0.2125,
      "step": 15485
    },
    {
      "epoch": 31.158953722334005,
      "grad_norm": 0.82984858751297,
      "learning_rate": 0.00013769594526612334,
      "loss": 0.227,
      "step": 15486
    },
    {
      "epoch": 31.16096579476861,
      "grad_norm": 0.7975910305976868,
      "learning_rate": 0.00013769192071636988,
      "loss": 0.1983,
      "step": 15487
    },
    {
      "epoch": 31.16297786720322,
      "grad_norm": 0.7772198915481567,
      "learning_rate": 0.00013768789616661636,
      "loss": 0.214,
      "step": 15488
    },
    {
      "epoch": 31.164989939637827,
      "grad_norm": 0.8116690516471863,
      "learning_rate": 0.00013768387161686288,
      "loss": 0.2104,
      "step": 15489
    },
    {
      "epoch": 31.167002012072434,
      "grad_norm": 0.8032377362251282,
      "learning_rate": 0.00013767984706710936,
      "loss": 0.2047,
      "step": 15490
    },
    {
      "epoch": 31.169014084507044,
      "grad_norm": 0.8003052473068237,
      "learning_rate": 0.00013767582251735587,
      "loss": 0.2309,
      "step": 15491
    },
    {
      "epoch": 31.17102615694165,
      "grad_norm": 0.8306785225868225,
      "learning_rate": 0.00013767179796760239,
      "loss": 0.2366,
      "step": 15492
    },
    {
      "epoch": 31.173038229376257,
      "grad_norm": 0.787874162197113,
      "learning_rate": 0.0001376677734178489,
      "loss": 0.2012,
      "step": 15493
    },
    {
      "epoch": 31.175050301810867,
      "grad_norm": 0.8426283001899719,
      "learning_rate": 0.00013766374886809538,
      "loss": 0.2113,
      "step": 15494
    },
    {
      "epoch": 31.177062374245473,
      "grad_norm": 0.8623311519622803,
      "learning_rate": 0.0001376597243183419,
      "loss": 0.2079,
      "step": 15495
    },
    {
      "epoch": 31.17907444668008,
      "grad_norm": 0.8547555804252625,
      "learning_rate": 0.00013765569976858838,
      "loss": 0.2193,
      "step": 15496
    },
    {
      "epoch": 31.18108651911469,
      "grad_norm": 0.811942994594574,
      "learning_rate": 0.00013765167521883492,
      "loss": 0.2098,
      "step": 15497
    },
    {
      "epoch": 31.183098591549296,
      "grad_norm": 0.8863232731819153,
      "learning_rate": 0.0001376476506690814,
      "loss": 0.2211,
      "step": 15498
    },
    {
      "epoch": 31.185110663983902,
      "grad_norm": 0.8247775435447693,
      "learning_rate": 0.00013764362611932792,
      "loss": 0.2147,
      "step": 15499
    },
    {
      "epoch": 31.187122736418512,
      "grad_norm": 0.8297696709632874,
      "learning_rate": 0.0001376396015695744,
      "loss": 0.2283,
      "step": 15500
    },
    {
      "epoch": 31.18913480885312,
      "grad_norm": 0.767596423625946,
      "learning_rate": 0.00013763557701982091,
      "loss": 0.2032,
      "step": 15501
    },
    {
      "epoch": 31.191146881287725,
      "grad_norm": 0.8558118939399719,
      "learning_rate": 0.00013763155247006743,
      "loss": 0.2246,
      "step": 15502
    },
    {
      "epoch": 31.193158953722335,
      "grad_norm": 0.7847992777824402,
      "learning_rate": 0.00013762752792031394,
      "loss": 0.2195,
      "step": 15503
    },
    {
      "epoch": 31.19517102615694,
      "grad_norm": 0.8174172639846802,
      "learning_rate": 0.00013762350337056042,
      "loss": 0.2045,
      "step": 15504
    },
    {
      "epoch": 31.197183098591548,
      "grad_norm": 0.8231632709503174,
      "learning_rate": 0.00013761947882080694,
      "loss": 0.2338,
      "step": 15505
    },
    {
      "epoch": 31.199195171026158,
      "grad_norm": 0.8268845677375793,
      "learning_rate": 0.00013761545427105342,
      "loss": 0.2164,
      "step": 15506
    },
    {
      "epoch": 31.201207243460765,
      "grad_norm": 0.7928465008735657,
      "learning_rate": 0.00013761142972129993,
      "loss": 0.2112,
      "step": 15507
    },
    {
      "epoch": 31.20321931589537,
      "grad_norm": 0.8350203633308411,
      "learning_rate": 0.00013760740517154645,
      "loss": 0.2199,
      "step": 15508
    },
    {
      "epoch": 31.20523138832998,
      "grad_norm": 0.8017814755439758,
      "learning_rate": 0.00013760338062179293,
      "loss": 0.2092,
      "step": 15509
    },
    {
      "epoch": 31.207243460764587,
      "grad_norm": 0.8982919454574585,
      "learning_rate": 0.00013759935607203944,
      "loss": 0.2259,
      "step": 15510
    },
    {
      "epoch": 31.209255533199194,
      "grad_norm": 0.8774532079696655,
      "learning_rate": 0.00013759533152228596,
      "loss": 0.2334,
      "step": 15511
    },
    {
      "epoch": 31.211267605633804,
      "grad_norm": 0.8434773087501526,
      "learning_rate": 0.00013759130697253247,
      "loss": 0.205,
      "step": 15512
    },
    {
      "epoch": 31.21327967806841,
      "grad_norm": 0.8741583228111267,
      "learning_rate": 0.00013758728242277895,
      "loss": 0.2405,
      "step": 15513
    },
    {
      "epoch": 31.215291750503017,
      "grad_norm": 0.8014403581619263,
      "learning_rate": 0.00013758325787302546,
      "loss": 0.2198,
      "step": 15514
    },
    {
      "epoch": 31.217303822937627,
      "grad_norm": 0.8446195721626282,
      "learning_rate": 0.00013757923332327195,
      "loss": 0.2277,
      "step": 15515
    },
    {
      "epoch": 31.219315895372233,
      "grad_norm": 0.8109134435653687,
      "learning_rate": 0.00013757520877351846,
      "loss": 0.222,
      "step": 15516
    },
    {
      "epoch": 31.22132796780684,
      "grad_norm": 0.8090015649795532,
      "learning_rate": 0.00013757118422376497,
      "loss": 0.2002,
      "step": 15517
    },
    {
      "epoch": 31.22334004024145,
      "grad_norm": 0.8099208474159241,
      "learning_rate": 0.00013756715967401149,
      "loss": 0.218,
      "step": 15518
    },
    {
      "epoch": 31.225352112676056,
      "grad_norm": 0.8381585478782654,
      "learning_rate": 0.00013756313512425797,
      "loss": 0.2222,
      "step": 15519
    },
    {
      "epoch": 31.227364185110662,
      "grad_norm": 1.0265493392944336,
      "learning_rate": 0.00013755911057450448,
      "loss": 0.2254,
      "step": 15520
    },
    {
      "epoch": 31.229376257545272,
      "grad_norm": 0.8793835043907166,
      "learning_rate": 0.00013755508602475097,
      "loss": 0.2146,
      "step": 15521
    },
    {
      "epoch": 31.23138832997988,
      "grad_norm": 0.8666786551475525,
      "learning_rate": 0.0001375510614749975,
      "loss": 0.2308,
      "step": 15522
    },
    {
      "epoch": 31.233400402414485,
      "grad_norm": 0.8559123873710632,
      "learning_rate": 0.000137547036925244,
      "loss": 0.2295,
      "step": 15523
    },
    {
      "epoch": 31.235412474849095,
      "grad_norm": 0.8222296833992004,
      "learning_rate": 0.0001375430123754905,
      "loss": 0.2248,
      "step": 15524
    },
    {
      "epoch": 31.2374245472837,
      "grad_norm": 0.8139151930809021,
      "learning_rate": 0.000137538987825737,
      "loss": 0.2236,
      "step": 15525
    },
    {
      "epoch": 31.239436619718308,
      "grad_norm": 0.8609309196472168,
      "learning_rate": 0.0001375349632759835,
      "loss": 0.2213,
      "step": 15526
    },
    {
      "epoch": 31.241448692152918,
      "grad_norm": 0.8201842308044434,
      "learning_rate": 0.00013753093872623002,
      "loss": 0.2225,
      "step": 15527
    },
    {
      "epoch": 31.243460764587525,
      "grad_norm": 0.827582061290741,
      "learning_rate": 0.00013752691417647653,
      "loss": 0.2183,
      "step": 15528
    },
    {
      "epoch": 31.24547283702213,
      "grad_norm": 0.8174412846565247,
      "learning_rate": 0.000137522889626723,
      "loss": 0.2234,
      "step": 15529
    },
    {
      "epoch": 31.24748490945674,
      "grad_norm": 0.8296369314193726,
      "learning_rate": 0.00013751886507696952,
      "loss": 0.2246,
      "step": 15530
    },
    {
      "epoch": 31.249496981891348,
      "grad_norm": 0.8698607683181763,
      "learning_rate": 0.000137514840527216,
      "loss": 0.2229,
      "step": 15531
    },
    {
      "epoch": 31.251509054325957,
      "grad_norm": 0.7959563136100769,
      "learning_rate": 0.00013751081597746255,
      "loss": 0.2154,
      "step": 15532
    },
    {
      "epoch": 31.253521126760564,
      "grad_norm": 0.8596091270446777,
      "learning_rate": 0.00013750679142770903,
      "loss": 0.2339,
      "step": 15533
    },
    {
      "epoch": 31.25553319919517,
      "grad_norm": 0.851848840713501,
      "learning_rate": 0.00013750276687795555,
      "loss": 0.2473,
      "step": 15534
    },
    {
      "epoch": 31.25754527162978,
      "grad_norm": 0.8071101903915405,
      "learning_rate": 0.00013749874232820203,
      "loss": 0.218,
      "step": 15535
    },
    {
      "epoch": 31.259557344064387,
      "grad_norm": 0.8569501638412476,
      "learning_rate": 0.00013749471777844854,
      "loss": 0.2244,
      "step": 15536
    },
    {
      "epoch": 31.261569416498993,
      "grad_norm": 0.8568503260612488,
      "learning_rate": 0.00013749069322869506,
      "loss": 0.2092,
      "step": 15537
    },
    {
      "epoch": 31.263581488933603,
      "grad_norm": 0.8499374985694885,
      "learning_rate": 0.00013748666867894157,
      "loss": 0.226,
      "step": 15538
    },
    {
      "epoch": 31.26559356136821,
      "grad_norm": 0.8082484006881714,
      "learning_rate": 0.00013748264412918805,
      "loss": 0.2062,
      "step": 15539
    },
    {
      "epoch": 31.267605633802816,
      "grad_norm": 0.8095690011978149,
      "learning_rate": 0.00013747861957943457,
      "loss": 0.2252,
      "step": 15540
    },
    {
      "epoch": 31.269617706237426,
      "grad_norm": 0.8280799984931946,
      "learning_rate": 0.00013747459502968105,
      "loss": 0.2229,
      "step": 15541
    },
    {
      "epoch": 31.271629778672033,
      "grad_norm": 0.8476287126541138,
      "learning_rate": 0.00013747057047992756,
      "loss": 0.2203,
      "step": 15542
    },
    {
      "epoch": 31.27364185110664,
      "grad_norm": 0.8360562324523926,
      "learning_rate": 0.00013746654593017408,
      "loss": 0.2267,
      "step": 15543
    },
    {
      "epoch": 31.27565392354125,
      "grad_norm": 0.8062669038772583,
      "learning_rate": 0.00013746252138042056,
      "loss": 0.2104,
      "step": 15544
    },
    {
      "epoch": 31.277665995975855,
      "grad_norm": 0.8791807889938354,
      "learning_rate": 0.00013745849683066707,
      "loss": 0.2198,
      "step": 15545
    },
    {
      "epoch": 31.279678068410462,
      "grad_norm": 0.8645393252372742,
      "learning_rate": 0.00013745447228091356,
      "loss": 0.2338,
      "step": 15546
    },
    {
      "epoch": 31.281690140845072,
      "grad_norm": 0.7993946075439453,
      "learning_rate": 0.0001374504477311601,
      "loss": 0.2094,
      "step": 15547
    },
    {
      "epoch": 31.28370221327968,
      "grad_norm": 0.8686280250549316,
      "learning_rate": 0.00013744642318140658,
      "loss": 0.2135,
      "step": 15548
    },
    {
      "epoch": 31.285714285714285,
      "grad_norm": 0.856225311756134,
      "learning_rate": 0.0001374423986316531,
      "loss": 0.2186,
      "step": 15549
    },
    {
      "epoch": 31.287726358148895,
      "grad_norm": 0.8627641201019287,
      "learning_rate": 0.00013743837408189958,
      "loss": 0.2307,
      "step": 15550
    },
    {
      "epoch": 31.2897384305835,
      "grad_norm": 0.8529717922210693,
      "learning_rate": 0.0001374343495321461,
      "loss": 0.2306,
      "step": 15551
    },
    {
      "epoch": 31.291750503018108,
      "grad_norm": 0.9155053496360779,
      "learning_rate": 0.0001374303249823926,
      "loss": 0.2394,
      "step": 15552
    },
    {
      "epoch": 31.293762575452718,
      "grad_norm": 0.8236141800880432,
      "learning_rate": 0.00013742630043263912,
      "loss": 0.2211,
      "step": 15553
    },
    {
      "epoch": 31.295774647887324,
      "grad_norm": 0.8426524996757507,
      "learning_rate": 0.0001374222758828856,
      "loss": 0.2298,
      "step": 15554
    },
    {
      "epoch": 31.29778672032193,
      "grad_norm": 0.8365496397018433,
      "learning_rate": 0.0001374182513331321,
      "loss": 0.2329,
      "step": 15555
    },
    {
      "epoch": 31.29979879275654,
      "grad_norm": 0.8813737630844116,
      "learning_rate": 0.0001374142267833786,
      "loss": 0.2349,
      "step": 15556
    },
    {
      "epoch": 31.301810865191147,
      "grad_norm": 0.9147855043411255,
      "learning_rate": 0.00013741020223362514,
      "loss": 0.2263,
      "step": 15557
    },
    {
      "epoch": 31.303822937625753,
      "grad_norm": 0.8425214886665344,
      "learning_rate": 0.00013740617768387162,
      "loss": 0.2212,
      "step": 15558
    },
    {
      "epoch": 31.305835010060363,
      "grad_norm": 0.8469846248626709,
      "learning_rate": 0.00013740215313411813,
      "loss": 0.2127,
      "step": 15559
    },
    {
      "epoch": 31.30784708249497,
      "grad_norm": 0.8208426237106323,
      "learning_rate": 0.00013739812858436462,
      "loss": 0.2276,
      "step": 15560
    },
    {
      "epoch": 31.309859154929576,
      "grad_norm": 0.8405640125274658,
      "learning_rate": 0.00013739410403461113,
      "loss": 0.2186,
      "step": 15561
    },
    {
      "epoch": 31.311871227364186,
      "grad_norm": 0.882274866104126,
      "learning_rate": 0.00013739007948485764,
      "loss": 0.2321,
      "step": 15562
    },
    {
      "epoch": 31.313883299798793,
      "grad_norm": 0.8530182838439941,
      "learning_rate": 0.00013738605493510416,
      "loss": 0.2172,
      "step": 15563
    },
    {
      "epoch": 31.3158953722334,
      "grad_norm": 0.8412654399871826,
      "learning_rate": 0.00013738203038535064,
      "loss": 0.2242,
      "step": 15564
    },
    {
      "epoch": 31.31790744466801,
      "grad_norm": 0.8019186854362488,
      "learning_rate": 0.00013737800583559715,
      "loss": 0.2142,
      "step": 15565
    },
    {
      "epoch": 31.319919517102615,
      "grad_norm": 0.8946965932846069,
      "learning_rate": 0.00013737398128584364,
      "loss": 0.2438,
      "step": 15566
    },
    {
      "epoch": 31.321931589537222,
      "grad_norm": 0.9026685953140259,
      "learning_rate": 0.00013736995673609018,
      "loss": 0.2211,
      "step": 15567
    },
    {
      "epoch": 31.323943661971832,
      "grad_norm": 0.8770466446876526,
      "learning_rate": 0.00013736593218633666,
      "loss": 0.2325,
      "step": 15568
    },
    {
      "epoch": 31.32595573440644,
      "grad_norm": 0.8781543374061584,
      "learning_rate": 0.00013736190763658318,
      "loss": 0.2337,
      "step": 15569
    },
    {
      "epoch": 31.327967806841045,
      "grad_norm": 0.8043292164802551,
      "learning_rate": 0.00013735788308682966,
      "loss": 0.2363,
      "step": 15570
    },
    {
      "epoch": 31.329979879275655,
      "grad_norm": 0.8843056559562683,
      "learning_rate": 0.00013735385853707617,
      "loss": 0.2175,
      "step": 15571
    },
    {
      "epoch": 31.33199195171026,
      "grad_norm": 0.8951497077941895,
      "learning_rate": 0.00013734983398732269,
      "loss": 0.2372,
      "step": 15572
    },
    {
      "epoch": 31.334004024144868,
      "grad_norm": 0.855789065361023,
      "learning_rate": 0.00013734580943756917,
      "loss": 0.2367,
      "step": 15573
    },
    {
      "epoch": 31.336016096579478,
      "grad_norm": 0.8459048271179199,
      "learning_rate": 0.00013734178488781568,
      "loss": 0.2343,
      "step": 15574
    },
    {
      "epoch": 31.338028169014084,
      "grad_norm": 0.8956583738327026,
      "learning_rate": 0.0001373377603380622,
      "loss": 0.2234,
      "step": 15575
    },
    {
      "epoch": 31.34004024144869,
      "grad_norm": 0.8536121845245361,
      "learning_rate": 0.00013733373578830868,
      "loss": 0.2301,
      "step": 15576
    },
    {
      "epoch": 31.3420523138833,
      "grad_norm": 0.8748155236244202,
      "learning_rate": 0.0001373297112385552,
      "loss": 0.2417,
      "step": 15577
    },
    {
      "epoch": 31.344064386317907,
      "grad_norm": 0.8148542046546936,
      "learning_rate": 0.0001373256866888017,
      "loss": 0.2333,
      "step": 15578
    },
    {
      "epoch": 31.346076458752513,
      "grad_norm": 0.8519185185432434,
      "learning_rate": 0.0001373216621390482,
      "loss": 0.2241,
      "step": 15579
    },
    {
      "epoch": 31.348088531187123,
      "grad_norm": 0.8855398893356323,
      "learning_rate": 0.0001373176375892947,
      "loss": 0.2465,
      "step": 15580
    },
    {
      "epoch": 31.35010060362173,
      "grad_norm": 0.8993279337882996,
      "learning_rate": 0.0001373136130395412,
      "loss": 0.2502,
      "step": 15581
    },
    {
      "epoch": 31.352112676056336,
      "grad_norm": 0.8448525667190552,
      "learning_rate": 0.00013730958848978773,
      "loss": 0.2237,
      "step": 15582
    },
    {
      "epoch": 31.354124748490946,
      "grad_norm": 0.9222791194915771,
      "learning_rate": 0.0001373055639400342,
      "loss": 0.2434,
      "step": 15583
    },
    {
      "epoch": 31.356136820925553,
      "grad_norm": 0.872399091720581,
      "learning_rate": 0.00013730153939028072,
      "loss": 0.2396,
      "step": 15584
    },
    {
      "epoch": 31.358148893360163,
      "grad_norm": 0.9497354626655579,
      "learning_rate": 0.0001372975148405272,
      "loss": 0.2607,
      "step": 15585
    },
    {
      "epoch": 31.36016096579477,
      "grad_norm": 0.8585996627807617,
      "learning_rate": 0.00013729349029077372,
      "loss": 0.2289,
      "step": 15586
    },
    {
      "epoch": 31.362173038229376,
      "grad_norm": 0.885679304599762,
      "learning_rate": 0.00013728946574102023,
      "loss": 0.2162,
      "step": 15587
    },
    {
      "epoch": 31.364185110663986,
      "grad_norm": 0.8321983218193054,
      "learning_rate": 0.00013728544119126675,
      "loss": 0.2051,
      "step": 15588
    },
    {
      "epoch": 31.366197183098592,
      "grad_norm": 0.8973840475082397,
      "learning_rate": 0.00013728141664151323,
      "loss": 0.2502,
      "step": 15589
    },
    {
      "epoch": 31.3682092555332,
      "grad_norm": 0.9572541117668152,
      "learning_rate": 0.00013727739209175974,
      "loss": 0.2458,
      "step": 15590
    },
    {
      "epoch": 31.37022132796781,
      "grad_norm": 0.8768220543861389,
      "learning_rate": 0.00013727336754200623,
      "loss": 0.2304,
      "step": 15591
    },
    {
      "epoch": 31.372233400402415,
      "grad_norm": 0.8428841829299927,
      "learning_rate": 0.00013726934299225277,
      "loss": 0.2248,
      "step": 15592
    },
    {
      "epoch": 31.37424547283702,
      "grad_norm": 0.8751917481422424,
      "learning_rate": 0.00013726531844249925,
      "loss": 0.2301,
      "step": 15593
    },
    {
      "epoch": 31.37625754527163,
      "grad_norm": 0.9098411798477173,
      "learning_rate": 0.00013726129389274576,
      "loss": 0.242,
      "step": 15594
    },
    {
      "epoch": 31.378269617706238,
      "grad_norm": 0.9065494537353516,
      "learning_rate": 0.00013725726934299225,
      "loss": 0.2378,
      "step": 15595
    },
    {
      "epoch": 31.380281690140844,
      "grad_norm": 0.8795666098594666,
      "learning_rate": 0.00013725324479323876,
      "loss": 0.2604,
      "step": 15596
    },
    {
      "epoch": 31.382293762575454,
      "grad_norm": 0.8970555663108826,
      "learning_rate": 0.00013724922024348527,
      "loss": 0.2389,
      "step": 15597
    },
    {
      "epoch": 31.38430583501006,
      "grad_norm": 0.8724802732467651,
      "learning_rate": 0.00013724519569373179,
      "loss": 0.2334,
      "step": 15598
    },
    {
      "epoch": 31.386317907444667,
      "grad_norm": 0.8397870063781738,
      "learning_rate": 0.00013724117114397827,
      "loss": 0.2221,
      "step": 15599
    },
    {
      "epoch": 31.388329979879277,
      "grad_norm": 0.8357391953468323,
      "learning_rate": 0.00013723714659422478,
      "loss": 0.2403,
      "step": 15600
    },
    {
      "epoch": 31.390342052313883,
      "grad_norm": 0.9020176529884338,
      "learning_rate": 0.00013723312204447127,
      "loss": 0.2379,
      "step": 15601
    },
    {
      "epoch": 31.39235412474849,
      "grad_norm": 0.9556928873062134,
      "learning_rate": 0.0001372290974947178,
      "loss": 0.2423,
      "step": 15602
    },
    {
      "epoch": 31.3943661971831,
      "grad_norm": 0.8856515288352966,
      "learning_rate": 0.0001372250729449643,
      "loss": 0.2336,
      "step": 15603
    },
    {
      "epoch": 31.396378269617706,
      "grad_norm": 0.8796279430389404,
      "learning_rate": 0.0001372210483952108,
      "loss": 0.2349,
      "step": 15604
    },
    {
      "epoch": 31.398390342052313,
      "grad_norm": 0.8302906155586243,
      "learning_rate": 0.0001372170238454573,
      "loss": 0.2253,
      "step": 15605
    },
    {
      "epoch": 31.400402414486923,
      "grad_norm": 0.8458828330039978,
      "learning_rate": 0.0001372129992957038,
      "loss": 0.2203,
      "step": 15606
    },
    {
      "epoch": 31.40241448692153,
      "grad_norm": 0.8920853137969971,
      "learning_rate": 0.00013720897474595031,
      "loss": 0.2526,
      "step": 15607
    },
    {
      "epoch": 31.404426559356136,
      "grad_norm": 0.8806695342063904,
      "learning_rate": 0.0001372049501961968,
      "loss": 0.251,
      "step": 15608
    },
    {
      "epoch": 31.406438631790746,
      "grad_norm": 0.8745479583740234,
      "learning_rate": 0.0001372009256464433,
      "loss": 0.244,
      "step": 15609
    },
    {
      "epoch": 31.408450704225352,
      "grad_norm": 0.8651832342147827,
      "learning_rate": 0.00013719690109668982,
      "loss": 0.2239,
      "step": 15610
    },
    {
      "epoch": 31.41046277665996,
      "grad_norm": 0.9123551249504089,
      "learning_rate": 0.0001371928765469363,
      "loss": 0.2392,
      "step": 15611
    },
    {
      "epoch": 31.41247484909457,
      "grad_norm": 0.950908899307251,
      "learning_rate": 0.00013718885199718282,
      "loss": 0.2459,
      "step": 15612
    },
    {
      "epoch": 31.414486921529175,
      "grad_norm": 0.8464515209197998,
      "learning_rate": 0.00013718482744742933,
      "loss": 0.2332,
      "step": 15613
    },
    {
      "epoch": 31.41649899396378,
      "grad_norm": 0.8818692564964294,
      "learning_rate": 0.00013718080289767582,
      "loss": 0.2368,
      "step": 15614
    },
    {
      "epoch": 31.41851106639839,
      "grad_norm": 0.9250711798667908,
      "learning_rate": 0.00013717677834792233,
      "loss": 0.2637,
      "step": 15615
    },
    {
      "epoch": 31.420523138832998,
      "grad_norm": 0.9296479821205139,
      "learning_rate": 0.00013717275379816882,
      "loss": 0.2355,
      "step": 15616
    },
    {
      "epoch": 31.422535211267604,
      "grad_norm": 0.8926807641983032,
      "learning_rate": 0.00013716872924841536,
      "loss": 0.2267,
      "step": 15617
    },
    {
      "epoch": 31.424547283702214,
      "grad_norm": 0.8769047856330872,
      "learning_rate": 0.00013716470469866184,
      "loss": 0.2299,
      "step": 15618
    },
    {
      "epoch": 31.42655935613682,
      "grad_norm": 0.8961014747619629,
      "learning_rate": 0.00013716068014890835,
      "loss": 0.2408,
      "step": 15619
    },
    {
      "epoch": 31.428571428571427,
      "grad_norm": 0.8836803436279297,
      "learning_rate": 0.00013715665559915484,
      "loss": 0.2403,
      "step": 15620
    },
    {
      "epoch": 31.430583501006037,
      "grad_norm": 0.8406482338905334,
      "learning_rate": 0.00013715263104940135,
      "loss": 0.241,
      "step": 15621
    },
    {
      "epoch": 31.432595573440643,
      "grad_norm": 0.8618224263191223,
      "learning_rate": 0.00013714860649964786,
      "loss": 0.2464,
      "step": 15622
    },
    {
      "epoch": 31.43460764587525,
      "grad_norm": 0.9203383922576904,
      "learning_rate": 0.00013714458194989437,
      "loss": 0.2446,
      "step": 15623
    },
    {
      "epoch": 31.43661971830986,
      "grad_norm": 0.8488037586212158,
      "learning_rate": 0.00013714055740014086,
      "loss": 0.2485,
      "step": 15624
    },
    {
      "epoch": 31.438631790744466,
      "grad_norm": 0.8478712439537048,
      "learning_rate": 0.00013713653285038737,
      "loss": 0.2273,
      "step": 15625
    },
    {
      "epoch": 31.440643863179073,
      "grad_norm": 0.90174400806427,
      "learning_rate": 0.00013713250830063386,
      "loss": 0.2383,
      "step": 15626
    },
    {
      "epoch": 31.442655935613683,
      "grad_norm": 0.8810673356056213,
      "learning_rate": 0.0001371284837508804,
      "loss": 0.2402,
      "step": 15627
    },
    {
      "epoch": 31.44466800804829,
      "grad_norm": 0.8404309749603271,
      "learning_rate": 0.00013712445920112688,
      "loss": 0.237,
      "step": 15628
    },
    {
      "epoch": 31.446680080482896,
      "grad_norm": 0.863048791885376,
      "learning_rate": 0.0001371204346513734,
      "loss": 0.2107,
      "step": 15629
    },
    {
      "epoch": 31.448692152917506,
      "grad_norm": 0.9023630619049072,
      "learning_rate": 0.00013711641010161988,
      "loss": 0.2319,
      "step": 15630
    },
    {
      "epoch": 31.450704225352112,
      "grad_norm": 0.9142447113990784,
      "learning_rate": 0.0001371123855518664,
      "loss": 0.2568,
      "step": 15631
    },
    {
      "epoch": 31.452716297786722,
      "grad_norm": 0.9226392507553101,
      "learning_rate": 0.0001371083610021129,
      "loss": 0.2526,
      "step": 15632
    },
    {
      "epoch": 31.45472837022133,
      "grad_norm": 0.8603261113166809,
      "learning_rate": 0.00013710433645235942,
      "loss": 0.2528,
      "step": 15633
    },
    {
      "epoch": 31.456740442655935,
      "grad_norm": 0.88080894947052,
      "learning_rate": 0.0001371003119026059,
      "loss": 0.2583,
      "step": 15634
    },
    {
      "epoch": 31.458752515090545,
      "grad_norm": 0.9435941576957703,
      "learning_rate": 0.0001370962873528524,
      "loss": 0.2717,
      "step": 15635
    },
    {
      "epoch": 31.46076458752515,
      "grad_norm": 0.889779806137085,
      "learning_rate": 0.0001370922628030989,
      "loss": 0.2321,
      "step": 15636
    },
    {
      "epoch": 31.462776659959758,
      "grad_norm": 0.8846989274024963,
      "learning_rate": 0.00013708823825334544,
      "loss": 0.2507,
      "step": 15637
    },
    {
      "epoch": 31.464788732394368,
      "grad_norm": 0.8322478532791138,
      "learning_rate": 0.00013708421370359192,
      "loss": 0.2417,
      "step": 15638
    },
    {
      "epoch": 31.466800804828974,
      "grad_norm": 0.8041991591453552,
      "learning_rate": 0.00013708018915383843,
      "loss": 0.2191,
      "step": 15639
    },
    {
      "epoch": 31.46881287726358,
      "grad_norm": 0.870225727558136,
      "learning_rate": 0.00013707616460408492,
      "loss": 0.2554,
      "step": 15640
    },
    {
      "epoch": 31.47082494969819,
      "grad_norm": 0.8932133316993713,
      "learning_rate": 0.00013707214005433143,
      "loss": 0.2532,
      "step": 15641
    },
    {
      "epoch": 31.472837022132797,
      "grad_norm": 0.888278603553772,
      "learning_rate": 0.00013706811550457792,
      "loss": 0.2529,
      "step": 15642
    },
    {
      "epoch": 31.474849094567404,
      "grad_norm": 0.8606619238853455,
      "learning_rate": 0.00013706409095482443,
      "loss": 0.2362,
      "step": 15643
    },
    {
      "epoch": 31.476861167002014,
      "grad_norm": 0.9288837313652039,
      "learning_rate": 0.00013706006640507094,
      "loss": 0.2481,
      "step": 15644
    },
    {
      "epoch": 31.47887323943662,
      "grad_norm": 0.8778534531593323,
      "learning_rate": 0.00013705604185531745,
      "loss": 0.2227,
      "step": 15645
    },
    {
      "epoch": 31.480885311871226,
      "grad_norm": 0.9260960817337036,
      "learning_rate": 0.00013705201730556394,
      "loss": 0.222,
      "step": 15646
    },
    {
      "epoch": 31.482897384305836,
      "grad_norm": 0.9068009257316589,
      "learning_rate": 0.00013704799275581045,
      "loss": 0.2718,
      "step": 15647
    },
    {
      "epoch": 31.484909456740443,
      "grad_norm": 0.909162700176239,
      "learning_rate": 0.00013704396820605696,
      "loss": 0.2295,
      "step": 15648
    },
    {
      "epoch": 31.48692152917505,
      "grad_norm": 0.8627090454101562,
      "learning_rate": 0.00013703994365630345,
      "loss": 0.2319,
      "step": 15649
    },
    {
      "epoch": 31.48893360160966,
      "grad_norm": 0.8488824367523193,
      "learning_rate": 0.00013703591910654996,
      "loss": 0.236,
      "step": 15650
    },
    {
      "epoch": 31.490945674044266,
      "grad_norm": 0.805569052696228,
      "learning_rate": 0.00013703189455679645,
      "loss": 0.2279,
      "step": 15651
    },
    {
      "epoch": 31.492957746478872,
      "grad_norm": 0.8526210784912109,
      "learning_rate": 0.00013702787000704296,
      "loss": 0.2478,
      "step": 15652
    },
    {
      "epoch": 31.494969818913482,
      "grad_norm": 0.9354504942893982,
      "learning_rate": 0.00013702384545728947,
      "loss": 0.2551,
      "step": 15653
    },
    {
      "epoch": 31.49698189134809,
      "grad_norm": 0.8825525641441345,
      "learning_rate": 0.00013701982090753598,
      "loss": 0.258,
      "step": 15654
    },
    {
      "epoch": 31.498993963782695,
      "grad_norm": 0.8923562169075012,
      "learning_rate": 0.00013701579635778247,
      "loss": 0.2579,
      "step": 15655
    },
    {
      "epoch": 31.501006036217305,
      "grad_norm": 0.8697331547737122,
      "learning_rate": 0.00013701177180802898,
      "loss": 0.2421,
      "step": 15656
    },
    {
      "epoch": 31.50301810865191,
      "grad_norm": 0.8858678340911865,
      "learning_rate": 0.00013700774725827546,
      "loss": 0.2539,
      "step": 15657
    },
    {
      "epoch": 31.505030181086518,
      "grad_norm": 0.8981514573097229,
      "learning_rate": 0.000137003722708522,
      "loss": 0.2325,
      "step": 15658
    },
    {
      "epoch": 31.507042253521128,
      "grad_norm": 0.9469447135925293,
      "learning_rate": 0.0001369996981587685,
      "loss": 0.2557,
      "step": 15659
    },
    {
      "epoch": 31.509054325955734,
      "grad_norm": 0.8298227787017822,
      "learning_rate": 0.000136995673609015,
      "loss": 0.2108,
      "step": 15660
    },
    {
      "epoch": 31.51106639839034,
      "grad_norm": 0.9021052718162537,
      "learning_rate": 0.0001369916490592615,
      "loss": 0.2365,
      "step": 15661
    },
    {
      "epoch": 31.51307847082495,
      "grad_norm": 0.9000788927078247,
      "learning_rate": 0.000136987624509508,
      "loss": 0.2249,
      "step": 15662
    },
    {
      "epoch": 31.515090543259557,
      "grad_norm": 0.94741290807724,
      "learning_rate": 0.0001369835999597545,
      "loss": 0.2523,
      "step": 15663
    },
    {
      "epoch": 31.517102615694164,
      "grad_norm": 0.8760009407997131,
      "learning_rate": 0.00013697957541000102,
      "loss": 0.2439,
      "step": 15664
    },
    {
      "epoch": 31.519114688128774,
      "grad_norm": 0.9115458726882935,
      "learning_rate": 0.0001369755508602475,
      "loss": 0.2682,
      "step": 15665
    },
    {
      "epoch": 31.52112676056338,
      "grad_norm": 0.8661531209945679,
      "learning_rate": 0.00013697152631049402,
      "loss": 0.2604,
      "step": 15666
    },
    {
      "epoch": 31.523138832997986,
      "grad_norm": 0.8786716461181641,
      "learning_rate": 0.0001369675017607405,
      "loss": 0.2241,
      "step": 15667
    },
    {
      "epoch": 31.525150905432596,
      "grad_norm": 0.9866284728050232,
      "learning_rate": 0.00013696347721098705,
      "loss": 0.2305,
      "step": 15668
    },
    {
      "epoch": 31.527162977867203,
      "grad_norm": 0.8742234706878662,
      "learning_rate": 0.00013695945266123353,
      "loss": 0.2333,
      "step": 15669
    },
    {
      "epoch": 31.52917505030181,
      "grad_norm": 0.9531199932098389,
      "learning_rate": 0.00013695542811148004,
      "loss": 0.2403,
      "step": 15670
    },
    {
      "epoch": 31.53118712273642,
      "grad_norm": 0.8572613596916199,
      "learning_rate": 0.00013695140356172653,
      "loss": 0.2271,
      "step": 15671
    },
    {
      "epoch": 31.533199195171026,
      "grad_norm": 0.9048373699188232,
      "learning_rate": 0.00013694737901197304,
      "loss": 0.226,
      "step": 15672
    },
    {
      "epoch": 31.535211267605632,
      "grad_norm": 0.9432060122489929,
      "learning_rate": 0.00013694335446221955,
      "loss": 0.2515,
      "step": 15673
    },
    {
      "epoch": 31.537223340040242,
      "grad_norm": 0.9063748121261597,
      "learning_rate": 0.00013693932991246606,
      "loss": 0.2329,
      "step": 15674
    },
    {
      "epoch": 31.53923541247485,
      "grad_norm": 0.8514799475669861,
      "learning_rate": 0.00013693530536271255,
      "loss": 0.2379,
      "step": 15675
    },
    {
      "epoch": 31.541247484909455,
      "grad_norm": 0.873182475566864,
      "learning_rate": 0.00013693128081295906,
      "loss": 0.2458,
      "step": 15676
    },
    {
      "epoch": 31.543259557344065,
      "grad_norm": 0.927881121635437,
      "learning_rate": 0.00013692725626320555,
      "loss": 0.2601,
      "step": 15677
    },
    {
      "epoch": 31.54527162977867,
      "grad_norm": 0.9049221277236938,
      "learning_rate": 0.00013692323171345206,
      "loss": 0.2367,
      "step": 15678
    },
    {
      "epoch": 31.547283702213278,
      "grad_norm": 0.8854960203170776,
      "learning_rate": 0.00013691920716369857,
      "loss": 0.2277,
      "step": 15679
    },
    {
      "epoch": 31.549295774647888,
      "grad_norm": 0.9255106449127197,
      "learning_rate": 0.00013691518261394508,
      "loss": 0.2278,
      "step": 15680
    },
    {
      "epoch": 31.551307847082494,
      "grad_norm": 0.9006636142730713,
      "learning_rate": 0.00013691115806419157,
      "loss": 0.2492,
      "step": 15681
    },
    {
      "epoch": 31.5533199195171,
      "grad_norm": 0.8391447067260742,
      "learning_rate": 0.00013690713351443808,
      "loss": 0.2511,
      "step": 15682
    },
    {
      "epoch": 31.55533199195171,
      "grad_norm": 0.8866220116615295,
      "learning_rate": 0.0001369031089646846,
      "loss": 0.2675,
      "step": 15683
    },
    {
      "epoch": 31.557344064386317,
      "grad_norm": 0.9373726844787598,
      "learning_rate": 0.00013689908441493108,
      "loss": 0.2518,
      "step": 15684
    },
    {
      "epoch": 31.559356136820927,
      "grad_norm": 0.8542059063911438,
      "learning_rate": 0.0001368950598651776,
      "loss": 0.2368,
      "step": 15685
    },
    {
      "epoch": 31.561368209255534,
      "grad_norm": 0.8217856287956238,
      "learning_rate": 0.00013689103531542408,
      "loss": 0.2338,
      "step": 15686
    },
    {
      "epoch": 31.56338028169014,
      "grad_norm": 0.8182448148727417,
      "learning_rate": 0.0001368870107656706,
      "loss": 0.2381,
      "step": 15687
    },
    {
      "epoch": 31.56539235412475,
      "grad_norm": 0.8538166880607605,
      "learning_rate": 0.0001368829862159171,
      "loss": 0.2198,
      "step": 15688
    },
    {
      "epoch": 31.567404426559357,
      "grad_norm": 0.9537061452865601,
      "learning_rate": 0.0001368789616661636,
      "loss": 0.2453,
      "step": 15689
    },
    {
      "epoch": 31.569416498993963,
      "grad_norm": 0.8502647876739502,
      "learning_rate": 0.0001368749371164101,
      "loss": 0.2419,
      "step": 15690
    },
    {
      "epoch": 31.571428571428573,
      "grad_norm": 0.9953131675720215,
      "learning_rate": 0.0001368709125666566,
      "loss": 0.2563,
      "step": 15691
    },
    {
      "epoch": 31.57344064386318,
      "grad_norm": 0.8623875379562378,
      "learning_rate": 0.0001368668880169031,
      "loss": 0.2354,
      "step": 15692
    },
    {
      "epoch": 31.575452716297786,
      "grad_norm": 0.8971331119537354,
      "learning_rate": 0.00013686286346714963,
      "loss": 0.2418,
      "step": 15693
    },
    {
      "epoch": 31.577464788732396,
      "grad_norm": 0.9444844722747803,
      "learning_rate": 0.00013685883891739612,
      "loss": 0.2411,
      "step": 15694
    },
    {
      "epoch": 31.579476861167002,
      "grad_norm": 0.888349175453186,
      "learning_rate": 0.00013685481436764263,
      "loss": 0.2614,
      "step": 15695
    },
    {
      "epoch": 31.58148893360161,
      "grad_norm": 0.8807230591773987,
      "learning_rate": 0.00013685078981788912,
      "loss": 0.2411,
      "step": 15696
    },
    {
      "epoch": 31.58350100603622,
      "grad_norm": 0.8977493047714233,
      "learning_rate": 0.00013684676526813563,
      "loss": 0.2522,
      "step": 15697
    },
    {
      "epoch": 31.585513078470825,
      "grad_norm": 0.8623537421226501,
      "learning_rate": 0.00013684274071838214,
      "loss": 0.2552,
      "step": 15698
    },
    {
      "epoch": 31.58752515090543,
      "grad_norm": 0.8966230750083923,
      "learning_rate": 0.00013683871616862865,
      "loss": 0.2683,
      "step": 15699
    },
    {
      "epoch": 31.58953722334004,
      "grad_norm": 0.9004924893379211,
      "learning_rate": 0.00013683469161887514,
      "loss": 0.2673,
      "step": 15700
    },
    {
      "epoch": 31.591549295774648,
      "grad_norm": 0.8595832586288452,
      "learning_rate": 0.00013683066706912165,
      "loss": 0.2513,
      "step": 15701
    },
    {
      "epoch": 31.593561368209254,
      "grad_norm": 0.9119585752487183,
      "learning_rate": 0.00013682664251936814,
      "loss": 0.2487,
      "step": 15702
    },
    {
      "epoch": 31.595573440643864,
      "grad_norm": 0.8998015522956848,
      "learning_rate": 0.00013682261796961467,
      "loss": 0.2406,
      "step": 15703
    },
    {
      "epoch": 31.59758551307847,
      "grad_norm": 1.0084832906723022,
      "learning_rate": 0.00013681859341986116,
      "loss": 0.2496,
      "step": 15704
    },
    {
      "epoch": 31.599597585513077,
      "grad_norm": 0.9031922817230225,
      "learning_rate": 0.00013681456887010767,
      "loss": 0.2303,
      "step": 15705
    },
    {
      "epoch": 31.601609657947687,
      "grad_norm": 0.8798689246177673,
      "learning_rate": 0.00013681054432035416,
      "loss": 0.2278,
      "step": 15706
    },
    {
      "epoch": 31.603621730382294,
      "grad_norm": 0.9299245476722717,
      "learning_rate": 0.00013680651977060067,
      "loss": 0.2742,
      "step": 15707
    },
    {
      "epoch": 31.6056338028169,
      "grad_norm": 0.9611142873764038,
      "learning_rate": 0.00013680249522084718,
      "loss": 0.271,
      "step": 15708
    },
    {
      "epoch": 31.60764587525151,
      "grad_norm": 0.8900795578956604,
      "learning_rate": 0.0001367984706710937,
      "loss": 0.2509,
      "step": 15709
    },
    {
      "epoch": 31.609657947686117,
      "grad_norm": 0.8899118900299072,
      "learning_rate": 0.00013679444612134018,
      "loss": 0.2122,
      "step": 15710
    },
    {
      "epoch": 31.611670020120723,
      "grad_norm": 0.9221314787864685,
      "learning_rate": 0.0001367904215715867,
      "loss": 0.2542,
      "step": 15711
    },
    {
      "epoch": 31.613682092555333,
      "grad_norm": 0.9030988812446594,
      "learning_rate": 0.00013678639702183318,
      "loss": 0.2505,
      "step": 15712
    },
    {
      "epoch": 31.61569416498994,
      "grad_norm": 0.9253975749015808,
      "learning_rate": 0.0001367823724720797,
      "loss": 0.2432,
      "step": 15713
    },
    {
      "epoch": 31.617706237424546,
      "grad_norm": 0.8901515007019043,
      "learning_rate": 0.0001367783479223262,
      "loss": 0.2481,
      "step": 15714
    },
    {
      "epoch": 31.619718309859156,
      "grad_norm": 0.8650879859924316,
      "learning_rate": 0.0001367743233725727,
      "loss": 0.2513,
      "step": 15715
    },
    {
      "epoch": 31.621730382293762,
      "grad_norm": 0.8855646252632141,
      "learning_rate": 0.0001367702988228192,
      "loss": 0.2316,
      "step": 15716
    },
    {
      "epoch": 31.62374245472837,
      "grad_norm": 0.8570902943611145,
      "learning_rate": 0.0001367662742730657,
      "loss": 0.2398,
      "step": 15717
    },
    {
      "epoch": 31.62575452716298,
      "grad_norm": 0.8894339799880981,
      "learning_rate": 0.00013676224972331222,
      "loss": 0.2531,
      "step": 15718
    },
    {
      "epoch": 31.627766599597585,
      "grad_norm": 0.7959885001182556,
      "learning_rate": 0.0001367582251735587,
      "loss": 0.2251,
      "step": 15719
    },
    {
      "epoch": 31.62977867203219,
      "grad_norm": 0.909288227558136,
      "learning_rate": 0.00013675420062380522,
      "loss": 0.2448,
      "step": 15720
    },
    {
      "epoch": 31.6317907444668,
      "grad_norm": 0.9322657585144043,
      "learning_rate": 0.0001367501760740517,
      "loss": 0.2562,
      "step": 15721
    },
    {
      "epoch": 31.633802816901408,
      "grad_norm": 0.9093136787414551,
      "learning_rate": 0.00013674615152429822,
      "loss": 0.2416,
      "step": 15722
    },
    {
      "epoch": 31.635814889336014,
      "grad_norm": 0.8522169589996338,
      "learning_rate": 0.00013674212697454473,
      "loss": 0.2371,
      "step": 15723
    },
    {
      "epoch": 31.637826961770624,
      "grad_norm": 0.8527307510375977,
      "learning_rate": 0.00013673810242479124,
      "loss": 0.2445,
      "step": 15724
    },
    {
      "epoch": 31.63983903420523,
      "grad_norm": 0.9122931957244873,
      "learning_rate": 0.00013673407787503773,
      "loss": 0.2713,
      "step": 15725
    },
    {
      "epoch": 31.641851106639837,
      "grad_norm": 0.8853662014007568,
      "learning_rate": 0.00013673005332528424,
      "loss": 0.2568,
      "step": 15726
    },
    {
      "epoch": 31.643863179074447,
      "grad_norm": 0.9361461997032166,
      "learning_rate": 0.00013672602877553072,
      "loss": 0.2442,
      "step": 15727
    },
    {
      "epoch": 31.645875251509054,
      "grad_norm": 0.8695757985115051,
      "learning_rate": 0.00013672200422577726,
      "loss": 0.2422,
      "step": 15728
    },
    {
      "epoch": 31.647887323943664,
      "grad_norm": 0.9079208970069885,
      "learning_rate": 0.00013671797967602375,
      "loss": 0.2612,
      "step": 15729
    },
    {
      "epoch": 31.64989939637827,
      "grad_norm": 0.9333636164665222,
      "learning_rate": 0.00013671395512627026,
      "loss": 0.2522,
      "step": 15730
    },
    {
      "epoch": 31.651911468812877,
      "grad_norm": 0.8386440277099609,
      "learning_rate": 0.00013670993057651675,
      "loss": 0.2301,
      "step": 15731
    },
    {
      "epoch": 31.653923541247487,
      "grad_norm": 0.8463783860206604,
      "learning_rate": 0.00013670590602676326,
      "loss": 0.2209,
      "step": 15732
    },
    {
      "epoch": 31.655935613682093,
      "grad_norm": 0.8981230854988098,
      "learning_rate": 0.00013670188147700977,
      "loss": 0.2482,
      "step": 15733
    },
    {
      "epoch": 31.6579476861167,
      "grad_norm": 0.9303646683692932,
      "learning_rate": 0.00013669785692725628,
      "loss": 0.2409,
      "step": 15734
    },
    {
      "epoch": 31.65995975855131,
      "grad_norm": 0.9400476217269897,
      "learning_rate": 0.00013669383237750277,
      "loss": 0.2574,
      "step": 15735
    },
    {
      "epoch": 31.661971830985916,
      "grad_norm": 1.006291389465332,
      "learning_rate": 0.00013668980782774928,
      "loss": 0.2725,
      "step": 15736
    },
    {
      "epoch": 31.663983903420522,
      "grad_norm": 0.8871076107025146,
      "learning_rate": 0.00013668578327799576,
      "loss": 0.2186,
      "step": 15737
    },
    {
      "epoch": 31.665995975855132,
      "grad_norm": 1.0120929479599,
      "learning_rate": 0.0001366817587282423,
      "loss": 0.2489,
      "step": 15738
    },
    {
      "epoch": 31.66800804828974,
      "grad_norm": 0.8727719187736511,
      "learning_rate": 0.0001366777341784888,
      "loss": 0.2387,
      "step": 15739
    },
    {
      "epoch": 31.670020120724345,
      "grad_norm": 0.9279732704162598,
      "learning_rate": 0.0001366737096287353,
      "loss": 0.2568,
      "step": 15740
    },
    {
      "epoch": 31.672032193158955,
      "grad_norm": 0.9491274356842041,
      "learning_rate": 0.00013666968507898179,
      "loss": 0.2483,
      "step": 15741
    },
    {
      "epoch": 31.67404426559356,
      "grad_norm": 0.9451152682304382,
      "learning_rate": 0.0001366656605292283,
      "loss": 0.2663,
      "step": 15742
    },
    {
      "epoch": 31.676056338028168,
      "grad_norm": 0.9335343241691589,
      "learning_rate": 0.0001366616359794748,
      "loss": 0.2751,
      "step": 15743
    },
    {
      "epoch": 31.678068410462778,
      "grad_norm": 0.8952315449714661,
      "learning_rate": 0.00013665761142972132,
      "loss": 0.2356,
      "step": 15744
    },
    {
      "epoch": 31.680080482897385,
      "grad_norm": 0.9171292185783386,
      "learning_rate": 0.0001366535868799678,
      "loss": 0.2369,
      "step": 15745
    },
    {
      "epoch": 31.68209255533199,
      "grad_norm": 0.9584678411483765,
      "learning_rate": 0.00013664956233021432,
      "loss": 0.2513,
      "step": 15746
    },
    {
      "epoch": 31.6841046277666,
      "grad_norm": 0.8616412281990051,
      "learning_rate": 0.0001366455377804608,
      "loss": 0.2349,
      "step": 15747
    },
    {
      "epoch": 31.686116700201207,
      "grad_norm": 0.9292634129524231,
      "learning_rate": 0.00013664151323070732,
      "loss": 0.25,
      "step": 15748
    },
    {
      "epoch": 31.688128772635814,
      "grad_norm": 0.902253270149231,
      "learning_rate": 0.00013663748868095383,
      "loss": 0.2558,
      "step": 15749
    },
    {
      "epoch": 31.690140845070424,
      "grad_norm": 0.9263367056846619,
      "learning_rate": 0.00013663346413120031,
      "loss": 0.2473,
      "step": 15750
    },
    {
      "epoch": 31.69215291750503,
      "grad_norm": 0.9629538655281067,
      "learning_rate": 0.00013662943958144683,
      "loss": 0.2341,
      "step": 15751
    },
    {
      "epoch": 31.694164989939637,
      "grad_norm": 0.9451164603233337,
      "learning_rate": 0.00013662541503169334,
      "loss": 0.2496,
      "step": 15752
    },
    {
      "epoch": 31.696177062374247,
      "grad_norm": 0.9207264184951782,
      "learning_rate": 0.00013662139048193985,
      "loss": 0.2419,
      "step": 15753
    },
    {
      "epoch": 31.698189134808853,
      "grad_norm": 0.9051032662391663,
      "learning_rate": 0.00013661736593218634,
      "loss": 0.2553,
      "step": 15754
    },
    {
      "epoch": 31.70020120724346,
      "grad_norm": 0.9014767408370972,
      "learning_rate": 0.00013661334138243285,
      "loss": 0.2551,
      "step": 15755
    },
    {
      "epoch": 31.70221327967807,
      "grad_norm": 0.9775294661521912,
      "learning_rate": 0.00013660931683267933,
      "loss": 0.2688,
      "step": 15756
    },
    {
      "epoch": 31.704225352112676,
      "grad_norm": 0.8930744528770447,
      "learning_rate": 0.00013660529228292585,
      "loss": 0.258,
      "step": 15757
    },
    {
      "epoch": 31.706237424547282,
      "grad_norm": 0.9390021562576294,
      "learning_rate": 0.00013660126773317236,
      "loss": 0.26,
      "step": 15758
    },
    {
      "epoch": 31.708249496981892,
      "grad_norm": 0.962214469909668,
      "learning_rate": 0.00013659724318341887,
      "loss": 0.262,
      "step": 15759
    },
    {
      "epoch": 31.7102615694165,
      "grad_norm": 0.8941515684127808,
      "learning_rate": 0.00013659321863366536,
      "loss": 0.2521,
      "step": 15760
    },
    {
      "epoch": 31.712273641851105,
      "grad_norm": 0.8874472379684448,
      "learning_rate": 0.00013658919408391187,
      "loss": 0.2623,
      "step": 15761
    },
    {
      "epoch": 31.714285714285715,
      "grad_norm": 0.9154279232025146,
      "learning_rate": 0.00013658516953415835,
      "loss": 0.246,
      "step": 15762
    },
    {
      "epoch": 31.71629778672032,
      "grad_norm": 0.9219905734062195,
      "learning_rate": 0.0001365811449844049,
      "loss": 0.2694,
      "step": 15763
    },
    {
      "epoch": 31.718309859154928,
      "grad_norm": 0.9190697073936462,
      "learning_rate": 0.00013657712043465138,
      "loss": 0.262,
      "step": 15764
    },
    {
      "epoch": 31.720321931589538,
      "grad_norm": 0.8850100040435791,
      "learning_rate": 0.0001365730958848979,
      "loss": 0.2518,
      "step": 15765
    },
    {
      "epoch": 31.722334004024145,
      "grad_norm": 0.861049234867096,
      "learning_rate": 0.00013656907133514437,
      "loss": 0.2305,
      "step": 15766
    },
    {
      "epoch": 31.72434607645875,
      "grad_norm": 0.9212812185287476,
      "learning_rate": 0.0001365650467853909,
      "loss": 0.2509,
      "step": 15767
    },
    {
      "epoch": 31.72635814889336,
      "grad_norm": 0.9297105073928833,
      "learning_rate": 0.0001365610222356374,
      "loss": 0.2521,
      "step": 15768
    },
    {
      "epoch": 31.728370221327967,
      "grad_norm": 0.8517775535583496,
      "learning_rate": 0.0001365569976858839,
      "loss": 0.2559,
      "step": 15769
    },
    {
      "epoch": 31.730382293762574,
      "grad_norm": 0.9131776094436646,
      "learning_rate": 0.0001365529731361304,
      "loss": 0.2714,
      "step": 15770
    },
    {
      "epoch": 31.732394366197184,
      "grad_norm": 0.8657332062721252,
      "learning_rate": 0.0001365489485863769,
      "loss": 0.2546,
      "step": 15771
    },
    {
      "epoch": 31.73440643863179,
      "grad_norm": 0.8905365467071533,
      "learning_rate": 0.0001365449240366234,
      "loss": 0.2587,
      "step": 15772
    },
    {
      "epoch": 31.736418511066397,
      "grad_norm": 0.9028576612472534,
      "learning_rate": 0.00013654089948686993,
      "loss": 0.2493,
      "step": 15773
    },
    {
      "epoch": 31.738430583501007,
      "grad_norm": 0.8575147390365601,
      "learning_rate": 0.00013653687493711642,
      "loss": 0.2477,
      "step": 15774
    },
    {
      "epoch": 31.740442655935613,
      "grad_norm": 0.9044115543365479,
      "learning_rate": 0.00013653285038736293,
      "loss": 0.2658,
      "step": 15775
    },
    {
      "epoch": 31.74245472837022,
      "grad_norm": 0.9222012162208557,
      "learning_rate": 0.00013652882583760942,
      "loss": 0.2536,
      "step": 15776
    },
    {
      "epoch": 31.74446680080483,
      "grad_norm": 0.919737696647644,
      "learning_rate": 0.00013652480128785593,
      "loss": 0.2671,
      "step": 15777
    },
    {
      "epoch": 31.746478873239436,
      "grad_norm": 0.8874986171722412,
      "learning_rate": 0.00013652077673810244,
      "loss": 0.2583,
      "step": 15778
    },
    {
      "epoch": 31.748490945674043,
      "grad_norm": 0.9957994818687439,
      "learning_rate": 0.00013651675218834895,
      "loss": 0.258,
      "step": 15779
    },
    {
      "epoch": 31.750503018108652,
      "grad_norm": 0.8982230424880981,
      "learning_rate": 0.00013651272763859544,
      "loss": 0.2539,
      "step": 15780
    },
    {
      "epoch": 31.75251509054326,
      "grad_norm": 0.9162499308586121,
      "learning_rate": 0.00013650870308884195,
      "loss": 0.2644,
      "step": 15781
    },
    {
      "epoch": 31.754527162977865,
      "grad_norm": 0.8844929933547974,
      "learning_rate": 0.00013650467853908843,
      "loss": 0.269,
      "step": 15782
    },
    {
      "epoch": 31.756539235412475,
      "grad_norm": 0.8702735900878906,
      "learning_rate": 0.00013650065398933495,
      "loss": 0.2484,
      "step": 15783
    },
    {
      "epoch": 31.758551307847082,
      "grad_norm": 0.9099149107933044,
      "learning_rate": 0.00013649662943958146,
      "loss": 0.2628,
      "step": 15784
    },
    {
      "epoch": 31.760563380281692,
      "grad_norm": 0.9155899286270142,
      "learning_rate": 0.00013649260488982794,
      "loss": 0.2684,
      "step": 15785
    },
    {
      "epoch": 31.7625754527163,
      "grad_norm": 0.9223642945289612,
      "learning_rate": 0.00013648858034007446,
      "loss": 0.2472,
      "step": 15786
    },
    {
      "epoch": 31.764587525150905,
      "grad_norm": 0.9018179774284363,
      "learning_rate": 0.00013648455579032097,
      "loss": 0.2548,
      "step": 15787
    },
    {
      "epoch": 31.766599597585515,
      "grad_norm": 0.8936326503753662,
      "learning_rate": 0.00013648053124056748,
      "loss": 0.2507,
      "step": 15788
    },
    {
      "epoch": 31.76861167002012,
      "grad_norm": 0.8859927654266357,
      "learning_rate": 0.00013647650669081397,
      "loss": 0.254,
      "step": 15789
    },
    {
      "epoch": 31.770623742454728,
      "grad_norm": 0.8826726675033569,
      "learning_rate": 0.00013647248214106048,
      "loss": 0.2342,
      "step": 15790
    },
    {
      "epoch": 31.772635814889338,
      "grad_norm": 0.8839385509490967,
      "learning_rate": 0.00013646845759130696,
      "loss": 0.2389,
      "step": 15791
    },
    {
      "epoch": 31.774647887323944,
      "grad_norm": 0.8327046036720276,
      "learning_rate": 0.00013646443304155348,
      "loss": 0.229,
      "step": 15792
    },
    {
      "epoch": 31.77665995975855,
      "grad_norm": 0.880222499370575,
      "learning_rate": 0.0001364604084918,
      "loss": 0.2554,
      "step": 15793
    },
    {
      "epoch": 31.77867203219316,
      "grad_norm": 0.8838518261909485,
      "learning_rate": 0.0001364563839420465,
      "loss": 0.2414,
      "step": 15794
    },
    {
      "epoch": 31.780684104627767,
      "grad_norm": 0.8740003108978271,
      "learning_rate": 0.00013645235939229299,
      "loss": 0.2388,
      "step": 15795
    },
    {
      "epoch": 31.782696177062373,
      "grad_norm": 0.898747444152832,
      "learning_rate": 0.0001364483348425395,
      "loss": 0.2595,
      "step": 15796
    },
    {
      "epoch": 31.784708249496983,
      "grad_norm": 0.8849849104881287,
      "learning_rate": 0.00013644431029278598,
      "loss": 0.2352,
      "step": 15797
    },
    {
      "epoch": 31.78672032193159,
      "grad_norm": 0.8769135475158691,
      "learning_rate": 0.00013644028574303252,
      "loss": 0.2391,
      "step": 15798
    },
    {
      "epoch": 31.788732394366196,
      "grad_norm": 0.8799877762794495,
      "learning_rate": 0.000136436261193279,
      "loss": 0.2717,
      "step": 15799
    },
    {
      "epoch": 31.790744466800806,
      "grad_norm": 0.887500524520874,
      "learning_rate": 0.00013643223664352552,
      "loss": 0.2587,
      "step": 15800
    },
    {
      "epoch": 31.792756539235413,
      "grad_norm": 0.8902589678764343,
      "learning_rate": 0.000136428212093772,
      "loss": 0.2604,
      "step": 15801
    },
    {
      "epoch": 31.79476861167002,
      "grad_norm": 0.8507870435714722,
      "learning_rate": 0.00013642418754401852,
      "loss": 0.2625,
      "step": 15802
    },
    {
      "epoch": 31.79678068410463,
      "grad_norm": 0.9152154922485352,
      "learning_rate": 0.00013642016299426503,
      "loss": 0.2551,
      "step": 15803
    },
    {
      "epoch": 31.798792756539235,
      "grad_norm": 0.8860019445419312,
      "learning_rate": 0.00013641613844451154,
      "loss": 0.2457,
      "step": 15804
    },
    {
      "epoch": 31.800804828973842,
      "grad_norm": 0.8989262580871582,
      "learning_rate": 0.00013641211389475803,
      "loss": 0.2371,
      "step": 15805
    },
    {
      "epoch": 31.802816901408452,
      "grad_norm": 0.8642129302024841,
      "learning_rate": 0.00013640808934500454,
      "loss": 0.248,
      "step": 15806
    },
    {
      "epoch": 31.80482897384306,
      "grad_norm": 0.9046289920806885,
      "learning_rate": 0.00013640406479525102,
      "loss": 0.2567,
      "step": 15807
    },
    {
      "epoch": 31.806841046277665,
      "grad_norm": 0.8729205131530762,
      "learning_rate": 0.00013640004024549756,
      "loss": 0.2456,
      "step": 15808
    },
    {
      "epoch": 31.808853118712275,
      "grad_norm": 0.899008572101593,
      "learning_rate": 0.00013639601569574405,
      "loss": 0.2415,
      "step": 15809
    },
    {
      "epoch": 31.81086519114688,
      "grad_norm": 0.9075558185577393,
      "learning_rate": 0.00013639199114599056,
      "loss": 0.2399,
      "step": 15810
    },
    {
      "epoch": 31.812877263581488,
      "grad_norm": 0.9021344184875488,
      "learning_rate": 0.00013638796659623705,
      "loss": 0.2404,
      "step": 15811
    },
    {
      "epoch": 31.814889336016098,
      "grad_norm": 0.8838016390800476,
      "learning_rate": 0.00013638394204648356,
      "loss": 0.2566,
      "step": 15812
    },
    {
      "epoch": 31.816901408450704,
      "grad_norm": 0.8669403791427612,
      "learning_rate": 0.00013637991749673007,
      "loss": 0.228,
      "step": 15813
    },
    {
      "epoch": 31.81891348088531,
      "grad_norm": 0.9423788189888,
      "learning_rate": 0.00013637589294697658,
      "loss": 0.2766,
      "step": 15814
    },
    {
      "epoch": 31.82092555331992,
      "grad_norm": 0.9273073673248291,
      "learning_rate": 0.00013637186839722307,
      "loss": 0.2577,
      "step": 15815
    },
    {
      "epoch": 31.822937625754527,
      "grad_norm": 0.8957730531692505,
      "learning_rate": 0.00013636784384746958,
      "loss": 0.2623,
      "step": 15816
    },
    {
      "epoch": 31.824949698189133,
      "grad_norm": 0.9071012139320374,
      "learning_rate": 0.00013636381929771606,
      "loss": 0.2619,
      "step": 15817
    },
    {
      "epoch": 31.826961770623743,
      "grad_norm": 0.9088788628578186,
      "learning_rate": 0.00013635979474796258,
      "loss": 0.2687,
      "step": 15818
    },
    {
      "epoch": 31.82897384305835,
      "grad_norm": 0.8768891096115112,
      "learning_rate": 0.0001363557701982091,
      "loss": 0.2559,
      "step": 15819
    },
    {
      "epoch": 31.830985915492956,
      "grad_norm": 0.8524460792541504,
      "learning_rate": 0.00013635174564845557,
      "loss": 0.2649,
      "step": 15820
    },
    {
      "epoch": 31.832997987927566,
      "grad_norm": 0.8741129040718079,
      "learning_rate": 0.00013634772109870209,
      "loss": 0.259,
      "step": 15821
    },
    {
      "epoch": 31.835010060362173,
      "grad_norm": 0.867560088634491,
      "learning_rate": 0.0001363436965489486,
      "loss": 0.2376,
      "step": 15822
    },
    {
      "epoch": 31.83702213279678,
      "grad_norm": 0.8799037337303162,
      "learning_rate": 0.0001363396719991951,
      "loss": 0.2278,
      "step": 15823
    },
    {
      "epoch": 31.83903420523139,
      "grad_norm": 0.9082118272781372,
      "learning_rate": 0.0001363356474494416,
      "loss": 0.2648,
      "step": 15824
    },
    {
      "epoch": 31.841046277665995,
      "grad_norm": 0.8743370771408081,
      "learning_rate": 0.0001363316228996881,
      "loss": 0.2438,
      "step": 15825
    },
    {
      "epoch": 31.843058350100602,
      "grad_norm": 0.8947073817253113,
      "learning_rate": 0.0001363275983499346,
      "loss": 0.2441,
      "step": 15826
    },
    {
      "epoch": 31.845070422535212,
      "grad_norm": 0.966017484664917,
      "learning_rate": 0.0001363235738001811,
      "loss": 0.2468,
      "step": 15827
    },
    {
      "epoch": 31.84708249496982,
      "grad_norm": 0.8777217268943787,
      "learning_rate": 0.00013631954925042762,
      "loss": 0.2534,
      "step": 15828
    },
    {
      "epoch": 31.84909456740443,
      "grad_norm": 0.8868147134780884,
      "learning_rate": 0.00013631552470067413,
      "loss": 0.2558,
      "step": 15829
    },
    {
      "epoch": 31.851106639839035,
      "grad_norm": 0.8972784280776978,
      "learning_rate": 0.00013631150015092061,
      "loss": 0.2527,
      "step": 15830
    },
    {
      "epoch": 31.85311871227364,
      "grad_norm": 0.9718735814094543,
      "learning_rate": 0.00013630747560116713,
      "loss": 0.2766,
      "step": 15831
    },
    {
      "epoch": 31.85513078470825,
      "grad_norm": 0.8986403942108154,
      "learning_rate": 0.0001363034510514136,
      "loss": 0.2479,
      "step": 15832
    },
    {
      "epoch": 31.857142857142858,
      "grad_norm": 0.906245768070221,
      "learning_rate": 0.00013629942650166015,
      "loss": 0.2437,
      "step": 15833
    },
    {
      "epoch": 31.859154929577464,
      "grad_norm": 0.8999554514884949,
      "learning_rate": 0.00013629540195190664,
      "loss": 0.2655,
      "step": 15834
    },
    {
      "epoch": 31.861167002012074,
      "grad_norm": 0.8992883563041687,
      "learning_rate": 0.00013629137740215315,
      "loss": 0.2545,
      "step": 15835
    },
    {
      "epoch": 31.86317907444668,
      "grad_norm": 0.959507167339325,
      "learning_rate": 0.00013628735285239963,
      "loss": 0.2517,
      "step": 15836
    },
    {
      "epoch": 31.865191146881287,
      "grad_norm": 0.8517088890075684,
      "learning_rate": 0.00013628332830264615,
      "loss": 0.2532,
      "step": 15837
    },
    {
      "epoch": 31.867203219315897,
      "grad_norm": 0.9143363237380981,
      "learning_rate": 0.00013627930375289266,
      "loss": 0.2718,
      "step": 15838
    },
    {
      "epoch": 31.869215291750503,
      "grad_norm": 0.8881130814552307,
      "learning_rate": 0.00013627527920313917,
      "loss": 0.2514,
      "step": 15839
    },
    {
      "epoch": 31.87122736418511,
      "grad_norm": 0.9194580912590027,
      "learning_rate": 0.00013627125465338566,
      "loss": 0.2708,
      "step": 15840
    },
    {
      "epoch": 31.87323943661972,
      "grad_norm": 0.9053749442100525,
      "learning_rate": 0.00013626723010363217,
      "loss": 0.2607,
      "step": 15841
    },
    {
      "epoch": 31.875251509054326,
      "grad_norm": 0.92638099193573,
      "learning_rate": 0.00013626320555387865,
      "loss": 0.2633,
      "step": 15842
    },
    {
      "epoch": 31.877263581488933,
      "grad_norm": 0.8585773706436157,
      "learning_rate": 0.0001362591810041252,
      "loss": 0.2444,
      "step": 15843
    },
    {
      "epoch": 31.879275653923543,
      "grad_norm": 0.904075026512146,
      "learning_rate": 0.00013625515645437168,
      "loss": 0.2822,
      "step": 15844
    },
    {
      "epoch": 31.88128772635815,
      "grad_norm": 0.9389036297798157,
      "learning_rate": 0.0001362511319046182,
      "loss": 0.272,
      "step": 15845
    },
    {
      "epoch": 31.883299798792756,
      "grad_norm": 0.8926337957382202,
      "learning_rate": 0.00013624710735486467,
      "loss": 0.2548,
      "step": 15846
    },
    {
      "epoch": 31.885311871227366,
      "grad_norm": 0.8920887112617493,
      "learning_rate": 0.0001362430828051112,
      "loss": 0.242,
      "step": 15847
    },
    {
      "epoch": 31.887323943661972,
      "grad_norm": 0.9062901139259338,
      "learning_rate": 0.0001362390582553577,
      "loss": 0.2737,
      "step": 15848
    },
    {
      "epoch": 31.88933601609658,
      "grad_norm": 0.9228900671005249,
      "learning_rate": 0.0001362350337056042,
      "loss": 0.2627,
      "step": 15849
    },
    {
      "epoch": 31.89134808853119,
      "grad_norm": 0.8918077349662781,
      "learning_rate": 0.0001362310091558507,
      "loss": 0.2678,
      "step": 15850
    },
    {
      "epoch": 31.893360160965795,
      "grad_norm": 0.8651652932167053,
      "learning_rate": 0.0001362269846060972,
      "loss": 0.2651,
      "step": 15851
    },
    {
      "epoch": 31.8953722334004,
      "grad_norm": 0.8865771293640137,
      "learning_rate": 0.0001362229600563437,
      "loss": 0.2415,
      "step": 15852
    },
    {
      "epoch": 31.89738430583501,
      "grad_norm": 0.8739382028579712,
      "learning_rate": 0.0001362189355065902,
      "loss": 0.2419,
      "step": 15853
    },
    {
      "epoch": 31.899396378269618,
      "grad_norm": 0.9675498604774475,
      "learning_rate": 0.00013621491095683672,
      "loss": 0.3082,
      "step": 15854
    },
    {
      "epoch": 31.901408450704224,
      "grad_norm": 0.9315505027770996,
      "learning_rate": 0.0001362108864070832,
      "loss": 0.2649,
      "step": 15855
    },
    {
      "epoch": 31.903420523138834,
      "grad_norm": 0.8872681856155396,
      "learning_rate": 0.00013620686185732972,
      "loss": 0.2571,
      "step": 15856
    },
    {
      "epoch": 31.90543259557344,
      "grad_norm": 0.8626466393470764,
      "learning_rate": 0.00013620283730757623,
      "loss": 0.2396,
      "step": 15857
    },
    {
      "epoch": 31.907444668008047,
      "grad_norm": 0.9178318381309509,
      "learning_rate": 0.00013619881275782274,
      "loss": 0.2714,
      "step": 15858
    },
    {
      "epoch": 31.909456740442657,
      "grad_norm": 0.9065622687339783,
      "learning_rate": 0.00013619478820806923,
      "loss": 0.2489,
      "step": 15859
    },
    {
      "epoch": 31.911468812877263,
      "grad_norm": 0.9223650693893433,
      "learning_rate": 0.00013619076365831574,
      "loss": 0.2744,
      "step": 15860
    },
    {
      "epoch": 31.91348088531187,
      "grad_norm": 0.8634883761405945,
      "learning_rate": 0.00013618673910856222,
      "loss": 0.2659,
      "step": 15861
    },
    {
      "epoch": 31.91549295774648,
      "grad_norm": 0.884390115737915,
      "learning_rate": 0.00013618271455880873,
      "loss": 0.2545,
      "step": 15862
    },
    {
      "epoch": 31.917505030181086,
      "grad_norm": 0.9316217303276062,
      "learning_rate": 0.00013617869000905525,
      "loss": 0.2474,
      "step": 15863
    },
    {
      "epoch": 31.919517102615693,
      "grad_norm": 0.961142361164093,
      "learning_rate": 0.00013617466545930176,
      "loss": 0.2643,
      "step": 15864
    },
    {
      "epoch": 31.921529175050303,
      "grad_norm": 0.9085052609443665,
      "learning_rate": 0.00013617064090954824,
      "loss": 0.2519,
      "step": 15865
    },
    {
      "epoch": 31.92354124748491,
      "grad_norm": 0.9680656790733337,
      "learning_rate": 0.00013616661635979476,
      "loss": 0.2778,
      "step": 15866
    },
    {
      "epoch": 31.925553319919516,
      "grad_norm": 0.8590702414512634,
      "learning_rate": 0.00013616259181004124,
      "loss": 0.2488,
      "step": 15867
    },
    {
      "epoch": 31.927565392354126,
      "grad_norm": 0.953561544418335,
      "learning_rate": 0.00013615856726028778,
      "loss": 0.2583,
      "step": 15868
    },
    {
      "epoch": 31.929577464788732,
      "grad_norm": 0.9321461915969849,
      "learning_rate": 0.00013615454271053427,
      "loss": 0.2704,
      "step": 15869
    },
    {
      "epoch": 31.93158953722334,
      "grad_norm": 0.8423317074775696,
      "learning_rate": 0.00013615051816078078,
      "loss": 0.2502,
      "step": 15870
    },
    {
      "epoch": 31.93360160965795,
      "grad_norm": 0.8601815104484558,
      "learning_rate": 0.00013614649361102726,
      "loss": 0.2619,
      "step": 15871
    },
    {
      "epoch": 31.935613682092555,
      "grad_norm": 0.911449670791626,
      "learning_rate": 0.00013614246906127378,
      "loss": 0.2655,
      "step": 15872
    },
    {
      "epoch": 31.93762575452716,
      "grad_norm": 0.8891765475273132,
      "learning_rate": 0.0001361384445115203,
      "loss": 0.2575,
      "step": 15873
    },
    {
      "epoch": 31.93963782696177,
      "grad_norm": 0.861136257648468,
      "learning_rate": 0.0001361344199617668,
      "loss": 0.2425,
      "step": 15874
    },
    {
      "epoch": 31.941649899396378,
      "grad_norm": 0.8629997968673706,
      "learning_rate": 0.00013613039541201328,
      "loss": 0.2609,
      "step": 15875
    },
    {
      "epoch": 31.943661971830984,
      "grad_norm": 0.9457661509513855,
      "learning_rate": 0.0001361263708622598,
      "loss": 0.2807,
      "step": 15876
    },
    {
      "epoch": 31.945674044265594,
      "grad_norm": 0.9903919696807861,
      "learning_rate": 0.00013612234631250628,
      "loss": 0.2659,
      "step": 15877
    },
    {
      "epoch": 31.9476861167002,
      "grad_norm": 0.9040325880050659,
      "learning_rate": 0.00013611832176275282,
      "loss": 0.258,
      "step": 15878
    },
    {
      "epoch": 31.949698189134807,
      "grad_norm": 0.9585115909576416,
      "learning_rate": 0.0001361142972129993,
      "loss": 0.2583,
      "step": 15879
    },
    {
      "epoch": 31.951710261569417,
      "grad_norm": 0.9280683994293213,
      "learning_rate": 0.00013611027266324582,
      "loss": 0.2664,
      "step": 15880
    },
    {
      "epoch": 31.953722334004024,
      "grad_norm": 0.9351930618286133,
      "learning_rate": 0.0001361062481134923,
      "loss": 0.2439,
      "step": 15881
    },
    {
      "epoch": 31.955734406438633,
      "grad_norm": 0.9096007943153381,
      "learning_rate": 0.00013610222356373882,
      "loss": 0.2916,
      "step": 15882
    },
    {
      "epoch": 31.95774647887324,
      "grad_norm": 0.8271018266677856,
      "learning_rate": 0.00013609819901398533,
      "loss": 0.2531,
      "step": 15883
    },
    {
      "epoch": 31.959758551307846,
      "grad_norm": 0.8729835748672485,
      "learning_rate": 0.00013609417446423184,
      "loss": 0.2596,
      "step": 15884
    },
    {
      "epoch": 31.961770623742456,
      "grad_norm": 0.8659248948097229,
      "learning_rate": 0.00013609014991447833,
      "loss": 0.2393,
      "step": 15885
    },
    {
      "epoch": 31.963782696177063,
      "grad_norm": 0.8965398073196411,
      "learning_rate": 0.00013608612536472484,
      "loss": 0.2707,
      "step": 15886
    },
    {
      "epoch": 31.96579476861167,
      "grad_norm": 0.8600456118583679,
      "learning_rate": 0.00013608210081497132,
      "loss": 0.2617,
      "step": 15887
    },
    {
      "epoch": 31.96780684104628,
      "grad_norm": 0.9434508085250854,
      "learning_rate": 0.00013607807626521784,
      "loss": 0.262,
      "step": 15888
    },
    {
      "epoch": 31.969818913480886,
      "grad_norm": 0.9639032483100891,
      "learning_rate": 0.00013607405171546435,
      "loss": 0.2757,
      "step": 15889
    },
    {
      "epoch": 31.971830985915492,
      "grad_norm": 0.9341928362846375,
      "learning_rate": 0.00013607002716571083,
      "loss": 0.2497,
      "step": 15890
    },
    {
      "epoch": 31.973843058350102,
      "grad_norm": 0.9028846621513367,
      "learning_rate": 0.00013606600261595734,
      "loss": 0.2664,
      "step": 15891
    },
    {
      "epoch": 31.97585513078471,
      "grad_norm": 0.9262832999229431,
      "learning_rate": 0.00013606197806620383,
      "loss": 0.264,
      "step": 15892
    },
    {
      "epoch": 31.977867203219315,
      "grad_norm": 0.9923152327537537,
      "learning_rate": 0.00013605795351645034,
      "loss": 0.2634,
      "step": 15893
    },
    {
      "epoch": 31.979879275653925,
      "grad_norm": 0.8556100726127625,
      "learning_rate": 0.00013605392896669685,
      "loss": 0.2681,
      "step": 15894
    },
    {
      "epoch": 31.98189134808853,
      "grad_norm": 0.921956479549408,
      "learning_rate": 0.00013604990441694337,
      "loss": 0.2633,
      "step": 15895
    },
    {
      "epoch": 31.983903420523138,
      "grad_norm": 0.8818544149398804,
      "learning_rate": 0.00013604587986718985,
      "loss": 0.2424,
      "step": 15896
    },
    {
      "epoch": 31.985915492957748,
      "grad_norm": 0.8677986860275269,
      "learning_rate": 0.00013604185531743636,
      "loss": 0.248,
      "step": 15897
    },
    {
      "epoch": 31.987927565392354,
      "grad_norm": 0.893387496471405,
      "learning_rate": 0.00013603783076768285,
      "loss": 0.2635,
      "step": 15898
    },
    {
      "epoch": 31.98993963782696,
      "grad_norm": 0.8873082399368286,
      "learning_rate": 0.0001360338062179294,
      "loss": 0.2524,
      "step": 15899
    },
    {
      "epoch": 31.99195171026157,
      "grad_norm": 0.8941742181777954,
      "learning_rate": 0.00013602978166817587,
      "loss": 0.2866,
      "step": 15900
    },
    {
      "epoch": 31.993963782696177,
      "grad_norm": 0.9754754304885864,
      "learning_rate": 0.00013602575711842239,
      "loss": 0.2624,
      "step": 15901
    },
    {
      "epoch": 31.995975855130784,
      "grad_norm": 0.9031473398208618,
      "learning_rate": 0.00013602173256866887,
      "loss": 0.24,
      "step": 15902
    },
    {
      "epoch": 31.997987927565394,
      "grad_norm": 0.926592230796814,
      "learning_rate": 0.00013601770801891538,
      "loss": 0.2727,
      "step": 15903
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.8777860403060913,
      "learning_rate": 0.0001360136834691619,
      "loss": 0.2493,
      "step": 15904
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.3607490062713623,
      "eval_runtime": 49.8019,
      "eval_samples_per_second": 19.919,
      "eval_steps_per_second": 2.49,
      "step": 15904
    },
    {
      "epoch": 32.00201207243461,
      "grad_norm": 0.7150154113769531,
      "learning_rate": 0.0001360096589194084,
      "loss": 0.1912,
      "step": 15905
    },
    {
      "epoch": 32.00402414486921,
      "grad_norm": 0.7869028449058533,
      "learning_rate": 0.0001360056343696549,
      "loss": 0.2102,
      "step": 15906
    },
    {
      "epoch": 32.00603621730382,
      "grad_norm": 0.7826861143112183,
      "learning_rate": 0.0001360016098199014,
      "loss": 0.2071,
      "step": 15907
    },
    {
      "epoch": 32.00804828973843,
      "grad_norm": 0.8124145865440369,
      "learning_rate": 0.0001359975852701479,
      "loss": 0.1833,
      "step": 15908
    },
    {
      "epoch": 32.010060362173036,
      "grad_norm": 0.8242446780204773,
      "learning_rate": 0.00013599356072039443,
      "loss": 0.2007,
      "step": 15909
    },
    {
      "epoch": 32.012072434607646,
      "grad_norm": 0.7993395924568176,
      "learning_rate": 0.00013598953617064091,
      "loss": 0.179,
      "step": 15910
    },
    {
      "epoch": 32.014084507042256,
      "grad_norm": 0.77145916223526,
      "learning_rate": 0.00013598551162088743,
      "loss": 0.199,
      "step": 15911
    },
    {
      "epoch": 32.01609657947686,
      "grad_norm": 0.8147225379943848,
      "learning_rate": 0.0001359814870711339,
      "loss": 0.1853,
      "step": 15912
    },
    {
      "epoch": 32.01810865191147,
      "grad_norm": 0.8732172846794128,
      "learning_rate": 0.00013597746252138042,
      "loss": 0.2169,
      "step": 15913
    },
    {
      "epoch": 32.02012072434608,
      "grad_norm": 0.7463902235031128,
      "learning_rate": 0.00013597343797162694,
      "loss": 0.21,
      "step": 15914
    },
    {
      "epoch": 32.02213279678068,
      "grad_norm": 0.751475989818573,
      "learning_rate": 0.00013596941342187345,
      "loss": 0.2005,
      "step": 15915
    },
    {
      "epoch": 32.02414486921529,
      "grad_norm": 0.8082079291343689,
      "learning_rate": 0.00013596538887211993,
      "loss": 0.2167,
      "step": 15916
    },
    {
      "epoch": 32.0261569416499,
      "grad_norm": 0.8535643219947815,
      "learning_rate": 0.00013596136432236645,
      "loss": 0.2151,
      "step": 15917
    },
    {
      "epoch": 32.028169014084504,
      "grad_norm": 0.8206613659858704,
      "learning_rate": 0.00013595733977261293,
      "loss": 0.2078,
      "step": 15918
    },
    {
      "epoch": 32.030181086519114,
      "grad_norm": 0.8000060319900513,
      "learning_rate": 0.00013595331522285944,
      "loss": 0.1979,
      "step": 15919
    },
    {
      "epoch": 32.032193158953724,
      "grad_norm": 0.7797231078147888,
      "learning_rate": 0.00013594929067310596,
      "loss": 0.2,
      "step": 15920
    },
    {
      "epoch": 32.03420523138833,
      "grad_norm": 0.7413157820701599,
      "learning_rate": 0.00013594526612335247,
      "loss": 0.1876,
      "step": 15921
    },
    {
      "epoch": 32.03621730382294,
      "grad_norm": 0.8220855593681335,
      "learning_rate": 0.00013594124157359895,
      "loss": 0.2153,
      "step": 15922
    },
    {
      "epoch": 32.03822937625755,
      "grad_norm": 0.8173365592956543,
      "learning_rate": 0.00013593721702384546,
      "loss": 0.2015,
      "step": 15923
    },
    {
      "epoch": 32.04024144869215,
      "grad_norm": 0.8520382046699524,
      "learning_rate": 0.00013593319247409198,
      "loss": 0.2185,
      "step": 15924
    },
    {
      "epoch": 32.04225352112676,
      "grad_norm": 0.833906352519989,
      "learning_rate": 0.00013592916792433846,
      "loss": 0.1973,
      "step": 15925
    },
    {
      "epoch": 32.04426559356137,
      "grad_norm": 0.7366182208061218,
      "learning_rate": 0.00013592514337458497,
      "loss": 0.187,
      "step": 15926
    },
    {
      "epoch": 32.04627766599597,
      "grad_norm": 0.8709272146224976,
      "learning_rate": 0.00013592111882483146,
      "loss": 0.2147,
      "step": 15927
    },
    {
      "epoch": 32.04828973843058,
      "grad_norm": 0.851814866065979,
      "learning_rate": 0.00013591709427507797,
      "loss": 0.2016,
      "step": 15928
    },
    {
      "epoch": 32.05030181086519,
      "grad_norm": 0.8487421274185181,
      "learning_rate": 0.00013591306972532448,
      "loss": 0.2229,
      "step": 15929
    },
    {
      "epoch": 32.052313883299796,
      "grad_norm": 0.8452164530754089,
      "learning_rate": 0.000135909045175571,
      "loss": 0.2154,
      "step": 15930
    },
    {
      "epoch": 32.054325955734406,
      "grad_norm": 0.8298094272613525,
      "learning_rate": 0.00013590502062581748,
      "loss": 0.2146,
      "step": 15931
    },
    {
      "epoch": 32.056338028169016,
      "grad_norm": 0.8025491237640381,
      "learning_rate": 0.000135900996076064,
      "loss": 0.2089,
      "step": 15932
    },
    {
      "epoch": 32.05835010060362,
      "grad_norm": 0.7862149477005005,
      "learning_rate": 0.00013589697152631048,
      "loss": 0.204,
      "step": 15933
    },
    {
      "epoch": 32.06036217303823,
      "grad_norm": 0.7912437319755554,
      "learning_rate": 0.00013589294697655702,
      "loss": 0.1869,
      "step": 15934
    },
    {
      "epoch": 32.06237424547284,
      "grad_norm": 0.7881579995155334,
      "learning_rate": 0.0001358889224268035,
      "loss": 0.2004,
      "step": 15935
    },
    {
      "epoch": 32.06438631790744,
      "grad_norm": 0.8019194602966309,
      "learning_rate": 0.00013588489787705002,
      "loss": 0.1886,
      "step": 15936
    },
    {
      "epoch": 32.06639839034205,
      "grad_norm": 0.7568671107292175,
      "learning_rate": 0.0001358808733272965,
      "loss": 0.1721,
      "step": 15937
    },
    {
      "epoch": 32.06841046277666,
      "grad_norm": 0.8517454862594604,
      "learning_rate": 0.000135876848777543,
      "loss": 0.2067,
      "step": 15938
    },
    {
      "epoch": 32.070422535211264,
      "grad_norm": 0.7746936082839966,
      "learning_rate": 0.00013587282422778952,
      "loss": 0.1977,
      "step": 15939
    },
    {
      "epoch": 32.072434607645874,
      "grad_norm": 0.7845594882965088,
      "learning_rate": 0.00013586879967803604,
      "loss": 0.2023,
      "step": 15940
    },
    {
      "epoch": 32.074446680080484,
      "grad_norm": 0.8077641129493713,
      "learning_rate": 0.00013586477512828252,
      "loss": 0.1901,
      "step": 15941
    },
    {
      "epoch": 32.07645875251509,
      "grad_norm": 0.8074753880500793,
      "learning_rate": 0.00013586075057852903,
      "loss": 0.1744,
      "step": 15942
    },
    {
      "epoch": 32.0784708249497,
      "grad_norm": 0.8295554518699646,
      "learning_rate": 0.00013585672602877552,
      "loss": 0.1908,
      "step": 15943
    },
    {
      "epoch": 32.08048289738431,
      "grad_norm": 0.7958266735076904,
      "learning_rate": 0.00013585270147902206,
      "loss": 0.1969,
      "step": 15944
    },
    {
      "epoch": 32.08249496981891,
      "grad_norm": 0.8109015822410583,
      "learning_rate": 0.00013584867692926854,
      "loss": 0.2187,
      "step": 15945
    },
    {
      "epoch": 32.08450704225352,
      "grad_norm": 0.8256819248199463,
      "learning_rate": 0.00013584465237951506,
      "loss": 0.1962,
      "step": 15946
    },
    {
      "epoch": 32.08651911468813,
      "grad_norm": 0.7865285277366638,
      "learning_rate": 0.00013584062782976154,
      "loss": 0.2198,
      "step": 15947
    },
    {
      "epoch": 32.08853118712273,
      "grad_norm": 0.830845832824707,
      "learning_rate": 0.00013583660328000805,
      "loss": 0.2119,
      "step": 15948
    },
    {
      "epoch": 32.09054325955734,
      "grad_norm": 0.8245295882225037,
      "learning_rate": 0.00013583257873025457,
      "loss": 0.194,
      "step": 15949
    },
    {
      "epoch": 32.09255533199195,
      "grad_norm": 0.8009548783302307,
      "learning_rate": 0.00013582855418050108,
      "loss": 0.2065,
      "step": 15950
    },
    {
      "epoch": 32.094567404426556,
      "grad_norm": 0.8130697011947632,
      "learning_rate": 0.00013582452963074756,
      "loss": 0.2096,
      "step": 15951
    },
    {
      "epoch": 32.096579476861166,
      "grad_norm": 0.8143240809440613,
      "learning_rate": 0.00013582050508099408,
      "loss": 0.1958,
      "step": 15952
    },
    {
      "epoch": 32.098591549295776,
      "grad_norm": 0.8332030773162842,
      "learning_rate": 0.00013581648053124056,
      "loss": 0.2121,
      "step": 15953
    },
    {
      "epoch": 32.100603621730386,
      "grad_norm": 0.8366901278495789,
      "learning_rate": 0.00013581245598148707,
      "loss": 0.2308,
      "step": 15954
    },
    {
      "epoch": 32.10261569416499,
      "grad_norm": 0.7642009258270264,
      "learning_rate": 0.00013580843143173358,
      "loss": 0.1881,
      "step": 15955
    },
    {
      "epoch": 32.1046277665996,
      "grad_norm": 0.7526729106903076,
      "learning_rate": 0.0001358044068819801,
      "loss": 0.2122,
      "step": 15956
    },
    {
      "epoch": 32.10663983903421,
      "grad_norm": 0.7721143960952759,
      "learning_rate": 0.00013580038233222658,
      "loss": 0.1846,
      "step": 15957
    },
    {
      "epoch": 32.10865191146881,
      "grad_norm": 0.8288601040840149,
      "learning_rate": 0.0001357963577824731,
      "loss": 0.2062,
      "step": 15958
    },
    {
      "epoch": 32.11066398390342,
      "grad_norm": 0.8460414409637451,
      "learning_rate": 0.0001357923332327196,
      "loss": 0.2012,
      "step": 15959
    },
    {
      "epoch": 32.11267605633803,
      "grad_norm": 0.8303143382072449,
      "learning_rate": 0.0001357883086829661,
      "loss": 0.2097,
      "step": 15960
    },
    {
      "epoch": 32.114688128772634,
      "grad_norm": 0.7859522700309753,
      "learning_rate": 0.0001357842841332126,
      "loss": 0.173,
      "step": 15961
    },
    {
      "epoch": 32.116700201207244,
      "grad_norm": 0.8021812438964844,
      "learning_rate": 0.0001357802595834591,
      "loss": 0.2088,
      "step": 15962
    },
    {
      "epoch": 32.118712273641854,
      "grad_norm": 0.8122722506523132,
      "learning_rate": 0.0001357762350337056,
      "loss": 0.2093,
      "step": 15963
    },
    {
      "epoch": 32.12072434607646,
      "grad_norm": 0.8261445164680481,
      "learning_rate": 0.0001357722104839521,
      "loss": 0.2036,
      "step": 15964
    },
    {
      "epoch": 32.12273641851107,
      "grad_norm": 0.807080090045929,
      "learning_rate": 0.00013576818593419863,
      "loss": 0.1911,
      "step": 15965
    },
    {
      "epoch": 32.12474849094568,
      "grad_norm": 0.7856994271278381,
      "learning_rate": 0.0001357641613844451,
      "loss": 0.1942,
      "step": 15966
    },
    {
      "epoch": 32.12676056338028,
      "grad_norm": 0.8161177635192871,
      "learning_rate": 0.00013576013683469162,
      "loss": 0.2082,
      "step": 15967
    },
    {
      "epoch": 32.12877263581489,
      "grad_norm": 0.8506578207015991,
      "learning_rate": 0.0001357561122849381,
      "loss": 0.2083,
      "step": 15968
    },
    {
      "epoch": 32.1307847082495,
      "grad_norm": 0.8703933954238892,
      "learning_rate": 0.00013575208773518465,
      "loss": 0.206,
      "step": 15969
    },
    {
      "epoch": 32.1327967806841,
      "grad_norm": 0.8373337388038635,
      "learning_rate": 0.00013574806318543113,
      "loss": 0.2033,
      "step": 15970
    },
    {
      "epoch": 32.13480885311871,
      "grad_norm": 0.84971022605896,
      "learning_rate": 0.00013574403863567764,
      "loss": 0.21,
      "step": 15971
    },
    {
      "epoch": 32.13682092555332,
      "grad_norm": 0.8470948934555054,
      "learning_rate": 0.00013574001408592413,
      "loss": 0.2091,
      "step": 15972
    },
    {
      "epoch": 32.138832997987926,
      "grad_norm": 0.8449073433876038,
      "learning_rate": 0.00013573598953617064,
      "loss": 0.2159,
      "step": 15973
    },
    {
      "epoch": 32.140845070422536,
      "grad_norm": 0.8140027523040771,
      "learning_rate": 0.00013573196498641715,
      "loss": 0.2045,
      "step": 15974
    },
    {
      "epoch": 32.142857142857146,
      "grad_norm": 0.8358930349349976,
      "learning_rate": 0.00013572794043666367,
      "loss": 0.2263,
      "step": 15975
    },
    {
      "epoch": 32.14486921529175,
      "grad_norm": 0.8722131848335266,
      "learning_rate": 0.00013572391588691015,
      "loss": 0.2019,
      "step": 15976
    },
    {
      "epoch": 32.14688128772636,
      "grad_norm": 0.8313561677932739,
      "learning_rate": 0.00013571989133715666,
      "loss": 0.211,
      "step": 15977
    },
    {
      "epoch": 32.14889336016097,
      "grad_norm": 0.8132882118225098,
      "learning_rate": 0.00013571586678740315,
      "loss": 0.2005,
      "step": 15978
    },
    {
      "epoch": 32.15090543259557,
      "grad_norm": 0.8755271434783936,
      "learning_rate": 0.0001357118422376497,
      "loss": 0.2057,
      "step": 15979
    },
    {
      "epoch": 32.15291750503018,
      "grad_norm": 0.8292023539543152,
      "learning_rate": 0.00013570781768789617,
      "loss": 0.2214,
      "step": 15980
    },
    {
      "epoch": 32.15492957746479,
      "grad_norm": 0.8009081482887268,
      "learning_rate": 0.00013570379313814269,
      "loss": 0.215,
      "step": 15981
    },
    {
      "epoch": 32.156941649899395,
      "grad_norm": 0.9041582345962524,
      "learning_rate": 0.00013569976858838917,
      "loss": 0.2243,
      "step": 15982
    },
    {
      "epoch": 32.158953722334005,
      "grad_norm": 0.8659648895263672,
      "learning_rate": 0.00013569574403863568,
      "loss": 0.2157,
      "step": 15983
    },
    {
      "epoch": 32.160965794768615,
      "grad_norm": 0.8208624720573425,
      "learning_rate": 0.0001356917194888822,
      "loss": 0.2122,
      "step": 15984
    },
    {
      "epoch": 32.16297786720322,
      "grad_norm": 0.7974827885627747,
      "learning_rate": 0.0001356876949391287,
      "loss": 0.2021,
      "step": 15985
    },
    {
      "epoch": 32.16498993963783,
      "grad_norm": 0.8498311042785645,
      "learning_rate": 0.0001356836703893752,
      "loss": 0.2079,
      "step": 15986
    },
    {
      "epoch": 32.16700201207244,
      "grad_norm": 0.9573697447776794,
      "learning_rate": 0.0001356796458396217,
      "loss": 0.2202,
      "step": 15987
    },
    {
      "epoch": 32.16901408450704,
      "grad_norm": 0.8126676678657532,
      "learning_rate": 0.0001356756212898682,
      "loss": 0.1945,
      "step": 15988
    },
    {
      "epoch": 32.17102615694165,
      "grad_norm": 0.9226963520050049,
      "learning_rate": 0.0001356715967401147,
      "loss": 0.2411,
      "step": 15989
    },
    {
      "epoch": 32.17303822937626,
      "grad_norm": 0.8414748311042786,
      "learning_rate": 0.00013566757219036121,
      "loss": 0.2026,
      "step": 15990
    },
    {
      "epoch": 32.17505030181086,
      "grad_norm": 0.8288074731826782,
      "learning_rate": 0.00013566354764060773,
      "loss": 0.1998,
      "step": 15991
    },
    {
      "epoch": 32.17706237424547,
      "grad_norm": 0.8532222509384155,
      "learning_rate": 0.0001356595230908542,
      "loss": 0.2136,
      "step": 15992
    },
    {
      "epoch": 32.17907444668008,
      "grad_norm": 0.8564881682395935,
      "learning_rate": 0.00013565549854110072,
      "loss": 0.2049,
      "step": 15993
    },
    {
      "epoch": 32.181086519114686,
      "grad_norm": 0.8352370262145996,
      "learning_rate": 0.00013565147399134724,
      "loss": 0.2016,
      "step": 15994
    },
    {
      "epoch": 32.183098591549296,
      "grad_norm": 0.8720434904098511,
      "learning_rate": 0.00013564744944159372,
      "loss": 0.2261,
      "step": 15995
    },
    {
      "epoch": 32.185110663983906,
      "grad_norm": 0.8728755712509155,
      "learning_rate": 0.00013564342489184023,
      "loss": 0.2002,
      "step": 15996
    },
    {
      "epoch": 32.18712273641851,
      "grad_norm": 0.8244792222976685,
      "learning_rate": 0.00013563940034208672,
      "loss": 0.2079,
      "step": 15997
    },
    {
      "epoch": 32.18913480885312,
      "grad_norm": 0.824660062789917,
      "learning_rate": 0.00013563537579233323,
      "loss": 0.2165,
      "step": 15998
    },
    {
      "epoch": 32.19114688128773,
      "grad_norm": 0.8781567811965942,
      "learning_rate": 0.00013563135124257974,
      "loss": 0.2108,
      "step": 15999
    },
    {
      "epoch": 32.19315895372233,
      "grad_norm": 0.8978358507156372,
      "learning_rate": 0.00013562732669282626,
      "loss": 0.2249,
      "step": 16000
    },
    {
      "epoch": 32.19517102615694,
      "grad_norm": 0.8474695682525635,
      "learning_rate": 0.00013562330214307274,
      "loss": 0.2146,
      "step": 16001
    },
    {
      "epoch": 32.19718309859155,
      "grad_norm": 0.8762299418449402,
      "learning_rate": 0.00013561927759331925,
      "loss": 0.2122,
      "step": 16002
    },
    {
      "epoch": 32.199195171026155,
      "grad_norm": 0.85357266664505,
      "learning_rate": 0.00013561525304356574,
      "loss": 0.1865,
      "step": 16003
    },
    {
      "epoch": 32.201207243460765,
      "grad_norm": 0.8844016194343567,
      "learning_rate": 0.00013561122849381228,
      "loss": 0.2073,
      "step": 16004
    },
    {
      "epoch": 32.203219315895375,
      "grad_norm": 0.8871591091156006,
      "learning_rate": 0.00013560720394405876,
      "loss": 0.1966,
      "step": 16005
    },
    {
      "epoch": 32.20523138832998,
      "grad_norm": 0.9016461372375488,
      "learning_rate": 0.00013560317939430527,
      "loss": 0.1987,
      "step": 16006
    },
    {
      "epoch": 32.20724346076459,
      "grad_norm": 0.8488925695419312,
      "learning_rate": 0.00013559915484455176,
      "loss": 0.2264,
      "step": 16007
    },
    {
      "epoch": 32.2092555331992,
      "grad_norm": 0.8216913342475891,
      "learning_rate": 0.00013559513029479827,
      "loss": 0.2006,
      "step": 16008
    },
    {
      "epoch": 32.2112676056338,
      "grad_norm": 0.83226078748703,
      "learning_rate": 0.00013559110574504478,
      "loss": 0.2205,
      "step": 16009
    },
    {
      "epoch": 32.21327967806841,
      "grad_norm": 0.8794342875480652,
      "learning_rate": 0.0001355870811952913,
      "loss": 0.222,
      "step": 16010
    },
    {
      "epoch": 32.21529175050302,
      "grad_norm": 0.8454188108444214,
      "learning_rate": 0.00013558305664553778,
      "loss": 0.2212,
      "step": 16011
    },
    {
      "epoch": 32.21730382293762,
      "grad_norm": 0.8636046051979065,
      "learning_rate": 0.0001355790320957843,
      "loss": 0.2075,
      "step": 16012
    },
    {
      "epoch": 32.21931589537223,
      "grad_norm": 0.8685774207115173,
      "learning_rate": 0.00013557500754603078,
      "loss": 0.2013,
      "step": 16013
    },
    {
      "epoch": 32.22132796780684,
      "grad_norm": 0.86738121509552,
      "learning_rate": 0.00013557098299627732,
      "loss": 0.2228,
      "step": 16014
    },
    {
      "epoch": 32.223340040241446,
      "grad_norm": 0.8382818102836609,
      "learning_rate": 0.0001355669584465238,
      "loss": 0.2148,
      "step": 16015
    },
    {
      "epoch": 32.225352112676056,
      "grad_norm": 0.8545313477516174,
      "learning_rate": 0.00013556293389677031,
      "loss": 0.2077,
      "step": 16016
    },
    {
      "epoch": 32.227364185110666,
      "grad_norm": 0.8651078343391418,
      "learning_rate": 0.0001355589093470168,
      "loss": 0.2094,
      "step": 16017
    },
    {
      "epoch": 32.22937625754527,
      "grad_norm": 0.8702012300491333,
      "learning_rate": 0.0001355548847972633,
      "loss": 0.2158,
      "step": 16018
    },
    {
      "epoch": 32.23138832997988,
      "grad_norm": 0.930675745010376,
      "learning_rate": 0.00013555086024750982,
      "loss": 0.2241,
      "step": 16019
    },
    {
      "epoch": 32.23340040241449,
      "grad_norm": 0.9471601843833923,
      "learning_rate": 0.00013554683569775634,
      "loss": 0.2118,
      "step": 16020
    },
    {
      "epoch": 32.23541247484909,
      "grad_norm": 0.8799822926521301,
      "learning_rate": 0.00013554281114800282,
      "loss": 0.2188,
      "step": 16021
    },
    {
      "epoch": 32.2374245472837,
      "grad_norm": 0.8002187013626099,
      "learning_rate": 0.00013553878659824933,
      "loss": 0.2044,
      "step": 16022
    },
    {
      "epoch": 32.23943661971831,
      "grad_norm": 0.8603873252868652,
      "learning_rate": 0.00013553476204849582,
      "loss": 0.2148,
      "step": 16023
    },
    {
      "epoch": 32.241448692152915,
      "grad_norm": 0.8951877355575562,
      "learning_rate": 0.00013553073749874233,
      "loss": 0.2194,
      "step": 16024
    },
    {
      "epoch": 32.243460764587525,
      "grad_norm": 0.8107613325119019,
      "learning_rate": 0.00013552671294898884,
      "loss": 0.2222,
      "step": 16025
    },
    {
      "epoch": 32.245472837022135,
      "grad_norm": 0.8631802797317505,
      "learning_rate": 0.00013552268839923536,
      "loss": 0.2243,
      "step": 16026
    },
    {
      "epoch": 32.24748490945674,
      "grad_norm": 0.8451249003410339,
      "learning_rate": 0.00013551866384948184,
      "loss": 0.2129,
      "step": 16027
    },
    {
      "epoch": 32.24949698189135,
      "grad_norm": 0.8862402439117432,
      "learning_rate": 0.00013551463929972835,
      "loss": 0.22,
      "step": 16028
    },
    {
      "epoch": 32.25150905432596,
      "grad_norm": 0.793100118637085,
      "learning_rate": 0.00013551061474997487,
      "loss": 0.1985,
      "step": 16029
    },
    {
      "epoch": 32.25352112676056,
      "grad_norm": 0.8497390747070312,
      "learning_rate": 0.00013550659020022135,
      "loss": 0.2003,
      "step": 16030
    },
    {
      "epoch": 32.25553319919517,
      "grad_norm": 0.8715998530387878,
      "learning_rate": 0.00013550256565046786,
      "loss": 0.2291,
      "step": 16031
    },
    {
      "epoch": 32.25754527162978,
      "grad_norm": 0.8693658709526062,
      "learning_rate": 0.00013549854110071435,
      "loss": 0.2135,
      "step": 16032
    },
    {
      "epoch": 32.25955734406438,
      "grad_norm": 0.862524151802063,
      "learning_rate": 0.00013549451655096086,
      "loss": 0.2023,
      "step": 16033
    },
    {
      "epoch": 32.26156941649899,
      "grad_norm": 0.9031442403793335,
      "learning_rate": 0.00013549049200120737,
      "loss": 0.2384,
      "step": 16034
    },
    {
      "epoch": 32.2635814889336,
      "grad_norm": 0.8651794791221619,
      "learning_rate": 0.00013548646745145388,
      "loss": 0.2256,
      "step": 16035
    },
    {
      "epoch": 32.265593561368206,
      "grad_norm": 0.8840665221214294,
      "learning_rate": 0.00013548244290170037,
      "loss": 0.201,
      "step": 16036
    },
    {
      "epoch": 32.267605633802816,
      "grad_norm": 0.881686270236969,
      "learning_rate": 0.00013547841835194688,
      "loss": 0.2225,
      "step": 16037
    },
    {
      "epoch": 32.269617706237426,
      "grad_norm": 0.8431927561759949,
      "learning_rate": 0.00013547439380219337,
      "loss": 0.2141,
      "step": 16038
    },
    {
      "epoch": 32.27162977867203,
      "grad_norm": 0.871386706829071,
      "learning_rate": 0.0001354703692524399,
      "loss": 0.2171,
      "step": 16039
    },
    {
      "epoch": 32.27364185110664,
      "grad_norm": 0.8833690285682678,
      "learning_rate": 0.0001354663447026864,
      "loss": 0.2232,
      "step": 16040
    },
    {
      "epoch": 32.27565392354125,
      "grad_norm": 0.8493334650993347,
      "learning_rate": 0.0001354623201529329,
      "loss": 0.2248,
      "step": 16041
    },
    {
      "epoch": 32.27766599597585,
      "grad_norm": 0.8960721492767334,
      "learning_rate": 0.0001354582956031794,
      "loss": 0.2239,
      "step": 16042
    },
    {
      "epoch": 32.27967806841046,
      "grad_norm": 0.8850306272506714,
      "learning_rate": 0.0001354542710534259,
      "loss": 0.2152,
      "step": 16043
    },
    {
      "epoch": 32.28169014084507,
      "grad_norm": 0.8414797186851501,
      "learning_rate": 0.0001354502465036724,
      "loss": 0.2023,
      "step": 16044
    },
    {
      "epoch": 32.283702213279675,
      "grad_norm": 0.9008194208145142,
      "learning_rate": 0.00013544622195391893,
      "loss": 0.2413,
      "step": 16045
    },
    {
      "epoch": 32.285714285714285,
      "grad_norm": 0.8555318117141724,
      "learning_rate": 0.0001354421974041654,
      "loss": 0.232,
      "step": 16046
    },
    {
      "epoch": 32.287726358148895,
      "grad_norm": 0.8978758454322815,
      "learning_rate": 0.00013543817285441192,
      "loss": 0.2294,
      "step": 16047
    },
    {
      "epoch": 32.2897384305835,
      "grad_norm": 0.9515752792358398,
      "learning_rate": 0.0001354341483046584,
      "loss": 0.2291,
      "step": 16048
    },
    {
      "epoch": 32.29175050301811,
      "grad_norm": 0.875274121761322,
      "learning_rate": 0.00013543012375490495,
      "loss": 0.2296,
      "step": 16049
    },
    {
      "epoch": 32.29376257545272,
      "grad_norm": 0.8761144876480103,
      "learning_rate": 0.00013542609920515143,
      "loss": 0.2238,
      "step": 16050
    },
    {
      "epoch": 32.29577464788732,
      "grad_norm": 0.8378172516822815,
      "learning_rate": 0.00013542207465539794,
      "loss": 0.2177,
      "step": 16051
    },
    {
      "epoch": 32.29778672032193,
      "grad_norm": 0.8632956147193909,
      "learning_rate": 0.00013541805010564443,
      "loss": 0.2448,
      "step": 16052
    },
    {
      "epoch": 32.29979879275654,
      "grad_norm": 0.841534435749054,
      "learning_rate": 0.00013541402555589094,
      "loss": 0.2309,
      "step": 16053
    },
    {
      "epoch": 32.30181086519114,
      "grad_norm": 0.8311383724212646,
      "learning_rate": 0.00013541000100613745,
      "loss": 0.2122,
      "step": 16054
    },
    {
      "epoch": 32.30382293762575,
      "grad_norm": 0.8730985522270203,
      "learning_rate": 0.00013540597645638397,
      "loss": 0.2185,
      "step": 16055
    },
    {
      "epoch": 32.30583501006036,
      "grad_norm": 0.8648189306259155,
      "learning_rate": 0.00013540195190663045,
      "loss": 0.2217,
      "step": 16056
    },
    {
      "epoch": 32.30784708249497,
      "grad_norm": 0.8576266169548035,
      "learning_rate": 0.00013539792735687696,
      "loss": 0.2097,
      "step": 16057
    },
    {
      "epoch": 32.309859154929576,
      "grad_norm": 0.9269458055496216,
      "learning_rate": 0.00013539390280712345,
      "loss": 0.2377,
      "step": 16058
    },
    {
      "epoch": 32.311871227364186,
      "grad_norm": 0.879268229007721,
      "learning_rate": 0.00013538987825736996,
      "loss": 0.2216,
      "step": 16059
    },
    {
      "epoch": 32.313883299798796,
      "grad_norm": 0.906700849533081,
      "learning_rate": 0.00013538585370761647,
      "loss": 0.2111,
      "step": 16060
    },
    {
      "epoch": 32.3158953722334,
      "grad_norm": 0.8886867165565491,
      "learning_rate": 0.00013538182915786296,
      "loss": 0.2339,
      "step": 16061
    },
    {
      "epoch": 32.31790744466801,
      "grad_norm": 0.9093060493469238,
      "learning_rate": 0.00013537780460810947,
      "loss": 0.2299,
      "step": 16062
    },
    {
      "epoch": 32.31991951710262,
      "grad_norm": 0.8727355003356934,
      "learning_rate": 0.00013537378005835598,
      "loss": 0.2119,
      "step": 16063
    },
    {
      "epoch": 32.32193158953722,
      "grad_norm": 0.8963690996170044,
      "learning_rate": 0.0001353697555086025,
      "loss": 0.2323,
      "step": 16064
    },
    {
      "epoch": 32.32394366197183,
      "grad_norm": 0.8930625319480896,
      "learning_rate": 0.00013536573095884898,
      "loss": 0.2293,
      "step": 16065
    },
    {
      "epoch": 32.32595573440644,
      "grad_norm": 0.8694223165512085,
      "learning_rate": 0.0001353617064090955,
      "loss": 0.2123,
      "step": 16066
    },
    {
      "epoch": 32.327967806841045,
      "grad_norm": 0.890591025352478,
      "learning_rate": 0.00013535768185934198,
      "loss": 0.198,
      "step": 16067
    },
    {
      "epoch": 32.329979879275655,
      "grad_norm": 0.8312090039253235,
      "learning_rate": 0.0001353536573095885,
      "loss": 0.2154,
      "step": 16068
    },
    {
      "epoch": 32.331991951710265,
      "grad_norm": 0.8461349606513977,
      "learning_rate": 0.000135349632759835,
      "loss": 0.2134,
      "step": 16069
    },
    {
      "epoch": 32.33400402414487,
      "grad_norm": 0.875789225101471,
      "learning_rate": 0.00013534560821008151,
      "loss": 0.2252,
      "step": 16070
    },
    {
      "epoch": 32.33601609657948,
      "grad_norm": 0.8260918855667114,
      "learning_rate": 0.000135341583660328,
      "loss": 0.2168,
      "step": 16071
    },
    {
      "epoch": 32.33802816901409,
      "grad_norm": 0.8839207887649536,
      "learning_rate": 0.0001353375591105745,
      "loss": 0.2199,
      "step": 16072
    },
    {
      "epoch": 32.34004024144869,
      "grad_norm": 0.847825288772583,
      "learning_rate": 0.000135333534560821,
      "loss": 0.2263,
      "step": 16073
    },
    {
      "epoch": 32.3420523138833,
      "grad_norm": 0.9307084679603577,
      "learning_rate": 0.00013532951001106754,
      "loss": 0.2224,
      "step": 16074
    },
    {
      "epoch": 32.34406438631791,
      "grad_norm": 0.8573852777481079,
      "learning_rate": 0.00013532548546131402,
      "loss": 0.2108,
      "step": 16075
    },
    {
      "epoch": 32.34607645875251,
      "grad_norm": 0.902565598487854,
      "learning_rate": 0.00013532146091156053,
      "loss": 0.229,
      "step": 16076
    },
    {
      "epoch": 32.34808853118712,
      "grad_norm": 0.8613728284835815,
      "learning_rate": 0.00013531743636180702,
      "loss": 0.2007,
      "step": 16077
    },
    {
      "epoch": 32.35010060362173,
      "grad_norm": 0.8396845459938049,
      "learning_rate": 0.00013531341181205353,
      "loss": 0.2239,
      "step": 16078
    },
    {
      "epoch": 32.352112676056336,
      "grad_norm": 0.8733789920806885,
      "learning_rate": 0.00013530938726230004,
      "loss": 0.2142,
      "step": 16079
    },
    {
      "epoch": 32.354124748490946,
      "grad_norm": 0.8515836596488953,
      "learning_rate": 0.00013530536271254655,
      "loss": 0.2322,
      "step": 16080
    },
    {
      "epoch": 32.356136820925556,
      "grad_norm": 0.8771836161613464,
      "learning_rate": 0.00013530133816279304,
      "loss": 0.234,
      "step": 16081
    },
    {
      "epoch": 32.35814889336016,
      "grad_norm": 0.8825030326843262,
      "learning_rate": 0.00013529731361303955,
      "loss": 0.2244,
      "step": 16082
    },
    {
      "epoch": 32.36016096579477,
      "grad_norm": 0.9028156399726868,
      "learning_rate": 0.00013529328906328604,
      "loss": 0.2305,
      "step": 16083
    },
    {
      "epoch": 32.36217303822938,
      "grad_norm": 0.8769059777259827,
      "learning_rate": 0.00013528926451353258,
      "loss": 0.2141,
      "step": 16084
    },
    {
      "epoch": 32.36418511066398,
      "grad_norm": 0.8954336047172546,
      "learning_rate": 0.00013528523996377906,
      "loss": 0.219,
      "step": 16085
    },
    {
      "epoch": 32.36619718309859,
      "grad_norm": 0.8351802229881287,
      "learning_rate": 0.00013528121541402557,
      "loss": 0.213,
      "step": 16086
    },
    {
      "epoch": 32.3682092555332,
      "grad_norm": 0.8800368905067444,
      "learning_rate": 0.00013527719086427206,
      "loss": 0.2157,
      "step": 16087
    },
    {
      "epoch": 32.370221327967805,
      "grad_norm": 0.8508679866790771,
      "learning_rate": 0.00013527316631451857,
      "loss": 0.2245,
      "step": 16088
    },
    {
      "epoch": 32.372233400402415,
      "grad_norm": 0.8736992478370667,
      "learning_rate": 0.00013526914176476508,
      "loss": 0.2164,
      "step": 16089
    },
    {
      "epoch": 32.374245472837025,
      "grad_norm": 0.8790943622589111,
      "learning_rate": 0.0001352651172150116,
      "loss": 0.2153,
      "step": 16090
    },
    {
      "epoch": 32.37625754527163,
      "grad_norm": 0.8615111708641052,
      "learning_rate": 0.00013526109266525808,
      "loss": 0.2125,
      "step": 16091
    },
    {
      "epoch": 32.37826961770624,
      "grad_norm": 0.8915367126464844,
      "learning_rate": 0.0001352570681155046,
      "loss": 0.2208,
      "step": 16092
    },
    {
      "epoch": 32.38028169014085,
      "grad_norm": 0.9294642210006714,
      "learning_rate": 0.00013525304356575108,
      "loss": 0.2174,
      "step": 16093
    },
    {
      "epoch": 32.38229376257545,
      "grad_norm": 0.8966430425643921,
      "learning_rate": 0.0001352490190159976,
      "loss": 0.2233,
      "step": 16094
    },
    {
      "epoch": 32.38430583501006,
      "grad_norm": 0.8768733739852905,
      "learning_rate": 0.0001352449944662441,
      "loss": 0.2306,
      "step": 16095
    },
    {
      "epoch": 32.38631790744467,
      "grad_norm": 0.8975704908370972,
      "learning_rate": 0.0001352409699164906,
      "loss": 0.2381,
      "step": 16096
    },
    {
      "epoch": 32.38832997987927,
      "grad_norm": 0.8714473247528076,
      "learning_rate": 0.0001352369453667371,
      "loss": 0.2179,
      "step": 16097
    },
    {
      "epoch": 32.39034205231388,
      "grad_norm": 0.8916423320770264,
      "learning_rate": 0.0001352329208169836,
      "loss": 0.2041,
      "step": 16098
    },
    {
      "epoch": 32.39235412474849,
      "grad_norm": 0.8653781414031982,
      "learning_rate": 0.00013522889626723012,
      "loss": 0.2146,
      "step": 16099
    },
    {
      "epoch": 32.394366197183096,
      "grad_norm": 0.8437178730964661,
      "learning_rate": 0.0001352248717174766,
      "loss": 0.2191,
      "step": 16100
    },
    {
      "epoch": 32.396378269617706,
      "grad_norm": 0.8924698829650879,
      "learning_rate": 0.00013522084716772312,
      "loss": 0.2191,
      "step": 16101
    },
    {
      "epoch": 32.398390342052316,
      "grad_norm": 0.9077267050743103,
      "learning_rate": 0.0001352168226179696,
      "loss": 0.2149,
      "step": 16102
    },
    {
      "epoch": 32.40040241448692,
      "grad_norm": 0.9004972577095032,
      "learning_rate": 0.00013521279806821612,
      "loss": 0.2266,
      "step": 16103
    },
    {
      "epoch": 32.40241448692153,
      "grad_norm": 0.9045451879501343,
      "learning_rate": 0.00013520877351846263,
      "loss": 0.2041,
      "step": 16104
    },
    {
      "epoch": 32.40442655935614,
      "grad_norm": 0.8906834125518799,
      "learning_rate": 0.00013520474896870914,
      "loss": 0.2242,
      "step": 16105
    },
    {
      "epoch": 32.40643863179074,
      "grad_norm": 0.8730897903442383,
      "learning_rate": 0.00013520072441895563,
      "loss": 0.2156,
      "step": 16106
    },
    {
      "epoch": 32.40845070422535,
      "grad_norm": 0.9153575301170349,
      "learning_rate": 0.00013519669986920214,
      "loss": 0.2298,
      "step": 16107
    },
    {
      "epoch": 32.41046277665996,
      "grad_norm": 0.9348970651626587,
      "learning_rate": 0.00013519267531944863,
      "loss": 0.2547,
      "step": 16108
    },
    {
      "epoch": 32.412474849094565,
      "grad_norm": 0.8771618008613586,
      "learning_rate": 0.00013518865076969517,
      "loss": 0.213,
      "step": 16109
    },
    {
      "epoch": 32.414486921529175,
      "grad_norm": 0.9170366525650024,
      "learning_rate": 0.00013518462621994165,
      "loss": 0.224,
      "step": 16110
    },
    {
      "epoch": 32.416498993963785,
      "grad_norm": 0.8783041834831238,
      "learning_rate": 0.00013518060167018816,
      "loss": 0.2309,
      "step": 16111
    },
    {
      "epoch": 32.41851106639839,
      "grad_norm": 0.8828943967819214,
      "learning_rate": 0.00013517657712043465,
      "loss": 0.2111,
      "step": 16112
    },
    {
      "epoch": 32.420523138833,
      "grad_norm": 0.8949130773544312,
      "learning_rate": 0.00013517255257068116,
      "loss": 0.2383,
      "step": 16113
    },
    {
      "epoch": 32.42253521126761,
      "grad_norm": 0.8922437429428101,
      "learning_rate": 0.00013516852802092767,
      "loss": 0.2265,
      "step": 16114
    },
    {
      "epoch": 32.42454728370221,
      "grad_norm": 0.8544114232063293,
      "learning_rate": 0.00013516450347117418,
      "loss": 0.227,
      "step": 16115
    },
    {
      "epoch": 32.42655935613682,
      "grad_norm": 0.8856784701347351,
      "learning_rate": 0.00013516047892142067,
      "loss": 0.2287,
      "step": 16116
    },
    {
      "epoch": 32.42857142857143,
      "grad_norm": 0.8606318831443787,
      "learning_rate": 0.00013515645437166718,
      "loss": 0.2424,
      "step": 16117
    },
    {
      "epoch": 32.43058350100603,
      "grad_norm": 0.8424882292747498,
      "learning_rate": 0.00013515242982191367,
      "loss": 0.2101,
      "step": 16118
    },
    {
      "epoch": 32.43259557344064,
      "grad_norm": 0.8214181065559387,
      "learning_rate": 0.0001351484052721602,
      "loss": 0.2051,
      "step": 16119
    },
    {
      "epoch": 32.43460764587525,
      "grad_norm": 0.8720529079437256,
      "learning_rate": 0.0001351443807224067,
      "loss": 0.233,
      "step": 16120
    },
    {
      "epoch": 32.436619718309856,
      "grad_norm": 0.8904371857643127,
      "learning_rate": 0.0001351403561726532,
      "loss": 0.2238,
      "step": 16121
    },
    {
      "epoch": 32.438631790744466,
      "grad_norm": 0.9342654347419739,
      "learning_rate": 0.0001351363316228997,
      "loss": 0.2317,
      "step": 16122
    },
    {
      "epoch": 32.440643863179076,
      "grad_norm": 0.8267857432365417,
      "learning_rate": 0.0001351323070731462,
      "loss": 0.2159,
      "step": 16123
    },
    {
      "epoch": 32.44265593561368,
      "grad_norm": 0.8745841383934021,
      "learning_rate": 0.0001351282825233927,
      "loss": 0.2154,
      "step": 16124
    },
    {
      "epoch": 32.44466800804829,
      "grad_norm": 0.8951922655105591,
      "learning_rate": 0.00013512425797363923,
      "loss": 0.2212,
      "step": 16125
    },
    {
      "epoch": 32.4466800804829,
      "grad_norm": 0.8856378793716431,
      "learning_rate": 0.0001351202334238857,
      "loss": 0.2405,
      "step": 16126
    },
    {
      "epoch": 32.4486921529175,
      "grad_norm": 0.9439899921417236,
      "learning_rate": 0.00013511620887413222,
      "loss": 0.2372,
      "step": 16127
    },
    {
      "epoch": 32.45070422535211,
      "grad_norm": 0.9127382636070251,
      "learning_rate": 0.0001351121843243787,
      "loss": 0.2386,
      "step": 16128
    },
    {
      "epoch": 32.45271629778672,
      "grad_norm": 0.9197236895561218,
      "learning_rate": 0.00013510815977462522,
      "loss": 0.2417,
      "step": 16129
    },
    {
      "epoch": 32.454728370221325,
      "grad_norm": 0.9480250477790833,
      "learning_rate": 0.00013510413522487173,
      "loss": 0.2399,
      "step": 16130
    },
    {
      "epoch": 32.456740442655935,
      "grad_norm": 0.8610296249389648,
      "learning_rate": 0.00013510011067511822,
      "loss": 0.2277,
      "step": 16131
    },
    {
      "epoch": 32.458752515090545,
      "grad_norm": 0.8606876134872437,
      "learning_rate": 0.00013509608612536473,
      "loss": 0.2046,
      "step": 16132
    },
    {
      "epoch": 32.46076458752515,
      "grad_norm": 0.9025923609733582,
      "learning_rate": 0.00013509206157561124,
      "loss": 0.2507,
      "step": 16133
    },
    {
      "epoch": 32.46277665995976,
      "grad_norm": 0.8740364909172058,
      "learning_rate": 0.00013508803702585773,
      "loss": 0.202,
      "step": 16134
    },
    {
      "epoch": 32.46478873239437,
      "grad_norm": 1.0481356382369995,
      "learning_rate": 0.00013508401247610424,
      "loss": 0.2317,
      "step": 16135
    },
    {
      "epoch": 32.46680080482897,
      "grad_norm": 0.9367722272872925,
      "learning_rate": 0.00013507998792635075,
      "loss": 0.2487,
      "step": 16136
    },
    {
      "epoch": 32.46881287726358,
      "grad_norm": 0.900581955909729,
      "learning_rate": 0.00013507596337659724,
      "loss": 0.234,
      "step": 16137
    },
    {
      "epoch": 32.47082494969819,
      "grad_norm": 0.9242993593215942,
      "learning_rate": 0.00013507193882684375,
      "loss": 0.2375,
      "step": 16138
    },
    {
      "epoch": 32.47283702213279,
      "grad_norm": 0.9058263897895813,
      "learning_rate": 0.00013506791427709023,
      "loss": 0.2219,
      "step": 16139
    },
    {
      "epoch": 32.4748490945674,
      "grad_norm": 0.9239590167999268,
      "learning_rate": 0.00013506388972733677,
      "loss": 0.237,
      "step": 16140
    },
    {
      "epoch": 32.47686116700201,
      "grad_norm": 0.9013855457305908,
      "learning_rate": 0.00013505986517758326,
      "loss": 0.2319,
      "step": 16141
    },
    {
      "epoch": 32.478873239436616,
      "grad_norm": 0.9152894020080566,
      "learning_rate": 0.00013505584062782977,
      "loss": 0.2279,
      "step": 16142
    },
    {
      "epoch": 32.480885311871226,
      "grad_norm": 0.872731626033783,
      "learning_rate": 0.00013505181607807626,
      "loss": 0.2235,
      "step": 16143
    },
    {
      "epoch": 32.482897384305836,
      "grad_norm": 0.9157727956771851,
      "learning_rate": 0.00013504779152832277,
      "loss": 0.2264,
      "step": 16144
    },
    {
      "epoch": 32.48490945674044,
      "grad_norm": 0.9284242391586304,
      "learning_rate": 0.00013504376697856928,
      "loss": 0.2456,
      "step": 16145
    },
    {
      "epoch": 32.48692152917505,
      "grad_norm": 0.8688830733299255,
      "learning_rate": 0.0001350397424288158,
      "loss": 0.2158,
      "step": 16146
    },
    {
      "epoch": 32.48893360160966,
      "grad_norm": 0.9453546404838562,
      "learning_rate": 0.00013503571787906228,
      "loss": 0.236,
      "step": 16147
    },
    {
      "epoch": 32.49094567404426,
      "grad_norm": 0.8800227046012878,
      "learning_rate": 0.0001350316933293088,
      "loss": 0.234,
      "step": 16148
    },
    {
      "epoch": 32.49295774647887,
      "grad_norm": 0.9079465866088867,
      "learning_rate": 0.00013502766877955527,
      "loss": 0.2138,
      "step": 16149
    },
    {
      "epoch": 32.49496981891348,
      "grad_norm": 0.9422582387924194,
      "learning_rate": 0.00013502364422980181,
      "loss": 0.2461,
      "step": 16150
    },
    {
      "epoch": 32.496981891348085,
      "grad_norm": 0.9049001932144165,
      "learning_rate": 0.0001350196196800483,
      "loss": 0.237,
      "step": 16151
    },
    {
      "epoch": 32.498993963782695,
      "grad_norm": 0.8747110962867737,
      "learning_rate": 0.0001350155951302948,
      "loss": 0.2237,
      "step": 16152
    },
    {
      "epoch": 32.501006036217305,
      "grad_norm": 0.844585657119751,
      "learning_rate": 0.0001350115705805413,
      "loss": 0.2298,
      "step": 16153
    },
    {
      "epoch": 32.503018108651915,
      "grad_norm": 0.9295705556869507,
      "learning_rate": 0.0001350075460307878,
      "loss": 0.2318,
      "step": 16154
    },
    {
      "epoch": 32.50503018108652,
      "grad_norm": 0.8527714014053345,
      "learning_rate": 0.00013500352148103432,
      "loss": 0.2092,
      "step": 16155
    },
    {
      "epoch": 32.50704225352113,
      "grad_norm": 0.8879218697547913,
      "learning_rate": 0.00013499949693128083,
      "loss": 0.2308,
      "step": 16156
    },
    {
      "epoch": 32.50905432595574,
      "grad_norm": 0.9610982537269592,
      "learning_rate": 0.00013499547238152732,
      "loss": 0.2403,
      "step": 16157
    },
    {
      "epoch": 32.51106639839034,
      "grad_norm": 0.9438725709915161,
      "learning_rate": 0.00013499144783177383,
      "loss": 0.239,
      "step": 16158
    },
    {
      "epoch": 32.51307847082495,
      "grad_norm": 0.8773340582847595,
      "learning_rate": 0.00013498742328202032,
      "loss": 0.2177,
      "step": 16159
    },
    {
      "epoch": 32.51509054325956,
      "grad_norm": 0.9392076730728149,
      "learning_rate": 0.00013498339873226685,
      "loss": 0.2401,
      "step": 16160
    },
    {
      "epoch": 32.517102615694164,
      "grad_norm": 0.9235720634460449,
      "learning_rate": 0.00013497937418251334,
      "loss": 0.241,
      "step": 16161
    },
    {
      "epoch": 32.519114688128774,
      "grad_norm": 0.947609007358551,
      "learning_rate": 0.00013497534963275985,
      "loss": 0.2278,
      "step": 16162
    },
    {
      "epoch": 32.521126760563384,
      "grad_norm": 0.9798822402954102,
      "learning_rate": 0.00013497132508300634,
      "loss": 0.2472,
      "step": 16163
    },
    {
      "epoch": 32.52313883299799,
      "grad_norm": 0.9254293441772461,
      "learning_rate": 0.00013496730053325285,
      "loss": 0.2415,
      "step": 16164
    },
    {
      "epoch": 32.5251509054326,
      "grad_norm": 0.9120258688926697,
      "learning_rate": 0.00013496327598349936,
      "loss": 0.2266,
      "step": 16165
    },
    {
      "epoch": 32.52716297786721,
      "grad_norm": 0.8730674982070923,
      "learning_rate": 0.00013495925143374585,
      "loss": 0.2252,
      "step": 16166
    },
    {
      "epoch": 32.52917505030181,
      "grad_norm": 0.9189746975898743,
      "learning_rate": 0.00013495522688399236,
      "loss": 0.2481,
      "step": 16167
    },
    {
      "epoch": 32.53118712273642,
      "grad_norm": 0.8955355286598206,
      "learning_rate": 0.00013495120233423887,
      "loss": 0.2215,
      "step": 16168
    },
    {
      "epoch": 32.53319919517103,
      "grad_norm": 0.9241647720336914,
      "learning_rate": 0.00013494717778448536,
      "loss": 0.24,
      "step": 16169
    },
    {
      "epoch": 32.53521126760563,
      "grad_norm": 0.927169144153595,
      "learning_rate": 0.00013494315323473187,
      "loss": 0.2248,
      "step": 16170
    },
    {
      "epoch": 32.53722334004024,
      "grad_norm": 0.875503659248352,
      "learning_rate": 0.00013493912868497838,
      "loss": 0.2218,
      "step": 16171
    },
    {
      "epoch": 32.53923541247485,
      "grad_norm": 0.9260950684547424,
      "learning_rate": 0.00013493510413522487,
      "loss": 0.2295,
      "step": 16172
    },
    {
      "epoch": 32.541247484909455,
      "grad_norm": 0.8642088174819946,
      "learning_rate": 0.00013493107958547138,
      "loss": 0.24,
      "step": 16173
    },
    {
      "epoch": 32.543259557344065,
      "grad_norm": 0.8629691004753113,
      "learning_rate": 0.00013492705503571786,
      "loss": 0.2219,
      "step": 16174
    },
    {
      "epoch": 32.545271629778675,
      "grad_norm": 0.8743793964385986,
      "learning_rate": 0.0001349230304859644,
      "loss": 0.2219,
      "step": 16175
    },
    {
      "epoch": 32.54728370221328,
      "grad_norm": 0.9793595671653748,
      "learning_rate": 0.0001349190059362109,
      "loss": 0.2381,
      "step": 16176
    },
    {
      "epoch": 32.54929577464789,
      "grad_norm": 0.8725584149360657,
      "learning_rate": 0.0001349149813864574,
      "loss": 0.223,
      "step": 16177
    },
    {
      "epoch": 32.5513078470825,
      "grad_norm": 0.8761054873466492,
      "learning_rate": 0.00013491095683670388,
      "loss": 0.2242,
      "step": 16178
    },
    {
      "epoch": 32.5533199195171,
      "grad_norm": 0.878226101398468,
      "learning_rate": 0.0001349069322869504,
      "loss": 0.232,
      "step": 16179
    },
    {
      "epoch": 32.55533199195171,
      "grad_norm": 0.9179592132568359,
      "learning_rate": 0.0001349029077371969,
      "loss": 0.2249,
      "step": 16180
    },
    {
      "epoch": 32.55734406438632,
      "grad_norm": 0.9694803357124329,
      "learning_rate": 0.00013489888318744342,
      "loss": 0.2523,
      "step": 16181
    },
    {
      "epoch": 32.559356136820924,
      "grad_norm": 0.8835263252258301,
      "learning_rate": 0.0001348948586376899,
      "loss": 0.2226,
      "step": 16182
    },
    {
      "epoch": 32.561368209255534,
      "grad_norm": 0.9345055222511292,
      "learning_rate": 0.00013489083408793642,
      "loss": 0.2307,
      "step": 16183
    },
    {
      "epoch": 32.563380281690144,
      "grad_norm": 0.9426712989807129,
      "learning_rate": 0.0001348868095381829,
      "loss": 0.2557,
      "step": 16184
    },
    {
      "epoch": 32.56539235412475,
      "grad_norm": 0.8756088614463806,
      "learning_rate": 0.00013488278498842944,
      "loss": 0.2227,
      "step": 16185
    },
    {
      "epoch": 32.56740442655936,
      "grad_norm": 0.9261702299118042,
      "learning_rate": 0.00013487876043867593,
      "loss": 0.24,
      "step": 16186
    },
    {
      "epoch": 32.56941649899397,
      "grad_norm": 0.899574339389801,
      "learning_rate": 0.00013487473588892244,
      "loss": 0.226,
      "step": 16187
    },
    {
      "epoch": 32.57142857142857,
      "grad_norm": 0.8892384171485901,
      "learning_rate": 0.00013487071133916893,
      "loss": 0.2464,
      "step": 16188
    },
    {
      "epoch": 32.57344064386318,
      "grad_norm": 0.9061773419380188,
      "learning_rate": 0.00013486668678941544,
      "loss": 0.2407,
      "step": 16189
    },
    {
      "epoch": 32.57545271629779,
      "grad_norm": 0.9229567646980286,
      "learning_rate": 0.00013486266223966195,
      "loss": 0.243,
      "step": 16190
    },
    {
      "epoch": 32.57746478873239,
      "grad_norm": 0.8648979067802429,
      "learning_rate": 0.00013485863768990846,
      "loss": 0.2307,
      "step": 16191
    },
    {
      "epoch": 32.579476861167,
      "grad_norm": 0.845248818397522,
      "learning_rate": 0.00013485461314015495,
      "loss": 0.2172,
      "step": 16192
    },
    {
      "epoch": 32.58148893360161,
      "grad_norm": 0.8557721972465515,
      "learning_rate": 0.00013485058859040146,
      "loss": 0.2213,
      "step": 16193
    },
    {
      "epoch": 32.583501006036215,
      "grad_norm": 0.9351282715797424,
      "learning_rate": 0.00013484656404064794,
      "loss": 0.2549,
      "step": 16194
    },
    {
      "epoch": 32.585513078470825,
      "grad_norm": 0.9713473916053772,
      "learning_rate": 0.00013484253949089448,
      "loss": 0.2452,
      "step": 16195
    },
    {
      "epoch": 32.587525150905435,
      "grad_norm": 0.8959417343139648,
      "learning_rate": 0.00013483851494114097,
      "loss": 0.2253,
      "step": 16196
    },
    {
      "epoch": 32.58953722334004,
      "grad_norm": 0.918362021446228,
      "learning_rate": 0.00013483449039138748,
      "loss": 0.2277,
      "step": 16197
    },
    {
      "epoch": 32.59154929577465,
      "grad_norm": 0.8963546752929688,
      "learning_rate": 0.00013483046584163397,
      "loss": 0.2412,
      "step": 16198
    },
    {
      "epoch": 32.59356136820926,
      "grad_norm": 0.9453227519989014,
      "learning_rate": 0.00013482644129188048,
      "loss": 0.2351,
      "step": 16199
    },
    {
      "epoch": 32.59557344064386,
      "grad_norm": 0.8987835645675659,
      "learning_rate": 0.000134822416742127,
      "loss": 0.2371,
      "step": 16200
    },
    {
      "epoch": 32.59758551307847,
      "grad_norm": 0.9022980332374573,
      "learning_rate": 0.00013481839219237348,
      "loss": 0.2479,
      "step": 16201
    },
    {
      "epoch": 32.59959758551308,
      "grad_norm": 0.9047520756721497,
      "learning_rate": 0.00013481436764262,
      "loss": 0.2368,
      "step": 16202
    },
    {
      "epoch": 32.601609657947684,
      "grad_norm": 0.9308319687843323,
      "learning_rate": 0.0001348103430928665,
      "loss": 0.2496,
      "step": 16203
    },
    {
      "epoch": 32.603621730382294,
      "grad_norm": 0.9366011023521423,
      "learning_rate": 0.00013480631854311299,
      "loss": 0.2261,
      "step": 16204
    },
    {
      "epoch": 32.605633802816904,
      "grad_norm": 0.9064623713493347,
      "learning_rate": 0.0001348022939933595,
      "loss": 0.2323,
      "step": 16205
    },
    {
      "epoch": 32.60764587525151,
      "grad_norm": 0.939113199710846,
      "learning_rate": 0.000134798269443606,
      "loss": 0.2492,
      "step": 16206
    },
    {
      "epoch": 32.60965794768612,
      "grad_norm": 0.9190784692764282,
      "learning_rate": 0.0001347942448938525,
      "loss": 0.2416,
      "step": 16207
    },
    {
      "epoch": 32.61167002012073,
      "grad_norm": 0.9440175890922546,
      "learning_rate": 0.000134790220344099,
      "loss": 0.2536,
      "step": 16208
    },
    {
      "epoch": 32.61368209255533,
      "grad_norm": 0.9624481201171875,
      "learning_rate": 0.0001347861957943455,
      "loss": 0.2485,
      "step": 16209
    },
    {
      "epoch": 32.61569416498994,
      "grad_norm": 0.9010081887245178,
      "learning_rate": 0.00013478217124459203,
      "loss": 0.2217,
      "step": 16210
    },
    {
      "epoch": 32.61770623742455,
      "grad_norm": 0.9293292164802551,
      "learning_rate": 0.00013477814669483852,
      "loss": 0.2526,
      "step": 16211
    },
    {
      "epoch": 32.61971830985915,
      "grad_norm": 0.8537871837615967,
      "learning_rate": 0.00013477412214508503,
      "loss": 0.2258,
      "step": 16212
    },
    {
      "epoch": 32.62173038229376,
      "grad_norm": 0.958308219909668,
      "learning_rate": 0.00013477009759533151,
      "loss": 0.2477,
      "step": 16213
    },
    {
      "epoch": 32.62374245472837,
      "grad_norm": 0.8772197961807251,
      "learning_rate": 0.00013476607304557803,
      "loss": 0.2214,
      "step": 16214
    },
    {
      "epoch": 32.625754527162975,
      "grad_norm": 0.9459072351455688,
      "learning_rate": 0.00013476204849582454,
      "loss": 0.2353,
      "step": 16215
    },
    {
      "epoch": 32.627766599597585,
      "grad_norm": 0.8582454323768616,
      "learning_rate": 0.00013475802394607105,
      "loss": 0.2212,
      "step": 16216
    },
    {
      "epoch": 32.629778672032195,
      "grad_norm": 0.929592490196228,
      "learning_rate": 0.00013475399939631754,
      "loss": 0.2444,
      "step": 16217
    },
    {
      "epoch": 32.6317907444668,
      "grad_norm": 0.9712732434272766,
      "learning_rate": 0.00013474997484656405,
      "loss": 0.2523,
      "step": 16218
    },
    {
      "epoch": 32.63380281690141,
      "grad_norm": 0.9073153138160706,
      "learning_rate": 0.00013474595029681053,
      "loss": 0.2428,
      "step": 16219
    },
    {
      "epoch": 32.63581488933602,
      "grad_norm": 0.9060573577880859,
      "learning_rate": 0.00013474192574705707,
      "loss": 0.2353,
      "step": 16220
    },
    {
      "epoch": 32.63782696177062,
      "grad_norm": 0.895751416683197,
      "learning_rate": 0.00013473790119730356,
      "loss": 0.2649,
      "step": 16221
    },
    {
      "epoch": 32.63983903420523,
      "grad_norm": 0.8488219976425171,
      "learning_rate": 0.00013473387664755007,
      "loss": 0.247,
      "step": 16222
    },
    {
      "epoch": 32.64185110663984,
      "grad_norm": 0.9654547572135925,
      "learning_rate": 0.00013472985209779655,
      "loss": 0.2394,
      "step": 16223
    },
    {
      "epoch": 32.643863179074444,
      "grad_norm": 0.8892507553100586,
      "learning_rate": 0.00013472582754804307,
      "loss": 0.2211,
      "step": 16224
    },
    {
      "epoch": 32.645875251509054,
      "grad_norm": 0.8776664733886719,
      "learning_rate": 0.00013472180299828958,
      "loss": 0.2373,
      "step": 16225
    },
    {
      "epoch": 32.647887323943664,
      "grad_norm": 0.9357100129127502,
      "learning_rate": 0.0001347177784485361,
      "loss": 0.2562,
      "step": 16226
    },
    {
      "epoch": 32.64989939637827,
      "grad_norm": 0.9199707508087158,
      "learning_rate": 0.00013471375389878258,
      "loss": 0.23,
      "step": 16227
    },
    {
      "epoch": 32.65191146881288,
      "grad_norm": 0.8643736243247986,
      "learning_rate": 0.0001347097293490291,
      "loss": 0.2168,
      "step": 16228
    },
    {
      "epoch": 32.65392354124749,
      "grad_norm": 0.870634138584137,
      "learning_rate": 0.00013470570479927557,
      "loss": 0.2395,
      "step": 16229
    },
    {
      "epoch": 32.65593561368209,
      "grad_norm": 0.932101309299469,
      "learning_rate": 0.00013470168024952209,
      "loss": 0.2571,
      "step": 16230
    },
    {
      "epoch": 32.6579476861167,
      "grad_norm": 0.9667810797691345,
      "learning_rate": 0.0001346976556997686,
      "loss": 0.2473,
      "step": 16231
    },
    {
      "epoch": 32.65995975855131,
      "grad_norm": 0.9235559105873108,
      "learning_rate": 0.0001346936311500151,
      "loss": 0.2428,
      "step": 16232
    },
    {
      "epoch": 32.66197183098591,
      "grad_norm": 0.9353784918785095,
      "learning_rate": 0.0001346896066002616,
      "loss": 0.2431,
      "step": 16233
    },
    {
      "epoch": 32.66398390342052,
      "grad_norm": 0.9369134306907654,
      "learning_rate": 0.0001346855820505081,
      "loss": 0.2522,
      "step": 16234
    },
    {
      "epoch": 32.66599597585513,
      "grad_norm": 0.8864253163337708,
      "learning_rate": 0.00013468155750075462,
      "loss": 0.2519,
      "step": 16235
    },
    {
      "epoch": 32.668008048289735,
      "grad_norm": 0.9188212156295776,
      "learning_rate": 0.0001346775329510011,
      "loss": 0.24,
      "step": 16236
    },
    {
      "epoch": 32.670020120724345,
      "grad_norm": 0.8893222212791443,
      "learning_rate": 0.00013467350840124762,
      "loss": 0.2348,
      "step": 16237
    },
    {
      "epoch": 32.672032193158955,
      "grad_norm": 0.9033092856407166,
      "learning_rate": 0.0001346694838514941,
      "loss": 0.2476,
      "step": 16238
    },
    {
      "epoch": 32.67404426559356,
      "grad_norm": 0.9161955714225769,
      "learning_rate": 0.00013466545930174061,
      "loss": 0.2322,
      "step": 16239
    },
    {
      "epoch": 32.67605633802817,
      "grad_norm": 0.9927821159362793,
      "learning_rate": 0.00013466143475198713,
      "loss": 0.2357,
      "step": 16240
    },
    {
      "epoch": 32.67806841046278,
      "grad_norm": 0.9411352276802063,
      "learning_rate": 0.00013465741020223364,
      "loss": 0.2357,
      "step": 16241
    },
    {
      "epoch": 32.68008048289738,
      "grad_norm": 0.8818212151527405,
      "learning_rate": 0.00013465338565248012,
      "loss": 0.2277,
      "step": 16242
    },
    {
      "epoch": 32.68209255533199,
      "grad_norm": 0.8876983523368835,
      "learning_rate": 0.00013464936110272664,
      "loss": 0.2356,
      "step": 16243
    },
    {
      "epoch": 32.6841046277666,
      "grad_norm": 0.9117029309272766,
      "learning_rate": 0.00013464533655297312,
      "loss": 0.2331,
      "step": 16244
    },
    {
      "epoch": 32.686116700201204,
      "grad_norm": 0.9259527325630188,
      "learning_rate": 0.00013464131200321966,
      "loss": 0.2383,
      "step": 16245
    },
    {
      "epoch": 32.688128772635814,
      "grad_norm": 0.955772340297699,
      "learning_rate": 0.00013463728745346615,
      "loss": 0.2485,
      "step": 16246
    },
    {
      "epoch": 32.690140845070424,
      "grad_norm": 0.9340936541557312,
      "learning_rate": 0.00013463326290371266,
      "loss": 0.2511,
      "step": 16247
    },
    {
      "epoch": 32.69215291750503,
      "grad_norm": 0.8989590406417847,
      "learning_rate": 0.00013462923835395914,
      "loss": 0.2306,
      "step": 16248
    },
    {
      "epoch": 32.69416498993964,
      "grad_norm": 0.9374040961265564,
      "learning_rate": 0.00013462521380420566,
      "loss": 0.2543,
      "step": 16249
    },
    {
      "epoch": 32.69617706237425,
      "grad_norm": 0.9042682647705078,
      "learning_rate": 0.00013462118925445217,
      "loss": 0.2293,
      "step": 16250
    },
    {
      "epoch": 32.69818913480886,
      "grad_norm": 0.8921368718147278,
      "learning_rate": 0.00013461716470469868,
      "loss": 0.2278,
      "step": 16251
    },
    {
      "epoch": 32.70020120724346,
      "grad_norm": 0.9082352519035339,
      "learning_rate": 0.00013461314015494517,
      "loss": 0.2362,
      "step": 16252
    },
    {
      "epoch": 32.70221327967807,
      "grad_norm": 0.911227822303772,
      "learning_rate": 0.00013460911560519168,
      "loss": 0.2514,
      "step": 16253
    },
    {
      "epoch": 32.70422535211267,
      "grad_norm": 0.9286149144172668,
      "learning_rate": 0.00013460509105543816,
      "loss": 0.2411,
      "step": 16254
    },
    {
      "epoch": 32.70623742454728,
      "grad_norm": 0.9733046889305115,
      "learning_rate": 0.0001346010665056847,
      "loss": 0.2232,
      "step": 16255
    },
    {
      "epoch": 32.70824949698189,
      "grad_norm": 0.8919411301612854,
      "learning_rate": 0.0001345970419559312,
      "loss": 0.2471,
      "step": 16256
    },
    {
      "epoch": 32.7102615694165,
      "grad_norm": 0.9151968955993652,
      "learning_rate": 0.0001345930174061777,
      "loss": 0.243,
      "step": 16257
    },
    {
      "epoch": 32.712273641851105,
      "grad_norm": 0.9384251236915588,
      "learning_rate": 0.00013458899285642418,
      "loss": 0.2368,
      "step": 16258
    },
    {
      "epoch": 32.714285714285715,
      "grad_norm": 0.8885552287101746,
      "learning_rate": 0.0001345849683066707,
      "loss": 0.2325,
      "step": 16259
    },
    {
      "epoch": 32.716297786720325,
      "grad_norm": 0.9156029224395752,
      "learning_rate": 0.0001345809437569172,
      "loss": 0.2621,
      "step": 16260
    },
    {
      "epoch": 32.71830985915493,
      "grad_norm": 0.9204376935958862,
      "learning_rate": 0.00013457691920716372,
      "loss": 0.2499,
      "step": 16261
    },
    {
      "epoch": 32.72032193158954,
      "grad_norm": 0.9529396891593933,
      "learning_rate": 0.0001345728946574102,
      "loss": 0.2594,
      "step": 16262
    },
    {
      "epoch": 32.72233400402415,
      "grad_norm": 0.8964037299156189,
      "learning_rate": 0.00013456887010765672,
      "loss": 0.2371,
      "step": 16263
    },
    {
      "epoch": 32.72434607645875,
      "grad_norm": 0.8724239468574524,
      "learning_rate": 0.0001345648455579032,
      "loss": 0.2406,
      "step": 16264
    },
    {
      "epoch": 32.72635814889336,
      "grad_norm": 0.88132643699646,
      "learning_rate": 0.00013456082100814972,
      "loss": 0.2282,
      "step": 16265
    },
    {
      "epoch": 32.72837022132797,
      "grad_norm": 0.9454065561294556,
      "learning_rate": 0.00013455679645839623,
      "loss": 0.2332,
      "step": 16266
    },
    {
      "epoch": 32.730382293762574,
      "grad_norm": 0.8610668778419495,
      "learning_rate": 0.00013455277190864274,
      "loss": 0.222,
      "step": 16267
    },
    {
      "epoch": 32.732394366197184,
      "grad_norm": 0.8508679270744324,
      "learning_rate": 0.00013454874735888923,
      "loss": 0.2147,
      "step": 16268
    },
    {
      "epoch": 32.734406438631794,
      "grad_norm": 0.9325414896011353,
      "learning_rate": 0.00013454472280913574,
      "loss": 0.2356,
      "step": 16269
    },
    {
      "epoch": 32.7364185110664,
      "grad_norm": 0.9869071841239929,
      "learning_rate": 0.00013454069825938225,
      "loss": 0.2316,
      "step": 16270
    },
    {
      "epoch": 32.73843058350101,
      "grad_norm": 0.9217109084129333,
      "learning_rate": 0.00013453667370962873,
      "loss": 0.2527,
      "step": 16271
    },
    {
      "epoch": 32.74044265593562,
      "grad_norm": 0.9902647733688354,
      "learning_rate": 0.00013453264915987525,
      "loss": 0.2493,
      "step": 16272
    },
    {
      "epoch": 32.74245472837022,
      "grad_norm": 0.9203089475631714,
      "learning_rate": 0.00013452862461012173,
      "loss": 0.2245,
      "step": 16273
    },
    {
      "epoch": 32.74446680080483,
      "grad_norm": 0.8966065049171448,
      "learning_rate": 0.00013452460006036824,
      "loss": 0.2265,
      "step": 16274
    },
    {
      "epoch": 32.74647887323944,
      "grad_norm": 0.8919091820716858,
      "learning_rate": 0.00013452057551061476,
      "loss": 0.2406,
      "step": 16275
    },
    {
      "epoch": 32.74849094567404,
      "grad_norm": 0.9144299626350403,
      "learning_rate": 0.00013451655096086127,
      "loss": 0.2364,
      "step": 16276
    },
    {
      "epoch": 32.75050301810865,
      "grad_norm": 0.9085191488265991,
      "learning_rate": 0.00013451252641110775,
      "loss": 0.2412,
      "step": 16277
    },
    {
      "epoch": 32.75251509054326,
      "grad_norm": 0.917652428150177,
      "learning_rate": 0.00013450850186135427,
      "loss": 0.234,
      "step": 16278
    },
    {
      "epoch": 32.754527162977865,
      "grad_norm": 0.9411056637763977,
      "learning_rate": 0.00013450447731160075,
      "loss": 0.2568,
      "step": 16279
    },
    {
      "epoch": 32.756539235412475,
      "grad_norm": 0.9868043661117554,
      "learning_rate": 0.0001345004527618473,
      "loss": 0.2626,
      "step": 16280
    },
    {
      "epoch": 32.758551307847085,
      "grad_norm": 0.8908182382583618,
      "learning_rate": 0.00013449642821209378,
      "loss": 0.2157,
      "step": 16281
    },
    {
      "epoch": 32.76056338028169,
      "grad_norm": 0.8950254321098328,
      "learning_rate": 0.0001344924036623403,
      "loss": 0.2583,
      "step": 16282
    },
    {
      "epoch": 32.7625754527163,
      "grad_norm": 0.9546413421630859,
      "learning_rate": 0.00013448837911258677,
      "loss": 0.2494,
      "step": 16283
    },
    {
      "epoch": 32.76458752515091,
      "grad_norm": 0.9504047632217407,
      "learning_rate": 0.00013448435456283329,
      "loss": 0.2718,
      "step": 16284
    },
    {
      "epoch": 32.76659959758551,
      "grad_norm": 0.9117364287376404,
      "learning_rate": 0.0001344803300130798,
      "loss": 0.2489,
      "step": 16285
    },
    {
      "epoch": 32.76861167002012,
      "grad_norm": 0.9378017783164978,
      "learning_rate": 0.0001344763054633263,
      "loss": 0.2445,
      "step": 16286
    },
    {
      "epoch": 32.77062374245473,
      "grad_norm": 0.9228271842002869,
      "learning_rate": 0.0001344722809135728,
      "loss": 0.2487,
      "step": 16287
    },
    {
      "epoch": 32.772635814889334,
      "grad_norm": 0.9500526785850525,
      "learning_rate": 0.0001344682563638193,
      "loss": 0.2449,
      "step": 16288
    },
    {
      "epoch": 32.774647887323944,
      "grad_norm": 0.965289831161499,
      "learning_rate": 0.0001344642318140658,
      "loss": 0.2421,
      "step": 16289
    },
    {
      "epoch": 32.776659959758554,
      "grad_norm": 0.9159558415412903,
      "learning_rate": 0.00013446020726431233,
      "loss": 0.2378,
      "step": 16290
    },
    {
      "epoch": 32.77867203219316,
      "grad_norm": 0.9297524690628052,
      "learning_rate": 0.00013445618271455882,
      "loss": 0.248,
      "step": 16291
    },
    {
      "epoch": 32.78068410462777,
      "grad_norm": 0.9175806641578674,
      "learning_rate": 0.00013445215816480533,
      "loss": 0.2442,
      "step": 16292
    },
    {
      "epoch": 32.78269617706238,
      "grad_norm": 0.8973377346992493,
      "learning_rate": 0.00013444813361505181,
      "loss": 0.2385,
      "step": 16293
    },
    {
      "epoch": 32.78470824949698,
      "grad_norm": 0.9135700464248657,
      "learning_rate": 0.00013444410906529833,
      "loss": 0.265,
      "step": 16294
    },
    {
      "epoch": 32.78672032193159,
      "grad_norm": 0.8582890033721924,
      "learning_rate": 0.00013444008451554484,
      "loss": 0.2355,
      "step": 16295
    },
    {
      "epoch": 32.7887323943662,
      "grad_norm": 0.9787898659706116,
      "learning_rate": 0.00013443605996579135,
      "loss": 0.2691,
      "step": 16296
    },
    {
      "epoch": 32.7907444668008,
      "grad_norm": 0.9128021001815796,
      "learning_rate": 0.00013443203541603784,
      "loss": 0.2288,
      "step": 16297
    },
    {
      "epoch": 32.79275653923541,
      "grad_norm": 0.8677827715873718,
      "learning_rate": 0.00013442801086628435,
      "loss": 0.2454,
      "step": 16298
    },
    {
      "epoch": 32.79476861167002,
      "grad_norm": 0.9302882552146912,
      "learning_rate": 0.00013442398631653083,
      "loss": 0.2154,
      "step": 16299
    },
    {
      "epoch": 32.796780684104625,
      "grad_norm": 0.8940302133560181,
      "learning_rate": 0.00013441996176677735,
      "loss": 0.2555,
      "step": 16300
    },
    {
      "epoch": 32.798792756539235,
      "grad_norm": 0.9435009956359863,
      "learning_rate": 0.00013441593721702386,
      "loss": 0.2586,
      "step": 16301
    },
    {
      "epoch": 32.800804828973845,
      "grad_norm": 1.0360578298568726,
      "learning_rate": 0.00013441191266727037,
      "loss": 0.2692,
      "step": 16302
    },
    {
      "epoch": 32.80281690140845,
      "grad_norm": 0.874775230884552,
      "learning_rate": 0.00013440788811751685,
      "loss": 0.2573,
      "step": 16303
    },
    {
      "epoch": 32.80482897384306,
      "grad_norm": 0.9373796582221985,
      "learning_rate": 0.00013440386356776337,
      "loss": 0.2541,
      "step": 16304
    },
    {
      "epoch": 32.80684104627767,
      "grad_norm": 0.9613326787948608,
      "learning_rate": 0.00013439983901800988,
      "loss": 0.2613,
      "step": 16305
    },
    {
      "epoch": 32.80885311871227,
      "grad_norm": 0.9689444303512573,
      "learning_rate": 0.00013439581446825636,
      "loss": 0.2464,
      "step": 16306
    },
    {
      "epoch": 32.81086519114688,
      "grad_norm": 0.824391782283783,
      "learning_rate": 0.00013439178991850288,
      "loss": 0.2143,
      "step": 16307
    },
    {
      "epoch": 32.81287726358149,
      "grad_norm": 0.9385294914245605,
      "learning_rate": 0.00013438776536874936,
      "loss": 0.2549,
      "step": 16308
    },
    {
      "epoch": 32.814889336016094,
      "grad_norm": 0.9249398708343506,
      "learning_rate": 0.00013438374081899587,
      "loss": 0.2438,
      "step": 16309
    },
    {
      "epoch": 32.816901408450704,
      "grad_norm": 0.9236788749694824,
      "learning_rate": 0.00013437971626924239,
      "loss": 0.2498,
      "step": 16310
    },
    {
      "epoch": 32.818913480885314,
      "grad_norm": 1.0082937479019165,
      "learning_rate": 0.0001343756917194889,
      "loss": 0.2592,
      "step": 16311
    },
    {
      "epoch": 32.82092555331992,
      "grad_norm": 0.9697746634483337,
      "learning_rate": 0.00013437166716973538,
      "loss": 0.25,
      "step": 16312
    },
    {
      "epoch": 32.82293762575453,
      "grad_norm": 0.9451034665107727,
      "learning_rate": 0.0001343676426199819,
      "loss": 0.2704,
      "step": 16313
    },
    {
      "epoch": 32.82494969818914,
      "grad_norm": 0.9494602680206299,
      "learning_rate": 0.00013436361807022838,
      "loss": 0.2709,
      "step": 16314
    },
    {
      "epoch": 32.82696177062374,
      "grad_norm": 0.9797571897506714,
      "learning_rate": 0.00013435959352047492,
      "loss": 0.2346,
      "step": 16315
    },
    {
      "epoch": 32.82897384305835,
      "grad_norm": 0.8692933320999146,
      "learning_rate": 0.0001343555689707214,
      "loss": 0.2294,
      "step": 16316
    },
    {
      "epoch": 32.83098591549296,
      "grad_norm": 0.9399703741073608,
      "learning_rate": 0.00013435154442096792,
      "loss": 0.2612,
      "step": 16317
    },
    {
      "epoch": 32.83299798792756,
      "grad_norm": 0.8710671067237854,
      "learning_rate": 0.0001343475198712144,
      "loss": 0.2349,
      "step": 16318
    },
    {
      "epoch": 32.83501006036217,
      "grad_norm": 0.8933453559875488,
      "learning_rate": 0.00013434349532146091,
      "loss": 0.2436,
      "step": 16319
    },
    {
      "epoch": 32.83702213279678,
      "grad_norm": 0.9799184203147888,
      "learning_rate": 0.00013433947077170743,
      "loss": 0.2642,
      "step": 16320
    },
    {
      "epoch": 32.839034205231385,
      "grad_norm": 1.0188636779785156,
      "learning_rate": 0.00013433544622195394,
      "loss": 0.2445,
      "step": 16321
    },
    {
      "epoch": 32.841046277665995,
      "grad_norm": 0.8948532938957214,
      "learning_rate": 0.00013433142167220042,
      "loss": 0.239,
      "step": 16322
    },
    {
      "epoch": 32.843058350100605,
      "grad_norm": 0.9391454458236694,
      "learning_rate": 0.00013432739712244694,
      "loss": 0.2552,
      "step": 16323
    },
    {
      "epoch": 32.84507042253521,
      "grad_norm": 0.9362341165542603,
      "learning_rate": 0.00013432337257269342,
      "loss": 0.2473,
      "step": 16324
    },
    {
      "epoch": 32.84708249496982,
      "grad_norm": 0.9361187219619751,
      "learning_rate": 0.00013431934802293996,
      "loss": 0.2637,
      "step": 16325
    },
    {
      "epoch": 32.84909456740443,
      "grad_norm": 0.9453022480010986,
      "learning_rate": 0.00013431532347318645,
      "loss": 0.2498,
      "step": 16326
    },
    {
      "epoch": 32.85110663983903,
      "grad_norm": 0.900301456451416,
      "learning_rate": 0.00013431129892343296,
      "loss": 0.2475,
      "step": 16327
    },
    {
      "epoch": 32.85311871227364,
      "grad_norm": 0.9093791842460632,
      "learning_rate": 0.00013430727437367944,
      "loss": 0.2485,
      "step": 16328
    },
    {
      "epoch": 32.85513078470825,
      "grad_norm": 0.9923558831214905,
      "learning_rate": 0.00013430324982392596,
      "loss": 0.2754,
      "step": 16329
    },
    {
      "epoch": 32.857142857142854,
      "grad_norm": 0.8868913650512695,
      "learning_rate": 0.00013429922527417247,
      "loss": 0.2407,
      "step": 16330
    },
    {
      "epoch": 32.859154929577464,
      "grad_norm": 0.9268302321434021,
      "learning_rate": 0.00013429520072441898,
      "loss": 0.2449,
      "step": 16331
    },
    {
      "epoch": 32.861167002012074,
      "grad_norm": 0.8902081251144409,
      "learning_rate": 0.00013429117617466546,
      "loss": 0.2503,
      "step": 16332
    },
    {
      "epoch": 32.86317907444668,
      "grad_norm": 0.9480552673339844,
      "learning_rate": 0.00013428715162491198,
      "loss": 0.2556,
      "step": 16333
    },
    {
      "epoch": 32.86519114688129,
      "grad_norm": 0.9648613333702087,
      "learning_rate": 0.00013428312707515846,
      "loss": 0.2468,
      "step": 16334
    },
    {
      "epoch": 32.8672032193159,
      "grad_norm": 0.9360777139663696,
      "learning_rate": 0.00013427910252540497,
      "loss": 0.2627,
      "step": 16335
    },
    {
      "epoch": 32.8692152917505,
      "grad_norm": 0.8897102475166321,
      "learning_rate": 0.0001342750779756515,
      "loss": 0.2357,
      "step": 16336
    },
    {
      "epoch": 32.87122736418511,
      "grad_norm": 0.9117878079414368,
      "learning_rate": 0.000134271053425898,
      "loss": 0.2523,
      "step": 16337
    },
    {
      "epoch": 32.87323943661972,
      "grad_norm": 0.9086817502975464,
      "learning_rate": 0.00013426702887614448,
      "loss": 0.2623,
      "step": 16338
    },
    {
      "epoch": 32.87525150905432,
      "grad_norm": 0.8977389931678772,
      "learning_rate": 0.000134263004326391,
      "loss": 0.2349,
      "step": 16339
    },
    {
      "epoch": 32.87726358148893,
      "grad_norm": 0.9926025867462158,
      "learning_rate": 0.0001342589797766375,
      "loss": 0.2493,
      "step": 16340
    },
    {
      "epoch": 32.87927565392354,
      "grad_norm": 0.9356319904327393,
      "learning_rate": 0.000134254955226884,
      "loss": 0.263,
      "step": 16341
    },
    {
      "epoch": 32.881287726358146,
      "grad_norm": 0.9704381823539734,
      "learning_rate": 0.0001342509306771305,
      "loss": 0.2709,
      "step": 16342
    },
    {
      "epoch": 32.883299798792756,
      "grad_norm": 0.8715483546257019,
      "learning_rate": 0.000134246906127377,
      "loss": 0.2488,
      "step": 16343
    },
    {
      "epoch": 32.885311871227366,
      "grad_norm": 0.9901517033576965,
      "learning_rate": 0.0001342428815776235,
      "loss": 0.259,
      "step": 16344
    },
    {
      "epoch": 32.88732394366197,
      "grad_norm": 0.8886434435844421,
      "learning_rate": 0.00013423885702787002,
      "loss": 0.2423,
      "step": 16345
    },
    {
      "epoch": 32.88933601609658,
      "grad_norm": 0.9347260594367981,
      "learning_rate": 0.00013423483247811653,
      "loss": 0.2441,
      "step": 16346
    },
    {
      "epoch": 32.89134808853119,
      "grad_norm": 0.9917007684707642,
      "learning_rate": 0.000134230807928363,
      "loss": 0.2557,
      "step": 16347
    },
    {
      "epoch": 32.89336016096579,
      "grad_norm": 0.9243562817573547,
      "learning_rate": 0.00013422678337860952,
      "loss": 0.2447,
      "step": 16348
    },
    {
      "epoch": 32.8953722334004,
      "grad_norm": 0.9194724559783936,
      "learning_rate": 0.000134222758828856,
      "loss": 0.2549,
      "step": 16349
    },
    {
      "epoch": 32.89738430583501,
      "grad_norm": 0.9781703948974609,
      "learning_rate": 0.00013421873427910255,
      "loss": 0.2669,
      "step": 16350
    },
    {
      "epoch": 32.899396378269614,
      "grad_norm": 0.9800481200218201,
      "learning_rate": 0.00013421470972934903,
      "loss": 0.247,
      "step": 16351
    },
    {
      "epoch": 32.901408450704224,
      "grad_norm": 0.9245476722717285,
      "learning_rate": 0.00013421068517959555,
      "loss": 0.2515,
      "step": 16352
    },
    {
      "epoch": 32.903420523138834,
      "grad_norm": 0.9916117191314697,
      "learning_rate": 0.00013420666062984203,
      "loss": 0.247,
      "step": 16353
    },
    {
      "epoch": 32.905432595573444,
      "grad_norm": 0.917715847492218,
      "learning_rate": 0.00013420263608008854,
      "loss": 0.2664,
      "step": 16354
    },
    {
      "epoch": 32.90744466800805,
      "grad_norm": 0.9063113331794739,
      "learning_rate": 0.00013419861153033506,
      "loss": 0.2595,
      "step": 16355
    },
    {
      "epoch": 32.90945674044266,
      "grad_norm": 0.9620538353919983,
      "learning_rate": 0.00013419458698058157,
      "loss": 0.2543,
      "step": 16356
    },
    {
      "epoch": 32.91146881287727,
      "grad_norm": 0.9305092692375183,
      "learning_rate": 0.00013419056243082805,
      "loss": 0.2656,
      "step": 16357
    },
    {
      "epoch": 32.91348088531187,
      "grad_norm": 0.9492226243019104,
      "learning_rate": 0.00013418653788107457,
      "loss": 0.2546,
      "step": 16358
    },
    {
      "epoch": 32.91549295774648,
      "grad_norm": 0.9367003440856934,
      "learning_rate": 0.00013418251333132105,
      "loss": 0.249,
      "step": 16359
    },
    {
      "epoch": 32.91750503018109,
      "grad_norm": 0.9222030639648438,
      "learning_rate": 0.0001341784887815676,
      "loss": 0.2453,
      "step": 16360
    },
    {
      "epoch": 32.91951710261569,
      "grad_norm": 0.9033185839653015,
      "learning_rate": 0.00013417446423181408,
      "loss": 0.2368,
      "step": 16361
    },
    {
      "epoch": 32.9215291750503,
      "grad_norm": 0.9419541358947754,
      "learning_rate": 0.0001341704396820606,
      "loss": 0.2558,
      "step": 16362
    },
    {
      "epoch": 32.92354124748491,
      "grad_norm": 0.9781562089920044,
      "learning_rate": 0.00013416641513230707,
      "loss": 0.2494,
      "step": 16363
    },
    {
      "epoch": 32.925553319919516,
      "grad_norm": 0.9139412641525269,
      "learning_rate": 0.00013416239058255358,
      "loss": 0.2471,
      "step": 16364
    },
    {
      "epoch": 32.927565392354126,
      "grad_norm": 0.9437610507011414,
      "learning_rate": 0.0001341583660328001,
      "loss": 0.2704,
      "step": 16365
    },
    {
      "epoch": 32.929577464788736,
      "grad_norm": 0.9490137696266174,
      "learning_rate": 0.0001341543414830466,
      "loss": 0.2524,
      "step": 16366
    },
    {
      "epoch": 32.93158953722334,
      "grad_norm": 0.9319348931312561,
      "learning_rate": 0.0001341503169332931,
      "loss": 0.2459,
      "step": 16367
    },
    {
      "epoch": 32.93360160965795,
      "grad_norm": 0.9446123838424683,
      "learning_rate": 0.0001341462923835396,
      "loss": 0.2452,
      "step": 16368
    },
    {
      "epoch": 32.93561368209256,
      "grad_norm": 0.9585648775100708,
      "learning_rate": 0.0001341422678337861,
      "loss": 0.2571,
      "step": 16369
    },
    {
      "epoch": 32.93762575452716,
      "grad_norm": 0.8998937010765076,
      "learning_rate": 0.0001341382432840326,
      "loss": 0.2395,
      "step": 16370
    },
    {
      "epoch": 32.93963782696177,
      "grad_norm": 0.9303855299949646,
      "learning_rate": 0.00013413421873427912,
      "loss": 0.262,
      "step": 16371
    },
    {
      "epoch": 32.94164989939638,
      "grad_norm": 0.9061956405639648,
      "learning_rate": 0.00013413019418452563,
      "loss": 0.2433,
      "step": 16372
    },
    {
      "epoch": 32.943661971830984,
      "grad_norm": 0.9683659076690674,
      "learning_rate": 0.0001341261696347721,
      "loss": 0.2522,
      "step": 16373
    },
    {
      "epoch": 32.945674044265594,
      "grad_norm": 0.9086587429046631,
      "learning_rate": 0.00013412214508501863,
      "loss": 0.2383,
      "step": 16374
    },
    {
      "epoch": 32.947686116700204,
      "grad_norm": 0.9246821999549866,
      "learning_rate": 0.0001341181205352651,
      "loss": 0.2605,
      "step": 16375
    },
    {
      "epoch": 32.94969818913481,
      "grad_norm": 0.9265273809432983,
      "learning_rate": 0.00013411409598551162,
      "loss": 0.2357,
      "step": 16376
    },
    {
      "epoch": 32.95171026156942,
      "grad_norm": 0.935390830039978,
      "learning_rate": 0.00013411007143575814,
      "loss": 0.252,
      "step": 16377
    },
    {
      "epoch": 32.95372233400403,
      "grad_norm": 0.9399122595787048,
      "learning_rate": 0.00013410604688600462,
      "loss": 0.2644,
      "step": 16378
    },
    {
      "epoch": 32.95573440643863,
      "grad_norm": 0.9149465560913086,
      "learning_rate": 0.00013410202233625113,
      "loss": 0.227,
      "step": 16379
    },
    {
      "epoch": 32.95774647887324,
      "grad_norm": 0.899982213973999,
      "learning_rate": 0.00013409799778649762,
      "loss": 0.2515,
      "step": 16380
    },
    {
      "epoch": 32.95975855130785,
      "grad_norm": 0.9221957921981812,
      "learning_rate": 0.00013409397323674416,
      "loss": 0.272,
      "step": 16381
    },
    {
      "epoch": 32.96177062374245,
      "grad_norm": 0.948671281337738,
      "learning_rate": 0.00013408994868699064,
      "loss": 0.2466,
      "step": 16382
    },
    {
      "epoch": 32.96378269617706,
      "grad_norm": 0.8854348659515381,
      "learning_rate": 0.00013408592413723715,
      "loss": 0.2418,
      "step": 16383
    },
    {
      "epoch": 32.96579476861167,
      "grad_norm": 0.9563421607017517,
      "learning_rate": 0.00013408189958748364,
      "loss": 0.2634,
      "step": 16384
    },
    {
      "epoch": 32.967806841046276,
      "grad_norm": 0.9267104268074036,
      "learning_rate": 0.00013407787503773015,
      "loss": 0.2536,
      "step": 16385
    },
    {
      "epoch": 32.969818913480886,
      "grad_norm": 0.9635161757469177,
      "learning_rate": 0.00013407385048797666,
      "loss": 0.2665,
      "step": 16386
    },
    {
      "epoch": 32.971830985915496,
      "grad_norm": 0.9032198786735535,
      "learning_rate": 0.00013406982593822318,
      "loss": 0.2439,
      "step": 16387
    },
    {
      "epoch": 32.9738430583501,
      "grad_norm": 0.9465917944908142,
      "learning_rate": 0.00013406580138846966,
      "loss": 0.2551,
      "step": 16388
    },
    {
      "epoch": 32.97585513078471,
      "grad_norm": 0.9848271608352661,
      "learning_rate": 0.00013406177683871617,
      "loss": 0.2686,
      "step": 16389
    },
    {
      "epoch": 32.97786720321932,
      "grad_norm": 0.9039728045463562,
      "learning_rate": 0.00013405775228896266,
      "loss": 0.2471,
      "step": 16390
    },
    {
      "epoch": 32.97987927565392,
      "grad_norm": 0.9459385871887207,
      "learning_rate": 0.0001340537277392092,
      "loss": 0.2593,
      "step": 16391
    },
    {
      "epoch": 32.98189134808853,
      "grad_norm": 0.8836459517478943,
      "learning_rate": 0.00013404970318945568,
      "loss": 0.2451,
      "step": 16392
    },
    {
      "epoch": 32.98390342052314,
      "grad_norm": 0.9124230742454529,
      "learning_rate": 0.0001340456786397022,
      "loss": 0.2441,
      "step": 16393
    },
    {
      "epoch": 32.985915492957744,
      "grad_norm": 0.898694634437561,
      "learning_rate": 0.00013404165408994868,
      "loss": 0.2457,
      "step": 16394
    },
    {
      "epoch": 32.987927565392354,
      "grad_norm": 0.9785674810409546,
      "learning_rate": 0.0001340376295401952,
      "loss": 0.2674,
      "step": 16395
    },
    {
      "epoch": 32.989939637826964,
      "grad_norm": 0.9396650791168213,
      "learning_rate": 0.0001340336049904417,
      "loss": 0.2541,
      "step": 16396
    },
    {
      "epoch": 32.99195171026157,
      "grad_norm": 0.9061352610588074,
      "learning_rate": 0.00013402958044068822,
      "loss": 0.2494,
      "step": 16397
    },
    {
      "epoch": 32.99396378269618,
      "grad_norm": 0.9218763113021851,
      "learning_rate": 0.0001340255558909347,
      "loss": 0.2511,
      "step": 16398
    },
    {
      "epoch": 32.99597585513079,
      "grad_norm": 0.8953972458839417,
      "learning_rate": 0.00013402153134118121,
      "loss": 0.2442,
      "step": 16399
    },
    {
      "epoch": 32.99798792756539,
      "grad_norm": 0.913779616355896,
      "learning_rate": 0.0001340175067914277,
      "loss": 0.2439,
      "step": 16400
    },
    {
      "epoch": 33.0,
      "grad_norm": 0.9562866687774658,
      "learning_rate": 0.00013401348224167424,
      "loss": 0.2522,
      "step": 16401
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.3963274955749512,
      "eval_runtime": 49.8388,
      "eval_samples_per_second": 19.904,
      "eval_steps_per_second": 2.488,
      "step": 16401
    },
    {
      "epoch": 33.00201207243461,
      "grad_norm": 0.7704271674156189,
      "learning_rate": 0.00013400945769192072,
      "loss": 0.1945,
      "step": 16402
    },
    {
      "epoch": 33.00402414486921,
      "grad_norm": 0.7689773440361023,
      "learning_rate": 0.00013400543314216724,
      "loss": 0.1951,
      "step": 16403
    },
    {
      "epoch": 33.00603621730382,
      "grad_norm": 0.8245940804481506,
      "learning_rate": 0.00013400140859241372,
      "loss": 0.1994,
      "step": 16404
    },
    {
      "epoch": 33.00804828973843,
      "grad_norm": 0.7991722226142883,
      "learning_rate": 0.00013399738404266023,
      "loss": 0.1833,
      "step": 16405
    },
    {
      "epoch": 33.010060362173036,
      "grad_norm": 0.8501407504081726,
      "learning_rate": 0.00013399335949290675,
      "loss": 0.1993,
      "step": 16406
    },
    {
      "epoch": 33.012072434607646,
      "grad_norm": 0.9214339256286621,
      "learning_rate": 0.00013398933494315323,
      "loss": 0.187,
      "step": 16407
    },
    {
      "epoch": 33.014084507042256,
      "grad_norm": 0.864931046962738,
      "learning_rate": 0.00013398531039339974,
      "loss": 0.1963,
      "step": 16408
    },
    {
      "epoch": 33.01609657947686,
      "grad_norm": 0.8308142423629761,
      "learning_rate": 0.00013398128584364626,
      "loss": 0.1957,
      "step": 16409
    },
    {
      "epoch": 33.01810865191147,
      "grad_norm": 0.9006310701370239,
      "learning_rate": 0.00013397726129389274,
      "loss": 0.205,
      "step": 16410
    },
    {
      "epoch": 33.02012072434608,
      "grad_norm": 0.8009387254714966,
      "learning_rate": 0.00013397323674413925,
      "loss": 0.1864,
      "step": 16411
    },
    {
      "epoch": 33.02213279678068,
      "grad_norm": 0.7937304377555847,
      "learning_rate": 0.00013396921219438576,
      "loss": 0.1914,
      "step": 16412
    },
    {
      "epoch": 33.02414486921529,
      "grad_norm": 0.7997809052467346,
      "learning_rate": 0.00013396518764463225,
      "loss": 0.1999,
      "step": 16413
    },
    {
      "epoch": 33.0261569416499,
      "grad_norm": 0.8957605957984924,
      "learning_rate": 0.00013396116309487876,
      "loss": 0.1982,
      "step": 16414
    },
    {
      "epoch": 33.028169014084504,
      "grad_norm": 0.7907642722129822,
      "learning_rate": 0.00013395713854512525,
      "loss": 0.2052,
      "step": 16415
    },
    {
      "epoch": 33.030181086519114,
      "grad_norm": 0.7908604741096497,
      "learning_rate": 0.0001339531139953718,
      "loss": 0.1899,
      "step": 16416
    },
    {
      "epoch": 33.032193158953724,
      "grad_norm": 0.8358743786811829,
      "learning_rate": 0.00013394908944561827,
      "loss": 0.2047,
      "step": 16417
    },
    {
      "epoch": 33.03420523138833,
      "grad_norm": 0.8656349182128906,
      "learning_rate": 0.00013394506489586478,
      "loss": 0.1912,
      "step": 16418
    },
    {
      "epoch": 33.03621730382294,
      "grad_norm": 0.8514154553413391,
      "learning_rate": 0.00013394104034611127,
      "loss": 0.1861,
      "step": 16419
    },
    {
      "epoch": 33.03822937625755,
      "grad_norm": 0.8590317368507385,
      "learning_rate": 0.00013393701579635778,
      "loss": 0.2043,
      "step": 16420
    },
    {
      "epoch": 33.04024144869215,
      "grad_norm": 0.7794538736343384,
      "learning_rate": 0.0001339329912466043,
      "loss": 0.1705,
      "step": 16421
    },
    {
      "epoch": 33.04225352112676,
      "grad_norm": 0.8184836506843567,
      "learning_rate": 0.0001339289666968508,
      "loss": 0.1985,
      "step": 16422
    },
    {
      "epoch": 33.04426559356137,
      "grad_norm": 0.8409410715103149,
      "learning_rate": 0.0001339249421470973,
      "loss": 0.2186,
      "step": 16423
    },
    {
      "epoch": 33.04627766599597,
      "grad_norm": 0.8361729979515076,
      "learning_rate": 0.0001339209175973438,
      "loss": 0.1781,
      "step": 16424
    },
    {
      "epoch": 33.04828973843058,
      "grad_norm": 0.8266171813011169,
      "learning_rate": 0.0001339168930475903,
      "loss": 0.2177,
      "step": 16425
    },
    {
      "epoch": 33.05030181086519,
      "grad_norm": 0.8452509045600891,
      "learning_rate": 0.00013391286849783683,
      "loss": 0.2043,
      "step": 16426
    },
    {
      "epoch": 33.052313883299796,
      "grad_norm": 0.788126528263092,
      "learning_rate": 0.0001339088439480833,
      "loss": 0.196,
      "step": 16427
    },
    {
      "epoch": 33.054325955734406,
      "grad_norm": 0.9242691993713379,
      "learning_rate": 0.00013390481939832982,
      "loss": 0.2158,
      "step": 16428
    },
    {
      "epoch": 33.056338028169016,
      "grad_norm": 0.7947772741317749,
      "learning_rate": 0.0001339007948485763,
      "loss": 0.1909,
      "step": 16429
    },
    {
      "epoch": 33.05835010060362,
      "grad_norm": 0.8021226525306702,
      "learning_rate": 0.00013389677029882282,
      "loss": 0.1873,
      "step": 16430
    },
    {
      "epoch": 33.06036217303823,
      "grad_norm": 0.85102379322052,
      "learning_rate": 0.00013389274574906933,
      "loss": 0.2067,
      "step": 16431
    },
    {
      "epoch": 33.06237424547284,
      "grad_norm": 0.8212797045707703,
      "learning_rate": 0.00013388872119931585,
      "loss": 0.2,
      "step": 16432
    },
    {
      "epoch": 33.06438631790744,
      "grad_norm": 0.8584569096565247,
      "learning_rate": 0.00013388469664956233,
      "loss": 0.1994,
      "step": 16433
    },
    {
      "epoch": 33.06639839034205,
      "grad_norm": 0.8535066246986389,
      "learning_rate": 0.00013388067209980884,
      "loss": 0.214,
      "step": 16434
    },
    {
      "epoch": 33.06841046277666,
      "grad_norm": 0.8939406871795654,
      "learning_rate": 0.00013387664755005533,
      "loss": 0.1983,
      "step": 16435
    },
    {
      "epoch": 33.070422535211264,
      "grad_norm": 0.9149838089942932,
      "learning_rate": 0.00013387262300030187,
      "loss": 0.2055,
      "step": 16436
    },
    {
      "epoch": 33.072434607645874,
      "grad_norm": 0.8047590255737305,
      "learning_rate": 0.00013386859845054835,
      "loss": 0.1767,
      "step": 16437
    },
    {
      "epoch": 33.074446680080484,
      "grad_norm": 0.8437239527702332,
      "learning_rate": 0.00013386457390079487,
      "loss": 0.2006,
      "step": 16438
    },
    {
      "epoch": 33.07645875251509,
      "grad_norm": 0.8695418834686279,
      "learning_rate": 0.00013386054935104135,
      "loss": 0.1872,
      "step": 16439
    },
    {
      "epoch": 33.0784708249497,
      "grad_norm": 0.8220437169075012,
      "learning_rate": 0.00013385652480128786,
      "loss": 0.1914,
      "step": 16440
    },
    {
      "epoch": 33.08048289738431,
      "grad_norm": 0.794273316860199,
      "learning_rate": 0.00013385250025153438,
      "loss": 0.1828,
      "step": 16441
    },
    {
      "epoch": 33.08249496981891,
      "grad_norm": 0.8182568550109863,
      "learning_rate": 0.00013384847570178086,
      "loss": 0.1965,
      "step": 16442
    },
    {
      "epoch": 33.08450704225352,
      "grad_norm": 0.8342868685722351,
      "learning_rate": 0.00013384445115202737,
      "loss": 0.198,
      "step": 16443
    },
    {
      "epoch": 33.08651911468813,
      "grad_norm": 0.7682849764823914,
      "learning_rate": 0.00013384042660227388,
      "loss": 0.1841,
      "step": 16444
    },
    {
      "epoch": 33.08853118712273,
      "grad_norm": 0.7999646663665771,
      "learning_rate": 0.00013383640205252037,
      "loss": 0.1997,
      "step": 16445
    },
    {
      "epoch": 33.09054325955734,
      "grad_norm": 0.8474413752555847,
      "learning_rate": 0.00013383237750276688,
      "loss": 0.2002,
      "step": 16446
    },
    {
      "epoch": 33.09255533199195,
      "grad_norm": 0.8484143614768982,
      "learning_rate": 0.0001338283529530134,
      "loss": 0.2004,
      "step": 16447
    },
    {
      "epoch": 33.094567404426556,
      "grad_norm": 0.8743097186088562,
      "learning_rate": 0.00013382432840325988,
      "loss": 0.1886,
      "step": 16448
    },
    {
      "epoch": 33.096579476861166,
      "grad_norm": 0.8881310224533081,
      "learning_rate": 0.0001338203038535064,
      "loss": 0.1891,
      "step": 16449
    },
    {
      "epoch": 33.098591549295776,
      "grad_norm": 0.8400725722312927,
      "learning_rate": 0.00013381627930375288,
      "loss": 0.2047,
      "step": 16450
    },
    {
      "epoch": 33.100603621730386,
      "grad_norm": 0.8260915875434875,
      "learning_rate": 0.00013381225475399942,
      "loss": 0.1891,
      "step": 16451
    },
    {
      "epoch": 33.10261569416499,
      "grad_norm": 0.8253250122070312,
      "learning_rate": 0.0001338082302042459,
      "loss": 0.2148,
      "step": 16452
    },
    {
      "epoch": 33.1046277665996,
      "grad_norm": 0.7909274697303772,
      "learning_rate": 0.0001338042056544924,
      "loss": 0.189,
      "step": 16453
    },
    {
      "epoch": 33.10663983903421,
      "grad_norm": 0.8751067519187927,
      "learning_rate": 0.0001338001811047389,
      "loss": 0.1951,
      "step": 16454
    },
    {
      "epoch": 33.10865191146881,
      "grad_norm": 0.8959618806838989,
      "learning_rate": 0.0001337961565549854,
      "loss": 0.1952,
      "step": 16455
    },
    {
      "epoch": 33.11066398390342,
      "grad_norm": 0.8066778779029846,
      "learning_rate": 0.00013379213200523192,
      "loss": 0.1768,
      "step": 16456
    },
    {
      "epoch": 33.11267605633803,
      "grad_norm": 0.8794789910316467,
      "learning_rate": 0.00013378810745547843,
      "loss": 0.1991,
      "step": 16457
    },
    {
      "epoch": 33.114688128772634,
      "grad_norm": 0.8494008779525757,
      "learning_rate": 0.00013378408290572492,
      "loss": 0.1809,
      "step": 16458
    },
    {
      "epoch": 33.116700201207244,
      "grad_norm": 0.8428831696510315,
      "learning_rate": 0.00013378005835597143,
      "loss": 0.2,
      "step": 16459
    },
    {
      "epoch": 33.118712273641854,
      "grad_norm": 0.8140990138053894,
      "learning_rate": 0.00013377603380621792,
      "loss": 0.1918,
      "step": 16460
    },
    {
      "epoch": 33.12072434607646,
      "grad_norm": 0.89664626121521,
      "learning_rate": 0.00013377200925646446,
      "loss": 0.2057,
      "step": 16461
    },
    {
      "epoch": 33.12273641851107,
      "grad_norm": 0.9003411531448364,
      "learning_rate": 0.00013376798470671094,
      "loss": 0.1803,
      "step": 16462
    },
    {
      "epoch": 33.12474849094568,
      "grad_norm": 0.9806281924247742,
      "learning_rate": 0.00013376396015695745,
      "loss": 0.2217,
      "step": 16463
    },
    {
      "epoch": 33.12676056338028,
      "grad_norm": 0.899503767490387,
      "learning_rate": 0.00013375993560720394,
      "loss": 0.2201,
      "step": 16464
    },
    {
      "epoch": 33.12877263581489,
      "grad_norm": 0.7633633017539978,
      "learning_rate": 0.00013375591105745045,
      "loss": 0.1791,
      "step": 16465
    },
    {
      "epoch": 33.1307847082495,
      "grad_norm": 0.8672937750816345,
      "learning_rate": 0.00013375188650769696,
      "loss": 0.1957,
      "step": 16466
    },
    {
      "epoch": 33.1327967806841,
      "grad_norm": 0.8258533477783203,
      "learning_rate": 0.00013374786195794348,
      "loss": 0.1914,
      "step": 16467
    },
    {
      "epoch": 33.13480885311871,
      "grad_norm": 0.8193762898445129,
      "learning_rate": 0.00013374383740818996,
      "loss": 0.1879,
      "step": 16468
    },
    {
      "epoch": 33.13682092555332,
      "grad_norm": 0.8563128709793091,
      "learning_rate": 0.00013373981285843647,
      "loss": 0.2042,
      "step": 16469
    },
    {
      "epoch": 33.138832997987926,
      "grad_norm": 0.9027361869812012,
      "learning_rate": 0.00013373578830868296,
      "loss": 0.2099,
      "step": 16470
    },
    {
      "epoch": 33.140845070422536,
      "grad_norm": 0.8941889405250549,
      "learning_rate": 0.0001337317637589295,
      "loss": 0.1987,
      "step": 16471
    },
    {
      "epoch": 33.142857142857146,
      "grad_norm": 0.8845445513725281,
      "learning_rate": 0.00013372773920917598,
      "loss": 0.2104,
      "step": 16472
    },
    {
      "epoch": 33.14486921529175,
      "grad_norm": 0.8622274398803711,
      "learning_rate": 0.0001337237146594225,
      "loss": 0.2084,
      "step": 16473
    },
    {
      "epoch": 33.14688128772636,
      "grad_norm": 0.7824422717094421,
      "learning_rate": 0.00013371969010966898,
      "loss": 0.1959,
      "step": 16474
    },
    {
      "epoch": 33.14889336016097,
      "grad_norm": 0.8504946827888489,
      "learning_rate": 0.0001337156655599155,
      "loss": 0.2173,
      "step": 16475
    },
    {
      "epoch": 33.15090543259557,
      "grad_norm": 0.814926028251648,
      "learning_rate": 0.000133711641010162,
      "loss": 0.1934,
      "step": 16476
    },
    {
      "epoch": 33.15291750503018,
      "grad_norm": 0.8099921345710754,
      "learning_rate": 0.0001337076164604085,
      "loss": 0.208,
      "step": 16477
    },
    {
      "epoch": 33.15492957746479,
      "grad_norm": 0.8778567910194397,
      "learning_rate": 0.000133703591910655,
      "loss": 0.2034,
      "step": 16478
    },
    {
      "epoch": 33.156941649899395,
      "grad_norm": 0.8326832056045532,
      "learning_rate": 0.00013369956736090151,
      "loss": 0.1971,
      "step": 16479
    },
    {
      "epoch": 33.158953722334005,
      "grad_norm": 0.8616719245910645,
      "learning_rate": 0.000133695542811148,
      "loss": 0.2067,
      "step": 16480
    },
    {
      "epoch": 33.160965794768615,
      "grad_norm": 0.7930328845977783,
      "learning_rate": 0.0001336915182613945,
      "loss": 0.1891,
      "step": 16481
    },
    {
      "epoch": 33.16297786720322,
      "grad_norm": 0.8930314779281616,
      "learning_rate": 0.00013368749371164102,
      "loss": 0.1916,
      "step": 16482
    },
    {
      "epoch": 33.16498993963783,
      "grad_norm": 0.8274452090263367,
      "learning_rate": 0.0001336834691618875,
      "loss": 0.2021,
      "step": 16483
    },
    {
      "epoch": 33.16700201207244,
      "grad_norm": 0.8363159894943237,
      "learning_rate": 0.00013367944461213402,
      "loss": 0.1997,
      "step": 16484
    },
    {
      "epoch": 33.16901408450704,
      "grad_norm": 0.8021714687347412,
      "learning_rate": 0.0001336754200623805,
      "loss": 0.1948,
      "step": 16485
    },
    {
      "epoch": 33.17102615694165,
      "grad_norm": 0.8102827668190002,
      "learning_rate": 0.00013367139551262705,
      "loss": 0.1962,
      "step": 16486
    },
    {
      "epoch": 33.17303822937626,
      "grad_norm": 0.8361331224441528,
      "learning_rate": 0.00013366737096287353,
      "loss": 0.1817,
      "step": 16487
    },
    {
      "epoch": 33.17505030181086,
      "grad_norm": 0.8719465136528015,
      "learning_rate": 0.00013366334641312004,
      "loss": 0.1971,
      "step": 16488
    },
    {
      "epoch": 33.17706237424547,
      "grad_norm": 0.9004098176956177,
      "learning_rate": 0.00013365932186336653,
      "loss": 0.1982,
      "step": 16489
    },
    {
      "epoch": 33.17907444668008,
      "grad_norm": 0.8265906572341919,
      "learning_rate": 0.00013365529731361304,
      "loss": 0.1905,
      "step": 16490
    },
    {
      "epoch": 33.181086519114686,
      "grad_norm": 0.8325778841972351,
      "learning_rate": 0.00013365127276385955,
      "loss": 0.1782,
      "step": 16491
    },
    {
      "epoch": 33.183098591549296,
      "grad_norm": 0.8382527828216553,
      "learning_rate": 0.00013364724821410606,
      "loss": 0.1723,
      "step": 16492
    },
    {
      "epoch": 33.185110663983906,
      "grad_norm": 0.8712735772132874,
      "learning_rate": 0.00013364322366435255,
      "loss": 0.2056,
      "step": 16493
    },
    {
      "epoch": 33.18712273641851,
      "grad_norm": 0.8634233474731445,
      "learning_rate": 0.00013363919911459906,
      "loss": 0.217,
      "step": 16494
    },
    {
      "epoch": 33.18913480885312,
      "grad_norm": 0.8249738216400146,
      "learning_rate": 0.00013363517456484555,
      "loss": 0.2082,
      "step": 16495
    },
    {
      "epoch": 33.19114688128773,
      "grad_norm": 0.8380264043807983,
      "learning_rate": 0.00013363115001509209,
      "loss": 0.2057,
      "step": 16496
    },
    {
      "epoch": 33.19315895372233,
      "grad_norm": 0.8337588310241699,
      "learning_rate": 0.00013362712546533857,
      "loss": 0.1962,
      "step": 16497
    },
    {
      "epoch": 33.19517102615694,
      "grad_norm": 0.8001990914344788,
      "learning_rate": 0.00013362310091558508,
      "loss": 0.188,
      "step": 16498
    },
    {
      "epoch": 33.19718309859155,
      "grad_norm": 0.8934292197227478,
      "learning_rate": 0.00013361907636583157,
      "loss": 0.2084,
      "step": 16499
    },
    {
      "epoch": 33.199195171026155,
      "grad_norm": 0.919879138469696,
      "learning_rate": 0.00013361505181607808,
      "loss": 0.2181,
      "step": 16500
    },
    {
      "epoch": 33.201207243460765,
      "grad_norm": 0.8411775827407837,
      "learning_rate": 0.0001336110272663246,
      "loss": 0.2077,
      "step": 16501
    },
    {
      "epoch": 33.203219315895375,
      "grad_norm": 0.8734918832778931,
      "learning_rate": 0.0001336070027165711,
      "loss": 0.1957,
      "step": 16502
    },
    {
      "epoch": 33.20523138832998,
      "grad_norm": 0.844973087310791,
      "learning_rate": 0.0001336029781668176,
      "loss": 0.2132,
      "step": 16503
    },
    {
      "epoch": 33.20724346076459,
      "grad_norm": 0.8637869358062744,
      "learning_rate": 0.0001335989536170641,
      "loss": 0.211,
      "step": 16504
    },
    {
      "epoch": 33.2092555331992,
      "grad_norm": 0.799888551235199,
      "learning_rate": 0.0001335949290673106,
      "loss": 0.1763,
      "step": 16505
    },
    {
      "epoch": 33.2112676056338,
      "grad_norm": 0.8674293756484985,
      "learning_rate": 0.00013359090451755713,
      "loss": 0.1875,
      "step": 16506
    },
    {
      "epoch": 33.21327967806841,
      "grad_norm": 0.8936686515808105,
      "learning_rate": 0.0001335868799678036,
      "loss": 0.2197,
      "step": 16507
    },
    {
      "epoch": 33.21529175050302,
      "grad_norm": 0.856885552406311,
      "learning_rate": 0.00013358285541805012,
      "loss": 0.1927,
      "step": 16508
    },
    {
      "epoch": 33.21730382293762,
      "grad_norm": 0.8321756720542908,
      "learning_rate": 0.0001335788308682966,
      "loss": 0.1887,
      "step": 16509
    },
    {
      "epoch": 33.21931589537223,
      "grad_norm": 0.8848329782485962,
      "learning_rate": 0.00013357480631854312,
      "loss": 0.2211,
      "step": 16510
    },
    {
      "epoch": 33.22132796780684,
      "grad_norm": 0.8552739024162292,
      "learning_rate": 0.00013357078176878963,
      "loss": 0.1936,
      "step": 16511
    },
    {
      "epoch": 33.223340040241446,
      "grad_norm": 0.903731107711792,
      "learning_rate": 0.00013356675721903612,
      "loss": 0.2126,
      "step": 16512
    },
    {
      "epoch": 33.225352112676056,
      "grad_norm": 0.9334290027618408,
      "learning_rate": 0.00013356273266928263,
      "loss": 0.2231,
      "step": 16513
    },
    {
      "epoch": 33.227364185110666,
      "grad_norm": 0.932978093624115,
      "learning_rate": 0.00013355870811952914,
      "loss": 0.2137,
      "step": 16514
    },
    {
      "epoch": 33.22937625754527,
      "grad_norm": 0.879968523979187,
      "learning_rate": 0.00013355468356977563,
      "loss": 0.1948,
      "step": 16515
    },
    {
      "epoch": 33.23138832997988,
      "grad_norm": 0.7864372730255127,
      "learning_rate": 0.00013355065902002214,
      "loss": 0.1777,
      "step": 16516
    },
    {
      "epoch": 33.23340040241449,
      "grad_norm": 0.8382912278175354,
      "learning_rate": 0.00013354663447026865,
      "loss": 0.1931,
      "step": 16517
    },
    {
      "epoch": 33.23541247484909,
      "grad_norm": 0.885699450969696,
      "learning_rate": 0.00013354260992051514,
      "loss": 0.1957,
      "step": 16518
    },
    {
      "epoch": 33.2374245472837,
      "grad_norm": 0.9053827524185181,
      "learning_rate": 0.00013353858537076165,
      "loss": 0.2076,
      "step": 16519
    },
    {
      "epoch": 33.23943661971831,
      "grad_norm": 0.8055322766304016,
      "learning_rate": 0.00013353456082100814,
      "loss": 0.1965,
      "step": 16520
    },
    {
      "epoch": 33.241448692152915,
      "grad_norm": 0.8645317554473877,
      "learning_rate": 0.00013353053627125467,
      "loss": 0.2051,
      "step": 16521
    },
    {
      "epoch": 33.243460764587525,
      "grad_norm": 0.8360434174537659,
      "learning_rate": 0.00013352651172150116,
      "loss": 0.1882,
      "step": 16522
    },
    {
      "epoch": 33.245472837022135,
      "grad_norm": 0.9088742733001709,
      "learning_rate": 0.00013352248717174767,
      "loss": 0.2213,
      "step": 16523
    },
    {
      "epoch": 33.24748490945674,
      "grad_norm": 1.0105390548706055,
      "learning_rate": 0.00013351846262199416,
      "loss": 0.21,
      "step": 16524
    },
    {
      "epoch": 33.24949698189135,
      "grad_norm": 0.8728423714637756,
      "learning_rate": 0.00013351443807224067,
      "loss": 0.2145,
      "step": 16525
    },
    {
      "epoch": 33.25150905432596,
      "grad_norm": 0.8532262444496155,
      "learning_rate": 0.00013351041352248718,
      "loss": 0.2045,
      "step": 16526
    },
    {
      "epoch": 33.25352112676056,
      "grad_norm": 0.9244996905326843,
      "learning_rate": 0.0001335063889727337,
      "loss": 0.2151,
      "step": 16527
    },
    {
      "epoch": 33.25553319919517,
      "grad_norm": 0.9144952297210693,
      "learning_rate": 0.00013350236442298018,
      "loss": 0.2156,
      "step": 16528
    },
    {
      "epoch": 33.25754527162978,
      "grad_norm": 0.8501438498497009,
      "learning_rate": 0.0001334983398732267,
      "loss": 0.1932,
      "step": 16529
    },
    {
      "epoch": 33.25955734406438,
      "grad_norm": 0.8697398900985718,
      "learning_rate": 0.00013349431532347318,
      "loss": 0.2098,
      "step": 16530
    },
    {
      "epoch": 33.26156941649899,
      "grad_norm": 0.8966754078865051,
      "learning_rate": 0.00013349029077371972,
      "loss": 0.2059,
      "step": 16531
    },
    {
      "epoch": 33.2635814889336,
      "grad_norm": 0.8616755604743958,
      "learning_rate": 0.0001334862662239662,
      "loss": 0.2013,
      "step": 16532
    },
    {
      "epoch": 33.265593561368206,
      "grad_norm": 0.8593750596046448,
      "learning_rate": 0.0001334822416742127,
      "loss": 0.1993,
      "step": 16533
    },
    {
      "epoch": 33.267605633802816,
      "grad_norm": 0.8314719200134277,
      "learning_rate": 0.0001334782171244592,
      "loss": 0.194,
      "step": 16534
    },
    {
      "epoch": 33.269617706237426,
      "grad_norm": 0.8879266977310181,
      "learning_rate": 0.0001334741925747057,
      "loss": 0.2119,
      "step": 16535
    },
    {
      "epoch": 33.27162977867203,
      "grad_norm": 0.8834802508354187,
      "learning_rate": 0.00013347016802495222,
      "loss": 0.2144,
      "step": 16536
    },
    {
      "epoch": 33.27364185110664,
      "grad_norm": 0.8544790148735046,
      "learning_rate": 0.00013346614347519873,
      "loss": 0.224,
      "step": 16537
    },
    {
      "epoch": 33.27565392354125,
      "grad_norm": 0.8579297065734863,
      "learning_rate": 0.00013346211892544522,
      "loss": 0.2052,
      "step": 16538
    },
    {
      "epoch": 33.27766599597585,
      "grad_norm": 0.8389206528663635,
      "learning_rate": 0.00013345809437569173,
      "loss": 0.196,
      "step": 16539
    },
    {
      "epoch": 33.27967806841046,
      "grad_norm": 0.8615491986274719,
      "learning_rate": 0.00013345406982593822,
      "loss": 0.2146,
      "step": 16540
    },
    {
      "epoch": 33.28169014084507,
      "grad_norm": 0.9010640382766724,
      "learning_rate": 0.00013345004527618476,
      "loss": 0.2199,
      "step": 16541
    },
    {
      "epoch": 33.283702213279675,
      "grad_norm": 0.8566854596138,
      "learning_rate": 0.00013344602072643124,
      "loss": 0.1998,
      "step": 16542
    },
    {
      "epoch": 33.285714285714285,
      "grad_norm": 0.8980017304420471,
      "learning_rate": 0.00013344199617667775,
      "loss": 0.2113,
      "step": 16543
    },
    {
      "epoch": 33.287726358148895,
      "grad_norm": 0.8607750535011292,
      "learning_rate": 0.00013343797162692424,
      "loss": 0.2199,
      "step": 16544
    },
    {
      "epoch": 33.2897384305835,
      "grad_norm": 0.9653878211975098,
      "learning_rate": 0.00013343394707717075,
      "loss": 0.2139,
      "step": 16545
    },
    {
      "epoch": 33.29175050301811,
      "grad_norm": 0.8667147755622864,
      "learning_rate": 0.00013342992252741726,
      "loss": 0.1986,
      "step": 16546
    },
    {
      "epoch": 33.29376257545272,
      "grad_norm": 0.846642017364502,
      "learning_rate": 0.00013342589797766375,
      "loss": 0.2108,
      "step": 16547
    },
    {
      "epoch": 33.29577464788732,
      "grad_norm": 0.8610600829124451,
      "learning_rate": 0.00013342187342791026,
      "loss": 0.2071,
      "step": 16548
    },
    {
      "epoch": 33.29778672032193,
      "grad_norm": 0.9585382342338562,
      "learning_rate": 0.00013341784887815675,
      "loss": 0.2337,
      "step": 16549
    },
    {
      "epoch": 33.29979879275654,
      "grad_norm": 0.8735119104385376,
      "learning_rate": 0.00013341382432840326,
      "loss": 0.218,
      "step": 16550
    },
    {
      "epoch": 33.30181086519114,
      "grad_norm": 0.8649448752403259,
      "learning_rate": 0.00013340979977864977,
      "loss": 0.2049,
      "step": 16551
    },
    {
      "epoch": 33.30382293762575,
      "grad_norm": 1.0323050022125244,
      "learning_rate": 0.00013340577522889628,
      "loss": 0.2279,
      "step": 16552
    },
    {
      "epoch": 33.30583501006036,
      "grad_norm": 0.9003932476043701,
      "learning_rate": 0.00013340175067914277,
      "loss": 0.2165,
      "step": 16553
    },
    {
      "epoch": 33.30784708249497,
      "grad_norm": 0.8741356730461121,
      "learning_rate": 0.00013339772612938928,
      "loss": 0.208,
      "step": 16554
    },
    {
      "epoch": 33.309859154929576,
      "grad_norm": 0.9115269780158997,
      "learning_rate": 0.00013339370157963576,
      "loss": 0.2171,
      "step": 16555
    },
    {
      "epoch": 33.311871227364186,
      "grad_norm": 0.8857072591781616,
      "learning_rate": 0.0001333896770298823,
      "loss": 0.2102,
      "step": 16556
    },
    {
      "epoch": 33.313883299798796,
      "grad_norm": 0.8755795955657959,
      "learning_rate": 0.0001333856524801288,
      "loss": 0.2168,
      "step": 16557
    },
    {
      "epoch": 33.3158953722334,
      "grad_norm": 0.8308460116386414,
      "learning_rate": 0.0001333816279303753,
      "loss": 0.2153,
      "step": 16558
    },
    {
      "epoch": 33.31790744466801,
      "grad_norm": 0.8813769221305847,
      "learning_rate": 0.0001333776033806218,
      "loss": 0.2225,
      "step": 16559
    },
    {
      "epoch": 33.31991951710262,
      "grad_norm": 0.8605495095252991,
      "learning_rate": 0.0001333735788308683,
      "loss": 0.1977,
      "step": 16560
    },
    {
      "epoch": 33.32193158953722,
      "grad_norm": 0.8542138934135437,
      "learning_rate": 0.0001333695542811148,
      "loss": 0.2112,
      "step": 16561
    },
    {
      "epoch": 33.32394366197183,
      "grad_norm": 0.8695781826972961,
      "learning_rate": 0.00013336552973136132,
      "loss": 0.2052,
      "step": 16562
    },
    {
      "epoch": 33.32595573440644,
      "grad_norm": 0.9608610272407532,
      "learning_rate": 0.0001333615051816078,
      "loss": 0.2136,
      "step": 16563
    },
    {
      "epoch": 33.327967806841045,
      "grad_norm": 0.9852889776229858,
      "learning_rate": 0.00013335748063185432,
      "loss": 0.2258,
      "step": 16564
    },
    {
      "epoch": 33.329979879275655,
      "grad_norm": 0.9017279744148254,
      "learning_rate": 0.0001333534560821008,
      "loss": 0.2121,
      "step": 16565
    },
    {
      "epoch": 33.331991951710265,
      "grad_norm": 0.8502106666564941,
      "learning_rate": 0.00013334943153234735,
      "loss": 0.1796,
      "step": 16566
    },
    {
      "epoch": 33.33400402414487,
      "grad_norm": 0.9137067794799805,
      "learning_rate": 0.00013334540698259383,
      "loss": 0.2203,
      "step": 16567
    },
    {
      "epoch": 33.33601609657948,
      "grad_norm": 0.89580899477005,
      "learning_rate": 0.00013334138243284034,
      "loss": 0.241,
      "step": 16568
    },
    {
      "epoch": 33.33802816901409,
      "grad_norm": 0.9162084460258484,
      "learning_rate": 0.00013333735788308683,
      "loss": 0.2205,
      "step": 16569
    },
    {
      "epoch": 33.34004024144869,
      "grad_norm": 0.9807171821594238,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.2221,
      "step": 16570
    },
    {
      "epoch": 33.3420523138833,
      "grad_norm": 0.9209048748016357,
      "learning_rate": 0.00013332930878357985,
      "loss": 0.2296,
      "step": 16571
    },
    {
      "epoch": 33.34406438631791,
      "grad_norm": 0.8758506774902344,
      "learning_rate": 0.00013332528423382636,
      "loss": 0.2148,
      "step": 16572
    },
    {
      "epoch": 33.34607645875251,
      "grad_norm": 0.9269891977310181,
      "learning_rate": 0.00013332125968407285,
      "loss": 0.2294,
      "step": 16573
    },
    {
      "epoch": 33.34808853118712,
      "grad_norm": 0.8648640513420105,
      "learning_rate": 0.00013331723513431936,
      "loss": 0.2082,
      "step": 16574
    },
    {
      "epoch": 33.35010060362173,
      "grad_norm": 0.8626517653465271,
      "learning_rate": 0.00013331321058456585,
      "loss": 0.1827,
      "step": 16575
    },
    {
      "epoch": 33.352112676056336,
      "grad_norm": 0.9908879995346069,
      "learning_rate": 0.00013330918603481236,
      "loss": 0.2187,
      "step": 16576
    },
    {
      "epoch": 33.354124748490946,
      "grad_norm": 0.970634937286377,
      "learning_rate": 0.00013330516148505887,
      "loss": 0.2255,
      "step": 16577
    },
    {
      "epoch": 33.356136820925556,
      "grad_norm": 0.901452898979187,
      "learning_rate": 0.00013330113693530538,
      "loss": 0.2081,
      "step": 16578
    },
    {
      "epoch": 33.35814889336016,
      "grad_norm": 0.8748210072517395,
      "learning_rate": 0.00013329711238555187,
      "loss": 0.1909,
      "step": 16579
    },
    {
      "epoch": 33.36016096579477,
      "grad_norm": 0.9041958451271057,
      "learning_rate": 0.00013329308783579838,
      "loss": 0.208,
      "step": 16580
    },
    {
      "epoch": 33.36217303822938,
      "grad_norm": 0.9365037083625793,
      "learning_rate": 0.0001332890632860449,
      "loss": 0.242,
      "step": 16581
    },
    {
      "epoch": 33.36418511066398,
      "grad_norm": 0.8851521611213684,
      "learning_rate": 0.00013328503873629138,
      "loss": 0.2091,
      "step": 16582
    },
    {
      "epoch": 33.36619718309859,
      "grad_norm": 0.8747254014015198,
      "learning_rate": 0.0001332810141865379,
      "loss": 0.2068,
      "step": 16583
    },
    {
      "epoch": 33.3682092555332,
      "grad_norm": 0.8699468970298767,
      "learning_rate": 0.00013327698963678438,
      "loss": 0.2156,
      "step": 16584
    },
    {
      "epoch": 33.370221327967805,
      "grad_norm": 0.9398938417434692,
      "learning_rate": 0.0001332729650870309,
      "loss": 0.2162,
      "step": 16585
    },
    {
      "epoch": 33.372233400402415,
      "grad_norm": 0.9329670667648315,
      "learning_rate": 0.0001332689405372774,
      "loss": 0.2034,
      "step": 16586
    },
    {
      "epoch": 33.374245472837025,
      "grad_norm": 0.9626985788345337,
      "learning_rate": 0.0001332649159875239,
      "loss": 0.2378,
      "step": 16587
    },
    {
      "epoch": 33.37625754527163,
      "grad_norm": 0.8698062896728516,
      "learning_rate": 0.0001332608914377704,
      "loss": 0.222,
      "step": 16588
    },
    {
      "epoch": 33.37826961770624,
      "grad_norm": 0.8275094628334045,
      "learning_rate": 0.0001332568668880169,
      "loss": 0.206,
      "step": 16589
    },
    {
      "epoch": 33.38028169014085,
      "grad_norm": 0.8638141751289368,
      "learning_rate": 0.0001332528423382634,
      "loss": 0.2056,
      "step": 16590
    },
    {
      "epoch": 33.38229376257545,
      "grad_norm": 0.8922492861747742,
      "learning_rate": 0.00013324881778850993,
      "loss": 0.2079,
      "step": 16591
    },
    {
      "epoch": 33.38430583501006,
      "grad_norm": 0.9220682382583618,
      "learning_rate": 0.00013324479323875642,
      "loss": 0.2326,
      "step": 16592
    },
    {
      "epoch": 33.38631790744467,
      "grad_norm": 0.8970378041267395,
      "learning_rate": 0.00013324076868900293,
      "loss": 0.207,
      "step": 16593
    },
    {
      "epoch": 33.38832997987927,
      "grad_norm": 0.9348203539848328,
      "learning_rate": 0.00013323674413924942,
      "loss": 0.2151,
      "step": 16594
    },
    {
      "epoch": 33.39034205231388,
      "grad_norm": 0.8878771662712097,
      "learning_rate": 0.00013323271958949593,
      "loss": 0.2092,
      "step": 16595
    },
    {
      "epoch": 33.39235412474849,
      "grad_norm": 0.9507925510406494,
      "learning_rate": 0.00013322869503974244,
      "loss": 0.2405,
      "step": 16596
    },
    {
      "epoch": 33.394366197183096,
      "grad_norm": 0.8742895722389221,
      "learning_rate": 0.00013322467048998895,
      "loss": 0.1992,
      "step": 16597
    },
    {
      "epoch": 33.396378269617706,
      "grad_norm": 0.9319334030151367,
      "learning_rate": 0.00013322064594023544,
      "loss": 0.1953,
      "step": 16598
    },
    {
      "epoch": 33.398390342052316,
      "grad_norm": 0.8694406747817993,
      "learning_rate": 0.00013321662139048195,
      "loss": 0.204,
      "step": 16599
    },
    {
      "epoch": 33.40040241448692,
      "grad_norm": 1.0062965154647827,
      "learning_rate": 0.00013321259684072844,
      "loss": 0.2601,
      "step": 16600
    },
    {
      "epoch": 33.40241448692153,
      "grad_norm": 0.9221378564834595,
      "learning_rate": 0.00013320857229097497,
      "loss": 0.2152,
      "step": 16601
    },
    {
      "epoch": 33.40442655935614,
      "grad_norm": 0.9517123103141785,
      "learning_rate": 0.00013320454774122146,
      "loss": 0.2121,
      "step": 16602
    },
    {
      "epoch": 33.40643863179074,
      "grad_norm": 0.9166377186775208,
      "learning_rate": 0.00013320052319146797,
      "loss": 0.2093,
      "step": 16603
    },
    {
      "epoch": 33.40845070422535,
      "grad_norm": 0.9005870819091797,
      "learning_rate": 0.00013319649864171446,
      "loss": 0.22,
      "step": 16604
    },
    {
      "epoch": 33.41046277665996,
      "grad_norm": 0.879307210445404,
      "learning_rate": 0.00013319247409196097,
      "loss": 0.2118,
      "step": 16605
    },
    {
      "epoch": 33.412474849094565,
      "grad_norm": 0.9454063773155212,
      "learning_rate": 0.00013318844954220748,
      "loss": 0.2287,
      "step": 16606
    },
    {
      "epoch": 33.414486921529175,
      "grad_norm": 0.8557078242301941,
      "learning_rate": 0.000133184424992454,
      "loss": 0.217,
      "step": 16607
    },
    {
      "epoch": 33.416498993963785,
      "grad_norm": 0.9033324122428894,
      "learning_rate": 0.00013318040044270048,
      "loss": 0.2238,
      "step": 16608
    },
    {
      "epoch": 33.41851106639839,
      "grad_norm": 0.9102693796157837,
      "learning_rate": 0.000133176375892947,
      "loss": 0.2155,
      "step": 16609
    },
    {
      "epoch": 33.420523138833,
      "grad_norm": 0.966143786907196,
      "learning_rate": 0.00013317235134319348,
      "loss": 0.2266,
      "step": 16610
    },
    {
      "epoch": 33.42253521126761,
      "grad_norm": 0.9837670922279358,
      "learning_rate": 0.00013316832679344,
      "loss": 0.2345,
      "step": 16611
    },
    {
      "epoch": 33.42454728370221,
      "grad_norm": 0.9886000156402588,
      "learning_rate": 0.0001331643022436865,
      "loss": 0.245,
      "step": 16612
    },
    {
      "epoch": 33.42655935613682,
      "grad_norm": 0.9317257404327393,
      "learning_rate": 0.000133160277693933,
      "loss": 0.2215,
      "step": 16613
    },
    {
      "epoch": 33.42857142857143,
      "grad_norm": 0.9455558061599731,
      "learning_rate": 0.0001331562531441795,
      "loss": 0.2376,
      "step": 16614
    },
    {
      "epoch": 33.43058350100603,
      "grad_norm": 0.9843431711196899,
      "learning_rate": 0.000133152228594426,
      "loss": 0.2381,
      "step": 16615
    },
    {
      "epoch": 33.43259557344064,
      "grad_norm": 0.8587945699691772,
      "learning_rate": 0.0001331482040446725,
      "loss": 0.184,
      "step": 16616
    },
    {
      "epoch": 33.43460764587525,
      "grad_norm": 0.9862670302391052,
      "learning_rate": 0.000133144179494919,
      "loss": 0.2239,
      "step": 16617
    },
    {
      "epoch": 33.436619718309856,
      "grad_norm": 0.9530860185623169,
      "learning_rate": 0.00013314015494516552,
      "loss": 0.1967,
      "step": 16618
    },
    {
      "epoch": 33.438631790744466,
      "grad_norm": 0.9631986021995544,
      "learning_rate": 0.000133136130395412,
      "loss": 0.2246,
      "step": 16619
    },
    {
      "epoch": 33.440643863179076,
      "grad_norm": 0.9117036461830139,
      "learning_rate": 0.00013313210584565852,
      "loss": 0.2124,
      "step": 16620
    },
    {
      "epoch": 33.44265593561368,
      "grad_norm": 0.9730744361877441,
      "learning_rate": 0.00013312808129590503,
      "loss": 0.2077,
      "step": 16621
    },
    {
      "epoch": 33.44466800804829,
      "grad_norm": 0.9302818179130554,
      "learning_rate": 0.00013312405674615154,
      "loss": 0.2097,
      "step": 16622
    },
    {
      "epoch": 33.4466800804829,
      "grad_norm": 0.9340733289718628,
      "learning_rate": 0.00013312003219639803,
      "loss": 0.2218,
      "step": 16623
    },
    {
      "epoch": 33.4486921529175,
      "grad_norm": 0.9461445212364197,
      "learning_rate": 0.00013311600764664454,
      "loss": 0.2329,
      "step": 16624
    },
    {
      "epoch": 33.45070422535211,
      "grad_norm": 0.8676621317863464,
      "learning_rate": 0.00013311198309689102,
      "loss": 0.2243,
      "step": 16625
    },
    {
      "epoch": 33.45271629778672,
      "grad_norm": 0.9241411685943604,
      "learning_rate": 0.00013310795854713754,
      "loss": 0.2321,
      "step": 16626
    },
    {
      "epoch": 33.454728370221325,
      "grad_norm": 0.9629576206207275,
      "learning_rate": 0.00013310393399738405,
      "loss": 0.235,
      "step": 16627
    },
    {
      "epoch": 33.456740442655935,
      "grad_norm": 0.8748312592506409,
      "learning_rate": 0.00013309990944763056,
      "loss": 0.2102,
      "step": 16628
    },
    {
      "epoch": 33.458752515090545,
      "grad_norm": 0.9846474528312683,
      "learning_rate": 0.00013309588489787705,
      "loss": 0.2211,
      "step": 16629
    },
    {
      "epoch": 33.46076458752515,
      "grad_norm": 0.9498851895332336,
      "learning_rate": 0.00013309186034812356,
      "loss": 0.2194,
      "step": 16630
    },
    {
      "epoch": 33.46277665995976,
      "grad_norm": 0.9200258255004883,
      "learning_rate": 0.00013308783579837004,
      "loss": 0.2129,
      "step": 16631
    },
    {
      "epoch": 33.46478873239437,
      "grad_norm": 0.9121514558792114,
      "learning_rate": 0.00013308381124861658,
      "loss": 0.2086,
      "step": 16632
    },
    {
      "epoch": 33.46680080482897,
      "grad_norm": 0.9680382013320923,
      "learning_rate": 0.00013307978669886307,
      "loss": 0.2216,
      "step": 16633
    },
    {
      "epoch": 33.46881287726358,
      "grad_norm": 0.8976304531097412,
      "learning_rate": 0.00013307576214910958,
      "loss": 0.2212,
      "step": 16634
    },
    {
      "epoch": 33.47082494969819,
      "grad_norm": 0.9133219122886658,
      "learning_rate": 0.00013307173759935606,
      "loss": 0.2255,
      "step": 16635
    },
    {
      "epoch": 33.47283702213279,
      "grad_norm": 0.9669681191444397,
      "learning_rate": 0.00013306771304960258,
      "loss": 0.2162,
      "step": 16636
    },
    {
      "epoch": 33.4748490945674,
      "grad_norm": 0.8612474799156189,
      "learning_rate": 0.0001330636884998491,
      "loss": 0.1913,
      "step": 16637
    },
    {
      "epoch": 33.47686116700201,
      "grad_norm": 0.9099434018135071,
      "learning_rate": 0.0001330596639500956,
      "loss": 0.22,
      "step": 16638
    },
    {
      "epoch": 33.478873239436616,
      "grad_norm": 0.8680287003517151,
      "learning_rate": 0.00013305563940034209,
      "loss": 0.2141,
      "step": 16639
    },
    {
      "epoch": 33.480885311871226,
      "grad_norm": 0.9593415260314941,
      "learning_rate": 0.0001330516148505886,
      "loss": 0.2369,
      "step": 16640
    },
    {
      "epoch": 33.482897384305836,
      "grad_norm": 0.9581537246704102,
      "learning_rate": 0.00013304759030083508,
      "loss": 0.2245,
      "step": 16641
    },
    {
      "epoch": 33.48490945674044,
      "grad_norm": 0.9426541924476624,
      "learning_rate": 0.00013304356575108162,
      "loss": 0.2078,
      "step": 16642
    },
    {
      "epoch": 33.48692152917505,
      "grad_norm": 0.9951554536819458,
      "learning_rate": 0.0001330395412013281,
      "loss": 0.2399,
      "step": 16643
    },
    {
      "epoch": 33.48893360160966,
      "grad_norm": 0.9124643802642822,
      "learning_rate": 0.00013303551665157462,
      "loss": 0.2151,
      "step": 16644
    },
    {
      "epoch": 33.49094567404426,
      "grad_norm": 0.9356587529182434,
      "learning_rate": 0.0001330314921018211,
      "loss": 0.223,
      "step": 16645
    },
    {
      "epoch": 33.49295774647887,
      "grad_norm": 0.9182233214378357,
      "learning_rate": 0.00013302746755206762,
      "loss": 0.2054,
      "step": 16646
    },
    {
      "epoch": 33.49496981891348,
      "grad_norm": 0.9378741979598999,
      "learning_rate": 0.00013302344300231413,
      "loss": 0.2469,
      "step": 16647
    },
    {
      "epoch": 33.496981891348085,
      "grad_norm": 0.9307813048362732,
      "learning_rate": 0.00013301941845256064,
      "loss": 0.2231,
      "step": 16648
    },
    {
      "epoch": 33.498993963782695,
      "grad_norm": 0.9366165399551392,
      "learning_rate": 0.00013301539390280713,
      "loss": 0.2308,
      "step": 16649
    },
    {
      "epoch": 33.501006036217305,
      "grad_norm": 0.9101365804672241,
      "learning_rate": 0.00013301136935305364,
      "loss": 0.1903,
      "step": 16650
    },
    {
      "epoch": 33.503018108651915,
      "grad_norm": 0.9423847794532776,
      "learning_rate": 0.00013300734480330012,
      "loss": 0.23,
      "step": 16651
    },
    {
      "epoch": 33.50503018108652,
      "grad_norm": 0.8820059895515442,
      "learning_rate": 0.00013300332025354664,
      "loss": 0.218,
      "step": 16652
    },
    {
      "epoch": 33.50704225352113,
      "grad_norm": 0.9317215085029602,
      "learning_rate": 0.00013299929570379315,
      "loss": 0.2502,
      "step": 16653
    },
    {
      "epoch": 33.50905432595574,
      "grad_norm": 0.8984970450401306,
      "learning_rate": 0.00013299527115403963,
      "loss": 0.2393,
      "step": 16654
    },
    {
      "epoch": 33.51106639839034,
      "grad_norm": 0.9075934290885925,
      "learning_rate": 0.00013299124660428615,
      "loss": 0.2154,
      "step": 16655
    },
    {
      "epoch": 33.51307847082495,
      "grad_norm": 0.9143767356872559,
      "learning_rate": 0.00013298722205453266,
      "loss": 0.246,
      "step": 16656
    },
    {
      "epoch": 33.51509054325956,
      "grad_norm": 0.9234647750854492,
      "learning_rate": 0.00013298319750477917,
      "loss": 0.2399,
      "step": 16657
    },
    {
      "epoch": 33.517102615694164,
      "grad_norm": 0.9468634128570557,
      "learning_rate": 0.00013297917295502566,
      "loss": 0.2087,
      "step": 16658
    },
    {
      "epoch": 33.519114688128774,
      "grad_norm": 0.9078788161277771,
      "learning_rate": 0.00013297514840527217,
      "loss": 0.2109,
      "step": 16659
    },
    {
      "epoch": 33.521126760563384,
      "grad_norm": 0.9313439130783081,
      "learning_rate": 0.00013297112385551865,
      "loss": 0.2233,
      "step": 16660
    },
    {
      "epoch": 33.52313883299799,
      "grad_norm": 0.9432468414306641,
      "learning_rate": 0.00013296709930576517,
      "loss": 0.2177,
      "step": 16661
    },
    {
      "epoch": 33.5251509054326,
      "grad_norm": 0.9265320301055908,
      "learning_rate": 0.00013296307475601168,
      "loss": 0.2217,
      "step": 16662
    },
    {
      "epoch": 33.52716297786721,
      "grad_norm": 0.9125291705131531,
      "learning_rate": 0.0001329590502062582,
      "loss": 0.2328,
      "step": 16663
    },
    {
      "epoch": 33.52917505030181,
      "grad_norm": 0.9167442917823792,
      "learning_rate": 0.00013295502565650467,
      "loss": 0.2102,
      "step": 16664
    },
    {
      "epoch": 33.53118712273642,
      "grad_norm": 0.8593303561210632,
      "learning_rate": 0.0001329510011067512,
      "loss": 0.189,
      "step": 16665
    },
    {
      "epoch": 33.53319919517103,
      "grad_norm": 0.996066153049469,
      "learning_rate": 0.00013294697655699767,
      "loss": 0.2229,
      "step": 16666
    },
    {
      "epoch": 33.53521126760563,
      "grad_norm": 0.8941502571105957,
      "learning_rate": 0.0001329429520072442,
      "loss": 0.2172,
      "step": 16667
    },
    {
      "epoch": 33.53722334004024,
      "grad_norm": 0.923189103603363,
      "learning_rate": 0.0001329389274574907,
      "loss": 0.2319,
      "step": 16668
    },
    {
      "epoch": 33.53923541247485,
      "grad_norm": 0.8770074248313904,
      "learning_rate": 0.0001329349029077372,
      "loss": 0.209,
      "step": 16669
    },
    {
      "epoch": 33.541247484909455,
      "grad_norm": 0.9287599921226501,
      "learning_rate": 0.0001329308783579837,
      "loss": 0.2142,
      "step": 16670
    },
    {
      "epoch": 33.543259557344065,
      "grad_norm": 0.9931045174598694,
      "learning_rate": 0.0001329268538082302,
      "loss": 0.2228,
      "step": 16671
    },
    {
      "epoch": 33.545271629778675,
      "grad_norm": 0.8783366680145264,
      "learning_rate": 0.00013292282925847672,
      "loss": 0.2205,
      "step": 16672
    },
    {
      "epoch": 33.54728370221328,
      "grad_norm": 0.9791635870933533,
      "learning_rate": 0.00013291880470872323,
      "loss": 0.2328,
      "step": 16673
    },
    {
      "epoch": 33.54929577464789,
      "grad_norm": 0.9428197145462036,
      "learning_rate": 0.00013291478015896972,
      "loss": 0.2193,
      "step": 16674
    },
    {
      "epoch": 33.5513078470825,
      "grad_norm": 0.9786261916160583,
      "learning_rate": 0.00013291075560921623,
      "loss": 0.2188,
      "step": 16675
    },
    {
      "epoch": 33.5533199195171,
      "grad_norm": 0.9473075866699219,
      "learning_rate": 0.0001329067310594627,
      "loss": 0.2222,
      "step": 16676
    },
    {
      "epoch": 33.55533199195171,
      "grad_norm": 0.9184849262237549,
      "learning_rate": 0.00013290270650970925,
      "loss": 0.2414,
      "step": 16677
    },
    {
      "epoch": 33.55734406438632,
      "grad_norm": 1.023874044418335,
      "learning_rate": 0.00013289868195995574,
      "loss": 0.2481,
      "step": 16678
    },
    {
      "epoch": 33.559356136820924,
      "grad_norm": 0.9741076827049255,
      "learning_rate": 0.00013289465741020225,
      "loss": 0.2331,
      "step": 16679
    },
    {
      "epoch": 33.561368209255534,
      "grad_norm": 0.9144705533981323,
      "learning_rate": 0.00013289063286044873,
      "loss": 0.2296,
      "step": 16680
    },
    {
      "epoch": 33.563380281690144,
      "grad_norm": 0.9373598694801331,
      "learning_rate": 0.00013288660831069525,
      "loss": 0.2162,
      "step": 16681
    },
    {
      "epoch": 33.56539235412475,
      "grad_norm": 0.9908466935157776,
      "learning_rate": 0.00013288258376094176,
      "loss": 0.2248,
      "step": 16682
    },
    {
      "epoch": 33.56740442655936,
      "grad_norm": 0.907472550868988,
      "learning_rate": 0.00013287855921118827,
      "loss": 0.2328,
      "step": 16683
    },
    {
      "epoch": 33.56941649899397,
      "grad_norm": 0.9632842540740967,
      "learning_rate": 0.00013287453466143476,
      "loss": 0.2262,
      "step": 16684
    },
    {
      "epoch": 33.57142857142857,
      "grad_norm": 0.8946851491928101,
      "learning_rate": 0.00013287051011168127,
      "loss": 0.2145,
      "step": 16685
    },
    {
      "epoch": 33.57344064386318,
      "grad_norm": 0.859937310218811,
      "learning_rate": 0.00013286648556192775,
      "loss": 0.2019,
      "step": 16686
    },
    {
      "epoch": 33.57545271629779,
      "grad_norm": 0.9302292466163635,
      "learning_rate": 0.00013286246101217427,
      "loss": 0.2278,
      "step": 16687
    },
    {
      "epoch": 33.57746478873239,
      "grad_norm": 0.9252919554710388,
      "learning_rate": 0.00013285843646242078,
      "loss": 0.2368,
      "step": 16688
    },
    {
      "epoch": 33.579476861167,
      "grad_norm": 0.9041001200675964,
      "learning_rate": 0.00013285441191266726,
      "loss": 0.2288,
      "step": 16689
    },
    {
      "epoch": 33.58148893360161,
      "grad_norm": 0.9119113087654114,
      "learning_rate": 0.00013285038736291378,
      "loss": 0.2304,
      "step": 16690
    },
    {
      "epoch": 33.583501006036215,
      "grad_norm": 0.9598339796066284,
      "learning_rate": 0.0001328463628131603,
      "loss": 0.2531,
      "step": 16691
    },
    {
      "epoch": 33.585513078470825,
      "grad_norm": 0.9555314779281616,
      "learning_rate": 0.0001328423382634068,
      "loss": 0.2353,
      "step": 16692
    },
    {
      "epoch": 33.587525150905435,
      "grad_norm": 0.884661853313446,
      "learning_rate": 0.00013283831371365329,
      "loss": 0.2137,
      "step": 16693
    },
    {
      "epoch": 33.58953722334004,
      "grad_norm": 0.9605499505996704,
      "learning_rate": 0.0001328342891638998,
      "loss": 0.2337,
      "step": 16694
    },
    {
      "epoch": 33.59154929577465,
      "grad_norm": 0.9189051985740662,
      "learning_rate": 0.00013283026461414628,
      "loss": 0.229,
      "step": 16695
    },
    {
      "epoch": 33.59356136820926,
      "grad_norm": 0.9417967796325684,
      "learning_rate": 0.0001328262400643928,
      "loss": 0.2279,
      "step": 16696
    },
    {
      "epoch": 33.59557344064386,
      "grad_norm": 0.9439882636070251,
      "learning_rate": 0.0001328222155146393,
      "loss": 0.2365,
      "step": 16697
    },
    {
      "epoch": 33.59758551307847,
      "grad_norm": 0.908696174621582,
      "learning_rate": 0.00013281819096488582,
      "loss": 0.2245,
      "step": 16698
    },
    {
      "epoch": 33.59959758551308,
      "grad_norm": 0.8887035846710205,
      "learning_rate": 0.0001328141664151323,
      "loss": 0.2127,
      "step": 16699
    },
    {
      "epoch": 33.601609657947684,
      "grad_norm": 0.9577518701553345,
      "learning_rate": 0.00013281014186537882,
      "loss": 0.236,
      "step": 16700
    },
    {
      "epoch": 33.603621730382294,
      "grad_norm": 0.9397512674331665,
      "learning_rate": 0.0001328061173156253,
      "loss": 0.242,
      "step": 16701
    },
    {
      "epoch": 33.605633802816904,
      "grad_norm": 0.9238530397415161,
      "learning_rate": 0.00013280209276587184,
      "loss": 0.2321,
      "step": 16702
    },
    {
      "epoch": 33.60764587525151,
      "grad_norm": 0.9357021450996399,
      "learning_rate": 0.00013279806821611833,
      "loss": 0.2212,
      "step": 16703
    },
    {
      "epoch": 33.60965794768612,
      "grad_norm": 0.9002459645271301,
      "learning_rate": 0.00013279404366636484,
      "loss": 0.233,
      "step": 16704
    },
    {
      "epoch": 33.61167002012073,
      "grad_norm": 0.9169738292694092,
      "learning_rate": 0.00013279001911661132,
      "loss": 0.2309,
      "step": 16705
    },
    {
      "epoch": 33.61368209255533,
      "grad_norm": 0.9184943437576294,
      "learning_rate": 0.00013278599456685784,
      "loss": 0.2169,
      "step": 16706
    },
    {
      "epoch": 33.61569416498994,
      "grad_norm": 0.8724359273910522,
      "learning_rate": 0.00013278197001710435,
      "loss": 0.2118,
      "step": 16707
    },
    {
      "epoch": 33.61770623742455,
      "grad_norm": 0.9061916470527649,
      "learning_rate": 0.00013277794546735086,
      "loss": 0.2256,
      "step": 16708
    },
    {
      "epoch": 33.61971830985915,
      "grad_norm": 0.953221321105957,
      "learning_rate": 0.00013277392091759735,
      "loss": 0.2375,
      "step": 16709
    },
    {
      "epoch": 33.62173038229376,
      "grad_norm": 0.988783597946167,
      "learning_rate": 0.00013276989636784386,
      "loss": 0.2438,
      "step": 16710
    },
    {
      "epoch": 33.62374245472837,
      "grad_norm": 0.8859258890151978,
      "learning_rate": 0.00013276587181809034,
      "loss": 0.2318,
      "step": 16711
    },
    {
      "epoch": 33.625754527162975,
      "grad_norm": 0.962598443031311,
      "learning_rate": 0.00013276184726833688,
      "loss": 0.2462,
      "step": 16712
    },
    {
      "epoch": 33.627766599597585,
      "grad_norm": 0.912602961063385,
      "learning_rate": 0.00013275782271858337,
      "loss": 0.2344,
      "step": 16713
    },
    {
      "epoch": 33.629778672032195,
      "grad_norm": 0.9189130663871765,
      "learning_rate": 0.00013275379816882988,
      "loss": 0.2235,
      "step": 16714
    },
    {
      "epoch": 33.6317907444668,
      "grad_norm": 0.8941552042961121,
      "learning_rate": 0.00013274977361907636,
      "loss": 0.224,
      "step": 16715
    },
    {
      "epoch": 33.63380281690141,
      "grad_norm": 0.909561812877655,
      "learning_rate": 0.00013274574906932288,
      "loss": 0.2096,
      "step": 16716
    },
    {
      "epoch": 33.63581488933602,
      "grad_norm": 0.9410206079483032,
      "learning_rate": 0.0001327417245195694,
      "loss": 0.2405,
      "step": 16717
    },
    {
      "epoch": 33.63782696177062,
      "grad_norm": 0.9754576683044434,
      "learning_rate": 0.00013273769996981587,
      "loss": 0.2282,
      "step": 16718
    },
    {
      "epoch": 33.63983903420523,
      "grad_norm": 0.9350359439849854,
      "learning_rate": 0.00013273367542006239,
      "loss": 0.2388,
      "step": 16719
    },
    {
      "epoch": 33.64185110663984,
      "grad_norm": 0.92425537109375,
      "learning_rate": 0.0001327296508703089,
      "loss": 0.2021,
      "step": 16720
    },
    {
      "epoch": 33.643863179074444,
      "grad_norm": 0.9473211765289307,
      "learning_rate": 0.00013272562632055538,
      "loss": 0.2338,
      "step": 16721
    },
    {
      "epoch": 33.645875251509054,
      "grad_norm": 0.8915117979049683,
      "learning_rate": 0.0001327216017708019,
      "loss": 0.2214,
      "step": 16722
    },
    {
      "epoch": 33.647887323943664,
      "grad_norm": 0.962648332118988,
      "learning_rate": 0.0001327175772210484,
      "loss": 0.2387,
      "step": 16723
    },
    {
      "epoch": 33.64989939637827,
      "grad_norm": 0.9408236742019653,
      "learning_rate": 0.0001327135526712949,
      "loss": 0.2338,
      "step": 16724
    },
    {
      "epoch": 33.65191146881288,
      "grad_norm": 0.8978135585784912,
      "learning_rate": 0.0001327095281215414,
      "loss": 0.2383,
      "step": 16725
    },
    {
      "epoch": 33.65392354124749,
      "grad_norm": 0.9321175813674927,
      "learning_rate": 0.0001327055035717879,
      "loss": 0.2278,
      "step": 16726
    },
    {
      "epoch": 33.65593561368209,
      "grad_norm": 0.8800516724586487,
      "learning_rate": 0.00013270147902203443,
      "loss": 0.239,
      "step": 16727
    },
    {
      "epoch": 33.6579476861167,
      "grad_norm": 0.9185343384742737,
      "learning_rate": 0.00013269745447228091,
      "loss": 0.2394,
      "step": 16728
    },
    {
      "epoch": 33.65995975855131,
      "grad_norm": 0.9351233243942261,
      "learning_rate": 0.00013269342992252743,
      "loss": 0.2074,
      "step": 16729
    },
    {
      "epoch": 33.66197183098591,
      "grad_norm": 0.8936792016029358,
      "learning_rate": 0.0001326894053727739,
      "loss": 0.2113,
      "step": 16730
    },
    {
      "epoch": 33.66398390342052,
      "grad_norm": 0.9442094564437866,
      "learning_rate": 0.00013268538082302042,
      "loss": 0.2325,
      "step": 16731
    },
    {
      "epoch": 33.66599597585513,
      "grad_norm": 0.9463943839073181,
      "learning_rate": 0.00013268135627326694,
      "loss": 0.2474,
      "step": 16732
    },
    {
      "epoch": 33.668008048289735,
      "grad_norm": 0.8994925618171692,
      "learning_rate": 0.00013267733172351345,
      "loss": 0.2146,
      "step": 16733
    },
    {
      "epoch": 33.670020120724345,
      "grad_norm": 0.8991875648498535,
      "learning_rate": 0.00013267330717375993,
      "loss": 0.2208,
      "step": 16734
    },
    {
      "epoch": 33.672032193158955,
      "grad_norm": 0.9002603888511658,
      "learning_rate": 0.00013266928262400645,
      "loss": 0.2363,
      "step": 16735
    },
    {
      "epoch": 33.67404426559356,
      "grad_norm": 0.8940801620483398,
      "learning_rate": 0.00013266525807425293,
      "loss": 0.2286,
      "step": 16736
    },
    {
      "epoch": 33.67605633802817,
      "grad_norm": 0.8876305818557739,
      "learning_rate": 0.00013266123352449947,
      "loss": 0.2221,
      "step": 16737
    },
    {
      "epoch": 33.67806841046278,
      "grad_norm": 0.8886632919311523,
      "learning_rate": 0.00013265720897474596,
      "loss": 0.2475,
      "step": 16738
    },
    {
      "epoch": 33.68008048289738,
      "grad_norm": 0.8905847668647766,
      "learning_rate": 0.00013265318442499247,
      "loss": 0.2363,
      "step": 16739
    },
    {
      "epoch": 33.68209255533199,
      "grad_norm": 0.9393068552017212,
      "learning_rate": 0.00013264915987523895,
      "loss": 0.2307,
      "step": 16740
    },
    {
      "epoch": 33.6841046277666,
      "grad_norm": 0.9598907828330994,
      "learning_rate": 0.00013264513532548547,
      "loss": 0.2406,
      "step": 16741
    },
    {
      "epoch": 33.686116700201204,
      "grad_norm": 0.9490541815757751,
      "learning_rate": 0.00013264111077573198,
      "loss": 0.2194,
      "step": 16742
    },
    {
      "epoch": 33.688128772635814,
      "grad_norm": 0.9444137811660767,
      "learning_rate": 0.0001326370862259785,
      "loss": 0.25,
      "step": 16743
    },
    {
      "epoch": 33.690140845070424,
      "grad_norm": 0.9176086783409119,
      "learning_rate": 0.00013263306167622497,
      "loss": 0.2297,
      "step": 16744
    },
    {
      "epoch": 33.69215291750503,
      "grad_norm": 0.9377778768539429,
      "learning_rate": 0.0001326290371264715,
      "loss": 0.2241,
      "step": 16745
    },
    {
      "epoch": 33.69416498993964,
      "grad_norm": 0.903574526309967,
      "learning_rate": 0.00013262501257671797,
      "loss": 0.2094,
      "step": 16746
    },
    {
      "epoch": 33.69617706237425,
      "grad_norm": 0.9026874899864197,
      "learning_rate": 0.0001326209880269645,
      "loss": 0.2339,
      "step": 16747
    },
    {
      "epoch": 33.69818913480886,
      "grad_norm": 0.9416581392288208,
      "learning_rate": 0.000132616963477211,
      "loss": 0.2437,
      "step": 16748
    },
    {
      "epoch": 33.70020120724346,
      "grad_norm": 0.9349195957183838,
      "learning_rate": 0.0001326129389274575,
      "loss": 0.2181,
      "step": 16749
    },
    {
      "epoch": 33.70221327967807,
      "grad_norm": 0.9709266424179077,
      "learning_rate": 0.000132608914377704,
      "loss": 0.2205,
      "step": 16750
    },
    {
      "epoch": 33.70422535211267,
      "grad_norm": 0.9708569049835205,
      "learning_rate": 0.0001326048898279505,
      "loss": 0.2369,
      "step": 16751
    },
    {
      "epoch": 33.70623742454728,
      "grad_norm": 0.9037628769874573,
      "learning_rate": 0.00013260086527819702,
      "loss": 0.2218,
      "step": 16752
    },
    {
      "epoch": 33.70824949698189,
      "grad_norm": 0.927652895450592,
      "learning_rate": 0.0001325968407284435,
      "loss": 0.234,
      "step": 16753
    },
    {
      "epoch": 33.7102615694165,
      "grad_norm": 0.9207442402839661,
      "learning_rate": 0.00013259281617869002,
      "loss": 0.2324,
      "step": 16754
    },
    {
      "epoch": 33.712273641851105,
      "grad_norm": 0.9416016936302185,
      "learning_rate": 0.00013258879162893653,
      "loss": 0.2462,
      "step": 16755
    },
    {
      "epoch": 33.714285714285715,
      "grad_norm": 0.9028497934341431,
      "learning_rate": 0.000132584767079183,
      "loss": 0.2202,
      "step": 16756
    },
    {
      "epoch": 33.716297786720325,
      "grad_norm": 0.9102214574813843,
      "learning_rate": 0.00013258074252942953,
      "loss": 0.2183,
      "step": 16757
    },
    {
      "epoch": 33.71830985915493,
      "grad_norm": 0.8740469217300415,
      "learning_rate": 0.00013257671797967604,
      "loss": 0.2258,
      "step": 16758
    },
    {
      "epoch": 33.72032193158954,
      "grad_norm": 1.0098756551742554,
      "learning_rate": 0.00013257269342992252,
      "loss": 0.247,
      "step": 16759
    },
    {
      "epoch": 33.72233400402415,
      "grad_norm": 0.8939957618713379,
      "learning_rate": 0.00013256866888016903,
      "loss": 0.2357,
      "step": 16760
    },
    {
      "epoch": 33.72434607645875,
      "grad_norm": 0.8917169570922852,
      "learning_rate": 0.00013256464433041552,
      "loss": 0.2225,
      "step": 16761
    },
    {
      "epoch": 33.72635814889336,
      "grad_norm": 0.8812328577041626,
      "learning_rate": 0.00013256061978066206,
      "loss": 0.2068,
      "step": 16762
    },
    {
      "epoch": 33.72837022132797,
      "grad_norm": 0.8590628504753113,
      "learning_rate": 0.00013255659523090854,
      "loss": 0.2205,
      "step": 16763
    },
    {
      "epoch": 33.730382293762574,
      "grad_norm": 0.9392284750938416,
      "learning_rate": 0.00013255257068115506,
      "loss": 0.2343,
      "step": 16764
    },
    {
      "epoch": 33.732394366197184,
      "grad_norm": 0.9623878598213196,
      "learning_rate": 0.00013254854613140154,
      "loss": 0.2471,
      "step": 16765
    },
    {
      "epoch": 33.734406438631794,
      "grad_norm": 0.9024651646614075,
      "learning_rate": 0.00013254452158164805,
      "loss": 0.2115,
      "step": 16766
    },
    {
      "epoch": 33.7364185110664,
      "grad_norm": 0.9314389824867249,
      "learning_rate": 0.00013254049703189457,
      "loss": 0.2339,
      "step": 16767
    },
    {
      "epoch": 33.73843058350101,
      "grad_norm": 1.0198390483856201,
      "learning_rate": 0.00013253647248214108,
      "loss": 0.2566,
      "step": 16768
    },
    {
      "epoch": 33.74044265593562,
      "grad_norm": 0.913664698600769,
      "learning_rate": 0.00013253244793238756,
      "loss": 0.222,
      "step": 16769
    },
    {
      "epoch": 33.74245472837022,
      "grad_norm": 0.9483703374862671,
      "learning_rate": 0.00013252842338263408,
      "loss": 0.2296,
      "step": 16770
    },
    {
      "epoch": 33.74446680080483,
      "grad_norm": 0.9096567630767822,
      "learning_rate": 0.00013252439883288056,
      "loss": 0.2292,
      "step": 16771
    },
    {
      "epoch": 33.74647887323944,
      "grad_norm": 0.9994786977767944,
      "learning_rate": 0.0001325203742831271,
      "loss": 0.2548,
      "step": 16772
    },
    {
      "epoch": 33.74849094567404,
      "grad_norm": 1.0151177644729614,
      "learning_rate": 0.00013251634973337359,
      "loss": 0.2513,
      "step": 16773
    },
    {
      "epoch": 33.75050301810865,
      "grad_norm": 0.9442866444587708,
      "learning_rate": 0.0001325123251836201,
      "loss": 0.2401,
      "step": 16774
    },
    {
      "epoch": 33.75251509054326,
      "grad_norm": 0.8781943917274475,
      "learning_rate": 0.00013250830063386658,
      "loss": 0.2277,
      "step": 16775
    },
    {
      "epoch": 33.754527162977865,
      "grad_norm": 0.9978465437889099,
      "learning_rate": 0.0001325042760841131,
      "loss": 0.251,
      "step": 16776
    },
    {
      "epoch": 33.756539235412475,
      "grad_norm": 1.0057185888290405,
      "learning_rate": 0.0001325002515343596,
      "loss": 0.2509,
      "step": 16777
    },
    {
      "epoch": 33.758551307847085,
      "grad_norm": 0.9178458452224731,
      "learning_rate": 0.00013249622698460612,
      "loss": 0.2358,
      "step": 16778
    },
    {
      "epoch": 33.76056338028169,
      "grad_norm": 1.0035055875778198,
      "learning_rate": 0.0001324922024348526,
      "loss": 0.255,
      "step": 16779
    },
    {
      "epoch": 33.7625754527163,
      "grad_norm": 0.8953616619110107,
      "learning_rate": 0.00013248817788509912,
      "loss": 0.2334,
      "step": 16780
    },
    {
      "epoch": 33.76458752515091,
      "grad_norm": 0.8912040591239929,
      "learning_rate": 0.0001324841533353456,
      "loss": 0.2311,
      "step": 16781
    },
    {
      "epoch": 33.76659959758551,
      "grad_norm": 0.8894933462142944,
      "learning_rate": 0.00013248012878559214,
      "loss": 0.2271,
      "step": 16782
    },
    {
      "epoch": 33.76861167002012,
      "grad_norm": 0.9511176347732544,
      "learning_rate": 0.00013247610423583863,
      "loss": 0.2483,
      "step": 16783
    },
    {
      "epoch": 33.77062374245473,
      "grad_norm": 0.9247732162475586,
      "learning_rate": 0.00013247207968608514,
      "loss": 0.2356,
      "step": 16784
    },
    {
      "epoch": 33.772635814889334,
      "grad_norm": 0.9287647008895874,
      "learning_rate": 0.00013246805513633162,
      "loss": 0.2436,
      "step": 16785
    },
    {
      "epoch": 33.774647887323944,
      "grad_norm": 0.9185590147972107,
      "learning_rate": 0.00013246403058657814,
      "loss": 0.2545,
      "step": 16786
    },
    {
      "epoch": 33.776659959758554,
      "grad_norm": 0.9177209138870239,
      "learning_rate": 0.00013246000603682465,
      "loss": 0.2343,
      "step": 16787
    },
    {
      "epoch": 33.77867203219316,
      "grad_norm": 0.9344502091407776,
      "learning_rate": 0.00013245598148707113,
      "loss": 0.2235,
      "step": 16788
    },
    {
      "epoch": 33.78068410462777,
      "grad_norm": 0.9526587128639221,
      "learning_rate": 0.00013245195693731764,
      "loss": 0.2456,
      "step": 16789
    },
    {
      "epoch": 33.78269617706238,
      "grad_norm": 0.9137445688247681,
      "learning_rate": 0.00013244793238756416,
      "loss": 0.2267,
      "step": 16790
    },
    {
      "epoch": 33.78470824949698,
      "grad_norm": 0.939255952835083,
      "learning_rate": 0.00013244390783781064,
      "loss": 0.2509,
      "step": 16791
    },
    {
      "epoch": 33.78672032193159,
      "grad_norm": 0.8940571546554565,
      "learning_rate": 0.00013243988328805715,
      "loss": 0.2389,
      "step": 16792
    },
    {
      "epoch": 33.7887323943662,
      "grad_norm": 0.9705114364624023,
      "learning_rate": 0.00013243585873830367,
      "loss": 0.2475,
      "step": 16793
    },
    {
      "epoch": 33.7907444668008,
      "grad_norm": 0.9762434363365173,
      "learning_rate": 0.00013243183418855015,
      "loss": 0.2586,
      "step": 16794
    },
    {
      "epoch": 33.79275653923541,
      "grad_norm": 1.007333755493164,
      "learning_rate": 0.00013242780963879666,
      "loss": 0.2405,
      "step": 16795
    },
    {
      "epoch": 33.79476861167002,
      "grad_norm": 0.9171215295791626,
      "learning_rate": 0.00013242378508904315,
      "loss": 0.2256,
      "step": 16796
    },
    {
      "epoch": 33.796780684104625,
      "grad_norm": 0.9568902850151062,
      "learning_rate": 0.0001324197605392897,
      "loss": 0.2308,
      "step": 16797
    },
    {
      "epoch": 33.798792756539235,
      "grad_norm": 0.987368106842041,
      "learning_rate": 0.00013241573598953617,
      "loss": 0.2389,
      "step": 16798
    },
    {
      "epoch": 33.800804828973845,
      "grad_norm": 0.9697045087814331,
      "learning_rate": 0.00013241171143978269,
      "loss": 0.2567,
      "step": 16799
    },
    {
      "epoch": 33.80281690140845,
      "grad_norm": 0.9745121598243713,
      "learning_rate": 0.00013240768689002917,
      "loss": 0.261,
      "step": 16800
    },
    {
      "epoch": 33.80482897384306,
      "grad_norm": 0.9045814275741577,
      "learning_rate": 0.00013240366234027568,
      "loss": 0.2347,
      "step": 16801
    },
    {
      "epoch": 33.80684104627767,
      "grad_norm": 0.9003201127052307,
      "learning_rate": 0.0001323996377905222,
      "loss": 0.2275,
      "step": 16802
    },
    {
      "epoch": 33.80885311871227,
      "grad_norm": 0.9448980093002319,
      "learning_rate": 0.0001323956132407687,
      "loss": 0.2347,
      "step": 16803
    },
    {
      "epoch": 33.81086519114688,
      "grad_norm": 0.942500114440918,
      "learning_rate": 0.0001323915886910152,
      "loss": 0.2295,
      "step": 16804
    },
    {
      "epoch": 33.81287726358149,
      "grad_norm": 0.9127532839775085,
      "learning_rate": 0.0001323875641412617,
      "loss": 0.2426,
      "step": 16805
    },
    {
      "epoch": 33.814889336016094,
      "grad_norm": 0.9590402245521545,
      "learning_rate": 0.0001323835395915082,
      "loss": 0.2276,
      "step": 16806
    },
    {
      "epoch": 33.816901408450704,
      "grad_norm": 0.9576518535614014,
      "learning_rate": 0.00013237951504175473,
      "loss": 0.2528,
      "step": 16807
    },
    {
      "epoch": 33.818913480885314,
      "grad_norm": 1.018647313117981,
      "learning_rate": 0.00013237549049200121,
      "loss": 0.2293,
      "step": 16808
    },
    {
      "epoch": 33.82092555331992,
      "grad_norm": 1.0038479566574097,
      "learning_rate": 0.00013237146594224773,
      "loss": 0.2507,
      "step": 16809
    },
    {
      "epoch": 33.82293762575453,
      "grad_norm": 0.9003289937973022,
      "learning_rate": 0.0001323674413924942,
      "loss": 0.2212,
      "step": 16810
    },
    {
      "epoch": 33.82494969818914,
      "grad_norm": 0.9929971694946289,
      "learning_rate": 0.00013236341684274072,
      "loss": 0.2037,
      "step": 16811
    },
    {
      "epoch": 33.82696177062374,
      "grad_norm": 0.9869759678840637,
      "learning_rate": 0.00013235939229298724,
      "loss": 0.2261,
      "step": 16812
    },
    {
      "epoch": 33.82897384305835,
      "grad_norm": 1.0363454818725586,
      "learning_rate": 0.00013235536774323375,
      "loss": 0.2546,
      "step": 16813
    },
    {
      "epoch": 33.83098591549296,
      "grad_norm": 0.9570452570915222,
      "learning_rate": 0.00013235134319348023,
      "loss": 0.2476,
      "step": 16814
    },
    {
      "epoch": 33.83299798792756,
      "grad_norm": 0.9657256603240967,
      "learning_rate": 0.00013234731864372675,
      "loss": 0.2366,
      "step": 16815
    },
    {
      "epoch": 33.83501006036217,
      "grad_norm": 0.8980322480201721,
      "learning_rate": 0.00013234329409397323,
      "loss": 0.2067,
      "step": 16816
    },
    {
      "epoch": 33.83702213279678,
      "grad_norm": 1.0278364419937134,
      "learning_rate": 0.00013233926954421977,
      "loss": 0.2363,
      "step": 16817
    },
    {
      "epoch": 33.839034205231385,
      "grad_norm": 0.9885185956954956,
      "learning_rate": 0.00013233524499446626,
      "loss": 0.2554,
      "step": 16818
    },
    {
      "epoch": 33.841046277665995,
      "grad_norm": 0.9664645195007324,
      "learning_rate": 0.00013233122044471277,
      "loss": 0.2452,
      "step": 16819
    },
    {
      "epoch": 33.843058350100605,
      "grad_norm": 0.908577024936676,
      "learning_rate": 0.00013232719589495925,
      "loss": 0.2513,
      "step": 16820
    },
    {
      "epoch": 33.84507042253521,
      "grad_norm": 0.9630692005157471,
      "learning_rate": 0.00013232317134520576,
      "loss": 0.2399,
      "step": 16821
    },
    {
      "epoch": 33.84708249496982,
      "grad_norm": 0.9244903922080994,
      "learning_rate": 0.00013231914679545228,
      "loss": 0.2361,
      "step": 16822
    },
    {
      "epoch": 33.84909456740443,
      "grad_norm": 0.90897136926651,
      "learning_rate": 0.00013231512224569876,
      "loss": 0.2323,
      "step": 16823
    },
    {
      "epoch": 33.85110663983903,
      "grad_norm": 0.9690884351730347,
      "learning_rate": 0.00013231109769594527,
      "loss": 0.2552,
      "step": 16824
    },
    {
      "epoch": 33.85311871227364,
      "grad_norm": 0.9664562344551086,
      "learning_rate": 0.0001323070731461918,
      "loss": 0.2567,
      "step": 16825
    },
    {
      "epoch": 33.85513078470825,
      "grad_norm": 0.9675042033195496,
      "learning_rate": 0.00013230304859643827,
      "loss": 0.2448,
      "step": 16826
    },
    {
      "epoch": 33.857142857142854,
      "grad_norm": 0.9624409675598145,
      "learning_rate": 0.00013229902404668478,
      "loss": 0.2435,
      "step": 16827
    },
    {
      "epoch": 33.859154929577464,
      "grad_norm": 0.9760599136352539,
      "learning_rate": 0.0001322949994969313,
      "loss": 0.2452,
      "step": 16828
    },
    {
      "epoch": 33.861167002012074,
      "grad_norm": 0.9130510687828064,
      "learning_rate": 0.00013229097494717778,
      "loss": 0.252,
      "step": 16829
    },
    {
      "epoch": 33.86317907444668,
      "grad_norm": 0.9022619724273682,
      "learning_rate": 0.0001322869503974243,
      "loss": 0.2216,
      "step": 16830
    },
    {
      "epoch": 33.86519114688129,
      "grad_norm": 0.8909265995025635,
      "learning_rate": 0.00013228292584767078,
      "loss": 0.2362,
      "step": 16831
    },
    {
      "epoch": 33.8672032193159,
      "grad_norm": 0.9652190804481506,
      "learning_rate": 0.00013227890129791732,
      "loss": 0.2383,
      "step": 16832
    },
    {
      "epoch": 33.8692152917505,
      "grad_norm": 0.978921115398407,
      "learning_rate": 0.0001322748767481638,
      "loss": 0.2436,
      "step": 16833
    },
    {
      "epoch": 33.87122736418511,
      "grad_norm": 0.9502670168876648,
      "learning_rate": 0.00013227085219841032,
      "loss": 0.2525,
      "step": 16834
    },
    {
      "epoch": 33.87323943661972,
      "grad_norm": 0.9508926272392273,
      "learning_rate": 0.0001322668276486568,
      "loss": 0.2403,
      "step": 16835
    },
    {
      "epoch": 33.87525150905432,
      "grad_norm": 0.9437682628631592,
      "learning_rate": 0.0001322628030989033,
      "loss": 0.2475,
      "step": 16836
    },
    {
      "epoch": 33.87726358148893,
      "grad_norm": 0.9100550413131714,
      "learning_rate": 0.00013225877854914982,
      "loss": 0.2301,
      "step": 16837
    },
    {
      "epoch": 33.87927565392354,
      "grad_norm": 0.9327102303504944,
      "learning_rate": 0.00013225475399939634,
      "loss": 0.2414,
      "step": 16838
    },
    {
      "epoch": 33.881287726358146,
      "grad_norm": 0.934312641620636,
      "learning_rate": 0.00013225072944964282,
      "loss": 0.237,
      "step": 16839
    },
    {
      "epoch": 33.883299798792756,
      "grad_norm": 0.8866591453552246,
      "learning_rate": 0.00013224670489988933,
      "loss": 0.2264,
      "step": 16840
    },
    {
      "epoch": 33.885311871227366,
      "grad_norm": 0.9409970641136169,
      "learning_rate": 0.00013224268035013582,
      "loss": 0.2364,
      "step": 16841
    },
    {
      "epoch": 33.88732394366197,
      "grad_norm": 0.8837622404098511,
      "learning_rate": 0.00013223865580038236,
      "loss": 0.2418,
      "step": 16842
    },
    {
      "epoch": 33.88933601609658,
      "grad_norm": 0.9878008365631104,
      "learning_rate": 0.00013223463125062884,
      "loss": 0.2379,
      "step": 16843
    },
    {
      "epoch": 33.89134808853119,
      "grad_norm": 0.908286988735199,
      "learning_rate": 0.00013223060670087536,
      "loss": 0.2383,
      "step": 16844
    },
    {
      "epoch": 33.89336016096579,
      "grad_norm": 0.9670401215553284,
      "learning_rate": 0.00013222658215112184,
      "loss": 0.2436,
      "step": 16845
    },
    {
      "epoch": 33.8953722334004,
      "grad_norm": 0.9395779967308044,
      "learning_rate": 0.00013222255760136835,
      "loss": 0.2471,
      "step": 16846
    },
    {
      "epoch": 33.89738430583501,
      "grad_norm": 0.9959339499473572,
      "learning_rate": 0.00013221853305161487,
      "loss": 0.271,
      "step": 16847
    },
    {
      "epoch": 33.899396378269614,
      "grad_norm": 0.9720542430877686,
      "learning_rate": 0.00013221450850186138,
      "loss": 0.264,
      "step": 16848
    },
    {
      "epoch": 33.901408450704224,
      "grad_norm": 0.9617632627487183,
      "learning_rate": 0.00013221048395210786,
      "loss": 0.237,
      "step": 16849
    },
    {
      "epoch": 33.903420523138834,
      "grad_norm": 0.9512280821800232,
      "learning_rate": 0.00013220645940235438,
      "loss": 0.2597,
      "step": 16850
    },
    {
      "epoch": 33.905432595573444,
      "grad_norm": 0.9886595010757446,
      "learning_rate": 0.00013220243485260086,
      "loss": 0.2328,
      "step": 16851
    },
    {
      "epoch": 33.90744466800805,
      "grad_norm": 0.9382359385490417,
      "learning_rate": 0.0001321984103028474,
      "loss": 0.2488,
      "step": 16852
    },
    {
      "epoch": 33.90945674044266,
      "grad_norm": 0.9243914484977722,
      "learning_rate": 0.00013219438575309388,
      "loss": 0.2201,
      "step": 16853
    },
    {
      "epoch": 33.91146881287727,
      "grad_norm": 1.003511905670166,
      "learning_rate": 0.0001321903612033404,
      "loss": 0.2657,
      "step": 16854
    },
    {
      "epoch": 33.91348088531187,
      "grad_norm": 1.0023928880691528,
      "learning_rate": 0.00013218633665358688,
      "loss": 0.2457,
      "step": 16855
    },
    {
      "epoch": 33.91549295774648,
      "grad_norm": 0.9930763840675354,
      "learning_rate": 0.0001321823121038334,
      "loss": 0.2383,
      "step": 16856
    },
    {
      "epoch": 33.91750503018109,
      "grad_norm": 0.9322211146354675,
      "learning_rate": 0.00013217828755407988,
      "loss": 0.2369,
      "step": 16857
    },
    {
      "epoch": 33.91951710261569,
      "grad_norm": 0.9631047248840332,
      "learning_rate": 0.0001321742630043264,
      "loss": 0.2461,
      "step": 16858
    },
    {
      "epoch": 33.9215291750503,
      "grad_norm": 0.9570584893226624,
      "learning_rate": 0.0001321702384545729,
      "loss": 0.2313,
      "step": 16859
    },
    {
      "epoch": 33.92354124748491,
      "grad_norm": 0.9477144479751587,
      "learning_rate": 0.00013216621390481942,
      "loss": 0.2267,
      "step": 16860
    },
    {
      "epoch": 33.925553319919516,
      "grad_norm": 0.9557381868362427,
      "learning_rate": 0.0001321621893550659,
      "loss": 0.2618,
      "step": 16861
    },
    {
      "epoch": 33.927565392354126,
      "grad_norm": 0.968248724937439,
      "learning_rate": 0.0001321581648053124,
      "loss": 0.241,
      "step": 16862
    },
    {
      "epoch": 33.929577464788736,
      "grad_norm": 0.888460099697113,
      "learning_rate": 0.00013215414025555893,
      "loss": 0.2345,
      "step": 16863
    },
    {
      "epoch": 33.93158953722334,
      "grad_norm": 0.9522536993026733,
      "learning_rate": 0.0001321501157058054,
      "loss": 0.2196,
      "step": 16864
    },
    {
      "epoch": 33.93360160965795,
      "grad_norm": 0.9271339774131775,
      "learning_rate": 0.00013214609115605192,
      "loss": 0.2293,
      "step": 16865
    },
    {
      "epoch": 33.93561368209256,
      "grad_norm": 0.9575521349906921,
      "learning_rate": 0.0001321420666062984,
      "loss": 0.2307,
      "step": 16866
    },
    {
      "epoch": 33.93762575452716,
      "grad_norm": 0.9457184672355652,
      "learning_rate": 0.00013213804205654492,
      "loss": 0.2405,
      "step": 16867
    },
    {
      "epoch": 33.93963782696177,
      "grad_norm": 0.9651877284049988,
      "learning_rate": 0.00013213401750679143,
      "loss": 0.2379,
      "step": 16868
    },
    {
      "epoch": 33.94164989939638,
      "grad_norm": 0.9269317984580994,
      "learning_rate": 0.00013212999295703794,
      "loss": 0.2492,
      "step": 16869
    },
    {
      "epoch": 33.943661971830984,
      "grad_norm": 0.9246951341629028,
      "learning_rate": 0.00013212596840728443,
      "loss": 0.224,
      "step": 16870
    },
    {
      "epoch": 33.945674044265594,
      "grad_norm": 0.9642372727394104,
      "learning_rate": 0.00013212194385753094,
      "loss": 0.2606,
      "step": 16871
    },
    {
      "epoch": 33.947686116700204,
      "grad_norm": 0.9026976823806763,
      "learning_rate": 0.00013211791930777743,
      "loss": 0.2276,
      "step": 16872
    },
    {
      "epoch": 33.94969818913481,
      "grad_norm": 0.9354981780052185,
      "learning_rate": 0.00013211389475802397,
      "loss": 0.2556,
      "step": 16873
    },
    {
      "epoch": 33.95171026156942,
      "grad_norm": 0.9423753023147583,
      "learning_rate": 0.00013210987020827045,
      "loss": 0.2537,
      "step": 16874
    },
    {
      "epoch": 33.95372233400403,
      "grad_norm": 0.9200581312179565,
      "learning_rate": 0.00013210584565851696,
      "loss": 0.2523,
      "step": 16875
    },
    {
      "epoch": 33.95573440643863,
      "grad_norm": 0.916174590587616,
      "learning_rate": 0.00013210182110876345,
      "loss": 0.2246,
      "step": 16876
    },
    {
      "epoch": 33.95774647887324,
      "grad_norm": 0.9398038387298584,
      "learning_rate": 0.00013209779655900996,
      "loss": 0.2333,
      "step": 16877
    },
    {
      "epoch": 33.95975855130785,
      "grad_norm": 0.9532273411750793,
      "learning_rate": 0.00013209377200925647,
      "loss": 0.2397,
      "step": 16878
    },
    {
      "epoch": 33.96177062374245,
      "grad_norm": 0.9324148893356323,
      "learning_rate": 0.00013208974745950299,
      "loss": 0.2494,
      "step": 16879
    },
    {
      "epoch": 33.96378269617706,
      "grad_norm": 0.9807098507881165,
      "learning_rate": 0.00013208572290974947,
      "loss": 0.2663,
      "step": 16880
    },
    {
      "epoch": 33.96579476861167,
      "grad_norm": 0.9785122871398926,
      "learning_rate": 0.00013208169835999598,
      "loss": 0.2556,
      "step": 16881
    },
    {
      "epoch": 33.967806841046276,
      "grad_norm": 0.9663333892822266,
      "learning_rate": 0.00013207767381024247,
      "loss": 0.251,
      "step": 16882
    },
    {
      "epoch": 33.969818913480886,
      "grad_norm": 0.8996056318283081,
      "learning_rate": 0.000132073649260489,
      "loss": 0.2267,
      "step": 16883
    },
    {
      "epoch": 33.971830985915496,
      "grad_norm": 0.9323552846908569,
      "learning_rate": 0.0001320696247107355,
      "loss": 0.2428,
      "step": 16884
    },
    {
      "epoch": 33.9738430583501,
      "grad_norm": 0.946432888507843,
      "learning_rate": 0.000132065600160982,
      "loss": 0.2415,
      "step": 16885
    },
    {
      "epoch": 33.97585513078471,
      "grad_norm": 0.9130482077598572,
      "learning_rate": 0.0001320615756112285,
      "loss": 0.2401,
      "step": 16886
    },
    {
      "epoch": 33.97786720321932,
      "grad_norm": 0.9704205393791199,
      "learning_rate": 0.000132057551061475,
      "loss": 0.2626,
      "step": 16887
    },
    {
      "epoch": 33.97987927565392,
      "grad_norm": 0.9240385890007019,
      "learning_rate": 0.00013205352651172151,
      "loss": 0.2656,
      "step": 16888
    },
    {
      "epoch": 33.98189134808853,
      "grad_norm": 0.8809676766395569,
      "learning_rate": 0.00013204950196196803,
      "loss": 0.2451,
      "step": 16889
    },
    {
      "epoch": 33.98390342052314,
      "grad_norm": 0.9459642767906189,
      "learning_rate": 0.0001320454774122145,
      "loss": 0.242,
      "step": 16890
    },
    {
      "epoch": 33.985915492957744,
      "grad_norm": 0.976043164730072,
      "learning_rate": 0.00013204145286246102,
      "loss": 0.2668,
      "step": 16891
    },
    {
      "epoch": 33.987927565392354,
      "grad_norm": 0.907644510269165,
      "learning_rate": 0.0001320374283127075,
      "loss": 0.243,
      "step": 16892
    },
    {
      "epoch": 33.989939637826964,
      "grad_norm": 0.9060919880867004,
      "learning_rate": 0.00013203340376295402,
      "loss": 0.2233,
      "step": 16893
    },
    {
      "epoch": 33.99195171026157,
      "grad_norm": 1.0239452123641968,
      "learning_rate": 0.00013202937921320053,
      "loss": 0.2722,
      "step": 16894
    },
    {
      "epoch": 33.99396378269618,
      "grad_norm": 0.910362958908081,
      "learning_rate": 0.00013202535466344702,
      "loss": 0.2363,
      "step": 16895
    },
    {
      "epoch": 33.99597585513079,
      "grad_norm": 0.8774628639221191,
      "learning_rate": 0.00013202133011369353,
      "loss": 0.2436,
      "step": 16896
    },
    {
      "epoch": 33.99798792756539,
      "grad_norm": 0.9777346849441528,
      "learning_rate": 0.00013201730556394004,
      "loss": 0.2629,
      "step": 16897
    },
    {
      "epoch": 34.0,
      "grad_norm": 0.9546379446983337,
      "learning_rate": 0.00013201328101418656,
      "loss": 0.2391,
      "step": 16898
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.4248327016830444,
      "eval_runtime": 49.8303,
      "eval_samples_per_second": 19.908,
      "eval_steps_per_second": 2.488,
      "step": 16898
    },
    {
      "epoch": 34.00201207243461,
      "grad_norm": 0.8150174021720886,
      "learning_rate": 0.00013200925646443304,
      "loss": 0.2049,
      "step": 16899
    },
    {
      "epoch": 34.00402414486921,
      "grad_norm": 0.8159786462783813,
      "learning_rate": 0.00013200523191467955,
      "loss": 0.2014,
      "step": 16900
    },
    {
      "epoch": 34.00603621730382,
      "grad_norm": 0.7965523600578308,
      "learning_rate": 0.00013200120736492604,
      "loss": 0.2051,
      "step": 16901
    },
    {
      "epoch": 34.00804828973843,
      "grad_norm": 0.8017539381980896,
      "learning_rate": 0.00013199718281517255,
      "loss": 0.1611,
      "step": 16902
    },
    {
      "epoch": 34.010060362173036,
      "grad_norm": 0.9002642631530762,
      "learning_rate": 0.00013199315826541906,
      "loss": 0.1895,
      "step": 16903
    },
    {
      "epoch": 34.012072434607646,
      "grad_norm": 0.8841384053230286,
      "learning_rate": 0.00013198913371566557,
      "loss": 0.177,
      "step": 16904
    },
    {
      "epoch": 34.014084507042256,
      "grad_norm": 0.8885663151741028,
      "learning_rate": 0.00013198510916591206,
      "loss": 0.1801,
      "step": 16905
    },
    {
      "epoch": 34.01609657947686,
      "grad_norm": 0.8891869783401489,
      "learning_rate": 0.00013198108461615857,
      "loss": 0.1895,
      "step": 16906
    },
    {
      "epoch": 34.01810865191147,
      "grad_norm": 0.7570995092391968,
      "learning_rate": 0.00013197706006640506,
      "loss": 0.1711,
      "step": 16907
    },
    {
      "epoch": 34.02012072434608,
      "grad_norm": 0.8062084913253784,
      "learning_rate": 0.0001319730355166516,
      "loss": 0.1611,
      "step": 16908
    },
    {
      "epoch": 34.02213279678068,
      "grad_norm": 0.8255929946899414,
      "learning_rate": 0.00013196901096689808,
      "loss": 0.177,
      "step": 16909
    },
    {
      "epoch": 34.02414486921529,
      "grad_norm": 0.7972189784049988,
      "learning_rate": 0.0001319649864171446,
      "loss": 0.1796,
      "step": 16910
    },
    {
      "epoch": 34.0261569416499,
      "grad_norm": 0.7925030589103699,
      "learning_rate": 0.00013196096186739108,
      "loss": 0.179,
      "step": 16911
    },
    {
      "epoch": 34.028169014084504,
      "grad_norm": 0.8016856908798218,
      "learning_rate": 0.0001319569373176376,
      "loss": 0.1792,
      "step": 16912
    },
    {
      "epoch": 34.030181086519114,
      "grad_norm": 0.8787609338760376,
      "learning_rate": 0.0001319529127678841,
      "loss": 0.2011,
      "step": 16913
    },
    {
      "epoch": 34.032193158953724,
      "grad_norm": 0.8293638229370117,
      "learning_rate": 0.00013194888821813061,
      "loss": 0.1848,
      "step": 16914
    },
    {
      "epoch": 34.03420523138833,
      "grad_norm": 0.8578966856002808,
      "learning_rate": 0.0001319448636683771,
      "loss": 0.1978,
      "step": 16915
    },
    {
      "epoch": 34.03621730382294,
      "grad_norm": 0.8469765782356262,
      "learning_rate": 0.0001319408391186236,
      "loss": 0.1916,
      "step": 16916
    },
    {
      "epoch": 34.03822937625755,
      "grad_norm": 0.8905779123306274,
      "learning_rate": 0.0001319368145688701,
      "loss": 0.1862,
      "step": 16917
    },
    {
      "epoch": 34.04024144869215,
      "grad_norm": 0.8328304290771484,
      "learning_rate": 0.00013193279001911664,
      "loss": 0.1934,
      "step": 16918
    },
    {
      "epoch": 34.04225352112676,
      "grad_norm": 0.7759032845497131,
      "learning_rate": 0.00013192876546936312,
      "loss": 0.1811,
      "step": 16919
    },
    {
      "epoch": 34.04426559356137,
      "grad_norm": 0.8714687824249268,
      "learning_rate": 0.00013192474091960963,
      "loss": 0.1795,
      "step": 16920
    },
    {
      "epoch": 34.04627766599597,
      "grad_norm": 0.7519596815109253,
      "learning_rate": 0.00013192071636985612,
      "loss": 0.1695,
      "step": 16921
    },
    {
      "epoch": 34.04828973843058,
      "grad_norm": 0.7864744067192078,
      "learning_rate": 0.00013191669182010263,
      "loss": 0.1674,
      "step": 16922
    },
    {
      "epoch": 34.05030181086519,
      "grad_norm": 0.844680905342102,
      "learning_rate": 0.00013191266727034914,
      "loss": 0.1814,
      "step": 16923
    },
    {
      "epoch": 34.052313883299796,
      "grad_norm": 0.9127360582351685,
      "learning_rate": 0.00013190864272059566,
      "loss": 0.1988,
      "step": 16924
    },
    {
      "epoch": 34.054325955734406,
      "grad_norm": 0.9499627947807312,
      "learning_rate": 0.00013190461817084214,
      "loss": 0.196,
      "step": 16925
    },
    {
      "epoch": 34.056338028169016,
      "grad_norm": 0.8863034248352051,
      "learning_rate": 0.00013190059362108865,
      "loss": 0.1878,
      "step": 16926
    },
    {
      "epoch": 34.05835010060362,
      "grad_norm": 0.9027806520462036,
      "learning_rate": 0.00013189656907133514,
      "loss": 0.1949,
      "step": 16927
    },
    {
      "epoch": 34.06036217303823,
      "grad_norm": 0.8249184489250183,
      "learning_rate": 0.00013189254452158165,
      "loss": 0.1827,
      "step": 16928
    },
    {
      "epoch": 34.06237424547284,
      "grad_norm": 0.8428325057029724,
      "learning_rate": 0.00013188851997182816,
      "loss": 0.202,
      "step": 16929
    },
    {
      "epoch": 34.06438631790744,
      "grad_norm": 0.8134125471115112,
      "learning_rate": 0.00013188449542207465,
      "loss": 0.1761,
      "step": 16930
    },
    {
      "epoch": 34.06639839034205,
      "grad_norm": 0.8430523872375488,
      "learning_rate": 0.00013188047087232116,
      "loss": 0.2006,
      "step": 16931
    },
    {
      "epoch": 34.06841046277666,
      "grad_norm": 0.8363484740257263,
      "learning_rate": 0.00013187644632256767,
      "loss": 0.1813,
      "step": 16932
    },
    {
      "epoch": 34.070422535211264,
      "grad_norm": 0.8313673138618469,
      "learning_rate": 0.00013187242177281418,
      "loss": 0.1948,
      "step": 16933
    },
    {
      "epoch": 34.072434607645874,
      "grad_norm": 0.8006082773208618,
      "learning_rate": 0.00013186839722306067,
      "loss": 0.1766,
      "step": 16934
    },
    {
      "epoch": 34.074446680080484,
      "grad_norm": 0.8753563165664673,
      "learning_rate": 0.00013186437267330718,
      "loss": 0.1921,
      "step": 16935
    },
    {
      "epoch": 34.07645875251509,
      "grad_norm": 0.8545250296592712,
      "learning_rate": 0.00013186034812355367,
      "loss": 0.1917,
      "step": 16936
    },
    {
      "epoch": 34.0784708249497,
      "grad_norm": 0.9014337062835693,
      "learning_rate": 0.00013185632357380018,
      "loss": 0.2045,
      "step": 16937
    },
    {
      "epoch": 34.08048289738431,
      "grad_norm": 0.8519719243049622,
      "learning_rate": 0.0001318522990240467,
      "loss": 0.1753,
      "step": 16938
    },
    {
      "epoch": 34.08249496981891,
      "grad_norm": 0.8976294994354248,
      "learning_rate": 0.0001318482744742932,
      "loss": 0.2016,
      "step": 16939
    },
    {
      "epoch": 34.08450704225352,
      "grad_norm": 0.822361946105957,
      "learning_rate": 0.0001318442499245397,
      "loss": 0.17,
      "step": 16940
    },
    {
      "epoch": 34.08651911468813,
      "grad_norm": 0.839516282081604,
      "learning_rate": 0.0001318402253747862,
      "loss": 0.1931,
      "step": 16941
    },
    {
      "epoch": 34.08853118712273,
      "grad_norm": 0.8074731230735779,
      "learning_rate": 0.00013183620082503269,
      "loss": 0.186,
      "step": 16942
    },
    {
      "epoch": 34.09054325955734,
      "grad_norm": 0.8096157312393188,
      "learning_rate": 0.00013183217627527923,
      "loss": 0.1772,
      "step": 16943
    },
    {
      "epoch": 34.09255533199195,
      "grad_norm": 0.7988398671150208,
      "learning_rate": 0.0001318281517255257,
      "loss": 0.1738,
      "step": 16944
    },
    {
      "epoch": 34.094567404426556,
      "grad_norm": 0.883175253868103,
      "learning_rate": 0.00013182412717577222,
      "loss": 0.2096,
      "step": 16945
    },
    {
      "epoch": 34.096579476861166,
      "grad_norm": 0.8582265377044678,
      "learning_rate": 0.0001318201026260187,
      "loss": 0.1922,
      "step": 16946
    },
    {
      "epoch": 34.098591549295776,
      "grad_norm": 0.7897682189941406,
      "learning_rate": 0.00013181607807626522,
      "loss": 0.1768,
      "step": 16947
    },
    {
      "epoch": 34.100603621730386,
      "grad_norm": 0.7826524972915649,
      "learning_rate": 0.00013181205352651173,
      "loss": 0.173,
      "step": 16948
    },
    {
      "epoch": 34.10261569416499,
      "grad_norm": 0.8427131175994873,
      "learning_rate": 0.00013180802897675824,
      "loss": 0.2008,
      "step": 16949
    },
    {
      "epoch": 34.1046277665996,
      "grad_norm": 0.8274943828582764,
      "learning_rate": 0.00013180400442700473,
      "loss": 0.1728,
      "step": 16950
    },
    {
      "epoch": 34.10663983903421,
      "grad_norm": 0.8678311109542847,
      "learning_rate": 0.00013179997987725124,
      "loss": 0.1901,
      "step": 16951
    },
    {
      "epoch": 34.10865191146881,
      "grad_norm": 0.9038770198822021,
      "learning_rate": 0.00013179595532749773,
      "loss": 0.1882,
      "step": 16952
    },
    {
      "epoch": 34.11066398390342,
      "grad_norm": 0.851478636264801,
      "learning_rate": 0.00013179193077774427,
      "loss": 0.1807,
      "step": 16953
    },
    {
      "epoch": 34.11267605633803,
      "grad_norm": 0.8660817742347717,
      "learning_rate": 0.00013178790622799075,
      "loss": 0.1749,
      "step": 16954
    },
    {
      "epoch": 34.114688128772634,
      "grad_norm": 0.8864260315895081,
      "learning_rate": 0.00013178388167823726,
      "loss": 0.1807,
      "step": 16955
    },
    {
      "epoch": 34.116700201207244,
      "grad_norm": 0.8545200824737549,
      "learning_rate": 0.00013177985712848375,
      "loss": 0.2019,
      "step": 16956
    },
    {
      "epoch": 34.118712273641854,
      "grad_norm": 0.875880777835846,
      "learning_rate": 0.00013177583257873026,
      "loss": 0.2082,
      "step": 16957
    },
    {
      "epoch": 34.12072434607646,
      "grad_norm": 0.8405564427375793,
      "learning_rate": 0.00013177180802897677,
      "loss": 0.1808,
      "step": 16958
    },
    {
      "epoch": 34.12273641851107,
      "grad_norm": 0.8454819917678833,
      "learning_rate": 0.00013176778347922329,
      "loss": 0.2024,
      "step": 16959
    },
    {
      "epoch": 34.12474849094568,
      "grad_norm": 0.8167949914932251,
      "learning_rate": 0.00013176375892946977,
      "loss": 0.2004,
      "step": 16960
    },
    {
      "epoch": 34.12676056338028,
      "grad_norm": 0.8910313844680786,
      "learning_rate": 0.00013175973437971628,
      "loss": 0.1957,
      "step": 16961
    },
    {
      "epoch": 34.12877263581489,
      "grad_norm": 0.7955310344696045,
      "learning_rate": 0.00013175570982996277,
      "loss": 0.174,
      "step": 16962
    },
    {
      "epoch": 34.1307847082495,
      "grad_norm": 0.8377141356468201,
      "learning_rate": 0.00013175168528020928,
      "loss": 0.1829,
      "step": 16963
    },
    {
      "epoch": 34.1327967806841,
      "grad_norm": 0.8188235759735107,
      "learning_rate": 0.0001317476607304558,
      "loss": 0.1761,
      "step": 16964
    },
    {
      "epoch": 34.13480885311871,
      "grad_norm": 0.831641674041748,
      "learning_rate": 0.00013174363618070228,
      "loss": 0.1921,
      "step": 16965
    },
    {
      "epoch": 34.13682092555332,
      "grad_norm": 0.8832799792289734,
      "learning_rate": 0.0001317396116309488,
      "loss": 0.1853,
      "step": 16966
    },
    {
      "epoch": 34.138832997987926,
      "grad_norm": 0.9416859149932861,
      "learning_rate": 0.0001317355870811953,
      "loss": 0.2082,
      "step": 16967
    },
    {
      "epoch": 34.140845070422536,
      "grad_norm": 0.8721871376037598,
      "learning_rate": 0.00013173156253144181,
      "loss": 0.1904,
      "step": 16968
    },
    {
      "epoch": 34.142857142857146,
      "grad_norm": 0.8094935417175293,
      "learning_rate": 0.0001317275379816883,
      "loss": 0.1752,
      "step": 16969
    },
    {
      "epoch": 34.14486921529175,
      "grad_norm": 0.8364916443824768,
      "learning_rate": 0.0001317235134319348,
      "loss": 0.199,
      "step": 16970
    },
    {
      "epoch": 34.14688128772636,
      "grad_norm": 0.8199982643127441,
      "learning_rate": 0.0001317194888821813,
      "loss": 0.1866,
      "step": 16971
    },
    {
      "epoch": 34.14889336016097,
      "grad_norm": 0.8489288687705994,
      "learning_rate": 0.0001317154643324278,
      "loss": 0.1782,
      "step": 16972
    },
    {
      "epoch": 34.15090543259557,
      "grad_norm": 0.8176286220550537,
      "learning_rate": 0.00013171143978267432,
      "loss": 0.1879,
      "step": 16973
    },
    {
      "epoch": 34.15291750503018,
      "grad_norm": 0.8205382823944092,
      "learning_rate": 0.00013170741523292083,
      "loss": 0.1787,
      "step": 16974
    },
    {
      "epoch": 34.15492957746479,
      "grad_norm": 0.976838231086731,
      "learning_rate": 0.00013170339068316732,
      "loss": 0.2082,
      "step": 16975
    },
    {
      "epoch": 34.156941649899395,
      "grad_norm": 0.9348282814025879,
      "learning_rate": 0.00013169936613341383,
      "loss": 0.2188,
      "step": 16976
    },
    {
      "epoch": 34.158953722334005,
      "grad_norm": 0.9271909594535828,
      "learning_rate": 0.00013169534158366032,
      "loss": 0.1875,
      "step": 16977
    },
    {
      "epoch": 34.160965794768615,
      "grad_norm": 0.8561478853225708,
      "learning_rate": 0.00013169131703390685,
      "loss": 0.1982,
      "step": 16978
    },
    {
      "epoch": 34.16297786720322,
      "grad_norm": 0.8475444912910461,
      "learning_rate": 0.00013168729248415334,
      "loss": 0.1886,
      "step": 16979
    },
    {
      "epoch": 34.16498993963783,
      "grad_norm": 0.8535822629928589,
      "learning_rate": 0.00013168326793439985,
      "loss": 0.1913,
      "step": 16980
    },
    {
      "epoch": 34.16700201207244,
      "grad_norm": 0.8854425549507141,
      "learning_rate": 0.00013167924338464634,
      "loss": 0.1945,
      "step": 16981
    },
    {
      "epoch": 34.16901408450704,
      "grad_norm": 0.8228936195373535,
      "learning_rate": 0.00013167521883489285,
      "loss": 0.1933,
      "step": 16982
    },
    {
      "epoch": 34.17102615694165,
      "grad_norm": 0.8284633755683899,
      "learning_rate": 0.00013167119428513936,
      "loss": 0.1727,
      "step": 16983
    },
    {
      "epoch": 34.17303822937626,
      "grad_norm": 0.8711363673210144,
      "learning_rate": 0.00013166716973538587,
      "loss": 0.1857,
      "step": 16984
    },
    {
      "epoch": 34.17505030181086,
      "grad_norm": 0.9152740836143494,
      "learning_rate": 0.00013166314518563236,
      "loss": 0.188,
      "step": 16985
    },
    {
      "epoch": 34.17706237424547,
      "grad_norm": 0.9200325608253479,
      "learning_rate": 0.00013165912063587887,
      "loss": 0.1885,
      "step": 16986
    },
    {
      "epoch": 34.17907444668008,
      "grad_norm": 0.956809401512146,
      "learning_rate": 0.00013165509608612536,
      "loss": 0.2127,
      "step": 16987
    },
    {
      "epoch": 34.181086519114686,
      "grad_norm": 0.8880290389060974,
      "learning_rate": 0.0001316510715363719,
      "loss": 0.1879,
      "step": 16988
    },
    {
      "epoch": 34.183098591549296,
      "grad_norm": 0.8622184991836548,
      "learning_rate": 0.00013164704698661838,
      "loss": 0.1974,
      "step": 16989
    },
    {
      "epoch": 34.185110663983906,
      "grad_norm": 0.8725395798683167,
      "learning_rate": 0.0001316430224368649,
      "loss": 0.2088,
      "step": 16990
    },
    {
      "epoch": 34.18712273641851,
      "grad_norm": 0.9216707348823547,
      "learning_rate": 0.00013163899788711138,
      "loss": 0.1902,
      "step": 16991
    },
    {
      "epoch": 34.18913480885312,
      "grad_norm": 0.923696756362915,
      "learning_rate": 0.0001316349733373579,
      "loss": 0.1962,
      "step": 16992
    },
    {
      "epoch": 34.19114688128773,
      "grad_norm": 0.8645631670951843,
      "learning_rate": 0.0001316309487876044,
      "loss": 0.1977,
      "step": 16993
    },
    {
      "epoch": 34.19315895372233,
      "grad_norm": 0.8876776099205017,
      "learning_rate": 0.00013162692423785091,
      "loss": 0.192,
      "step": 16994
    },
    {
      "epoch": 34.19517102615694,
      "grad_norm": 0.9372663497924805,
      "learning_rate": 0.0001316228996880974,
      "loss": 0.2146,
      "step": 16995
    },
    {
      "epoch": 34.19718309859155,
      "grad_norm": 0.9136713147163391,
      "learning_rate": 0.0001316188751383439,
      "loss": 0.2008,
      "step": 16996
    },
    {
      "epoch": 34.199195171026155,
      "grad_norm": 0.865583062171936,
      "learning_rate": 0.0001316148505885904,
      "loss": 0.1909,
      "step": 16997
    },
    {
      "epoch": 34.201207243460765,
      "grad_norm": 0.8460516929626465,
      "learning_rate": 0.0001316108260388369,
      "loss": 0.1932,
      "step": 16998
    },
    {
      "epoch": 34.203219315895375,
      "grad_norm": 0.8621319532394409,
      "learning_rate": 0.00013160680148908342,
      "loss": 0.1865,
      "step": 16999
    },
    {
      "epoch": 34.20523138832998,
      "grad_norm": 0.8592448830604553,
      "learning_rate": 0.0001316027769393299,
      "loss": 0.1776,
      "step": 17000
    },
    {
      "epoch": 34.20724346076459,
      "grad_norm": 0.8967415690422058,
      "learning_rate": 0.00013159875238957642,
      "loss": 0.199,
      "step": 17001
    },
    {
      "epoch": 34.2092555331992,
      "grad_norm": 0.9498505592346191,
      "learning_rate": 0.00013159472783982293,
      "loss": 0.2068,
      "step": 17002
    },
    {
      "epoch": 34.2112676056338,
      "grad_norm": 0.8732732534408569,
      "learning_rate": 0.00013159070329006944,
      "loss": 0.1923,
      "step": 17003
    },
    {
      "epoch": 34.21327967806841,
      "grad_norm": 0.8737416863441467,
      "learning_rate": 0.00013158667874031593,
      "loss": 0.2045,
      "step": 17004
    },
    {
      "epoch": 34.21529175050302,
      "grad_norm": 0.9289398789405823,
      "learning_rate": 0.00013158265419056244,
      "loss": 0.2107,
      "step": 17005
    },
    {
      "epoch": 34.21730382293762,
      "grad_norm": 0.8993688821792603,
      "learning_rate": 0.00013157862964080893,
      "loss": 0.1933,
      "step": 17006
    },
    {
      "epoch": 34.21931589537223,
      "grad_norm": 0.8869116306304932,
      "learning_rate": 0.00013157460509105544,
      "loss": 0.1921,
      "step": 17007
    },
    {
      "epoch": 34.22132796780684,
      "grad_norm": 0.8274705410003662,
      "learning_rate": 0.00013157058054130195,
      "loss": 0.1967,
      "step": 17008
    },
    {
      "epoch": 34.223340040241446,
      "grad_norm": 0.8424408435821533,
      "learning_rate": 0.00013156655599154846,
      "loss": 0.1816,
      "step": 17009
    },
    {
      "epoch": 34.225352112676056,
      "grad_norm": 0.7845021486282349,
      "learning_rate": 0.00013156253144179495,
      "loss": 0.1606,
      "step": 17010
    },
    {
      "epoch": 34.227364185110666,
      "grad_norm": 0.8521990180015564,
      "learning_rate": 0.00013155850689204146,
      "loss": 0.1804,
      "step": 17011
    },
    {
      "epoch": 34.22937625754527,
      "grad_norm": 0.9080262780189514,
      "learning_rate": 0.00013155448234228794,
      "loss": 0.1999,
      "step": 17012
    },
    {
      "epoch": 34.23138832997988,
      "grad_norm": 0.9097810983657837,
      "learning_rate": 0.00013155045779253448,
      "loss": 0.2034,
      "step": 17013
    },
    {
      "epoch": 34.23340040241449,
      "grad_norm": 0.8494960069656372,
      "learning_rate": 0.00013154643324278097,
      "loss": 0.1939,
      "step": 17014
    },
    {
      "epoch": 34.23541247484909,
      "grad_norm": 0.8833293318748474,
      "learning_rate": 0.00013154240869302748,
      "loss": 0.2094,
      "step": 17015
    },
    {
      "epoch": 34.2374245472837,
      "grad_norm": 0.9449289441108704,
      "learning_rate": 0.00013153838414327397,
      "loss": 0.2275,
      "step": 17016
    },
    {
      "epoch": 34.23943661971831,
      "grad_norm": 0.9886448979377747,
      "learning_rate": 0.00013153435959352048,
      "loss": 0.2049,
      "step": 17017
    },
    {
      "epoch": 34.241448692152915,
      "grad_norm": 0.8640136122703552,
      "learning_rate": 0.000131530335043767,
      "loss": 0.1827,
      "step": 17018
    },
    {
      "epoch": 34.243460764587525,
      "grad_norm": 0.904203474521637,
      "learning_rate": 0.0001315263104940135,
      "loss": 0.1978,
      "step": 17019
    },
    {
      "epoch": 34.245472837022135,
      "grad_norm": 0.8680272102355957,
      "learning_rate": 0.00013152228594426,
      "loss": 0.1985,
      "step": 17020
    },
    {
      "epoch": 34.24748490945674,
      "grad_norm": 0.8995068073272705,
      "learning_rate": 0.0001315182613945065,
      "loss": 0.2008,
      "step": 17021
    },
    {
      "epoch": 34.24949698189135,
      "grad_norm": 0.9164904952049255,
      "learning_rate": 0.00013151423684475299,
      "loss": 0.212,
      "step": 17022
    },
    {
      "epoch": 34.25150905432596,
      "grad_norm": 0.874295175075531,
      "learning_rate": 0.00013151021229499953,
      "loss": 0.2011,
      "step": 17023
    },
    {
      "epoch": 34.25352112676056,
      "grad_norm": 0.8624000549316406,
      "learning_rate": 0.000131506187745246,
      "loss": 0.19,
      "step": 17024
    },
    {
      "epoch": 34.25553319919517,
      "grad_norm": 0.9175317883491516,
      "learning_rate": 0.00013150216319549252,
      "loss": 0.214,
      "step": 17025
    },
    {
      "epoch": 34.25754527162978,
      "grad_norm": 0.8959582448005676,
      "learning_rate": 0.000131498138645739,
      "loss": 0.2112,
      "step": 17026
    },
    {
      "epoch": 34.25955734406438,
      "grad_norm": 0.9361572265625,
      "learning_rate": 0.00013149411409598552,
      "loss": 0.2076,
      "step": 17027
    },
    {
      "epoch": 34.26156941649899,
      "grad_norm": 0.8576866388320923,
      "learning_rate": 0.00013149008954623203,
      "loss": 0.2074,
      "step": 17028
    },
    {
      "epoch": 34.2635814889336,
      "grad_norm": 0.8091921806335449,
      "learning_rate": 0.00013148606499647854,
      "loss": 0.1708,
      "step": 17029
    },
    {
      "epoch": 34.265593561368206,
      "grad_norm": 0.8475973010063171,
      "learning_rate": 0.00013148204044672503,
      "loss": 0.1878,
      "step": 17030
    },
    {
      "epoch": 34.267605633802816,
      "grad_norm": 0.9525238871574402,
      "learning_rate": 0.00013147801589697154,
      "loss": 0.2084,
      "step": 17031
    },
    {
      "epoch": 34.269617706237426,
      "grad_norm": 0.8963715434074402,
      "learning_rate": 0.00013147399134721803,
      "loss": 0.1986,
      "step": 17032
    },
    {
      "epoch": 34.27162977867203,
      "grad_norm": 0.9459105134010315,
      "learning_rate": 0.00013146996679746454,
      "loss": 0.2097,
      "step": 17033
    },
    {
      "epoch": 34.27364185110664,
      "grad_norm": 0.9366505146026611,
      "learning_rate": 0.00013146594224771105,
      "loss": 0.2096,
      "step": 17034
    },
    {
      "epoch": 34.27565392354125,
      "grad_norm": 0.862551748752594,
      "learning_rate": 0.00013146191769795754,
      "loss": 0.1927,
      "step": 17035
    },
    {
      "epoch": 34.27766599597585,
      "grad_norm": 0.9510119557380676,
      "learning_rate": 0.00013145789314820405,
      "loss": 0.1716,
      "step": 17036
    },
    {
      "epoch": 34.27967806841046,
      "grad_norm": 0.9229265451431274,
      "learning_rate": 0.00013145386859845053,
      "loss": 0.2148,
      "step": 17037
    },
    {
      "epoch": 34.28169014084507,
      "grad_norm": 1.012486219406128,
      "learning_rate": 0.00013144984404869707,
      "loss": 0.2006,
      "step": 17038
    },
    {
      "epoch": 34.283702213279675,
      "grad_norm": 0.9036019444465637,
      "learning_rate": 0.00013144581949894356,
      "loss": 0.1993,
      "step": 17039
    },
    {
      "epoch": 34.285714285714285,
      "grad_norm": 0.9573531150817871,
      "learning_rate": 0.00013144179494919007,
      "loss": 0.2139,
      "step": 17040
    },
    {
      "epoch": 34.287726358148895,
      "grad_norm": 0.8973220586776733,
      "learning_rate": 0.00013143777039943656,
      "loss": 0.1865,
      "step": 17041
    },
    {
      "epoch": 34.2897384305835,
      "grad_norm": 0.8806455135345459,
      "learning_rate": 0.00013143374584968307,
      "loss": 0.1884,
      "step": 17042
    },
    {
      "epoch": 34.29175050301811,
      "grad_norm": 0.913404643535614,
      "learning_rate": 0.00013142972129992958,
      "loss": 0.2028,
      "step": 17043
    },
    {
      "epoch": 34.29376257545272,
      "grad_norm": 0.9499236345291138,
      "learning_rate": 0.0001314256967501761,
      "loss": 0.2184,
      "step": 17044
    },
    {
      "epoch": 34.29577464788732,
      "grad_norm": 0.9018320441246033,
      "learning_rate": 0.00013142167220042258,
      "loss": 0.1947,
      "step": 17045
    },
    {
      "epoch": 34.29778672032193,
      "grad_norm": 0.8966547250747681,
      "learning_rate": 0.0001314176476506691,
      "loss": 0.2054,
      "step": 17046
    },
    {
      "epoch": 34.29979879275654,
      "grad_norm": 0.8972246646881104,
      "learning_rate": 0.00013141362310091557,
      "loss": 0.2099,
      "step": 17047
    },
    {
      "epoch": 34.30181086519114,
      "grad_norm": 0.9141948223114014,
      "learning_rate": 0.00013140959855116211,
      "loss": 0.2065,
      "step": 17048
    },
    {
      "epoch": 34.30382293762575,
      "grad_norm": 0.9238578677177429,
      "learning_rate": 0.0001314055740014086,
      "loss": 0.1956,
      "step": 17049
    },
    {
      "epoch": 34.30583501006036,
      "grad_norm": 0.9303757548332214,
      "learning_rate": 0.0001314015494516551,
      "loss": 0.2126,
      "step": 17050
    },
    {
      "epoch": 34.30784708249497,
      "grad_norm": 0.9069876074790955,
      "learning_rate": 0.0001313975249019016,
      "loss": 0.1943,
      "step": 17051
    },
    {
      "epoch": 34.309859154929576,
      "grad_norm": 0.9198822379112244,
      "learning_rate": 0.0001313935003521481,
      "loss": 0.1786,
      "step": 17052
    },
    {
      "epoch": 34.311871227364186,
      "grad_norm": 0.9293592572212219,
      "learning_rate": 0.00013138947580239462,
      "loss": 0.1879,
      "step": 17053
    },
    {
      "epoch": 34.313883299798796,
      "grad_norm": 0.8852815628051758,
      "learning_rate": 0.00013138545125264113,
      "loss": 0.2052,
      "step": 17054
    },
    {
      "epoch": 34.3158953722334,
      "grad_norm": 0.9146045446395874,
      "learning_rate": 0.00013138142670288762,
      "loss": 0.2202,
      "step": 17055
    },
    {
      "epoch": 34.31790744466801,
      "grad_norm": 0.8830953240394592,
      "learning_rate": 0.00013137740215313413,
      "loss": 0.2005,
      "step": 17056
    },
    {
      "epoch": 34.31991951710262,
      "grad_norm": 0.8847543597221375,
      "learning_rate": 0.00013137337760338062,
      "loss": 0.1998,
      "step": 17057
    },
    {
      "epoch": 34.32193158953722,
      "grad_norm": 0.9469159841537476,
      "learning_rate": 0.00013136935305362715,
      "loss": 0.2168,
      "step": 17058
    },
    {
      "epoch": 34.32394366197183,
      "grad_norm": 0.8848874568939209,
      "learning_rate": 0.00013136532850387364,
      "loss": 0.1934,
      "step": 17059
    },
    {
      "epoch": 34.32595573440644,
      "grad_norm": 0.9041783809661865,
      "learning_rate": 0.00013136130395412015,
      "loss": 0.2138,
      "step": 17060
    },
    {
      "epoch": 34.327967806841045,
      "grad_norm": 0.9126596450805664,
      "learning_rate": 0.00013135727940436664,
      "loss": 0.204,
      "step": 17061
    },
    {
      "epoch": 34.329979879275655,
      "grad_norm": 0.9060472846031189,
      "learning_rate": 0.00013135325485461315,
      "loss": 0.198,
      "step": 17062
    },
    {
      "epoch": 34.331991951710265,
      "grad_norm": 0.9177349805831909,
      "learning_rate": 0.00013134923030485966,
      "loss": 0.228,
      "step": 17063
    },
    {
      "epoch": 34.33400402414487,
      "grad_norm": 0.8972971439361572,
      "learning_rate": 0.00013134520575510615,
      "loss": 0.1987,
      "step": 17064
    },
    {
      "epoch": 34.33601609657948,
      "grad_norm": 0.892049252986908,
      "learning_rate": 0.00013134118120535266,
      "loss": 0.2102,
      "step": 17065
    },
    {
      "epoch": 34.33802816901409,
      "grad_norm": 0.8931881785392761,
      "learning_rate": 0.00013133715665559917,
      "loss": 0.213,
      "step": 17066
    },
    {
      "epoch": 34.34004024144869,
      "grad_norm": 0.8577629327774048,
      "learning_rate": 0.00013133313210584566,
      "loss": 0.1943,
      "step": 17067
    },
    {
      "epoch": 34.3420523138833,
      "grad_norm": 0.933285653591156,
      "learning_rate": 0.00013132910755609217,
      "loss": 0.2105,
      "step": 17068
    },
    {
      "epoch": 34.34406438631791,
      "grad_norm": 0.8771935701370239,
      "learning_rate": 0.00013132508300633868,
      "loss": 0.1989,
      "step": 17069
    },
    {
      "epoch": 34.34607645875251,
      "grad_norm": 0.8950291872024536,
      "learning_rate": 0.00013132105845658517,
      "loss": 0.2074,
      "step": 17070
    },
    {
      "epoch": 34.34808853118712,
      "grad_norm": 0.8723315000534058,
      "learning_rate": 0.00013131703390683168,
      "loss": 0.1805,
      "step": 17071
    },
    {
      "epoch": 34.35010060362173,
      "grad_norm": 0.8866150379180908,
      "learning_rate": 0.00013131300935707816,
      "loss": 0.2043,
      "step": 17072
    },
    {
      "epoch": 34.352112676056336,
      "grad_norm": 0.9249880909919739,
      "learning_rate": 0.0001313089848073247,
      "loss": 0.1914,
      "step": 17073
    },
    {
      "epoch": 34.354124748490946,
      "grad_norm": 0.9233028888702393,
      "learning_rate": 0.0001313049602575712,
      "loss": 0.2158,
      "step": 17074
    },
    {
      "epoch": 34.356136820925556,
      "grad_norm": 0.9856772422790527,
      "learning_rate": 0.0001313009357078177,
      "loss": 0.2074,
      "step": 17075
    },
    {
      "epoch": 34.35814889336016,
      "grad_norm": 0.9222886562347412,
      "learning_rate": 0.00013129691115806418,
      "loss": 0.208,
      "step": 17076
    },
    {
      "epoch": 34.36016096579477,
      "grad_norm": 0.9884869456291199,
      "learning_rate": 0.0001312928866083107,
      "loss": 0.1893,
      "step": 17077
    },
    {
      "epoch": 34.36217303822938,
      "grad_norm": 0.9341756105422974,
      "learning_rate": 0.0001312888620585572,
      "loss": 0.208,
      "step": 17078
    },
    {
      "epoch": 34.36418511066398,
      "grad_norm": 0.9631781578063965,
      "learning_rate": 0.00013128483750880372,
      "loss": 0.2347,
      "step": 17079
    },
    {
      "epoch": 34.36619718309859,
      "grad_norm": 0.9192128777503967,
      "learning_rate": 0.0001312808129590502,
      "loss": 0.1996,
      "step": 17080
    },
    {
      "epoch": 34.3682092555332,
      "grad_norm": 0.9638307094573975,
      "learning_rate": 0.00013127678840929672,
      "loss": 0.1998,
      "step": 17081
    },
    {
      "epoch": 34.370221327967805,
      "grad_norm": 0.9144283533096313,
      "learning_rate": 0.0001312727638595432,
      "loss": 0.1967,
      "step": 17082
    },
    {
      "epoch": 34.372233400402415,
      "grad_norm": 0.89354407787323,
      "learning_rate": 0.00013126873930978974,
      "loss": 0.2027,
      "step": 17083
    },
    {
      "epoch": 34.374245472837025,
      "grad_norm": 0.9242205023765564,
      "learning_rate": 0.00013126471476003623,
      "loss": 0.209,
      "step": 17084
    },
    {
      "epoch": 34.37625754527163,
      "grad_norm": 0.8951159715652466,
      "learning_rate": 0.00013126069021028274,
      "loss": 0.2074,
      "step": 17085
    },
    {
      "epoch": 34.37826961770624,
      "grad_norm": 0.9643670916557312,
      "learning_rate": 0.00013125666566052923,
      "loss": 0.2141,
      "step": 17086
    },
    {
      "epoch": 34.38028169014085,
      "grad_norm": 0.9601932168006897,
      "learning_rate": 0.00013125264111077574,
      "loss": 0.2249,
      "step": 17087
    },
    {
      "epoch": 34.38229376257545,
      "grad_norm": 0.9410313963890076,
      "learning_rate": 0.00013124861656102225,
      "loss": 0.2196,
      "step": 17088
    },
    {
      "epoch": 34.38430583501006,
      "grad_norm": 0.9192293882369995,
      "learning_rate": 0.00013124459201126876,
      "loss": 0.2169,
      "step": 17089
    },
    {
      "epoch": 34.38631790744467,
      "grad_norm": 0.9024063944816589,
      "learning_rate": 0.00013124056746151525,
      "loss": 0.2147,
      "step": 17090
    },
    {
      "epoch": 34.38832997987927,
      "grad_norm": 0.9264136552810669,
      "learning_rate": 0.00013123654291176176,
      "loss": 0.2081,
      "step": 17091
    },
    {
      "epoch": 34.39034205231388,
      "grad_norm": 0.8812913298606873,
      "learning_rate": 0.00013123251836200824,
      "loss": 0.1919,
      "step": 17092
    },
    {
      "epoch": 34.39235412474849,
      "grad_norm": 0.9145898818969727,
      "learning_rate": 0.00013122849381225478,
      "loss": 0.2205,
      "step": 17093
    },
    {
      "epoch": 34.394366197183096,
      "grad_norm": 0.9484293460845947,
      "learning_rate": 0.00013122446926250127,
      "loss": 0.2111,
      "step": 17094
    },
    {
      "epoch": 34.396378269617706,
      "grad_norm": 0.9775664806365967,
      "learning_rate": 0.00013122044471274778,
      "loss": 0.2276,
      "step": 17095
    },
    {
      "epoch": 34.398390342052316,
      "grad_norm": 0.9027782678604126,
      "learning_rate": 0.00013121642016299427,
      "loss": 0.2097,
      "step": 17096
    },
    {
      "epoch": 34.40040241448692,
      "grad_norm": 0.8899332880973816,
      "learning_rate": 0.00013121239561324078,
      "loss": 0.2077,
      "step": 17097
    },
    {
      "epoch": 34.40241448692153,
      "grad_norm": 0.9070153832435608,
      "learning_rate": 0.00013120837106348726,
      "loss": 0.2108,
      "step": 17098
    },
    {
      "epoch": 34.40442655935614,
      "grad_norm": 1.0022152662277222,
      "learning_rate": 0.00013120434651373378,
      "loss": 0.2148,
      "step": 17099
    },
    {
      "epoch": 34.40643863179074,
      "grad_norm": 0.8897925615310669,
      "learning_rate": 0.0001312003219639803,
      "loss": 0.189,
      "step": 17100
    },
    {
      "epoch": 34.40845070422535,
      "grad_norm": 0.9052752256393433,
      "learning_rate": 0.0001311962974142268,
      "loss": 0.2288,
      "step": 17101
    },
    {
      "epoch": 34.41046277665996,
      "grad_norm": 0.9316849708557129,
      "learning_rate": 0.00013119227286447329,
      "loss": 0.219,
      "step": 17102
    },
    {
      "epoch": 34.412474849094565,
      "grad_norm": 0.8945265412330627,
      "learning_rate": 0.0001311882483147198,
      "loss": 0.2041,
      "step": 17103
    },
    {
      "epoch": 34.414486921529175,
      "grad_norm": 0.9094021916389465,
      "learning_rate": 0.0001311842237649663,
      "loss": 0.2112,
      "step": 17104
    },
    {
      "epoch": 34.416498993963785,
      "grad_norm": 0.9479153156280518,
      "learning_rate": 0.0001311801992152128,
      "loss": 0.2196,
      "step": 17105
    },
    {
      "epoch": 34.41851106639839,
      "grad_norm": 0.8647614121437073,
      "learning_rate": 0.0001311761746654593,
      "loss": 0.2073,
      "step": 17106
    },
    {
      "epoch": 34.420523138833,
      "grad_norm": 0.9700393676757812,
      "learning_rate": 0.0001311721501157058,
      "loss": 0.2208,
      "step": 17107
    },
    {
      "epoch": 34.42253521126761,
      "grad_norm": 0.9414393901824951,
      "learning_rate": 0.0001311681255659523,
      "loss": 0.2222,
      "step": 17108
    },
    {
      "epoch": 34.42454728370221,
      "grad_norm": 0.8848346471786499,
      "learning_rate": 0.00013116410101619882,
      "loss": 0.2179,
      "step": 17109
    },
    {
      "epoch": 34.42655935613682,
      "grad_norm": 0.9519568085670471,
      "learning_rate": 0.00013116007646644533,
      "loss": 0.2276,
      "step": 17110
    },
    {
      "epoch": 34.42857142857143,
      "grad_norm": 0.8513584136962891,
      "learning_rate": 0.00013115605191669181,
      "loss": 0.1898,
      "step": 17111
    },
    {
      "epoch": 34.43058350100603,
      "grad_norm": 0.9140923619270325,
      "learning_rate": 0.00013115202736693833,
      "loss": 0.2237,
      "step": 17112
    },
    {
      "epoch": 34.43259557344064,
      "grad_norm": 0.9159019589424133,
      "learning_rate": 0.0001311480028171848,
      "loss": 0.2237,
      "step": 17113
    },
    {
      "epoch": 34.43460764587525,
      "grad_norm": 0.9547569751739502,
      "learning_rate": 0.00013114397826743135,
      "loss": 0.2048,
      "step": 17114
    },
    {
      "epoch": 34.436619718309856,
      "grad_norm": 0.9824565052986145,
      "learning_rate": 0.00013113995371767784,
      "loss": 0.2186,
      "step": 17115
    },
    {
      "epoch": 34.438631790744466,
      "grad_norm": 0.9369039535522461,
      "learning_rate": 0.00013113592916792435,
      "loss": 0.2191,
      "step": 17116
    },
    {
      "epoch": 34.440643863179076,
      "grad_norm": 0.9067353010177612,
      "learning_rate": 0.00013113190461817083,
      "loss": 0.209,
      "step": 17117
    },
    {
      "epoch": 34.44265593561368,
      "grad_norm": 0.8999723792076111,
      "learning_rate": 0.00013112788006841735,
      "loss": 0.2067,
      "step": 17118
    },
    {
      "epoch": 34.44466800804829,
      "grad_norm": 0.896643340587616,
      "learning_rate": 0.00013112385551866386,
      "loss": 0.1884,
      "step": 17119
    },
    {
      "epoch": 34.4466800804829,
      "grad_norm": 0.9081739783287048,
      "learning_rate": 0.00013111983096891037,
      "loss": 0.2093,
      "step": 17120
    },
    {
      "epoch": 34.4486921529175,
      "grad_norm": 0.9554519057273865,
      "learning_rate": 0.00013111580641915685,
      "loss": 0.2135,
      "step": 17121
    },
    {
      "epoch": 34.45070422535211,
      "grad_norm": 0.9814403653144836,
      "learning_rate": 0.00013111178186940337,
      "loss": 0.2139,
      "step": 17122
    },
    {
      "epoch": 34.45271629778672,
      "grad_norm": 0.9016517996788025,
      "learning_rate": 0.00013110775731964985,
      "loss": 0.1948,
      "step": 17123
    },
    {
      "epoch": 34.454728370221325,
      "grad_norm": 0.9690101742744446,
      "learning_rate": 0.0001311037327698964,
      "loss": 0.2147,
      "step": 17124
    },
    {
      "epoch": 34.456740442655935,
      "grad_norm": 0.9137295484542847,
      "learning_rate": 0.00013109970822014288,
      "loss": 0.2109,
      "step": 17125
    },
    {
      "epoch": 34.458752515090545,
      "grad_norm": 1.0089861154556274,
      "learning_rate": 0.0001310956836703894,
      "loss": 0.2204,
      "step": 17126
    },
    {
      "epoch": 34.46076458752515,
      "grad_norm": 0.8802418112754822,
      "learning_rate": 0.00013109165912063587,
      "loss": 0.2068,
      "step": 17127
    },
    {
      "epoch": 34.46277665995976,
      "grad_norm": 0.9632600545883179,
      "learning_rate": 0.00013108763457088239,
      "loss": 0.2143,
      "step": 17128
    },
    {
      "epoch": 34.46478873239437,
      "grad_norm": 0.9676485657691956,
      "learning_rate": 0.0001310836100211289,
      "loss": 0.212,
      "step": 17129
    },
    {
      "epoch": 34.46680080482897,
      "grad_norm": 0.9265349507331848,
      "learning_rate": 0.0001310795854713754,
      "loss": 0.201,
      "step": 17130
    },
    {
      "epoch": 34.46881287726358,
      "grad_norm": 0.945083737373352,
      "learning_rate": 0.0001310755609216219,
      "loss": 0.2149,
      "step": 17131
    },
    {
      "epoch": 34.47082494969819,
      "grad_norm": 0.9655890464782715,
      "learning_rate": 0.0001310715363718684,
      "loss": 0.215,
      "step": 17132
    },
    {
      "epoch": 34.47283702213279,
      "grad_norm": 0.9141473770141602,
      "learning_rate": 0.0001310675118221149,
      "loss": 0.2085,
      "step": 17133
    },
    {
      "epoch": 34.4748490945674,
      "grad_norm": 0.9658684730529785,
      "learning_rate": 0.0001310634872723614,
      "loss": 0.2023,
      "step": 17134
    },
    {
      "epoch": 34.47686116700201,
      "grad_norm": 0.8890359997749329,
      "learning_rate": 0.00013105946272260792,
      "loss": 0.1919,
      "step": 17135
    },
    {
      "epoch": 34.478873239436616,
      "grad_norm": 1.0344414710998535,
      "learning_rate": 0.00013105543817285443,
      "loss": 0.2224,
      "step": 17136
    },
    {
      "epoch": 34.480885311871226,
      "grad_norm": 1.0090391635894775,
      "learning_rate": 0.00013105141362310091,
      "loss": 0.2153,
      "step": 17137
    },
    {
      "epoch": 34.482897384305836,
      "grad_norm": 0.8882624506950378,
      "learning_rate": 0.00013104738907334743,
      "loss": 0.2187,
      "step": 17138
    },
    {
      "epoch": 34.48490945674044,
      "grad_norm": 0.983900785446167,
      "learning_rate": 0.00013104336452359394,
      "loss": 0.2326,
      "step": 17139
    },
    {
      "epoch": 34.48692152917505,
      "grad_norm": 0.9654009342193604,
      "learning_rate": 0.00013103933997384042,
      "loss": 0.2231,
      "step": 17140
    },
    {
      "epoch": 34.48893360160966,
      "grad_norm": 0.949685275554657,
      "learning_rate": 0.00013103531542408694,
      "loss": 0.2148,
      "step": 17141
    },
    {
      "epoch": 34.49094567404426,
      "grad_norm": 0.9723498225212097,
      "learning_rate": 0.00013103129087433342,
      "loss": 0.1964,
      "step": 17142
    },
    {
      "epoch": 34.49295774647887,
      "grad_norm": 0.9675654768943787,
      "learning_rate": 0.00013102726632457993,
      "loss": 0.2294,
      "step": 17143
    },
    {
      "epoch": 34.49496981891348,
      "grad_norm": 0.9215133786201477,
      "learning_rate": 0.00013102324177482645,
      "loss": 0.2105,
      "step": 17144
    },
    {
      "epoch": 34.496981891348085,
      "grad_norm": 0.937629222869873,
      "learning_rate": 0.00013101921722507296,
      "loss": 0.2057,
      "step": 17145
    },
    {
      "epoch": 34.498993963782695,
      "grad_norm": 0.9296794533729553,
      "learning_rate": 0.00013101519267531944,
      "loss": 0.2014,
      "step": 17146
    },
    {
      "epoch": 34.501006036217305,
      "grad_norm": 0.9678443670272827,
      "learning_rate": 0.00013101116812556596,
      "loss": 0.2214,
      "step": 17147
    },
    {
      "epoch": 34.503018108651915,
      "grad_norm": 0.9085281491279602,
      "learning_rate": 0.00013100714357581244,
      "loss": 0.2272,
      "step": 17148
    },
    {
      "epoch": 34.50503018108652,
      "grad_norm": 0.9235975742340088,
      "learning_rate": 0.00013100311902605898,
      "loss": 0.2167,
      "step": 17149
    },
    {
      "epoch": 34.50704225352113,
      "grad_norm": 0.9281654953956604,
      "learning_rate": 0.00013099909447630547,
      "loss": 0.2123,
      "step": 17150
    },
    {
      "epoch": 34.50905432595574,
      "grad_norm": 0.9676985144615173,
      "learning_rate": 0.00013099506992655198,
      "loss": 0.2301,
      "step": 17151
    },
    {
      "epoch": 34.51106639839034,
      "grad_norm": 0.9756178259849548,
      "learning_rate": 0.00013099104537679846,
      "loss": 0.2102,
      "step": 17152
    },
    {
      "epoch": 34.51307847082495,
      "grad_norm": 0.9448084831237793,
      "learning_rate": 0.00013098702082704497,
      "loss": 0.21,
      "step": 17153
    },
    {
      "epoch": 34.51509054325956,
      "grad_norm": 1.0248392820358276,
      "learning_rate": 0.0001309829962772915,
      "loss": 0.2466,
      "step": 17154
    },
    {
      "epoch": 34.517102615694164,
      "grad_norm": 0.9529900550842285,
      "learning_rate": 0.000130978971727538,
      "loss": 0.212,
      "step": 17155
    },
    {
      "epoch": 34.519114688128774,
      "grad_norm": 0.8969861268997192,
      "learning_rate": 0.00013097494717778448,
      "loss": 0.2125,
      "step": 17156
    },
    {
      "epoch": 34.521126760563384,
      "grad_norm": 0.9309551119804382,
      "learning_rate": 0.000130970922628031,
      "loss": 0.2319,
      "step": 17157
    },
    {
      "epoch": 34.52313883299799,
      "grad_norm": 0.9618080258369446,
      "learning_rate": 0.00013096689807827748,
      "loss": 0.2432,
      "step": 17158
    },
    {
      "epoch": 34.5251509054326,
      "grad_norm": 0.9404389262199402,
      "learning_rate": 0.00013096287352852402,
      "loss": 0.2178,
      "step": 17159
    },
    {
      "epoch": 34.52716297786721,
      "grad_norm": 0.9709166884422302,
      "learning_rate": 0.0001309588489787705,
      "loss": 0.234,
      "step": 17160
    },
    {
      "epoch": 34.52917505030181,
      "grad_norm": 0.909407913684845,
      "learning_rate": 0.00013095482442901702,
      "loss": 0.2101,
      "step": 17161
    },
    {
      "epoch": 34.53118712273642,
      "grad_norm": 0.916487455368042,
      "learning_rate": 0.0001309507998792635,
      "loss": 0.2064,
      "step": 17162
    },
    {
      "epoch": 34.53319919517103,
      "grad_norm": 0.941535234451294,
      "learning_rate": 0.00013094677532951002,
      "loss": 0.212,
      "step": 17163
    },
    {
      "epoch": 34.53521126760563,
      "grad_norm": 0.9361158609390259,
      "learning_rate": 0.00013094275077975653,
      "loss": 0.2156,
      "step": 17164
    },
    {
      "epoch": 34.53722334004024,
      "grad_norm": 0.9704297184944153,
      "learning_rate": 0.00013093872623000304,
      "loss": 0.2167,
      "step": 17165
    },
    {
      "epoch": 34.53923541247485,
      "grad_norm": 0.971989095211029,
      "learning_rate": 0.00013093470168024953,
      "loss": 0.2301,
      "step": 17166
    },
    {
      "epoch": 34.541247484909455,
      "grad_norm": 0.9468706250190735,
      "learning_rate": 0.00013093067713049604,
      "loss": 0.2198,
      "step": 17167
    },
    {
      "epoch": 34.543259557344065,
      "grad_norm": 0.9596344828605652,
      "learning_rate": 0.00013092665258074252,
      "loss": 0.2253,
      "step": 17168
    },
    {
      "epoch": 34.545271629778675,
      "grad_norm": 0.8613156080245972,
      "learning_rate": 0.00013092262803098903,
      "loss": 0.2051,
      "step": 17169
    },
    {
      "epoch": 34.54728370221328,
      "grad_norm": 0.9301313161849976,
      "learning_rate": 0.00013091860348123555,
      "loss": 0.2234,
      "step": 17170
    },
    {
      "epoch": 34.54929577464789,
      "grad_norm": 0.9107948541641235,
      "learning_rate": 0.00013091457893148206,
      "loss": 0.1997,
      "step": 17171
    },
    {
      "epoch": 34.5513078470825,
      "grad_norm": 1.0133790969848633,
      "learning_rate": 0.00013091055438172854,
      "loss": 0.2331,
      "step": 17172
    },
    {
      "epoch": 34.5533199195171,
      "grad_norm": 0.9559786915779114,
      "learning_rate": 0.00013090652983197506,
      "loss": 0.2257,
      "step": 17173
    },
    {
      "epoch": 34.55533199195171,
      "grad_norm": 0.9371420741081238,
      "learning_rate": 0.00013090250528222157,
      "loss": 0.215,
      "step": 17174
    },
    {
      "epoch": 34.55734406438632,
      "grad_norm": 0.9365406036376953,
      "learning_rate": 0.00013089848073246805,
      "loss": 0.2191,
      "step": 17175
    },
    {
      "epoch": 34.559356136820924,
      "grad_norm": 0.9027827978134155,
      "learning_rate": 0.00013089445618271457,
      "loss": 0.2102,
      "step": 17176
    },
    {
      "epoch": 34.561368209255534,
      "grad_norm": 1.0125422477722168,
      "learning_rate": 0.00013089043163296105,
      "loss": 0.2108,
      "step": 17177
    },
    {
      "epoch": 34.563380281690144,
      "grad_norm": 0.9704571962356567,
      "learning_rate": 0.00013088640708320756,
      "loss": 0.2138,
      "step": 17178
    },
    {
      "epoch": 34.56539235412475,
      "grad_norm": 1.0229777097702026,
      "learning_rate": 0.00013088238253345408,
      "loss": 0.2237,
      "step": 17179
    },
    {
      "epoch": 34.56740442655936,
      "grad_norm": 0.9846722483634949,
      "learning_rate": 0.0001308783579837006,
      "loss": 0.2428,
      "step": 17180
    },
    {
      "epoch": 34.56941649899397,
      "grad_norm": 0.8964537382125854,
      "learning_rate": 0.00013087433343394707,
      "loss": 0.2157,
      "step": 17181
    },
    {
      "epoch": 34.57142857142857,
      "grad_norm": 0.9313422441482544,
      "learning_rate": 0.00013087030888419359,
      "loss": 0.2126,
      "step": 17182
    },
    {
      "epoch": 34.57344064386318,
      "grad_norm": 0.9359149932861328,
      "learning_rate": 0.00013086628433444007,
      "loss": 0.1987,
      "step": 17183
    },
    {
      "epoch": 34.57545271629779,
      "grad_norm": 0.899807870388031,
      "learning_rate": 0.0001308622597846866,
      "loss": 0.225,
      "step": 17184
    },
    {
      "epoch": 34.57746478873239,
      "grad_norm": 0.9041150808334351,
      "learning_rate": 0.0001308582352349331,
      "loss": 0.2279,
      "step": 17185
    },
    {
      "epoch": 34.579476861167,
      "grad_norm": 1.000470757484436,
      "learning_rate": 0.0001308542106851796,
      "loss": 0.2331,
      "step": 17186
    },
    {
      "epoch": 34.58148893360161,
      "grad_norm": 0.9365288019180298,
      "learning_rate": 0.0001308501861354261,
      "loss": 0.2079,
      "step": 17187
    },
    {
      "epoch": 34.583501006036215,
      "grad_norm": 0.9325724840164185,
      "learning_rate": 0.0001308461615856726,
      "loss": 0.2153,
      "step": 17188
    },
    {
      "epoch": 34.585513078470825,
      "grad_norm": 0.9221630692481995,
      "learning_rate": 0.00013084213703591912,
      "loss": 0.221,
      "step": 17189
    },
    {
      "epoch": 34.587525150905435,
      "grad_norm": 0.9176475405693054,
      "learning_rate": 0.00013083811248616563,
      "loss": 0.2195,
      "step": 17190
    },
    {
      "epoch": 34.58953722334004,
      "grad_norm": 0.8682895302772522,
      "learning_rate": 0.00013083408793641211,
      "loss": 0.2064,
      "step": 17191
    },
    {
      "epoch": 34.59154929577465,
      "grad_norm": 0.9391742944717407,
      "learning_rate": 0.00013083006338665863,
      "loss": 0.2217,
      "step": 17192
    },
    {
      "epoch": 34.59356136820926,
      "grad_norm": 0.9304002523422241,
      "learning_rate": 0.0001308260388369051,
      "loss": 0.2248,
      "step": 17193
    },
    {
      "epoch": 34.59557344064386,
      "grad_norm": 0.9562621116638184,
      "learning_rate": 0.00013082201428715165,
      "loss": 0.2081,
      "step": 17194
    },
    {
      "epoch": 34.59758551307847,
      "grad_norm": 0.9217517375946045,
      "learning_rate": 0.00013081798973739814,
      "loss": 0.219,
      "step": 17195
    },
    {
      "epoch": 34.59959758551308,
      "grad_norm": 0.9411880970001221,
      "learning_rate": 0.00013081396518764465,
      "loss": 0.2052,
      "step": 17196
    },
    {
      "epoch": 34.601609657947684,
      "grad_norm": 0.9667997360229492,
      "learning_rate": 0.00013080994063789113,
      "loss": 0.2344,
      "step": 17197
    },
    {
      "epoch": 34.603621730382294,
      "grad_norm": 0.9807972311973572,
      "learning_rate": 0.00013080591608813765,
      "loss": 0.2384,
      "step": 17198
    },
    {
      "epoch": 34.605633802816904,
      "grad_norm": 0.9154208898544312,
      "learning_rate": 0.00013080189153838416,
      "loss": 0.2369,
      "step": 17199
    },
    {
      "epoch": 34.60764587525151,
      "grad_norm": 0.8847465515136719,
      "learning_rate": 0.00013079786698863067,
      "loss": 0.2023,
      "step": 17200
    },
    {
      "epoch": 34.60965794768612,
      "grad_norm": 0.9424886703491211,
      "learning_rate": 0.00013079384243887715,
      "loss": 0.2235,
      "step": 17201
    },
    {
      "epoch": 34.61167002012073,
      "grad_norm": 0.8860781788825989,
      "learning_rate": 0.00013078981788912367,
      "loss": 0.2269,
      "step": 17202
    },
    {
      "epoch": 34.61368209255533,
      "grad_norm": 0.9364046454429626,
      "learning_rate": 0.00013078579333937015,
      "loss": 0.2082,
      "step": 17203
    },
    {
      "epoch": 34.61569416498994,
      "grad_norm": 0.9775519967079163,
      "learning_rate": 0.00013078176878961666,
      "loss": 0.2306,
      "step": 17204
    },
    {
      "epoch": 34.61770623742455,
      "grad_norm": 1.0078502893447876,
      "learning_rate": 0.00013077774423986318,
      "loss": 0.2295,
      "step": 17205
    },
    {
      "epoch": 34.61971830985915,
      "grad_norm": 1.0172288417816162,
      "learning_rate": 0.00013077371969010966,
      "loss": 0.221,
      "step": 17206
    },
    {
      "epoch": 34.62173038229376,
      "grad_norm": 0.9697965979576111,
      "learning_rate": 0.00013076969514035617,
      "loss": 0.2183,
      "step": 17207
    },
    {
      "epoch": 34.62374245472837,
      "grad_norm": 0.9552960991859436,
      "learning_rate": 0.00013076567059060269,
      "loss": 0.214,
      "step": 17208
    },
    {
      "epoch": 34.625754527162975,
      "grad_norm": 0.9856340885162354,
      "learning_rate": 0.0001307616460408492,
      "loss": 0.2267,
      "step": 17209
    },
    {
      "epoch": 34.627766599597585,
      "grad_norm": 0.9498088359832764,
      "learning_rate": 0.00013075762149109568,
      "loss": 0.2113,
      "step": 17210
    },
    {
      "epoch": 34.629778672032195,
      "grad_norm": 0.8662365078926086,
      "learning_rate": 0.0001307535969413422,
      "loss": 0.1975,
      "step": 17211
    },
    {
      "epoch": 34.6317907444668,
      "grad_norm": 0.8993239998817444,
      "learning_rate": 0.00013074957239158868,
      "loss": 0.1935,
      "step": 17212
    },
    {
      "epoch": 34.63380281690141,
      "grad_norm": 0.9438462853431702,
      "learning_rate": 0.0001307455478418352,
      "loss": 0.2297,
      "step": 17213
    },
    {
      "epoch": 34.63581488933602,
      "grad_norm": 0.9613158702850342,
      "learning_rate": 0.0001307415232920817,
      "loss": 0.2201,
      "step": 17214
    },
    {
      "epoch": 34.63782696177062,
      "grad_norm": 0.9278256297111511,
      "learning_rate": 0.00013073749874232822,
      "loss": 0.2098,
      "step": 17215
    },
    {
      "epoch": 34.63983903420523,
      "grad_norm": 0.8986772894859314,
      "learning_rate": 0.0001307334741925747,
      "loss": 0.2019,
      "step": 17216
    },
    {
      "epoch": 34.64185110663984,
      "grad_norm": 0.9810892939567566,
      "learning_rate": 0.00013072944964282121,
      "loss": 0.2127,
      "step": 17217
    },
    {
      "epoch": 34.643863179074444,
      "grad_norm": 0.9315049648284912,
      "learning_rate": 0.0001307254250930677,
      "loss": 0.2158,
      "step": 17218
    },
    {
      "epoch": 34.645875251509054,
      "grad_norm": 0.9157349467277527,
      "learning_rate": 0.00013072140054331424,
      "loss": 0.2158,
      "step": 17219
    },
    {
      "epoch": 34.647887323943664,
      "grad_norm": 0.9222981929779053,
      "learning_rate": 0.00013071737599356072,
      "loss": 0.218,
      "step": 17220
    },
    {
      "epoch": 34.64989939637827,
      "grad_norm": 0.9725945591926575,
      "learning_rate": 0.00013071335144380724,
      "loss": 0.2231,
      "step": 17221
    },
    {
      "epoch": 34.65191146881288,
      "grad_norm": 0.9488232135772705,
      "learning_rate": 0.00013070932689405372,
      "loss": 0.2093,
      "step": 17222
    },
    {
      "epoch": 34.65392354124749,
      "grad_norm": 0.9587270617485046,
      "learning_rate": 0.00013070530234430023,
      "loss": 0.2247,
      "step": 17223
    },
    {
      "epoch": 34.65593561368209,
      "grad_norm": 0.9339025616645813,
      "learning_rate": 0.00013070127779454675,
      "loss": 0.213,
      "step": 17224
    },
    {
      "epoch": 34.6579476861167,
      "grad_norm": 0.9369111657142639,
      "learning_rate": 0.00013069725324479326,
      "loss": 0.2284,
      "step": 17225
    },
    {
      "epoch": 34.65995975855131,
      "grad_norm": 0.968563437461853,
      "learning_rate": 0.00013069322869503974,
      "loss": 0.2239,
      "step": 17226
    },
    {
      "epoch": 34.66197183098591,
      "grad_norm": 0.9270166158676147,
      "learning_rate": 0.00013068920414528626,
      "loss": 0.2319,
      "step": 17227
    },
    {
      "epoch": 34.66398390342052,
      "grad_norm": 0.9531579613685608,
      "learning_rate": 0.00013068517959553274,
      "loss": 0.2184,
      "step": 17228
    },
    {
      "epoch": 34.66599597585513,
      "grad_norm": 0.9633930325508118,
      "learning_rate": 0.00013068115504577928,
      "loss": 0.2223,
      "step": 17229
    },
    {
      "epoch": 34.668008048289735,
      "grad_norm": 0.8952310681343079,
      "learning_rate": 0.00013067713049602577,
      "loss": 0.2205,
      "step": 17230
    },
    {
      "epoch": 34.670020120724345,
      "grad_norm": 0.9805777668952942,
      "learning_rate": 0.00013067310594627228,
      "loss": 0.2356,
      "step": 17231
    },
    {
      "epoch": 34.672032193158955,
      "grad_norm": 1.0020748376846313,
      "learning_rate": 0.00013066908139651876,
      "loss": 0.2238,
      "step": 17232
    },
    {
      "epoch": 34.67404426559356,
      "grad_norm": 0.9814352989196777,
      "learning_rate": 0.00013066505684676527,
      "loss": 0.2347,
      "step": 17233
    },
    {
      "epoch": 34.67605633802817,
      "grad_norm": 0.9736480116844177,
      "learning_rate": 0.0001306610322970118,
      "loss": 0.2127,
      "step": 17234
    },
    {
      "epoch": 34.67806841046278,
      "grad_norm": 0.9538018107414246,
      "learning_rate": 0.0001306570077472583,
      "loss": 0.2406,
      "step": 17235
    },
    {
      "epoch": 34.68008048289738,
      "grad_norm": 0.9539936184883118,
      "learning_rate": 0.00013065298319750478,
      "loss": 0.2208,
      "step": 17236
    },
    {
      "epoch": 34.68209255533199,
      "grad_norm": 0.950323760509491,
      "learning_rate": 0.0001306489586477513,
      "loss": 0.2196,
      "step": 17237
    },
    {
      "epoch": 34.6841046277666,
      "grad_norm": 0.981023371219635,
      "learning_rate": 0.00013064493409799778,
      "loss": 0.2326,
      "step": 17238
    },
    {
      "epoch": 34.686116700201204,
      "grad_norm": 0.9223816394805908,
      "learning_rate": 0.0001306409095482443,
      "loss": 0.225,
      "step": 17239
    },
    {
      "epoch": 34.688128772635814,
      "grad_norm": 0.9408308267593384,
      "learning_rate": 0.0001306368849984908,
      "loss": 0.2194,
      "step": 17240
    },
    {
      "epoch": 34.690140845070424,
      "grad_norm": 0.9903178215026855,
      "learning_rate": 0.0001306328604487373,
      "loss": 0.2355,
      "step": 17241
    },
    {
      "epoch": 34.69215291750503,
      "grad_norm": 0.935863733291626,
      "learning_rate": 0.0001306288358989838,
      "loss": 0.2145,
      "step": 17242
    },
    {
      "epoch": 34.69416498993964,
      "grad_norm": 0.9859751462936401,
      "learning_rate": 0.00013062481134923032,
      "loss": 0.2205,
      "step": 17243
    },
    {
      "epoch": 34.69617706237425,
      "grad_norm": 0.9994704127311707,
      "learning_rate": 0.00013062078679947683,
      "loss": 0.2437,
      "step": 17244
    },
    {
      "epoch": 34.69818913480886,
      "grad_norm": 0.9971388578414917,
      "learning_rate": 0.0001306167622497233,
      "loss": 0.2191,
      "step": 17245
    },
    {
      "epoch": 34.70020120724346,
      "grad_norm": 1.039732813835144,
      "learning_rate": 0.00013061273769996982,
      "loss": 0.2056,
      "step": 17246
    },
    {
      "epoch": 34.70221327967807,
      "grad_norm": 0.9552871584892273,
      "learning_rate": 0.0001306087131502163,
      "loss": 0.2365,
      "step": 17247
    },
    {
      "epoch": 34.70422535211267,
      "grad_norm": 0.9541986584663391,
      "learning_rate": 0.00013060468860046282,
      "loss": 0.2235,
      "step": 17248
    },
    {
      "epoch": 34.70623742454728,
      "grad_norm": 0.898944079875946,
      "learning_rate": 0.00013060066405070933,
      "loss": 0.1915,
      "step": 17249
    },
    {
      "epoch": 34.70824949698189,
      "grad_norm": 0.9643605947494507,
      "learning_rate": 0.00013059663950095585,
      "loss": 0.2311,
      "step": 17250
    },
    {
      "epoch": 34.7102615694165,
      "grad_norm": 0.9736409783363342,
      "learning_rate": 0.00013059261495120233,
      "loss": 0.2295,
      "step": 17251
    },
    {
      "epoch": 34.712273641851105,
      "grad_norm": 0.9699584245681763,
      "learning_rate": 0.00013058859040144884,
      "loss": 0.2316,
      "step": 17252
    },
    {
      "epoch": 34.714285714285715,
      "grad_norm": 0.8991554975509644,
      "learning_rate": 0.00013058456585169533,
      "loss": 0.2085,
      "step": 17253
    },
    {
      "epoch": 34.716297786720325,
      "grad_norm": 1.0601526498794556,
      "learning_rate": 0.00013058054130194187,
      "loss": 0.2274,
      "step": 17254
    },
    {
      "epoch": 34.71830985915493,
      "grad_norm": 0.9578395485877991,
      "learning_rate": 0.00013057651675218835,
      "loss": 0.222,
      "step": 17255
    },
    {
      "epoch": 34.72032193158954,
      "grad_norm": 0.9634131193161011,
      "learning_rate": 0.00013057249220243487,
      "loss": 0.2241,
      "step": 17256
    },
    {
      "epoch": 34.72233400402415,
      "grad_norm": 0.9156817197799683,
      "learning_rate": 0.00013056846765268135,
      "loss": 0.2226,
      "step": 17257
    },
    {
      "epoch": 34.72434607645875,
      "grad_norm": 1.0505180358886719,
      "learning_rate": 0.00013056444310292786,
      "loss": 0.2212,
      "step": 17258
    },
    {
      "epoch": 34.72635814889336,
      "grad_norm": 0.9277076721191406,
      "learning_rate": 0.00013056041855317438,
      "loss": 0.2039,
      "step": 17259
    },
    {
      "epoch": 34.72837022132797,
      "grad_norm": 0.9773260951042175,
      "learning_rate": 0.0001305563940034209,
      "loss": 0.2378,
      "step": 17260
    },
    {
      "epoch": 34.730382293762574,
      "grad_norm": 0.9585932493209839,
      "learning_rate": 0.00013055236945366737,
      "loss": 0.2429,
      "step": 17261
    },
    {
      "epoch": 34.732394366197184,
      "grad_norm": 0.9388836622238159,
      "learning_rate": 0.00013054834490391388,
      "loss": 0.219,
      "step": 17262
    },
    {
      "epoch": 34.734406438631794,
      "grad_norm": 0.9017069339752197,
      "learning_rate": 0.00013054432035416037,
      "loss": 0.2223,
      "step": 17263
    },
    {
      "epoch": 34.7364185110664,
      "grad_norm": 1.0254042148590088,
      "learning_rate": 0.0001305402958044069,
      "loss": 0.2443,
      "step": 17264
    },
    {
      "epoch": 34.73843058350101,
      "grad_norm": 1.0302790403366089,
      "learning_rate": 0.0001305362712546534,
      "loss": 0.2377,
      "step": 17265
    },
    {
      "epoch": 34.74044265593562,
      "grad_norm": 1.0265586376190186,
      "learning_rate": 0.0001305322467048999,
      "loss": 0.2369,
      "step": 17266
    },
    {
      "epoch": 34.74245472837022,
      "grad_norm": 0.9719921350479126,
      "learning_rate": 0.0001305282221551464,
      "loss": 0.2283,
      "step": 17267
    },
    {
      "epoch": 34.74446680080483,
      "grad_norm": 0.9679378271102905,
      "learning_rate": 0.0001305241976053929,
      "loss": 0.208,
      "step": 17268
    },
    {
      "epoch": 34.74647887323944,
      "grad_norm": 0.9847605228424072,
      "learning_rate": 0.00013052017305563942,
      "loss": 0.232,
      "step": 17269
    },
    {
      "epoch": 34.74849094567404,
      "grad_norm": 0.996536374092102,
      "learning_rate": 0.00013051614850588593,
      "loss": 0.2211,
      "step": 17270
    },
    {
      "epoch": 34.75050301810865,
      "grad_norm": 1.044433355331421,
      "learning_rate": 0.0001305121239561324,
      "loss": 0.2304,
      "step": 17271
    },
    {
      "epoch": 34.75251509054326,
      "grad_norm": 1.00823974609375,
      "learning_rate": 0.00013050809940637893,
      "loss": 0.2248,
      "step": 17272
    },
    {
      "epoch": 34.754527162977865,
      "grad_norm": 0.9925281405448914,
      "learning_rate": 0.0001305040748566254,
      "loss": 0.2188,
      "step": 17273
    },
    {
      "epoch": 34.756539235412475,
      "grad_norm": 0.9149764776229858,
      "learning_rate": 0.00013050005030687192,
      "loss": 0.213,
      "step": 17274
    },
    {
      "epoch": 34.758551307847085,
      "grad_norm": 0.973315417766571,
      "learning_rate": 0.00013049602575711844,
      "loss": 0.2247,
      "step": 17275
    },
    {
      "epoch": 34.76056338028169,
      "grad_norm": 0.9985859394073486,
      "learning_rate": 0.00013049200120736492,
      "loss": 0.2397,
      "step": 17276
    },
    {
      "epoch": 34.7625754527163,
      "grad_norm": 0.9782723188400269,
      "learning_rate": 0.00013048797665761143,
      "loss": 0.2271,
      "step": 17277
    },
    {
      "epoch": 34.76458752515091,
      "grad_norm": 0.9792711734771729,
      "learning_rate": 0.00013048395210785794,
      "loss": 0.2185,
      "step": 17278
    },
    {
      "epoch": 34.76659959758551,
      "grad_norm": 0.978878915309906,
      "learning_rate": 0.00013047992755810446,
      "loss": 0.2347,
      "step": 17279
    },
    {
      "epoch": 34.76861167002012,
      "grad_norm": 0.9562438726425171,
      "learning_rate": 0.00013047590300835094,
      "loss": 0.2443,
      "step": 17280
    },
    {
      "epoch": 34.77062374245473,
      "grad_norm": 0.9390535950660706,
      "learning_rate": 0.00013047187845859745,
      "loss": 0.2333,
      "step": 17281
    },
    {
      "epoch": 34.772635814889334,
      "grad_norm": 0.9742044806480408,
      "learning_rate": 0.00013046785390884394,
      "loss": 0.2208,
      "step": 17282
    },
    {
      "epoch": 34.774647887323944,
      "grad_norm": 0.9223741292953491,
      "learning_rate": 0.00013046382935909045,
      "loss": 0.2172,
      "step": 17283
    },
    {
      "epoch": 34.776659959758554,
      "grad_norm": 0.9672216176986694,
      "learning_rate": 0.00013045980480933696,
      "loss": 0.2336,
      "step": 17284
    },
    {
      "epoch": 34.77867203219316,
      "grad_norm": 0.9273627400398254,
      "learning_rate": 0.00013045578025958348,
      "loss": 0.2185,
      "step": 17285
    },
    {
      "epoch": 34.78068410462777,
      "grad_norm": 0.9062447547912598,
      "learning_rate": 0.00013045175570982996,
      "loss": 0.2225,
      "step": 17286
    },
    {
      "epoch": 34.78269617706238,
      "grad_norm": 0.9823598861694336,
      "learning_rate": 0.00013044773116007647,
      "loss": 0.2381,
      "step": 17287
    },
    {
      "epoch": 34.78470824949698,
      "grad_norm": 0.933600664138794,
      "learning_rate": 0.00013044370661032296,
      "loss": 0.2353,
      "step": 17288
    },
    {
      "epoch": 34.78672032193159,
      "grad_norm": 0.9468280076980591,
      "learning_rate": 0.0001304396820605695,
      "loss": 0.2337,
      "step": 17289
    },
    {
      "epoch": 34.7887323943662,
      "grad_norm": 0.9119967818260193,
      "learning_rate": 0.00013043565751081598,
      "loss": 0.232,
      "step": 17290
    },
    {
      "epoch": 34.7907444668008,
      "grad_norm": 0.9413313269615173,
      "learning_rate": 0.0001304316329610625,
      "loss": 0.219,
      "step": 17291
    },
    {
      "epoch": 34.79275653923541,
      "grad_norm": 0.9262340068817139,
      "learning_rate": 0.00013042760841130898,
      "loss": 0.2384,
      "step": 17292
    },
    {
      "epoch": 34.79476861167002,
      "grad_norm": 0.9179946184158325,
      "learning_rate": 0.0001304235838615555,
      "loss": 0.2195,
      "step": 17293
    },
    {
      "epoch": 34.796780684104625,
      "grad_norm": 0.9296375513076782,
      "learning_rate": 0.000130419559311802,
      "loss": 0.2176,
      "step": 17294
    },
    {
      "epoch": 34.798792756539235,
      "grad_norm": 0.9688955545425415,
      "learning_rate": 0.00013041553476204852,
      "loss": 0.2449,
      "step": 17295
    },
    {
      "epoch": 34.800804828973845,
      "grad_norm": 0.9886735081672668,
      "learning_rate": 0.000130411510212295,
      "loss": 0.2323,
      "step": 17296
    },
    {
      "epoch": 34.80281690140845,
      "grad_norm": 1.0207968950271606,
      "learning_rate": 0.00013040748566254151,
      "loss": 0.2385,
      "step": 17297
    },
    {
      "epoch": 34.80482897384306,
      "grad_norm": 0.9592064619064331,
      "learning_rate": 0.000130403461112788,
      "loss": 0.2402,
      "step": 17298
    },
    {
      "epoch": 34.80684104627767,
      "grad_norm": 0.9331934452056885,
      "learning_rate": 0.00013039943656303454,
      "loss": 0.2297,
      "step": 17299
    },
    {
      "epoch": 34.80885311871227,
      "grad_norm": 0.9427005648612976,
      "learning_rate": 0.00013039541201328102,
      "loss": 0.2233,
      "step": 17300
    },
    {
      "epoch": 34.81086519114688,
      "grad_norm": 0.9375874400138855,
      "learning_rate": 0.00013039138746352754,
      "loss": 0.2284,
      "step": 17301
    },
    {
      "epoch": 34.81287726358149,
      "grad_norm": 0.9830427169799805,
      "learning_rate": 0.00013038736291377402,
      "loss": 0.2391,
      "step": 17302
    },
    {
      "epoch": 34.814889336016094,
      "grad_norm": 0.932149350643158,
      "learning_rate": 0.00013038333836402053,
      "loss": 0.2168,
      "step": 17303
    },
    {
      "epoch": 34.816901408450704,
      "grad_norm": 0.8756966590881348,
      "learning_rate": 0.00013037931381426705,
      "loss": 0.2201,
      "step": 17304
    },
    {
      "epoch": 34.818913480885314,
      "grad_norm": 1.0072752237319946,
      "learning_rate": 0.00013037528926451356,
      "loss": 0.247,
      "step": 17305
    },
    {
      "epoch": 34.82092555331992,
      "grad_norm": 0.9328685998916626,
      "learning_rate": 0.00013037126471476004,
      "loss": 0.2148,
      "step": 17306
    },
    {
      "epoch": 34.82293762575453,
      "grad_norm": 0.9756011962890625,
      "learning_rate": 0.00013036724016500656,
      "loss": 0.2354,
      "step": 17307
    },
    {
      "epoch": 34.82494969818914,
      "grad_norm": 0.9221152067184448,
      "learning_rate": 0.00013036321561525304,
      "loss": 0.2246,
      "step": 17308
    },
    {
      "epoch": 34.82696177062374,
      "grad_norm": 0.994816780090332,
      "learning_rate": 0.00013035919106549955,
      "loss": 0.2476,
      "step": 17309
    },
    {
      "epoch": 34.82897384305835,
      "grad_norm": 0.9615049958229065,
      "learning_rate": 0.00013035516651574606,
      "loss": 0.2266,
      "step": 17310
    },
    {
      "epoch": 34.83098591549296,
      "grad_norm": 0.9377975463867188,
      "learning_rate": 0.00013035114196599255,
      "loss": 0.2257,
      "step": 17311
    },
    {
      "epoch": 34.83299798792756,
      "grad_norm": 0.8907846808433533,
      "learning_rate": 0.00013034711741623906,
      "loss": 0.2117,
      "step": 17312
    },
    {
      "epoch": 34.83501006036217,
      "grad_norm": 1.0186805725097656,
      "learning_rate": 0.00013034309286648557,
      "loss": 0.2289,
      "step": 17313
    },
    {
      "epoch": 34.83702213279678,
      "grad_norm": 0.9559678435325623,
      "learning_rate": 0.0001303390683167321,
      "loss": 0.2358,
      "step": 17314
    },
    {
      "epoch": 34.839034205231385,
      "grad_norm": 0.9360284805297852,
      "learning_rate": 0.00013033504376697857,
      "loss": 0.2371,
      "step": 17315
    },
    {
      "epoch": 34.841046277665995,
      "grad_norm": 0.9285511374473572,
      "learning_rate": 0.00013033101921722508,
      "loss": 0.2214,
      "step": 17316
    },
    {
      "epoch": 34.843058350100605,
      "grad_norm": 0.929671585559845,
      "learning_rate": 0.00013032699466747157,
      "loss": 0.241,
      "step": 17317
    },
    {
      "epoch": 34.84507042253521,
      "grad_norm": 0.9357861876487732,
      "learning_rate": 0.00013032297011771808,
      "loss": 0.2452,
      "step": 17318
    },
    {
      "epoch": 34.84708249496982,
      "grad_norm": 0.9426583051681519,
      "learning_rate": 0.0001303189455679646,
      "loss": 0.2455,
      "step": 17319
    },
    {
      "epoch": 34.84909456740443,
      "grad_norm": 0.9968428611755371,
      "learning_rate": 0.0001303149210182111,
      "loss": 0.2444,
      "step": 17320
    },
    {
      "epoch": 34.85110663983903,
      "grad_norm": 0.9587089419364929,
      "learning_rate": 0.0001303108964684576,
      "loss": 0.2278,
      "step": 17321
    },
    {
      "epoch": 34.85311871227364,
      "grad_norm": 0.9556952118873596,
      "learning_rate": 0.0001303068719187041,
      "loss": 0.2376,
      "step": 17322
    },
    {
      "epoch": 34.85513078470825,
      "grad_norm": 0.9399057030677795,
      "learning_rate": 0.0001303028473689506,
      "loss": 0.2137,
      "step": 17323
    },
    {
      "epoch": 34.857142857142854,
      "grad_norm": 1.0095821619033813,
      "learning_rate": 0.00013029882281919713,
      "loss": 0.2356,
      "step": 17324
    },
    {
      "epoch": 34.859154929577464,
      "grad_norm": 0.9915598630905151,
      "learning_rate": 0.0001302947982694436,
      "loss": 0.2224,
      "step": 17325
    },
    {
      "epoch": 34.861167002012074,
      "grad_norm": 0.9833425879478455,
      "learning_rate": 0.00013029077371969012,
      "loss": 0.2173,
      "step": 17326
    },
    {
      "epoch": 34.86317907444668,
      "grad_norm": 0.9490591883659363,
      "learning_rate": 0.0001302867491699366,
      "loss": 0.2335,
      "step": 17327
    },
    {
      "epoch": 34.86519114688129,
      "grad_norm": 0.9491779208183289,
      "learning_rate": 0.00013028272462018312,
      "loss": 0.2201,
      "step": 17328
    },
    {
      "epoch": 34.8672032193159,
      "grad_norm": 1.006260633468628,
      "learning_rate": 0.00013027870007042963,
      "loss": 0.2206,
      "step": 17329
    },
    {
      "epoch": 34.8692152917505,
      "grad_norm": 0.9923107624053955,
      "learning_rate": 0.00013027467552067615,
      "loss": 0.2157,
      "step": 17330
    },
    {
      "epoch": 34.87122736418511,
      "grad_norm": 0.9319900274276733,
      "learning_rate": 0.00013027065097092263,
      "loss": 0.2259,
      "step": 17331
    },
    {
      "epoch": 34.87323943661972,
      "grad_norm": 0.9672548174858093,
      "learning_rate": 0.00013026662642116914,
      "loss": 0.2494,
      "step": 17332
    },
    {
      "epoch": 34.87525150905432,
      "grad_norm": 1.0042753219604492,
      "learning_rate": 0.00013026260187141563,
      "loss": 0.2471,
      "step": 17333
    },
    {
      "epoch": 34.87726358148893,
      "grad_norm": 0.9375686049461365,
      "learning_rate": 0.00013025857732166217,
      "loss": 0.2275,
      "step": 17334
    },
    {
      "epoch": 34.87927565392354,
      "grad_norm": 1.0301785469055176,
      "learning_rate": 0.00013025455277190865,
      "loss": 0.2239,
      "step": 17335
    },
    {
      "epoch": 34.881287726358146,
      "grad_norm": 0.9287973046302795,
      "learning_rate": 0.00013025052822215517,
      "loss": 0.2162,
      "step": 17336
    },
    {
      "epoch": 34.883299798792756,
      "grad_norm": 0.9755651354789734,
      "learning_rate": 0.00013024650367240165,
      "loss": 0.2171,
      "step": 17337
    },
    {
      "epoch": 34.885311871227366,
      "grad_norm": 0.9256671667098999,
      "learning_rate": 0.00013024247912264816,
      "loss": 0.2241,
      "step": 17338
    },
    {
      "epoch": 34.88732394366197,
      "grad_norm": 0.9873470067977905,
      "learning_rate": 0.00013023845457289465,
      "loss": 0.2387,
      "step": 17339
    },
    {
      "epoch": 34.88933601609658,
      "grad_norm": 0.9885272979736328,
      "learning_rate": 0.0001302344300231412,
      "loss": 0.2404,
      "step": 17340
    },
    {
      "epoch": 34.89134808853119,
      "grad_norm": 0.9750944375991821,
      "learning_rate": 0.00013023040547338767,
      "loss": 0.2276,
      "step": 17341
    },
    {
      "epoch": 34.89336016096579,
      "grad_norm": 0.9516125917434692,
      "learning_rate": 0.00013022638092363418,
      "loss": 0.2227,
      "step": 17342
    },
    {
      "epoch": 34.8953722334004,
      "grad_norm": 0.9476714134216309,
      "learning_rate": 0.00013022235637388067,
      "loss": 0.2079,
      "step": 17343
    },
    {
      "epoch": 34.89738430583501,
      "grad_norm": 0.9910519123077393,
      "learning_rate": 0.00013021833182412718,
      "loss": 0.225,
      "step": 17344
    },
    {
      "epoch": 34.899396378269614,
      "grad_norm": 0.9243559837341309,
      "learning_rate": 0.0001302143072743737,
      "loss": 0.2262,
      "step": 17345
    },
    {
      "epoch": 34.901408450704224,
      "grad_norm": 0.9811403751373291,
      "learning_rate": 0.00013021028272462018,
      "loss": 0.2285,
      "step": 17346
    },
    {
      "epoch": 34.903420523138834,
      "grad_norm": 0.9721229076385498,
      "learning_rate": 0.0001302062581748667,
      "loss": 0.2225,
      "step": 17347
    },
    {
      "epoch": 34.905432595573444,
      "grad_norm": 0.9666800498962402,
      "learning_rate": 0.0001302022336251132,
      "loss": 0.2409,
      "step": 17348
    },
    {
      "epoch": 34.90744466800805,
      "grad_norm": 0.9796259999275208,
      "learning_rate": 0.0001301982090753597,
      "loss": 0.2358,
      "step": 17349
    },
    {
      "epoch": 34.90945674044266,
      "grad_norm": 0.9980068206787109,
      "learning_rate": 0.0001301941845256062,
      "loss": 0.2191,
      "step": 17350
    },
    {
      "epoch": 34.91146881287727,
      "grad_norm": 0.9441790580749512,
      "learning_rate": 0.0001301901599758527,
      "loss": 0.2262,
      "step": 17351
    },
    {
      "epoch": 34.91348088531187,
      "grad_norm": 0.9131670594215393,
      "learning_rate": 0.0001301861354260992,
      "loss": 0.224,
      "step": 17352
    },
    {
      "epoch": 34.91549295774648,
      "grad_norm": 0.983090877532959,
      "learning_rate": 0.0001301821108763457,
      "loss": 0.2401,
      "step": 17353
    },
    {
      "epoch": 34.91750503018109,
      "grad_norm": 0.9771197438240051,
      "learning_rate": 0.0001301780863265922,
      "loss": 0.2324,
      "step": 17354
    },
    {
      "epoch": 34.91951710261569,
      "grad_norm": 0.9854909777641296,
      "learning_rate": 0.00013017406177683874,
      "loss": 0.2334,
      "step": 17355
    },
    {
      "epoch": 34.9215291750503,
      "grad_norm": 0.9569815993309021,
      "learning_rate": 0.00013017003722708522,
      "loss": 0.2415,
      "step": 17356
    },
    {
      "epoch": 34.92354124748491,
      "grad_norm": 0.9480764269828796,
      "learning_rate": 0.00013016601267733173,
      "loss": 0.2305,
      "step": 17357
    },
    {
      "epoch": 34.925553319919516,
      "grad_norm": 0.9751197099685669,
      "learning_rate": 0.00013016198812757822,
      "loss": 0.2397,
      "step": 17358
    },
    {
      "epoch": 34.927565392354126,
      "grad_norm": 0.9973629713058472,
      "learning_rate": 0.00013015796357782473,
      "loss": 0.2341,
      "step": 17359
    },
    {
      "epoch": 34.929577464788736,
      "grad_norm": 0.919437050819397,
      "learning_rate": 0.00013015393902807124,
      "loss": 0.2127,
      "step": 17360
    },
    {
      "epoch": 34.93158953722334,
      "grad_norm": 0.9195647835731506,
      "learning_rate": 0.00013014991447831775,
      "loss": 0.2267,
      "step": 17361
    },
    {
      "epoch": 34.93360160965795,
      "grad_norm": 0.9512316584587097,
      "learning_rate": 0.00013014588992856424,
      "loss": 0.2362,
      "step": 17362
    },
    {
      "epoch": 34.93561368209256,
      "grad_norm": 0.9372302889823914,
      "learning_rate": 0.00013014186537881075,
      "loss": 0.2399,
      "step": 17363
    },
    {
      "epoch": 34.93762575452716,
      "grad_norm": 1.0045528411865234,
      "learning_rate": 0.00013013784082905724,
      "loss": 0.2521,
      "step": 17364
    },
    {
      "epoch": 34.93963782696177,
      "grad_norm": 0.9703143239021301,
      "learning_rate": 0.00013013381627930378,
      "loss": 0.2373,
      "step": 17365
    },
    {
      "epoch": 34.94164989939638,
      "grad_norm": 0.9760076403617859,
      "learning_rate": 0.00013012979172955026,
      "loss": 0.2372,
      "step": 17366
    },
    {
      "epoch": 34.943661971830984,
      "grad_norm": 1.0294301509857178,
      "learning_rate": 0.00013012576717979677,
      "loss": 0.2212,
      "step": 17367
    },
    {
      "epoch": 34.945674044265594,
      "grad_norm": 0.9717875123023987,
      "learning_rate": 0.00013012174263004326,
      "loss": 0.2211,
      "step": 17368
    },
    {
      "epoch": 34.947686116700204,
      "grad_norm": 0.9672818183898926,
      "learning_rate": 0.00013011771808028977,
      "loss": 0.2509,
      "step": 17369
    },
    {
      "epoch": 34.94969818913481,
      "grad_norm": 0.9013082385063171,
      "learning_rate": 0.00013011369353053628,
      "loss": 0.2306,
      "step": 17370
    },
    {
      "epoch": 34.95171026156942,
      "grad_norm": 0.9200848937034607,
      "learning_rate": 0.0001301096689807828,
      "loss": 0.2386,
      "step": 17371
    },
    {
      "epoch": 34.95372233400403,
      "grad_norm": 0.9337257742881775,
      "learning_rate": 0.00013010564443102928,
      "loss": 0.2246,
      "step": 17372
    },
    {
      "epoch": 34.95573440643863,
      "grad_norm": 1.0404932498931885,
      "learning_rate": 0.0001301016198812758,
      "loss": 0.2498,
      "step": 17373
    },
    {
      "epoch": 34.95774647887324,
      "grad_norm": 0.9092540740966797,
      "learning_rate": 0.00013009759533152228,
      "loss": 0.2247,
      "step": 17374
    },
    {
      "epoch": 34.95975855130785,
      "grad_norm": 0.9454008936882019,
      "learning_rate": 0.00013009357078176882,
      "loss": 0.2391,
      "step": 17375
    },
    {
      "epoch": 34.96177062374245,
      "grad_norm": 0.9649682641029358,
      "learning_rate": 0.0001300895462320153,
      "loss": 0.238,
      "step": 17376
    },
    {
      "epoch": 34.96378269617706,
      "grad_norm": 0.8967324495315552,
      "learning_rate": 0.00013008552168226181,
      "loss": 0.2203,
      "step": 17377
    },
    {
      "epoch": 34.96579476861167,
      "grad_norm": 0.9753783941268921,
      "learning_rate": 0.0001300814971325083,
      "loss": 0.2384,
      "step": 17378
    },
    {
      "epoch": 34.967806841046276,
      "grad_norm": 0.9662691354751587,
      "learning_rate": 0.0001300774725827548,
      "loss": 0.2275,
      "step": 17379
    },
    {
      "epoch": 34.969818913480886,
      "grad_norm": 0.9533759355545044,
      "learning_rate": 0.00013007344803300132,
      "loss": 0.2278,
      "step": 17380
    },
    {
      "epoch": 34.971830985915496,
      "grad_norm": 0.9300695061683655,
      "learning_rate": 0.0001300694234832478,
      "loss": 0.2123,
      "step": 17381
    },
    {
      "epoch": 34.9738430583501,
      "grad_norm": 0.9267885088920593,
      "learning_rate": 0.00013006539893349432,
      "loss": 0.2209,
      "step": 17382
    },
    {
      "epoch": 34.97585513078471,
      "grad_norm": 0.9379408955574036,
      "learning_rate": 0.0001300613743837408,
      "loss": 0.2228,
      "step": 17383
    },
    {
      "epoch": 34.97786720321932,
      "grad_norm": 0.9616529941558838,
      "learning_rate": 0.00013005734983398732,
      "loss": 0.2389,
      "step": 17384
    },
    {
      "epoch": 34.97987927565392,
      "grad_norm": 0.9487524032592773,
      "learning_rate": 0.00013005332528423383,
      "loss": 0.2219,
      "step": 17385
    },
    {
      "epoch": 34.98189134808853,
      "grad_norm": 0.9851840138435364,
      "learning_rate": 0.00013004930073448034,
      "loss": 0.2438,
      "step": 17386
    },
    {
      "epoch": 34.98390342052314,
      "grad_norm": 0.9621119499206543,
      "learning_rate": 0.00013004527618472683,
      "loss": 0.2153,
      "step": 17387
    },
    {
      "epoch": 34.985915492957744,
      "grad_norm": 0.9661129117012024,
      "learning_rate": 0.00013004125163497334,
      "loss": 0.254,
      "step": 17388
    },
    {
      "epoch": 34.987927565392354,
      "grad_norm": 0.9356842637062073,
      "learning_rate": 0.00013003722708521983,
      "loss": 0.2328,
      "step": 17389
    },
    {
      "epoch": 34.989939637826964,
      "grad_norm": 0.9469510912895203,
      "learning_rate": 0.00013003320253546636,
      "loss": 0.2446,
      "step": 17390
    },
    {
      "epoch": 34.99195171026157,
      "grad_norm": 1.0102031230926514,
      "learning_rate": 0.00013002917798571285,
      "loss": 0.2389,
      "step": 17391
    },
    {
      "epoch": 34.99396378269618,
      "grad_norm": 0.9026757478713989,
      "learning_rate": 0.00013002515343595936,
      "loss": 0.2216,
      "step": 17392
    },
    {
      "epoch": 34.99597585513079,
      "grad_norm": 0.8986660838127136,
      "learning_rate": 0.00013002112888620585,
      "loss": 0.2229,
      "step": 17393
    },
    {
      "epoch": 34.99798792756539,
      "grad_norm": 0.9486649632453918,
      "learning_rate": 0.00013001710433645236,
      "loss": 0.2346,
      "step": 17394
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.9912259578704834,
      "learning_rate": 0.00013001307978669887,
      "loss": 0.2317,
      "step": 17395
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.4638292789459229,
      "eval_runtime": 49.8555,
      "eval_samples_per_second": 19.898,
      "eval_steps_per_second": 2.487,
      "step": 17395
    },
    {
      "epoch": 35.00201207243461,
      "grad_norm": 0.7486368417739868,
      "learning_rate": 0.00013000905523694538,
      "loss": 0.1681,
      "step": 17396
    },
    {
      "epoch": 35.00402414486921,
      "grad_norm": 0.8267912268638611,
      "learning_rate": 0.00013000503068719187,
      "loss": 0.1803,
      "step": 17397
    },
    {
      "epoch": 35.00603621730382,
      "grad_norm": 0.8081842660903931,
      "learning_rate": 0.00013000100613743838,
      "loss": 0.1728,
      "step": 17398
    },
    {
      "epoch": 35.00804828973843,
      "grad_norm": 0.856379508972168,
      "learning_rate": 0.00012999698158768487,
      "loss": 0.1853,
      "step": 17399
    },
    {
      "epoch": 35.010060362173036,
      "grad_norm": 0.8744649887084961,
      "learning_rate": 0.0001299929570379314,
      "loss": 0.1876,
      "step": 17400
    },
    {
      "epoch": 35.012072434607646,
      "grad_norm": 0.8440601825714111,
      "learning_rate": 0.0001299889324881779,
      "loss": 0.1638,
      "step": 17401
    },
    {
      "epoch": 35.014084507042256,
      "grad_norm": 0.829413115978241,
      "learning_rate": 0.0001299849079384244,
      "loss": 0.1817,
      "step": 17402
    },
    {
      "epoch": 35.01609657947686,
      "grad_norm": 0.846921443939209,
      "learning_rate": 0.0001299808833886709,
      "loss": 0.1726,
      "step": 17403
    },
    {
      "epoch": 35.01810865191147,
      "grad_norm": 0.8165314197540283,
      "learning_rate": 0.0001299768588389174,
      "loss": 0.1709,
      "step": 17404
    },
    {
      "epoch": 35.02012072434608,
      "grad_norm": 0.8409253358840942,
      "learning_rate": 0.0001299728342891639,
      "loss": 0.1806,
      "step": 17405
    },
    {
      "epoch": 35.02213279678068,
      "grad_norm": 0.8828469514846802,
      "learning_rate": 0.00012996880973941042,
      "loss": 0.1856,
      "step": 17406
    },
    {
      "epoch": 35.02414486921529,
      "grad_norm": 0.8554297685623169,
      "learning_rate": 0.0001299647851896569,
      "loss": 0.1758,
      "step": 17407
    },
    {
      "epoch": 35.0261569416499,
      "grad_norm": 0.8295936584472656,
      "learning_rate": 0.00012996076063990342,
      "loss": 0.1725,
      "step": 17408
    },
    {
      "epoch": 35.028169014084504,
      "grad_norm": 0.8494076728820801,
      "learning_rate": 0.0001299567360901499,
      "loss": 0.1928,
      "step": 17409
    },
    {
      "epoch": 35.030181086519114,
      "grad_norm": 0.8197737336158752,
      "learning_rate": 0.00012995271154039642,
      "loss": 0.1688,
      "step": 17410
    },
    {
      "epoch": 35.032193158953724,
      "grad_norm": 0.7795675992965698,
      "learning_rate": 0.00012994868699064293,
      "loss": 0.1668,
      "step": 17411
    },
    {
      "epoch": 35.03420523138833,
      "grad_norm": 0.8910354971885681,
      "learning_rate": 0.00012994466244088944,
      "loss": 0.1813,
      "step": 17412
    },
    {
      "epoch": 35.03621730382294,
      "grad_norm": 0.8961685299873352,
      "learning_rate": 0.00012994063789113593,
      "loss": 0.1773,
      "step": 17413
    },
    {
      "epoch": 35.03822937625755,
      "grad_norm": 0.8665094971656799,
      "learning_rate": 0.00012993661334138244,
      "loss": 0.1815,
      "step": 17414
    },
    {
      "epoch": 35.04024144869215,
      "grad_norm": 0.8868305683135986,
      "learning_rate": 0.00012993258879162895,
      "loss": 0.1833,
      "step": 17415
    },
    {
      "epoch": 35.04225352112676,
      "grad_norm": 0.8399059176445007,
      "learning_rate": 0.00012992856424187544,
      "loss": 0.1753,
      "step": 17416
    },
    {
      "epoch": 35.04426559356137,
      "grad_norm": 0.8380879163742065,
      "learning_rate": 0.00012992453969212195,
      "loss": 0.1634,
      "step": 17417
    },
    {
      "epoch": 35.04627766599597,
      "grad_norm": 0.855414867401123,
      "learning_rate": 0.00012992051514236844,
      "loss": 0.1859,
      "step": 17418
    },
    {
      "epoch": 35.04828973843058,
      "grad_norm": 0.8370775580406189,
      "learning_rate": 0.00012991649059261495,
      "loss": 0.1954,
      "step": 17419
    },
    {
      "epoch": 35.05030181086519,
      "grad_norm": 0.8190999627113342,
      "learning_rate": 0.00012991246604286146,
      "loss": 0.1818,
      "step": 17420
    },
    {
      "epoch": 35.052313883299796,
      "grad_norm": 0.9762135148048401,
      "learning_rate": 0.00012990844149310797,
      "loss": 0.1967,
      "step": 17421
    },
    {
      "epoch": 35.054325955734406,
      "grad_norm": 0.8383378982543945,
      "learning_rate": 0.00012990441694335446,
      "loss": 0.1823,
      "step": 17422
    },
    {
      "epoch": 35.056338028169016,
      "grad_norm": 0.8611953258514404,
      "learning_rate": 0.00012990039239360097,
      "loss": 0.1809,
      "step": 17423
    },
    {
      "epoch": 35.05835010060362,
      "grad_norm": 0.8020380139350891,
      "learning_rate": 0.00012989636784384745,
      "loss": 0.1657,
      "step": 17424
    },
    {
      "epoch": 35.06036217303823,
      "grad_norm": 0.8314377665519714,
      "learning_rate": 0.000129892343294094,
      "loss": 0.1766,
      "step": 17425
    },
    {
      "epoch": 35.06237424547284,
      "grad_norm": 0.8636515140533447,
      "learning_rate": 0.00012988831874434048,
      "loss": 0.194,
      "step": 17426
    },
    {
      "epoch": 35.06438631790744,
      "grad_norm": 0.8676521182060242,
      "learning_rate": 0.000129884294194587,
      "loss": 0.1899,
      "step": 17427
    },
    {
      "epoch": 35.06639839034205,
      "grad_norm": 0.875770628452301,
      "learning_rate": 0.00012988026964483348,
      "loss": 0.191,
      "step": 17428
    },
    {
      "epoch": 35.06841046277666,
      "grad_norm": 0.9178328514099121,
      "learning_rate": 0.00012987624509508,
      "loss": 0.2038,
      "step": 17429
    },
    {
      "epoch": 35.070422535211264,
      "grad_norm": 0.8394383192062378,
      "learning_rate": 0.0001298722205453265,
      "loss": 0.1686,
      "step": 17430
    },
    {
      "epoch": 35.072434607645874,
      "grad_norm": 0.8614896535873413,
      "learning_rate": 0.000129868195995573,
      "loss": 0.1728,
      "step": 17431
    },
    {
      "epoch": 35.074446680080484,
      "grad_norm": 0.8478075861930847,
      "learning_rate": 0.0001298641714458195,
      "loss": 0.1822,
      "step": 17432
    },
    {
      "epoch": 35.07645875251509,
      "grad_norm": 0.8807971477508545,
      "learning_rate": 0.000129860146896066,
      "loss": 0.1875,
      "step": 17433
    },
    {
      "epoch": 35.0784708249497,
      "grad_norm": 0.8244522213935852,
      "learning_rate": 0.0001298561223463125,
      "loss": 0.1676,
      "step": 17434
    },
    {
      "epoch": 35.08048289738431,
      "grad_norm": 0.8531272411346436,
      "learning_rate": 0.00012985209779655903,
      "loss": 0.1572,
      "step": 17435
    },
    {
      "epoch": 35.08249496981891,
      "grad_norm": 0.8465277552604675,
      "learning_rate": 0.00012984807324680552,
      "loss": 0.1782,
      "step": 17436
    },
    {
      "epoch": 35.08450704225352,
      "grad_norm": 0.862636148929596,
      "learning_rate": 0.00012984404869705203,
      "loss": 0.1861,
      "step": 17437
    },
    {
      "epoch": 35.08651911468813,
      "grad_norm": 0.8570570945739746,
      "learning_rate": 0.00012984002414729852,
      "loss": 0.1831,
      "step": 17438
    },
    {
      "epoch": 35.08853118712273,
      "grad_norm": 0.8518198728561401,
      "learning_rate": 0.00012983599959754503,
      "loss": 0.1767,
      "step": 17439
    },
    {
      "epoch": 35.09054325955734,
      "grad_norm": 0.830255389213562,
      "learning_rate": 0.00012983197504779154,
      "loss": 0.1717,
      "step": 17440
    },
    {
      "epoch": 35.09255533199195,
      "grad_norm": 0.8487222194671631,
      "learning_rate": 0.00012982795049803805,
      "loss": 0.185,
      "step": 17441
    },
    {
      "epoch": 35.094567404426556,
      "grad_norm": 0.8740097284317017,
      "learning_rate": 0.00012982392594828454,
      "loss": 0.1826,
      "step": 17442
    },
    {
      "epoch": 35.096579476861166,
      "grad_norm": 0.8550547957420349,
      "learning_rate": 0.00012981990139853105,
      "loss": 0.1687,
      "step": 17443
    },
    {
      "epoch": 35.098591549295776,
      "grad_norm": 0.8700578808784485,
      "learning_rate": 0.00012981587684877754,
      "loss": 0.1697,
      "step": 17444
    },
    {
      "epoch": 35.100603621730386,
      "grad_norm": 0.8054949045181274,
      "learning_rate": 0.00012981185229902405,
      "loss": 0.161,
      "step": 17445
    },
    {
      "epoch": 35.10261569416499,
      "grad_norm": 0.8741614818572998,
      "learning_rate": 0.00012980782774927056,
      "loss": 0.1914,
      "step": 17446
    },
    {
      "epoch": 35.1046277665996,
      "grad_norm": 0.8503767251968384,
      "learning_rate": 0.00012980380319951707,
      "loss": 0.1875,
      "step": 17447
    },
    {
      "epoch": 35.10663983903421,
      "grad_norm": 0.835149347782135,
      "learning_rate": 0.00012979977864976356,
      "loss": 0.1598,
      "step": 17448
    },
    {
      "epoch": 35.10865191146881,
      "grad_norm": 0.8597439527511597,
      "learning_rate": 0.00012979575410001007,
      "loss": 0.1812,
      "step": 17449
    },
    {
      "epoch": 35.11066398390342,
      "grad_norm": 0.8423987030982971,
      "learning_rate": 0.00012979172955025658,
      "loss": 0.1728,
      "step": 17450
    },
    {
      "epoch": 35.11267605633803,
      "grad_norm": 0.8104554414749146,
      "learning_rate": 0.00012978770500050307,
      "loss": 0.1704,
      "step": 17451
    },
    {
      "epoch": 35.114688128772634,
      "grad_norm": 0.8720439672470093,
      "learning_rate": 0.00012978368045074958,
      "loss": 0.1771,
      "step": 17452
    },
    {
      "epoch": 35.116700201207244,
      "grad_norm": 0.884350061416626,
      "learning_rate": 0.00012977965590099606,
      "loss": 0.1866,
      "step": 17453
    },
    {
      "epoch": 35.118712273641854,
      "grad_norm": 0.7782466411590576,
      "learning_rate": 0.00012977563135124258,
      "loss": 0.1699,
      "step": 17454
    },
    {
      "epoch": 35.12072434607646,
      "grad_norm": 0.8090137243270874,
      "learning_rate": 0.0001297716068014891,
      "loss": 0.1517,
      "step": 17455
    },
    {
      "epoch": 35.12273641851107,
      "grad_norm": 0.7974878549575806,
      "learning_rate": 0.0001297675822517356,
      "loss": 0.1685,
      "step": 17456
    },
    {
      "epoch": 35.12474849094568,
      "grad_norm": 0.8722764849662781,
      "learning_rate": 0.0001297635577019821,
      "loss": 0.1877,
      "step": 17457
    },
    {
      "epoch": 35.12676056338028,
      "grad_norm": 0.9040709137916565,
      "learning_rate": 0.0001297595331522286,
      "loss": 0.1817,
      "step": 17458
    },
    {
      "epoch": 35.12877263581489,
      "grad_norm": 0.8846301436424255,
      "learning_rate": 0.00012975550860247508,
      "loss": 0.1962,
      "step": 17459
    },
    {
      "epoch": 35.1307847082495,
      "grad_norm": 0.8322501182556152,
      "learning_rate": 0.00012975148405272162,
      "loss": 0.1616,
      "step": 17460
    },
    {
      "epoch": 35.1327967806841,
      "grad_norm": 0.842593789100647,
      "learning_rate": 0.0001297474595029681,
      "loss": 0.1873,
      "step": 17461
    },
    {
      "epoch": 35.13480885311871,
      "grad_norm": 0.8881362676620483,
      "learning_rate": 0.00012974343495321462,
      "loss": 0.1818,
      "step": 17462
    },
    {
      "epoch": 35.13682092555332,
      "grad_norm": 0.8580336570739746,
      "learning_rate": 0.0001297394104034611,
      "loss": 0.1719,
      "step": 17463
    },
    {
      "epoch": 35.138832997987926,
      "grad_norm": 0.8312364220619202,
      "learning_rate": 0.00012973538585370762,
      "loss": 0.1661,
      "step": 17464
    },
    {
      "epoch": 35.140845070422536,
      "grad_norm": 0.8290542364120483,
      "learning_rate": 0.00012973136130395413,
      "loss": 0.1885,
      "step": 17465
    },
    {
      "epoch": 35.142857142857146,
      "grad_norm": 0.9257431626319885,
      "learning_rate": 0.00012972733675420064,
      "loss": 0.1992,
      "step": 17466
    },
    {
      "epoch": 35.14486921529175,
      "grad_norm": 0.8581078052520752,
      "learning_rate": 0.00012972331220444713,
      "loss": 0.1982,
      "step": 17467
    },
    {
      "epoch": 35.14688128772636,
      "grad_norm": 0.8335732221603394,
      "learning_rate": 0.00012971928765469364,
      "loss": 0.1704,
      "step": 17468
    },
    {
      "epoch": 35.14889336016097,
      "grad_norm": 0.9010444283485413,
      "learning_rate": 0.00012971526310494012,
      "loss": 0.1789,
      "step": 17469
    },
    {
      "epoch": 35.15090543259557,
      "grad_norm": 0.8698495030403137,
      "learning_rate": 0.00012971123855518666,
      "loss": 0.1851,
      "step": 17470
    },
    {
      "epoch": 35.15291750503018,
      "grad_norm": 0.8800617456436157,
      "learning_rate": 0.00012970721400543315,
      "loss": 0.1767,
      "step": 17471
    },
    {
      "epoch": 35.15492957746479,
      "grad_norm": 0.8821764588356018,
      "learning_rate": 0.00012970318945567966,
      "loss": 0.1747,
      "step": 17472
    },
    {
      "epoch": 35.156941649899395,
      "grad_norm": 0.9108636379241943,
      "learning_rate": 0.00012969916490592615,
      "loss": 0.1676,
      "step": 17473
    },
    {
      "epoch": 35.158953722334005,
      "grad_norm": 0.8968483805656433,
      "learning_rate": 0.00012969514035617266,
      "loss": 0.1821,
      "step": 17474
    },
    {
      "epoch": 35.160965794768615,
      "grad_norm": 0.9209678769111633,
      "learning_rate": 0.00012969111580641917,
      "loss": 0.2013,
      "step": 17475
    },
    {
      "epoch": 35.16297786720322,
      "grad_norm": 0.8844730854034424,
      "learning_rate": 0.00012968709125666568,
      "loss": 0.1908,
      "step": 17476
    },
    {
      "epoch": 35.16498993963783,
      "grad_norm": 0.887607753276825,
      "learning_rate": 0.00012968306670691217,
      "loss": 0.1817,
      "step": 17477
    },
    {
      "epoch": 35.16700201207244,
      "grad_norm": 0.965266764163971,
      "learning_rate": 0.00012967904215715868,
      "loss": 0.1993,
      "step": 17478
    },
    {
      "epoch": 35.16901408450704,
      "grad_norm": 0.8761069178581238,
      "learning_rate": 0.00012967501760740517,
      "loss": 0.1884,
      "step": 17479
    },
    {
      "epoch": 35.17102615694165,
      "grad_norm": 0.8742266893386841,
      "learning_rate": 0.00012967099305765168,
      "loss": 0.1717,
      "step": 17480
    },
    {
      "epoch": 35.17303822937626,
      "grad_norm": 0.8339689373970032,
      "learning_rate": 0.0001296669685078982,
      "loss": 0.1777,
      "step": 17481
    },
    {
      "epoch": 35.17505030181086,
      "grad_norm": 0.8964095115661621,
      "learning_rate": 0.0001296629439581447,
      "loss": 0.1902,
      "step": 17482
    },
    {
      "epoch": 35.17706237424547,
      "grad_norm": 0.8672776222229004,
      "learning_rate": 0.0001296589194083912,
      "loss": 0.1685,
      "step": 17483
    },
    {
      "epoch": 35.17907444668008,
      "grad_norm": 0.8484904766082764,
      "learning_rate": 0.0001296548948586377,
      "loss": 0.1824,
      "step": 17484
    },
    {
      "epoch": 35.181086519114686,
      "grad_norm": 0.9237667918205261,
      "learning_rate": 0.0001296508703088842,
      "loss": 0.1842,
      "step": 17485
    },
    {
      "epoch": 35.183098591549296,
      "grad_norm": 0.8892666697502136,
      "learning_rate": 0.0001296468457591307,
      "loss": 0.1905,
      "step": 17486
    },
    {
      "epoch": 35.185110663983906,
      "grad_norm": 0.8478413820266724,
      "learning_rate": 0.0001296428212093772,
      "loss": 0.1861,
      "step": 17487
    },
    {
      "epoch": 35.18712273641851,
      "grad_norm": 0.8642436265945435,
      "learning_rate": 0.0001296387966596237,
      "loss": 0.1711,
      "step": 17488
    },
    {
      "epoch": 35.18913480885312,
      "grad_norm": 0.8680187463760376,
      "learning_rate": 0.0001296347721098702,
      "loss": 0.1845,
      "step": 17489
    },
    {
      "epoch": 35.19114688128773,
      "grad_norm": 0.8776922225952148,
      "learning_rate": 0.00012963074756011672,
      "loss": 0.1829,
      "step": 17490
    },
    {
      "epoch": 35.19315895372233,
      "grad_norm": 0.8656283020973206,
      "learning_rate": 0.00012962672301036323,
      "loss": 0.1855,
      "step": 17491
    },
    {
      "epoch": 35.19517102615694,
      "grad_norm": 0.8989068865776062,
      "learning_rate": 0.00012962269846060972,
      "loss": 0.2039,
      "step": 17492
    },
    {
      "epoch": 35.19718309859155,
      "grad_norm": 0.9175065755844116,
      "learning_rate": 0.00012961867391085623,
      "loss": 0.199,
      "step": 17493
    },
    {
      "epoch": 35.199195171026155,
      "grad_norm": 0.8618743419647217,
      "learning_rate": 0.0001296146493611027,
      "loss": 0.1759,
      "step": 17494
    },
    {
      "epoch": 35.201207243460765,
      "grad_norm": 0.9213539361953735,
      "learning_rate": 0.00012961062481134925,
      "loss": 0.1911,
      "step": 17495
    },
    {
      "epoch": 35.203219315895375,
      "grad_norm": 0.8712509870529175,
      "learning_rate": 0.00012960660026159574,
      "loss": 0.1828,
      "step": 17496
    },
    {
      "epoch": 35.20523138832998,
      "grad_norm": 0.8841516971588135,
      "learning_rate": 0.00012960257571184225,
      "loss": 0.1952,
      "step": 17497
    },
    {
      "epoch": 35.20724346076459,
      "grad_norm": 0.8207917809486389,
      "learning_rate": 0.00012959855116208874,
      "loss": 0.16,
      "step": 17498
    },
    {
      "epoch": 35.2092555331992,
      "grad_norm": 0.9272242784500122,
      "learning_rate": 0.00012959452661233525,
      "loss": 0.1914,
      "step": 17499
    },
    {
      "epoch": 35.2112676056338,
      "grad_norm": 0.9103237390518188,
      "learning_rate": 0.00012959050206258176,
      "loss": 0.1835,
      "step": 17500
    },
    {
      "epoch": 35.21327967806841,
      "grad_norm": 0.8439513444900513,
      "learning_rate": 0.00012958647751282827,
      "loss": 0.1811,
      "step": 17501
    },
    {
      "epoch": 35.21529175050302,
      "grad_norm": 0.9213230013847351,
      "learning_rate": 0.00012958245296307476,
      "loss": 0.2042,
      "step": 17502
    },
    {
      "epoch": 35.21730382293762,
      "grad_norm": 0.820590615272522,
      "learning_rate": 0.00012957842841332127,
      "loss": 0.1707,
      "step": 17503
    },
    {
      "epoch": 35.21931589537223,
      "grad_norm": 0.9049411416053772,
      "learning_rate": 0.00012957440386356775,
      "loss": 0.1987,
      "step": 17504
    },
    {
      "epoch": 35.22132796780684,
      "grad_norm": 0.8311417698860168,
      "learning_rate": 0.0001295703793138143,
      "loss": 0.1755,
      "step": 17505
    },
    {
      "epoch": 35.223340040241446,
      "grad_norm": 0.8697291612625122,
      "learning_rate": 0.00012956635476406078,
      "loss": 0.1926,
      "step": 17506
    },
    {
      "epoch": 35.225352112676056,
      "grad_norm": 0.9072961211204529,
      "learning_rate": 0.0001295623302143073,
      "loss": 0.1932,
      "step": 17507
    },
    {
      "epoch": 35.227364185110666,
      "grad_norm": 0.9879475235939026,
      "learning_rate": 0.00012955830566455378,
      "loss": 0.1945,
      "step": 17508
    },
    {
      "epoch": 35.22937625754527,
      "grad_norm": 0.8692785501480103,
      "learning_rate": 0.0001295542811148003,
      "loss": 0.1841,
      "step": 17509
    },
    {
      "epoch": 35.23138832997988,
      "grad_norm": 0.8821274638175964,
      "learning_rate": 0.0001295502565650468,
      "loss": 0.1774,
      "step": 17510
    },
    {
      "epoch": 35.23340040241449,
      "grad_norm": 0.8583770990371704,
      "learning_rate": 0.0001295462320152933,
      "loss": 0.1833,
      "step": 17511
    },
    {
      "epoch": 35.23541247484909,
      "grad_norm": 0.9005333185195923,
      "learning_rate": 0.0001295422074655398,
      "loss": 0.1831,
      "step": 17512
    },
    {
      "epoch": 35.2374245472837,
      "grad_norm": 1.0469701290130615,
      "learning_rate": 0.0001295381829157863,
      "loss": 0.1979,
      "step": 17513
    },
    {
      "epoch": 35.23943661971831,
      "grad_norm": 0.9675573110580444,
      "learning_rate": 0.0001295341583660328,
      "loss": 0.1952,
      "step": 17514
    },
    {
      "epoch": 35.241448692152915,
      "grad_norm": 0.9321415424346924,
      "learning_rate": 0.0001295301338162793,
      "loss": 0.1911,
      "step": 17515
    },
    {
      "epoch": 35.243460764587525,
      "grad_norm": 0.889947772026062,
      "learning_rate": 0.00012952610926652582,
      "loss": 0.1864,
      "step": 17516
    },
    {
      "epoch": 35.245472837022135,
      "grad_norm": 0.9851560592651367,
      "learning_rate": 0.00012952208471677233,
      "loss": 0.2051,
      "step": 17517
    },
    {
      "epoch": 35.24748490945674,
      "grad_norm": 0.8996028900146484,
      "learning_rate": 0.00012951806016701882,
      "loss": 0.1912,
      "step": 17518
    },
    {
      "epoch": 35.24949698189135,
      "grad_norm": 0.9067355990409851,
      "learning_rate": 0.00012951403561726533,
      "loss": 0.1846,
      "step": 17519
    },
    {
      "epoch": 35.25150905432596,
      "grad_norm": 1.0082058906555176,
      "learning_rate": 0.00012951001106751184,
      "loss": 0.1922,
      "step": 17520
    },
    {
      "epoch": 35.25352112676056,
      "grad_norm": 0.9757077693939209,
      "learning_rate": 0.00012950598651775833,
      "loss": 0.1947,
      "step": 17521
    },
    {
      "epoch": 35.25553319919517,
      "grad_norm": 0.8440011739730835,
      "learning_rate": 0.00012950196196800484,
      "loss": 0.1896,
      "step": 17522
    },
    {
      "epoch": 35.25754527162978,
      "grad_norm": 0.9092804193496704,
      "learning_rate": 0.00012949793741825132,
      "loss": 0.1982,
      "step": 17523
    },
    {
      "epoch": 35.25955734406438,
      "grad_norm": 0.864690363407135,
      "learning_rate": 0.00012949391286849784,
      "loss": 0.1869,
      "step": 17524
    },
    {
      "epoch": 35.26156941649899,
      "grad_norm": 0.9259848594665527,
      "learning_rate": 0.00012948988831874435,
      "loss": 0.1865,
      "step": 17525
    },
    {
      "epoch": 35.2635814889336,
      "grad_norm": 0.8841737508773804,
      "learning_rate": 0.00012948586376899086,
      "loss": 0.1938,
      "step": 17526
    },
    {
      "epoch": 35.265593561368206,
      "grad_norm": 0.8747533559799194,
      "learning_rate": 0.00012948183921923735,
      "loss": 0.1798,
      "step": 17527
    },
    {
      "epoch": 35.267605633802816,
      "grad_norm": 0.9339291453361511,
      "learning_rate": 0.00012947781466948386,
      "loss": 0.182,
      "step": 17528
    },
    {
      "epoch": 35.269617706237426,
      "grad_norm": 0.9136425852775574,
      "learning_rate": 0.00012947379011973034,
      "loss": 0.1736,
      "step": 17529
    },
    {
      "epoch": 35.27162977867203,
      "grad_norm": 0.8904463052749634,
      "learning_rate": 0.00012946976556997688,
      "loss": 0.1847,
      "step": 17530
    },
    {
      "epoch": 35.27364185110664,
      "grad_norm": 0.9701859354972839,
      "learning_rate": 0.00012946574102022337,
      "loss": 0.1934,
      "step": 17531
    },
    {
      "epoch": 35.27565392354125,
      "grad_norm": 0.8713693022727966,
      "learning_rate": 0.00012946171647046988,
      "loss": 0.1849,
      "step": 17532
    },
    {
      "epoch": 35.27766599597585,
      "grad_norm": 0.9271275401115417,
      "learning_rate": 0.00012945769192071636,
      "loss": 0.1912,
      "step": 17533
    },
    {
      "epoch": 35.27967806841046,
      "grad_norm": 0.9141660332679749,
      "learning_rate": 0.00012945366737096288,
      "loss": 0.2033,
      "step": 17534
    },
    {
      "epoch": 35.28169014084507,
      "grad_norm": 0.879662811756134,
      "learning_rate": 0.0001294496428212094,
      "loss": 0.1886,
      "step": 17535
    },
    {
      "epoch": 35.283702213279675,
      "grad_norm": 0.9671444296836853,
      "learning_rate": 0.0001294456182714559,
      "loss": 0.1926,
      "step": 17536
    },
    {
      "epoch": 35.285714285714285,
      "grad_norm": 0.8840683102607727,
      "learning_rate": 0.00012944159372170239,
      "loss": 0.1984,
      "step": 17537
    },
    {
      "epoch": 35.287726358148895,
      "grad_norm": 0.900234580039978,
      "learning_rate": 0.0001294375691719489,
      "loss": 0.1914,
      "step": 17538
    },
    {
      "epoch": 35.2897384305835,
      "grad_norm": 0.911247193813324,
      "learning_rate": 0.00012943354462219538,
      "loss": 0.2083,
      "step": 17539
    },
    {
      "epoch": 35.29175050301811,
      "grad_norm": 0.864787220954895,
      "learning_rate": 0.00012942952007244192,
      "loss": 0.1736,
      "step": 17540
    },
    {
      "epoch": 35.29376257545272,
      "grad_norm": 0.8620366454124451,
      "learning_rate": 0.0001294254955226884,
      "loss": 0.1873,
      "step": 17541
    },
    {
      "epoch": 35.29577464788732,
      "grad_norm": 0.9108731150627136,
      "learning_rate": 0.00012942147097293492,
      "loss": 0.2008,
      "step": 17542
    },
    {
      "epoch": 35.29778672032193,
      "grad_norm": 0.9391005635261536,
      "learning_rate": 0.0001294174464231814,
      "loss": 0.1903,
      "step": 17543
    },
    {
      "epoch": 35.29979879275654,
      "grad_norm": 0.8653300404548645,
      "learning_rate": 0.00012941342187342792,
      "loss": 0.1819,
      "step": 17544
    },
    {
      "epoch": 35.30181086519114,
      "grad_norm": 0.9582154154777527,
      "learning_rate": 0.00012940939732367443,
      "loss": 0.2012,
      "step": 17545
    },
    {
      "epoch": 35.30382293762575,
      "grad_norm": 0.9061068296432495,
      "learning_rate": 0.00012940537277392094,
      "loss": 0.1943,
      "step": 17546
    },
    {
      "epoch": 35.30583501006036,
      "grad_norm": 0.948823869228363,
      "learning_rate": 0.00012940134822416743,
      "loss": 0.1854,
      "step": 17547
    },
    {
      "epoch": 35.30784708249497,
      "grad_norm": 0.9205658435821533,
      "learning_rate": 0.00012939732367441394,
      "loss": 0.1897,
      "step": 17548
    },
    {
      "epoch": 35.309859154929576,
      "grad_norm": 0.9154027104377747,
      "learning_rate": 0.00012939329912466042,
      "loss": 0.1996,
      "step": 17549
    },
    {
      "epoch": 35.311871227364186,
      "grad_norm": 0.9390048980712891,
      "learning_rate": 0.00012938927457490694,
      "loss": 0.1995,
      "step": 17550
    },
    {
      "epoch": 35.313883299798796,
      "grad_norm": 0.9071117043495178,
      "learning_rate": 0.00012938525002515345,
      "loss": 0.1935,
      "step": 17551
    },
    {
      "epoch": 35.3158953722334,
      "grad_norm": 0.955348551273346,
      "learning_rate": 0.00012938122547539993,
      "loss": 0.1941,
      "step": 17552
    },
    {
      "epoch": 35.31790744466801,
      "grad_norm": 0.9400554895401001,
      "learning_rate": 0.00012937720092564645,
      "loss": 0.2101,
      "step": 17553
    },
    {
      "epoch": 35.31991951710262,
      "grad_norm": 1.0958493947982788,
      "learning_rate": 0.00012937317637589296,
      "loss": 0.1957,
      "step": 17554
    },
    {
      "epoch": 35.32193158953722,
      "grad_norm": 0.8951528072357178,
      "learning_rate": 0.00012936915182613947,
      "loss": 0.1905,
      "step": 17555
    },
    {
      "epoch": 35.32394366197183,
      "grad_norm": 0.917998731136322,
      "learning_rate": 0.00012936512727638596,
      "loss": 0.1969,
      "step": 17556
    },
    {
      "epoch": 35.32595573440644,
      "grad_norm": 0.9261134266853333,
      "learning_rate": 0.00012936110272663247,
      "loss": 0.1899,
      "step": 17557
    },
    {
      "epoch": 35.327967806841045,
      "grad_norm": 0.927524745464325,
      "learning_rate": 0.00012935707817687895,
      "loss": 0.1985,
      "step": 17558
    },
    {
      "epoch": 35.329979879275655,
      "grad_norm": 1.0132369995117188,
      "learning_rate": 0.00012935305362712547,
      "loss": 0.2196,
      "step": 17559
    },
    {
      "epoch": 35.331991951710265,
      "grad_norm": 0.9256591796875,
      "learning_rate": 0.00012934902907737198,
      "loss": 0.1891,
      "step": 17560
    },
    {
      "epoch": 35.33400402414487,
      "grad_norm": 0.9054227471351624,
      "learning_rate": 0.0001293450045276185,
      "loss": 0.1751,
      "step": 17561
    },
    {
      "epoch": 35.33601609657948,
      "grad_norm": 0.9055402278900146,
      "learning_rate": 0.00012934097997786497,
      "loss": 0.1803,
      "step": 17562
    },
    {
      "epoch": 35.33802816901409,
      "grad_norm": 0.8959320187568665,
      "learning_rate": 0.0001293369554281115,
      "loss": 0.2029,
      "step": 17563
    },
    {
      "epoch": 35.34004024144869,
      "grad_norm": 0.882178008556366,
      "learning_rate": 0.00012933293087835797,
      "loss": 0.1983,
      "step": 17564
    },
    {
      "epoch": 35.3420523138833,
      "grad_norm": 0.9029141068458557,
      "learning_rate": 0.0001293289063286045,
      "loss": 0.189,
      "step": 17565
    },
    {
      "epoch": 35.34406438631791,
      "grad_norm": 0.9158803820610046,
      "learning_rate": 0.000129324881778851,
      "loss": 0.1792,
      "step": 17566
    },
    {
      "epoch": 35.34607645875251,
      "grad_norm": 0.9418448805809021,
      "learning_rate": 0.0001293208572290975,
      "loss": 0.2149,
      "step": 17567
    },
    {
      "epoch": 35.34808853118712,
      "grad_norm": 0.8927502036094666,
      "learning_rate": 0.000129316832679344,
      "loss": 0.2007,
      "step": 17568
    },
    {
      "epoch": 35.35010060362173,
      "grad_norm": 0.9838874936103821,
      "learning_rate": 0.0001293128081295905,
      "loss": 0.1955,
      "step": 17569
    },
    {
      "epoch": 35.352112676056336,
      "grad_norm": 0.9113550186157227,
      "learning_rate": 0.00012930878357983702,
      "loss": 0.1881,
      "step": 17570
    },
    {
      "epoch": 35.354124748490946,
      "grad_norm": 0.8978513479232788,
      "learning_rate": 0.00012930475903008353,
      "loss": 0.1947,
      "step": 17571
    },
    {
      "epoch": 35.356136820925556,
      "grad_norm": 0.8671289086341858,
      "learning_rate": 0.00012930073448033002,
      "loss": 0.1817,
      "step": 17572
    },
    {
      "epoch": 35.35814889336016,
      "grad_norm": 0.930755615234375,
      "learning_rate": 0.00012929670993057653,
      "loss": 0.1971,
      "step": 17573
    },
    {
      "epoch": 35.36016096579477,
      "grad_norm": 0.891159176826477,
      "learning_rate": 0.000129292685380823,
      "loss": 0.1694,
      "step": 17574
    },
    {
      "epoch": 35.36217303822938,
      "grad_norm": 0.9036560654640198,
      "learning_rate": 0.00012928866083106955,
      "loss": 0.1936,
      "step": 17575
    },
    {
      "epoch": 35.36418511066398,
      "grad_norm": 0.9671895503997803,
      "learning_rate": 0.00012928463628131604,
      "loss": 0.208,
      "step": 17576
    },
    {
      "epoch": 35.36619718309859,
      "grad_norm": 1.0180089473724365,
      "learning_rate": 0.00012928061173156255,
      "loss": 0.2156,
      "step": 17577
    },
    {
      "epoch": 35.3682092555332,
      "grad_norm": 0.9483146071434021,
      "learning_rate": 0.00012927658718180903,
      "loss": 0.2131,
      "step": 17578
    },
    {
      "epoch": 35.370221327967805,
      "grad_norm": 0.9203938841819763,
      "learning_rate": 0.00012927256263205555,
      "loss": 0.2033,
      "step": 17579
    },
    {
      "epoch": 35.372233400402415,
      "grad_norm": 0.9140435457229614,
      "learning_rate": 0.00012926853808230206,
      "loss": 0.1901,
      "step": 17580
    },
    {
      "epoch": 35.374245472837025,
      "grad_norm": 0.8921238780021667,
      "learning_rate": 0.00012926451353254857,
      "loss": 0.2,
      "step": 17581
    },
    {
      "epoch": 35.37625754527163,
      "grad_norm": 0.9185987114906311,
      "learning_rate": 0.00012926048898279506,
      "loss": 0.1952,
      "step": 17582
    },
    {
      "epoch": 35.37826961770624,
      "grad_norm": 0.8893930912017822,
      "learning_rate": 0.00012925646443304157,
      "loss": 0.199,
      "step": 17583
    },
    {
      "epoch": 35.38028169014085,
      "grad_norm": 0.9721096754074097,
      "learning_rate": 0.00012925243988328805,
      "loss": 0.2071,
      "step": 17584
    },
    {
      "epoch": 35.38229376257545,
      "grad_norm": 0.9417575597763062,
      "learning_rate": 0.00012924841533353457,
      "loss": 0.2062,
      "step": 17585
    },
    {
      "epoch": 35.38430583501006,
      "grad_norm": 0.9286898374557495,
      "learning_rate": 0.00012924439078378108,
      "loss": 0.2022,
      "step": 17586
    },
    {
      "epoch": 35.38631790744467,
      "grad_norm": 0.9411964416503906,
      "learning_rate": 0.00012924036623402756,
      "loss": 0.2052,
      "step": 17587
    },
    {
      "epoch": 35.38832997987927,
      "grad_norm": 0.9520074129104614,
      "learning_rate": 0.00012923634168427408,
      "loss": 0.2023,
      "step": 17588
    },
    {
      "epoch": 35.39034205231388,
      "grad_norm": 0.8840572834014893,
      "learning_rate": 0.0001292323171345206,
      "loss": 0.195,
      "step": 17589
    },
    {
      "epoch": 35.39235412474849,
      "grad_norm": 0.9791903495788574,
      "learning_rate": 0.00012922829258476707,
      "loss": 0.1785,
      "step": 17590
    },
    {
      "epoch": 35.394366197183096,
      "grad_norm": 0.9551851153373718,
      "learning_rate": 0.00012922426803501359,
      "loss": 0.214,
      "step": 17591
    },
    {
      "epoch": 35.396378269617706,
      "grad_norm": 0.9994834065437317,
      "learning_rate": 0.0001292202434852601,
      "loss": 0.2144,
      "step": 17592
    },
    {
      "epoch": 35.398390342052316,
      "grad_norm": 0.9374477863311768,
      "learning_rate": 0.00012921621893550658,
      "loss": 0.2009,
      "step": 17593
    },
    {
      "epoch": 35.40040241448692,
      "grad_norm": 0.9305450320243835,
      "learning_rate": 0.0001292121943857531,
      "loss": 0.2036,
      "step": 17594
    },
    {
      "epoch": 35.40241448692153,
      "grad_norm": 0.9862843155860901,
      "learning_rate": 0.00012920816983599958,
      "loss": 0.2066,
      "step": 17595
    },
    {
      "epoch": 35.40442655935614,
      "grad_norm": 0.9351469874382019,
      "learning_rate": 0.00012920414528624612,
      "loss": 0.2026,
      "step": 17596
    },
    {
      "epoch": 35.40643863179074,
      "grad_norm": 0.9319257736206055,
      "learning_rate": 0.0001292001207364926,
      "loss": 0.2225,
      "step": 17597
    },
    {
      "epoch": 35.40845070422535,
      "grad_norm": 0.9200127720832825,
      "learning_rate": 0.00012919609618673912,
      "loss": 0.2127,
      "step": 17598
    },
    {
      "epoch": 35.41046277665996,
      "grad_norm": 0.9229565858840942,
      "learning_rate": 0.0001291920716369856,
      "loss": 0.1965,
      "step": 17599
    },
    {
      "epoch": 35.412474849094565,
      "grad_norm": 0.9820329546928406,
      "learning_rate": 0.00012918804708723211,
      "loss": 0.2037,
      "step": 17600
    },
    {
      "epoch": 35.414486921529175,
      "grad_norm": 0.9357258081436157,
      "learning_rate": 0.00012918402253747863,
      "loss": 0.2014,
      "step": 17601
    },
    {
      "epoch": 35.416498993963785,
      "grad_norm": 1.0124112367630005,
      "learning_rate": 0.00012917999798772514,
      "loss": 0.2178,
      "step": 17602
    },
    {
      "epoch": 35.41851106639839,
      "grad_norm": 0.9527568221092224,
      "learning_rate": 0.00012917597343797162,
      "loss": 0.2083,
      "step": 17603
    },
    {
      "epoch": 35.420523138833,
      "grad_norm": 0.9619845747947693,
      "learning_rate": 0.00012917194888821814,
      "loss": 0.2044,
      "step": 17604
    },
    {
      "epoch": 35.42253521126761,
      "grad_norm": 0.9551780223846436,
      "learning_rate": 0.00012916792433846462,
      "loss": 0.2054,
      "step": 17605
    },
    {
      "epoch": 35.42454728370221,
      "grad_norm": 0.8681256175041199,
      "learning_rate": 0.00012916389978871116,
      "loss": 0.1823,
      "step": 17606
    },
    {
      "epoch": 35.42655935613682,
      "grad_norm": 1.0199921131134033,
      "learning_rate": 0.00012915987523895765,
      "loss": 0.2149,
      "step": 17607
    },
    {
      "epoch": 35.42857142857143,
      "grad_norm": 0.935933530330658,
      "learning_rate": 0.00012915585068920416,
      "loss": 0.2033,
      "step": 17608
    },
    {
      "epoch": 35.43058350100603,
      "grad_norm": 1.0047695636749268,
      "learning_rate": 0.00012915182613945064,
      "loss": 0.2076,
      "step": 17609
    },
    {
      "epoch": 35.43259557344064,
      "grad_norm": 0.9943056106567383,
      "learning_rate": 0.00012914780158969715,
      "loss": 0.2086,
      "step": 17610
    },
    {
      "epoch": 35.43460764587525,
      "grad_norm": 1.0042353868484497,
      "learning_rate": 0.00012914377703994367,
      "loss": 0.196,
      "step": 17611
    },
    {
      "epoch": 35.436619718309856,
      "grad_norm": 0.8770574331283569,
      "learning_rate": 0.00012913975249019018,
      "loss": 0.1894,
      "step": 17612
    },
    {
      "epoch": 35.438631790744466,
      "grad_norm": 0.9055371880531311,
      "learning_rate": 0.00012913572794043666,
      "loss": 0.1927,
      "step": 17613
    },
    {
      "epoch": 35.440643863179076,
      "grad_norm": 1.1082439422607422,
      "learning_rate": 0.00012913170339068318,
      "loss": 0.2059,
      "step": 17614
    },
    {
      "epoch": 35.44265593561368,
      "grad_norm": 0.940658450126648,
      "learning_rate": 0.00012912767884092966,
      "loss": 0.1731,
      "step": 17615
    },
    {
      "epoch": 35.44466800804829,
      "grad_norm": 0.9042263031005859,
      "learning_rate": 0.0001291236542911762,
      "loss": 0.1907,
      "step": 17616
    },
    {
      "epoch": 35.4466800804829,
      "grad_norm": 0.9140515923500061,
      "learning_rate": 0.00012911962974142269,
      "loss": 0.2036,
      "step": 17617
    },
    {
      "epoch": 35.4486921529175,
      "grad_norm": 0.9270559549331665,
      "learning_rate": 0.0001291156051916692,
      "loss": 0.1991,
      "step": 17618
    },
    {
      "epoch": 35.45070422535211,
      "grad_norm": 0.9397163987159729,
      "learning_rate": 0.00012911158064191568,
      "loss": 0.2031,
      "step": 17619
    },
    {
      "epoch": 35.45271629778672,
      "grad_norm": 0.9279176592826843,
      "learning_rate": 0.0001291075560921622,
      "loss": 0.2141,
      "step": 17620
    },
    {
      "epoch": 35.454728370221325,
      "grad_norm": 1.018994927406311,
      "learning_rate": 0.0001291035315424087,
      "loss": 0.2317,
      "step": 17621
    },
    {
      "epoch": 35.456740442655935,
      "grad_norm": 0.9463991522789001,
      "learning_rate": 0.0001290995069926552,
      "loss": 0.207,
      "step": 17622
    },
    {
      "epoch": 35.458752515090545,
      "grad_norm": 0.8946922421455383,
      "learning_rate": 0.0001290954824429017,
      "loss": 0.1958,
      "step": 17623
    },
    {
      "epoch": 35.46076458752515,
      "grad_norm": 0.9359086155891418,
      "learning_rate": 0.00012909145789314822,
      "loss": 0.2048,
      "step": 17624
    },
    {
      "epoch": 35.46277665995976,
      "grad_norm": 0.8960137963294983,
      "learning_rate": 0.0001290874333433947,
      "loss": 0.1999,
      "step": 17625
    },
    {
      "epoch": 35.46478873239437,
      "grad_norm": 0.8756409287452698,
      "learning_rate": 0.00012908340879364121,
      "loss": 0.1794,
      "step": 17626
    },
    {
      "epoch": 35.46680080482897,
      "grad_norm": 0.9515095353126526,
      "learning_rate": 0.00012907938424388773,
      "loss": 0.2029,
      "step": 17627
    },
    {
      "epoch": 35.46881287726358,
      "grad_norm": 0.9576642513275146,
      "learning_rate": 0.0001290753596941342,
      "loss": 0.2004,
      "step": 17628
    },
    {
      "epoch": 35.47082494969819,
      "grad_norm": 0.9255168437957764,
      "learning_rate": 0.00012907133514438072,
      "loss": 0.1988,
      "step": 17629
    },
    {
      "epoch": 35.47283702213279,
      "grad_norm": 0.9654390811920166,
      "learning_rate": 0.0001290673105946272,
      "loss": 0.2174,
      "step": 17630
    },
    {
      "epoch": 35.4748490945674,
      "grad_norm": 1.0528396368026733,
      "learning_rate": 0.00012906328604487375,
      "loss": 0.1953,
      "step": 17631
    },
    {
      "epoch": 35.47686116700201,
      "grad_norm": 0.9667808413505554,
      "learning_rate": 0.00012905926149512023,
      "loss": 0.2139,
      "step": 17632
    },
    {
      "epoch": 35.478873239436616,
      "grad_norm": 0.9151001572608948,
      "learning_rate": 0.00012905523694536675,
      "loss": 0.2024,
      "step": 17633
    },
    {
      "epoch": 35.480885311871226,
      "grad_norm": 0.9214359521865845,
      "learning_rate": 0.00012905121239561323,
      "loss": 0.1966,
      "step": 17634
    },
    {
      "epoch": 35.482897384305836,
      "grad_norm": 1.0161124467849731,
      "learning_rate": 0.00012904718784585974,
      "loss": 0.205,
      "step": 17635
    },
    {
      "epoch": 35.48490945674044,
      "grad_norm": 0.8974433541297913,
      "learning_rate": 0.00012904316329610626,
      "loss": 0.1998,
      "step": 17636
    },
    {
      "epoch": 35.48692152917505,
      "grad_norm": 1.0408401489257812,
      "learning_rate": 0.00012903913874635277,
      "loss": 0.2106,
      "step": 17637
    },
    {
      "epoch": 35.48893360160966,
      "grad_norm": 0.9601848125457764,
      "learning_rate": 0.00012903511419659925,
      "loss": 0.2015,
      "step": 17638
    },
    {
      "epoch": 35.49094567404426,
      "grad_norm": 0.9272364974021912,
      "learning_rate": 0.00012903108964684577,
      "loss": 0.2082,
      "step": 17639
    },
    {
      "epoch": 35.49295774647887,
      "grad_norm": 0.8846498131752014,
      "learning_rate": 0.00012902706509709225,
      "loss": 0.1869,
      "step": 17640
    },
    {
      "epoch": 35.49496981891348,
      "grad_norm": 0.939723551273346,
      "learning_rate": 0.0001290230405473388,
      "loss": 0.1907,
      "step": 17641
    },
    {
      "epoch": 35.496981891348085,
      "grad_norm": 0.9193785786628723,
      "learning_rate": 0.00012901901599758527,
      "loss": 0.1914,
      "step": 17642
    },
    {
      "epoch": 35.498993963782695,
      "grad_norm": 0.9503226280212402,
      "learning_rate": 0.0001290149914478318,
      "loss": 0.2181,
      "step": 17643
    },
    {
      "epoch": 35.501006036217305,
      "grad_norm": 0.958718478679657,
      "learning_rate": 0.00012901096689807827,
      "loss": 0.2023,
      "step": 17644
    },
    {
      "epoch": 35.503018108651915,
      "grad_norm": 0.9503077268600464,
      "learning_rate": 0.00012900694234832478,
      "loss": 0.2032,
      "step": 17645
    },
    {
      "epoch": 35.50503018108652,
      "grad_norm": 0.9451696872711182,
      "learning_rate": 0.0001290029177985713,
      "loss": 0.2072,
      "step": 17646
    },
    {
      "epoch": 35.50704225352113,
      "grad_norm": 0.9586084485054016,
      "learning_rate": 0.0001289988932488178,
      "loss": 0.2018,
      "step": 17647
    },
    {
      "epoch": 35.50905432595574,
      "grad_norm": 0.9876869916915894,
      "learning_rate": 0.0001289948686990643,
      "loss": 0.2199,
      "step": 17648
    },
    {
      "epoch": 35.51106639839034,
      "grad_norm": 0.9322842359542847,
      "learning_rate": 0.0001289908441493108,
      "loss": 0.2063,
      "step": 17649
    },
    {
      "epoch": 35.51307847082495,
      "grad_norm": 0.9721313118934631,
      "learning_rate": 0.0001289868195995573,
      "loss": 0.2242,
      "step": 17650
    },
    {
      "epoch": 35.51509054325956,
      "grad_norm": 0.9067829251289368,
      "learning_rate": 0.00012898279504980383,
      "loss": 0.205,
      "step": 17651
    },
    {
      "epoch": 35.517102615694164,
      "grad_norm": 0.9556900858879089,
      "learning_rate": 0.00012897877050005032,
      "loss": 0.2012,
      "step": 17652
    },
    {
      "epoch": 35.519114688128774,
      "grad_norm": 0.929056704044342,
      "learning_rate": 0.00012897474595029683,
      "loss": 0.1984,
      "step": 17653
    },
    {
      "epoch": 35.521126760563384,
      "grad_norm": 0.9395378828048706,
      "learning_rate": 0.0001289707214005433,
      "loss": 0.2128,
      "step": 17654
    },
    {
      "epoch": 35.52313883299799,
      "grad_norm": 0.9569399356842041,
      "learning_rate": 0.00012896669685078983,
      "loss": 0.198,
      "step": 17655
    },
    {
      "epoch": 35.5251509054326,
      "grad_norm": 0.9373518228530884,
      "learning_rate": 0.00012896267230103634,
      "loss": 0.2226,
      "step": 17656
    },
    {
      "epoch": 35.52716297786721,
      "grad_norm": 1.0016193389892578,
      "learning_rate": 0.00012895864775128282,
      "loss": 0.2203,
      "step": 17657
    },
    {
      "epoch": 35.52917505030181,
      "grad_norm": 1.0208765268325806,
      "learning_rate": 0.00012895462320152933,
      "loss": 0.2112,
      "step": 17658
    },
    {
      "epoch": 35.53118712273642,
      "grad_norm": 1.0126380920410156,
      "learning_rate": 0.00012895059865177585,
      "loss": 0.231,
      "step": 17659
    },
    {
      "epoch": 35.53319919517103,
      "grad_norm": 0.914763331413269,
      "learning_rate": 0.00012894657410202233,
      "loss": 0.1926,
      "step": 17660
    },
    {
      "epoch": 35.53521126760563,
      "grad_norm": 0.8788540959358215,
      "learning_rate": 0.00012894254955226884,
      "loss": 0.215,
      "step": 17661
    },
    {
      "epoch": 35.53722334004024,
      "grad_norm": 0.9736437797546387,
      "learning_rate": 0.00012893852500251536,
      "loss": 0.2231,
      "step": 17662
    },
    {
      "epoch": 35.53923541247485,
      "grad_norm": 1.0046672821044922,
      "learning_rate": 0.00012893450045276184,
      "loss": 0.2186,
      "step": 17663
    },
    {
      "epoch": 35.541247484909455,
      "grad_norm": 0.9064528942108154,
      "learning_rate": 0.00012893047590300835,
      "loss": 0.2068,
      "step": 17664
    },
    {
      "epoch": 35.543259557344065,
      "grad_norm": 0.9300785064697266,
      "learning_rate": 0.00012892645135325484,
      "loss": 0.1996,
      "step": 17665
    },
    {
      "epoch": 35.545271629778675,
      "grad_norm": 0.9497460722923279,
      "learning_rate": 0.00012892242680350138,
      "loss": 0.1934,
      "step": 17666
    },
    {
      "epoch": 35.54728370221328,
      "grad_norm": 0.9607955813407898,
      "learning_rate": 0.00012891840225374786,
      "loss": 0.2204,
      "step": 17667
    },
    {
      "epoch": 35.54929577464789,
      "grad_norm": 0.9977003335952759,
      "learning_rate": 0.00012891437770399438,
      "loss": 0.2183,
      "step": 17668
    },
    {
      "epoch": 35.5513078470825,
      "grad_norm": 0.9644545316696167,
      "learning_rate": 0.00012891035315424086,
      "loss": 0.2157,
      "step": 17669
    },
    {
      "epoch": 35.5533199195171,
      "grad_norm": 0.9448789954185486,
      "learning_rate": 0.00012890632860448737,
      "loss": 0.2253,
      "step": 17670
    },
    {
      "epoch": 35.55533199195171,
      "grad_norm": 1.0808433294296265,
      "learning_rate": 0.00012890230405473389,
      "loss": 0.212,
      "step": 17671
    },
    {
      "epoch": 35.55734406438632,
      "grad_norm": 0.9956624507904053,
      "learning_rate": 0.0001288982795049804,
      "loss": 0.225,
      "step": 17672
    },
    {
      "epoch": 35.559356136820924,
      "grad_norm": 0.9493257999420166,
      "learning_rate": 0.00012889425495522688,
      "loss": 0.2059,
      "step": 17673
    },
    {
      "epoch": 35.561368209255534,
      "grad_norm": 0.9368183612823486,
      "learning_rate": 0.0001288902304054734,
      "loss": 0.2001,
      "step": 17674
    },
    {
      "epoch": 35.563380281690144,
      "grad_norm": 0.999043345451355,
      "learning_rate": 0.00012888620585571988,
      "loss": 0.2247,
      "step": 17675
    },
    {
      "epoch": 35.56539235412475,
      "grad_norm": 0.9474442005157471,
      "learning_rate": 0.00012888218130596642,
      "loss": 0.2025,
      "step": 17676
    },
    {
      "epoch": 35.56740442655936,
      "grad_norm": 0.956935465335846,
      "learning_rate": 0.0001288781567562129,
      "loss": 0.2155,
      "step": 17677
    },
    {
      "epoch": 35.56941649899397,
      "grad_norm": 0.937611997127533,
      "learning_rate": 0.00012887413220645942,
      "loss": 0.2064,
      "step": 17678
    },
    {
      "epoch": 35.57142857142857,
      "grad_norm": 0.9592728614807129,
      "learning_rate": 0.0001288701076567059,
      "loss": 0.2007,
      "step": 17679
    },
    {
      "epoch": 35.57344064386318,
      "grad_norm": 1.03421151638031,
      "learning_rate": 0.00012886608310695241,
      "loss": 0.2109,
      "step": 17680
    },
    {
      "epoch": 35.57545271629779,
      "grad_norm": 1.0540903806686401,
      "learning_rate": 0.00012886205855719893,
      "loss": 0.2225,
      "step": 17681
    },
    {
      "epoch": 35.57746478873239,
      "grad_norm": 1.0111169815063477,
      "learning_rate": 0.00012885803400744544,
      "loss": 0.2235,
      "step": 17682
    },
    {
      "epoch": 35.579476861167,
      "grad_norm": 0.9808695912361145,
      "learning_rate": 0.00012885400945769192,
      "loss": 0.2051,
      "step": 17683
    },
    {
      "epoch": 35.58148893360161,
      "grad_norm": 1.0239177942276,
      "learning_rate": 0.00012884998490793844,
      "loss": 0.225,
      "step": 17684
    },
    {
      "epoch": 35.583501006036215,
      "grad_norm": 0.9636493921279907,
      "learning_rate": 0.00012884596035818492,
      "loss": 0.227,
      "step": 17685
    },
    {
      "epoch": 35.585513078470825,
      "grad_norm": 0.9325666427612305,
      "learning_rate": 0.00012884193580843146,
      "loss": 0.1961,
      "step": 17686
    },
    {
      "epoch": 35.587525150905435,
      "grad_norm": 0.9583547115325928,
      "learning_rate": 0.00012883791125867795,
      "loss": 0.2221,
      "step": 17687
    },
    {
      "epoch": 35.58953722334004,
      "grad_norm": 0.9767310619354248,
      "learning_rate": 0.00012883388670892446,
      "loss": 0.2212,
      "step": 17688
    },
    {
      "epoch": 35.59154929577465,
      "grad_norm": 0.9838771820068359,
      "learning_rate": 0.00012882986215917094,
      "loss": 0.2097,
      "step": 17689
    },
    {
      "epoch": 35.59356136820926,
      "grad_norm": 0.9256525039672852,
      "learning_rate": 0.00012882583760941745,
      "loss": 0.2076,
      "step": 17690
    },
    {
      "epoch": 35.59557344064386,
      "grad_norm": 0.9387519955635071,
      "learning_rate": 0.00012882181305966397,
      "loss": 0.2188,
      "step": 17691
    },
    {
      "epoch": 35.59758551307847,
      "grad_norm": 0.9176051020622253,
      "learning_rate": 0.00012881778850991045,
      "loss": 0.2095,
      "step": 17692
    },
    {
      "epoch": 35.59959758551308,
      "grad_norm": 0.9113680124282837,
      "learning_rate": 0.00012881376396015696,
      "loss": 0.2041,
      "step": 17693
    },
    {
      "epoch": 35.601609657947684,
      "grad_norm": 1.0292531251907349,
      "learning_rate": 0.00012880973941040345,
      "loss": 0.2139,
      "step": 17694
    },
    {
      "epoch": 35.603621730382294,
      "grad_norm": 0.9731569290161133,
      "learning_rate": 0.00012880571486064996,
      "loss": 0.2157,
      "step": 17695
    },
    {
      "epoch": 35.605633802816904,
      "grad_norm": 0.8908042907714844,
      "learning_rate": 0.00012880169031089647,
      "loss": 0.1858,
      "step": 17696
    },
    {
      "epoch": 35.60764587525151,
      "grad_norm": 0.9092551469802856,
      "learning_rate": 0.00012879766576114299,
      "loss": 0.1998,
      "step": 17697
    },
    {
      "epoch": 35.60965794768612,
      "grad_norm": 0.9713405966758728,
      "learning_rate": 0.00012879364121138947,
      "loss": 0.2167,
      "step": 17698
    },
    {
      "epoch": 35.61167002012073,
      "grad_norm": 0.9391534328460693,
      "learning_rate": 0.00012878961666163598,
      "loss": 0.2149,
      "step": 17699
    },
    {
      "epoch": 35.61368209255533,
      "grad_norm": 0.9821772575378418,
      "learning_rate": 0.00012878559211188247,
      "loss": 0.2343,
      "step": 17700
    },
    {
      "epoch": 35.61569416498994,
      "grad_norm": 0.9797156453132629,
      "learning_rate": 0.000128781567562129,
      "loss": 0.2103,
      "step": 17701
    },
    {
      "epoch": 35.61770623742455,
      "grad_norm": 0.9803285002708435,
      "learning_rate": 0.0001287775430123755,
      "loss": 0.2144,
      "step": 17702
    },
    {
      "epoch": 35.61971830985915,
      "grad_norm": 0.9666596055030823,
      "learning_rate": 0.000128773518462622,
      "loss": 0.221,
      "step": 17703
    },
    {
      "epoch": 35.62173038229376,
      "grad_norm": 0.947822630405426,
      "learning_rate": 0.0001287694939128685,
      "loss": 0.2179,
      "step": 17704
    },
    {
      "epoch": 35.62374245472837,
      "grad_norm": 0.9576910734176636,
      "learning_rate": 0.000128765469363115,
      "loss": 0.2235,
      "step": 17705
    },
    {
      "epoch": 35.625754527162975,
      "grad_norm": 0.9395918250083923,
      "learning_rate": 0.00012876144481336151,
      "loss": 0.2286,
      "step": 17706
    },
    {
      "epoch": 35.627766599597585,
      "grad_norm": 1.0093685388565063,
      "learning_rate": 0.00012875742026360803,
      "loss": 0.2257,
      "step": 17707
    },
    {
      "epoch": 35.629778672032195,
      "grad_norm": 0.9535293579101562,
      "learning_rate": 0.0001287533957138545,
      "loss": 0.2201,
      "step": 17708
    },
    {
      "epoch": 35.6317907444668,
      "grad_norm": 0.9566484093666077,
      "learning_rate": 0.00012874937116410102,
      "loss": 0.2023,
      "step": 17709
    },
    {
      "epoch": 35.63380281690141,
      "grad_norm": 0.9345186352729797,
      "learning_rate": 0.0001287453466143475,
      "loss": 0.2172,
      "step": 17710
    },
    {
      "epoch": 35.63581488933602,
      "grad_norm": 0.9336296916007996,
      "learning_rate": 0.00012874132206459405,
      "loss": 0.1827,
      "step": 17711
    },
    {
      "epoch": 35.63782696177062,
      "grad_norm": 0.8844968676567078,
      "learning_rate": 0.00012873729751484053,
      "loss": 0.2065,
      "step": 17712
    },
    {
      "epoch": 35.63983903420523,
      "grad_norm": 0.9911654591560364,
      "learning_rate": 0.00012873327296508705,
      "loss": 0.2142,
      "step": 17713
    },
    {
      "epoch": 35.64185110663984,
      "grad_norm": 0.9221712946891785,
      "learning_rate": 0.00012872924841533353,
      "loss": 0.1954,
      "step": 17714
    },
    {
      "epoch": 35.643863179074444,
      "grad_norm": 0.9458476305007935,
      "learning_rate": 0.00012872522386558004,
      "loss": 0.2224,
      "step": 17715
    },
    {
      "epoch": 35.645875251509054,
      "grad_norm": 0.9386484622955322,
      "learning_rate": 0.00012872119931582656,
      "loss": 0.2113,
      "step": 17716
    },
    {
      "epoch": 35.647887323943664,
      "grad_norm": 1.0037626028060913,
      "learning_rate": 0.00012871717476607307,
      "loss": 0.2242,
      "step": 17717
    },
    {
      "epoch": 35.64989939637827,
      "grad_norm": 0.9460468888282776,
      "learning_rate": 0.00012871315021631955,
      "loss": 0.2137,
      "step": 17718
    },
    {
      "epoch": 35.65191146881288,
      "grad_norm": 0.9372426867485046,
      "learning_rate": 0.00012870912566656606,
      "loss": 0.2044,
      "step": 17719
    },
    {
      "epoch": 35.65392354124749,
      "grad_norm": 0.9296014308929443,
      "learning_rate": 0.00012870510111681255,
      "loss": 0.1884,
      "step": 17720
    },
    {
      "epoch": 35.65593561368209,
      "grad_norm": 1.044840693473816,
      "learning_rate": 0.00012870107656705906,
      "loss": 0.2203,
      "step": 17721
    },
    {
      "epoch": 35.6579476861167,
      "grad_norm": 0.9676809906959534,
      "learning_rate": 0.00012869705201730557,
      "loss": 0.2058,
      "step": 17722
    },
    {
      "epoch": 35.65995975855131,
      "grad_norm": 1.000996470451355,
      "learning_rate": 0.0001286930274675521,
      "loss": 0.2246,
      "step": 17723
    },
    {
      "epoch": 35.66197183098591,
      "grad_norm": 1.0283480882644653,
      "learning_rate": 0.00012868900291779857,
      "loss": 0.234,
      "step": 17724
    },
    {
      "epoch": 35.66398390342052,
      "grad_norm": 0.9052811861038208,
      "learning_rate": 0.00012868497836804508,
      "loss": 0.189,
      "step": 17725
    },
    {
      "epoch": 35.66599597585513,
      "grad_norm": 1.016878366470337,
      "learning_rate": 0.0001286809538182916,
      "loss": 0.2221,
      "step": 17726
    },
    {
      "epoch": 35.668008048289735,
      "grad_norm": 0.9727521538734436,
      "learning_rate": 0.00012867692926853808,
      "loss": 0.2094,
      "step": 17727
    },
    {
      "epoch": 35.670020120724345,
      "grad_norm": 0.9806614518165588,
      "learning_rate": 0.0001286729047187846,
      "loss": 0.2314,
      "step": 17728
    },
    {
      "epoch": 35.672032193158955,
      "grad_norm": 1.0070899724960327,
      "learning_rate": 0.00012866888016903108,
      "loss": 0.2176,
      "step": 17729
    },
    {
      "epoch": 35.67404426559356,
      "grad_norm": 0.9247975945472717,
      "learning_rate": 0.0001286648556192776,
      "loss": 0.1978,
      "step": 17730
    },
    {
      "epoch": 35.67605633802817,
      "grad_norm": 0.9599868655204773,
      "learning_rate": 0.0001286608310695241,
      "loss": 0.2016,
      "step": 17731
    },
    {
      "epoch": 35.67806841046278,
      "grad_norm": 0.9443963766098022,
      "learning_rate": 0.00012865680651977062,
      "loss": 0.2045,
      "step": 17732
    },
    {
      "epoch": 35.68008048289738,
      "grad_norm": 0.9806057214736938,
      "learning_rate": 0.0001286527819700171,
      "loss": 0.2105,
      "step": 17733
    },
    {
      "epoch": 35.68209255533199,
      "grad_norm": 0.9834920167922974,
      "learning_rate": 0.0001286487574202636,
      "loss": 0.2297,
      "step": 17734
    },
    {
      "epoch": 35.6841046277666,
      "grad_norm": 0.9417713284492493,
      "learning_rate": 0.0001286447328705101,
      "loss": 0.2137,
      "step": 17735
    },
    {
      "epoch": 35.686116700201204,
      "grad_norm": 0.9771921038627625,
      "learning_rate": 0.00012864070832075664,
      "loss": 0.2319,
      "step": 17736
    },
    {
      "epoch": 35.688128772635814,
      "grad_norm": 0.9251146912574768,
      "learning_rate": 0.00012863668377100312,
      "loss": 0.1983,
      "step": 17737
    },
    {
      "epoch": 35.690140845070424,
      "grad_norm": 0.9511889815330505,
      "learning_rate": 0.00012863265922124963,
      "loss": 0.2088,
      "step": 17738
    },
    {
      "epoch": 35.69215291750503,
      "grad_norm": 0.971331775188446,
      "learning_rate": 0.00012862863467149612,
      "loss": 0.2072,
      "step": 17739
    },
    {
      "epoch": 35.69416498993964,
      "grad_norm": 0.9601869583129883,
      "learning_rate": 0.00012862461012174263,
      "loss": 0.2171,
      "step": 17740
    },
    {
      "epoch": 35.69617706237425,
      "grad_norm": 0.927903950214386,
      "learning_rate": 0.00012862058557198914,
      "loss": 0.2134,
      "step": 17741
    },
    {
      "epoch": 35.69818913480886,
      "grad_norm": 0.9293296933174133,
      "learning_rate": 0.00012861656102223566,
      "loss": 0.2236,
      "step": 17742
    },
    {
      "epoch": 35.70020120724346,
      "grad_norm": 1.0357837677001953,
      "learning_rate": 0.00012861253647248214,
      "loss": 0.2064,
      "step": 17743
    },
    {
      "epoch": 35.70221327967807,
      "grad_norm": 0.9355652332305908,
      "learning_rate": 0.00012860851192272865,
      "loss": 0.2123,
      "step": 17744
    },
    {
      "epoch": 35.70422535211267,
      "grad_norm": 0.9950340986251831,
      "learning_rate": 0.00012860448737297514,
      "loss": 0.2213,
      "step": 17745
    },
    {
      "epoch": 35.70623742454728,
      "grad_norm": 1.007670283317566,
      "learning_rate": 0.00012860046282322168,
      "loss": 0.2238,
      "step": 17746
    },
    {
      "epoch": 35.70824949698189,
      "grad_norm": 0.9246439933776855,
      "learning_rate": 0.00012859643827346816,
      "loss": 0.2037,
      "step": 17747
    },
    {
      "epoch": 35.7102615694165,
      "grad_norm": 0.9702295660972595,
      "learning_rate": 0.00012859241372371468,
      "loss": 0.2139,
      "step": 17748
    },
    {
      "epoch": 35.712273641851105,
      "grad_norm": 0.9728603363037109,
      "learning_rate": 0.00012858838917396116,
      "loss": 0.2075,
      "step": 17749
    },
    {
      "epoch": 35.714285714285715,
      "grad_norm": 0.9603451490402222,
      "learning_rate": 0.00012858436462420767,
      "loss": 0.2134,
      "step": 17750
    },
    {
      "epoch": 35.716297786720325,
      "grad_norm": 1.0716216564178467,
      "learning_rate": 0.00012858034007445418,
      "loss": 0.2207,
      "step": 17751
    },
    {
      "epoch": 35.71830985915493,
      "grad_norm": 0.925812840461731,
      "learning_rate": 0.0001285763155247007,
      "loss": 0.2102,
      "step": 17752
    },
    {
      "epoch": 35.72032193158954,
      "grad_norm": 0.9982312917709351,
      "learning_rate": 0.00012857229097494718,
      "loss": 0.2285,
      "step": 17753
    },
    {
      "epoch": 35.72233400402415,
      "grad_norm": 0.9893215298652649,
      "learning_rate": 0.0001285682664251937,
      "loss": 0.2186,
      "step": 17754
    },
    {
      "epoch": 35.72434607645875,
      "grad_norm": 1.0215269327163696,
      "learning_rate": 0.00012856424187544018,
      "loss": 0.2318,
      "step": 17755
    },
    {
      "epoch": 35.72635814889336,
      "grad_norm": 0.9837622046470642,
      "learning_rate": 0.0001285602173256867,
      "loss": 0.205,
      "step": 17756
    },
    {
      "epoch": 35.72837022132797,
      "grad_norm": 0.9902937412261963,
      "learning_rate": 0.0001285561927759332,
      "loss": 0.2173,
      "step": 17757
    },
    {
      "epoch": 35.730382293762574,
      "grad_norm": 1.0261574983596802,
      "learning_rate": 0.00012855216822617972,
      "loss": 0.2224,
      "step": 17758
    },
    {
      "epoch": 35.732394366197184,
      "grad_norm": 0.9883486032485962,
      "learning_rate": 0.0001285481436764262,
      "loss": 0.2251,
      "step": 17759
    },
    {
      "epoch": 35.734406438631794,
      "grad_norm": 1.0393728017807007,
      "learning_rate": 0.0001285441191266727,
      "loss": 0.2053,
      "step": 17760
    },
    {
      "epoch": 35.7364185110664,
      "grad_norm": 1.0629565715789795,
      "learning_rate": 0.00012854009457691923,
      "loss": 0.2176,
      "step": 17761
    },
    {
      "epoch": 35.73843058350101,
      "grad_norm": 0.9430050849914551,
      "learning_rate": 0.0001285360700271657,
      "loss": 0.2154,
      "step": 17762
    },
    {
      "epoch": 35.74044265593562,
      "grad_norm": 1.0026129484176636,
      "learning_rate": 0.00012853204547741222,
      "loss": 0.2254,
      "step": 17763
    },
    {
      "epoch": 35.74245472837022,
      "grad_norm": 1.0471442937850952,
      "learning_rate": 0.0001285280209276587,
      "loss": 0.2279,
      "step": 17764
    },
    {
      "epoch": 35.74446680080483,
      "grad_norm": 0.9488499164581299,
      "learning_rate": 0.00012852399637790522,
      "loss": 0.2055,
      "step": 17765
    },
    {
      "epoch": 35.74647887323944,
      "grad_norm": 0.9930274486541748,
      "learning_rate": 0.00012851997182815173,
      "loss": 0.2208,
      "step": 17766
    },
    {
      "epoch": 35.74849094567404,
      "grad_norm": 0.9703229665756226,
      "learning_rate": 0.00012851594727839824,
      "loss": 0.1948,
      "step": 17767
    },
    {
      "epoch": 35.75050301810865,
      "grad_norm": 0.9421427249908447,
      "learning_rate": 0.00012851192272864473,
      "loss": 0.2144,
      "step": 17768
    },
    {
      "epoch": 35.75251509054326,
      "grad_norm": 0.9759481549263,
      "learning_rate": 0.00012850789817889124,
      "loss": 0.2074,
      "step": 17769
    },
    {
      "epoch": 35.754527162977865,
      "grad_norm": 1.0259119272232056,
      "learning_rate": 0.00012850387362913773,
      "loss": 0.222,
      "step": 17770
    },
    {
      "epoch": 35.756539235412475,
      "grad_norm": 0.9963964223861694,
      "learning_rate": 0.00012849984907938427,
      "loss": 0.2368,
      "step": 17771
    },
    {
      "epoch": 35.758551307847085,
      "grad_norm": 1.001762866973877,
      "learning_rate": 0.00012849582452963075,
      "loss": 0.2116,
      "step": 17772
    },
    {
      "epoch": 35.76056338028169,
      "grad_norm": 1.0001509189605713,
      "learning_rate": 0.00012849179997987726,
      "loss": 0.2436,
      "step": 17773
    },
    {
      "epoch": 35.7625754527163,
      "grad_norm": 0.9426329135894775,
      "learning_rate": 0.00012848777543012375,
      "loss": 0.2138,
      "step": 17774
    },
    {
      "epoch": 35.76458752515091,
      "grad_norm": 0.9907121658325195,
      "learning_rate": 0.00012848375088037026,
      "loss": 0.224,
      "step": 17775
    },
    {
      "epoch": 35.76659959758551,
      "grad_norm": 1.0641653537750244,
      "learning_rate": 0.00012847972633061677,
      "loss": 0.2233,
      "step": 17776
    },
    {
      "epoch": 35.76861167002012,
      "grad_norm": 0.9860101342201233,
      "learning_rate": 0.00012847570178086329,
      "loss": 0.2132,
      "step": 17777
    },
    {
      "epoch": 35.77062374245473,
      "grad_norm": 0.918503999710083,
      "learning_rate": 0.00012847167723110977,
      "loss": 0.2098,
      "step": 17778
    },
    {
      "epoch": 35.772635814889334,
      "grad_norm": 0.9796370267868042,
      "learning_rate": 0.00012846765268135628,
      "loss": 0.2249,
      "step": 17779
    },
    {
      "epoch": 35.774647887323944,
      "grad_norm": 0.9919822216033936,
      "learning_rate": 0.00012846362813160277,
      "loss": 0.226,
      "step": 17780
    },
    {
      "epoch": 35.776659959758554,
      "grad_norm": 0.9980617165565491,
      "learning_rate": 0.0001284596035818493,
      "loss": 0.2216,
      "step": 17781
    },
    {
      "epoch": 35.77867203219316,
      "grad_norm": 0.958842396736145,
      "learning_rate": 0.0001284555790320958,
      "loss": 0.2194,
      "step": 17782
    },
    {
      "epoch": 35.78068410462777,
      "grad_norm": 0.9673293232917786,
      "learning_rate": 0.0001284515544823423,
      "loss": 0.2293,
      "step": 17783
    },
    {
      "epoch": 35.78269617706238,
      "grad_norm": 1.0655492544174194,
      "learning_rate": 0.0001284475299325888,
      "loss": 0.2315,
      "step": 17784
    },
    {
      "epoch": 35.78470824949698,
      "grad_norm": 0.9847625494003296,
      "learning_rate": 0.0001284435053828353,
      "loss": 0.2269,
      "step": 17785
    },
    {
      "epoch": 35.78672032193159,
      "grad_norm": 1.0031040906906128,
      "learning_rate": 0.00012843948083308181,
      "loss": 0.228,
      "step": 17786
    },
    {
      "epoch": 35.7887323943662,
      "grad_norm": 0.9832172989845276,
      "learning_rate": 0.00012843545628332833,
      "loss": 0.2319,
      "step": 17787
    },
    {
      "epoch": 35.7907444668008,
      "grad_norm": 0.9692426919937134,
      "learning_rate": 0.0001284314317335748,
      "loss": 0.2277,
      "step": 17788
    },
    {
      "epoch": 35.79275653923541,
      "grad_norm": 0.9751691222190857,
      "learning_rate": 0.00012842740718382132,
      "loss": 0.2179,
      "step": 17789
    },
    {
      "epoch": 35.79476861167002,
      "grad_norm": 1.0113601684570312,
      "learning_rate": 0.0001284233826340678,
      "loss": 0.2259,
      "step": 17790
    },
    {
      "epoch": 35.796780684104625,
      "grad_norm": 1.0362004041671753,
      "learning_rate": 0.00012841935808431432,
      "loss": 0.242,
      "step": 17791
    },
    {
      "epoch": 35.798792756539235,
      "grad_norm": 0.985473096370697,
      "learning_rate": 0.00012841533353456083,
      "loss": 0.2158,
      "step": 17792
    },
    {
      "epoch": 35.800804828973845,
      "grad_norm": 0.9162673950195312,
      "learning_rate": 0.00012841130898480735,
      "loss": 0.2143,
      "step": 17793
    },
    {
      "epoch": 35.80281690140845,
      "grad_norm": 1.0225579738616943,
      "learning_rate": 0.00012840728443505383,
      "loss": 0.2213,
      "step": 17794
    },
    {
      "epoch": 35.80482897384306,
      "grad_norm": 0.9800941944122314,
      "learning_rate": 0.00012840325988530034,
      "loss": 0.2006,
      "step": 17795
    },
    {
      "epoch": 35.80684104627767,
      "grad_norm": 0.9749043583869934,
      "learning_rate": 0.00012839923533554686,
      "loss": 0.2211,
      "step": 17796
    },
    {
      "epoch": 35.80885311871227,
      "grad_norm": 0.9373442530632019,
      "learning_rate": 0.00012839521078579334,
      "loss": 0.2121,
      "step": 17797
    },
    {
      "epoch": 35.81086519114688,
      "grad_norm": 0.9361129403114319,
      "learning_rate": 0.00012839118623603985,
      "loss": 0.1964,
      "step": 17798
    },
    {
      "epoch": 35.81287726358149,
      "grad_norm": 1.002819299697876,
      "learning_rate": 0.00012838716168628634,
      "loss": 0.2173,
      "step": 17799
    },
    {
      "epoch": 35.814889336016094,
      "grad_norm": 1.020780324935913,
      "learning_rate": 0.00012838313713653285,
      "loss": 0.2274,
      "step": 17800
    },
    {
      "epoch": 35.816901408450704,
      "grad_norm": 0.9225071668624878,
      "learning_rate": 0.00012837911258677936,
      "loss": 0.223,
      "step": 17801
    },
    {
      "epoch": 35.818913480885314,
      "grad_norm": 0.9472986459732056,
      "learning_rate": 0.00012837508803702587,
      "loss": 0.2172,
      "step": 17802
    },
    {
      "epoch": 35.82092555331992,
      "grad_norm": 0.9490165114402771,
      "learning_rate": 0.00012837106348727236,
      "loss": 0.2154,
      "step": 17803
    },
    {
      "epoch": 35.82293762575453,
      "grad_norm": 0.991941511631012,
      "learning_rate": 0.00012836703893751887,
      "loss": 0.2301,
      "step": 17804
    },
    {
      "epoch": 35.82494969818914,
      "grad_norm": 0.9605656266212463,
      "learning_rate": 0.00012836301438776536,
      "loss": 0.2391,
      "step": 17805
    },
    {
      "epoch": 35.82696177062374,
      "grad_norm": 0.9666117429733276,
      "learning_rate": 0.0001283589898380119,
      "loss": 0.2276,
      "step": 17806
    },
    {
      "epoch": 35.82897384305835,
      "grad_norm": 1.008536696434021,
      "learning_rate": 0.00012835496528825838,
      "loss": 0.2278,
      "step": 17807
    },
    {
      "epoch": 35.83098591549296,
      "grad_norm": 0.9394202828407288,
      "learning_rate": 0.0001283509407385049,
      "loss": 0.2085,
      "step": 17808
    },
    {
      "epoch": 35.83299798792756,
      "grad_norm": 0.9184179306030273,
      "learning_rate": 0.00012834691618875138,
      "loss": 0.1953,
      "step": 17809
    },
    {
      "epoch": 35.83501006036217,
      "grad_norm": 0.9960421323776245,
      "learning_rate": 0.0001283428916389979,
      "loss": 0.2229,
      "step": 17810
    },
    {
      "epoch": 35.83702213279678,
      "grad_norm": 1.0053988695144653,
      "learning_rate": 0.0001283388670892444,
      "loss": 0.2305,
      "step": 17811
    },
    {
      "epoch": 35.839034205231385,
      "grad_norm": 0.9966028928756714,
      "learning_rate": 0.00012833484253949092,
      "loss": 0.2425,
      "step": 17812
    },
    {
      "epoch": 35.841046277665995,
      "grad_norm": 0.9056999087333679,
      "learning_rate": 0.0001283308179897374,
      "loss": 0.1991,
      "step": 17813
    },
    {
      "epoch": 35.843058350100605,
      "grad_norm": 0.9969590306282043,
      "learning_rate": 0.0001283267934399839,
      "loss": 0.2217,
      "step": 17814
    },
    {
      "epoch": 35.84507042253521,
      "grad_norm": 0.9608518481254578,
      "learning_rate": 0.0001283227688902304,
      "loss": 0.2245,
      "step": 17815
    },
    {
      "epoch": 35.84708249496982,
      "grad_norm": 0.935615062713623,
      "learning_rate": 0.00012831874434047694,
      "loss": 0.2195,
      "step": 17816
    },
    {
      "epoch": 35.84909456740443,
      "grad_norm": 0.9508013725280762,
      "learning_rate": 0.00012831471979072342,
      "loss": 0.2139,
      "step": 17817
    },
    {
      "epoch": 35.85110663983903,
      "grad_norm": 0.9815751314163208,
      "learning_rate": 0.00012831069524096993,
      "loss": 0.2218,
      "step": 17818
    },
    {
      "epoch": 35.85311871227364,
      "grad_norm": 0.9963359236717224,
      "learning_rate": 0.00012830667069121642,
      "loss": 0.2373,
      "step": 17819
    },
    {
      "epoch": 35.85513078470825,
      "grad_norm": 0.9720097184181213,
      "learning_rate": 0.00012830264614146293,
      "loss": 0.2227,
      "step": 17820
    },
    {
      "epoch": 35.857142857142854,
      "grad_norm": 0.9648193120956421,
      "learning_rate": 0.00012829862159170944,
      "loss": 0.199,
      "step": 17821
    },
    {
      "epoch": 35.859154929577464,
      "grad_norm": 0.9600873589515686,
      "learning_rate": 0.00012829459704195596,
      "loss": 0.2263,
      "step": 17822
    },
    {
      "epoch": 35.861167002012074,
      "grad_norm": 0.9899447560310364,
      "learning_rate": 0.00012829057249220244,
      "loss": 0.2255,
      "step": 17823
    },
    {
      "epoch": 35.86317907444668,
      "grad_norm": 0.9893288612365723,
      "learning_rate": 0.00012828654794244895,
      "loss": 0.2258,
      "step": 17824
    },
    {
      "epoch": 35.86519114688129,
      "grad_norm": 0.9862136244773865,
      "learning_rate": 0.00012828252339269544,
      "loss": 0.2269,
      "step": 17825
    },
    {
      "epoch": 35.8672032193159,
      "grad_norm": 0.9760927557945251,
      "learning_rate": 0.00012827849884294195,
      "loss": 0.2265,
      "step": 17826
    },
    {
      "epoch": 35.8692152917505,
      "grad_norm": 0.9717034697532654,
      "learning_rate": 0.00012827447429318846,
      "loss": 0.2338,
      "step": 17827
    },
    {
      "epoch": 35.87122736418511,
      "grad_norm": 0.9478365778923035,
      "learning_rate": 0.00012827044974343497,
      "loss": 0.2154,
      "step": 17828
    },
    {
      "epoch": 35.87323943661972,
      "grad_norm": 0.9464346170425415,
      "learning_rate": 0.00012826642519368146,
      "loss": 0.2274,
      "step": 17829
    },
    {
      "epoch": 35.87525150905432,
      "grad_norm": 0.979520320892334,
      "learning_rate": 0.00012826240064392797,
      "loss": 0.2205,
      "step": 17830
    },
    {
      "epoch": 35.87726358148893,
      "grad_norm": 0.9819939136505127,
      "learning_rate": 0.00012825837609417446,
      "loss": 0.2273,
      "step": 17831
    },
    {
      "epoch": 35.87927565392354,
      "grad_norm": 0.9908938407897949,
      "learning_rate": 0.00012825435154442097,
      "loss": 0.2196,
      "step": 17832
    },
    {
      "epoch": 35.881287726358146,
      "grad_norm": 0.9311811923980713,
      "learning_rate": 0.00012825032699466748,
      "loss": 0.2151,
      "step": 17833
    },
    {
      "epoch": 35.883299798792756,
      "grad_norm": 0.9881625771522522,
      "learning_rate": 0.00012824630244491397,
      "loss": 0.2227,
      "step": 17834
    },
    {
      "epoch": 35.885311871227366,
      "grad_norm": 0.9645034670829773,
      "learning_rate": 0.00012824227789516048,
      "loss": 0.2165,
      "step": 17835
    },
    {
      "epoch": 35.88732394366197,
      "grad_norm": 1.040771484375,
      "learning_rate": 0.000128238253345407,
      "loss": 0.2466,
      "step": 17836
    },
    {
      "epoch": 35.88933601609658,
      "grad_norm": 0.9588963389396667,
      "learning_rate": 0.0001282342287956535,
      "loss": 0.2238,
      "step": 17837
    },
    {
      "epoch": 35.89134808853119,
      "grad_norm": 1.0536359548568726,
      "learning_rate": 0.0001282302042459,
      "loss": 0.2222,
      "step": 17838
    },
    {
      "epoch": 35.89336016096579,
      "grad_norm": 1.0006954669952393,
      "learning_rate": 0.0001282261796961465,
      "loss": 0.2027,
      "step": 17839
    },
    {
      "epoch": 35.8953722334004,
      "grad_norm": 0.9540011286735535,
      "learning_rate": 0.00012822215514639299,
      "loss": 0.2058,
      "step": 17840
    },
    {
      "epoch": 35.89738430583501,
      "grad_norm": 1.0186760425567627,
      "learning_rate": 0.0001282181305966395,
      "loss": 0.2316,
      "step": 17841
    },
    {
      "epoch": 35.899396378269614,
      "grad_norm": 1.0014721155166626,
      "learning_rate": 0.000128214106046886,
      "loss": 0.2138,
      "step": 17842
    },
    {
      "epoch": 35.901408450704224,
      "grad_norm": 1.0191954374313354,
      "learning_rate": 0.00012821008149713252,
      "loss": 0.2336,
      "step": 17843
    },
    {
      "epoch": 35.903420523138834,
      "grad_norm": 1.015950322151184,
      "learning_rate": 0.000128206056947379,
      "loss": 0.2263,
      "step": 17844
    },
    {
      "epoch": 35.905432595573444,
      "grad_norm": 0.9736288785934448,
      "learning_rate": 0.00012820203239762552,
      "loss": 0.2085,
      "step": 17845
    },
    {
      "epoch": 35.90744466800805,
      "grad_norm": 1.002026081085205,
      "learning_rate": 0.000128198007847872,
      "loss": 0.2413,
      "step": 17846
    },
    {
      "epoch": 35.90945674044266,
      "grad_norm": 1.0003564357757568,
      "learning_rate": 0.00012819398329811854,
      "loss": 0.2288,
      "step": 17847
    },
    {
      "epoch": 35.91146881287727,
      "grad_norm": 1.0120720863342285,
      "learning_rate": 0.00012818995874836503,
      "loss": 0.2267,
      "step": 17848
    },
    {
      "epoch": 35.91348088531187,
      "grad_norm": 0.9511741995811462,
      "learning_rate": 0.00012818593419861154,
      "loss": 0.2107,
      "step": 17849
    },
    {
      "epoch": 35.91549295774648,
      "grad_norm": 1.0234960317611694,
      "learning_rate": 0.00012818190964885803,
      "loss": 0.2428,
      "step": 17850
    },
    {
      "epoch": 35.91750503018109,
      "grad_norm": 0.9538518786430359,
      "learning_rate": 0.00012817788509910454,
      "loss": 0.2154,
      "step": 17851
    },
    {
      "epoch": 35.91951710261569,
      "grad_norm": 1.0054742097854614,
      "learning_rate": 0.00012817386054935105,
      "loss": 0.2423,
      "step": 17852
    },
    {
      "epoch": 35.9215291750503,
      "grad_norm": 0.9675396680831909,
      "learning_rate": 0.00012816983599959756,
      "loss": 0.2192,
      "step": 17853
    },
    {
      "epoch": 35.92354124748491,
      "grad_norm": 0.9909349083900452,
      "learning_rate": 0.00012816581144984405,
      "loss": 0.2178,
      "step": 17854
    },
    {
      "epoch": 35.925553319919516,
      "grad_norm": 0.980894148349762,
      "learning_rate": 0.00012816178690009056,
      "loss": 0.2343,
      "step": 17855
    },
    {
      "epoch": 35.927565392354126,
      "grad_norm": 1.0248692035675049,
      "learning_rate": 0.00012815776235033705,
      "loss": 0.2127,
      "step": 17856
    },
    {
      "epoch": 35.929577464788736,
      "grad_norm": 0.9961259365081787,
      "learning_rate": 0.00012815373780058359,
      "loss": 0.2364,
      "step": 17857
    },
    {
      "epoch": 35.93158953722334,
      "grad_norm": 1.0157681703567505,
      "learning_rate": 0.00012814971325083007,
      "loss": 0.23,
      "step": 17858
    },
    {
      "epoch": 35.93360160965795,
      "grad_norm": 0.9424781203269958,
      "learning_rate": 0.00012814568870107658,
      "loss": 0.2133,
      "step": 17859
    },
    {
      "epoch": 35.93561368209256,
      "grad_norm": 0.9501153230667114,
      "learning_rate": 0.00012814166415132307,
      "loss": 0.2184,
      "step": 17860
    },
    {
      "epoch": 35.93762575452716,
      "grad_norm": 0.9798370003700256,
      "learning_rate": 0.00012813763960156958,
      "loss": 0.2309,
      "step": 17861
    },
    {
      "epoch": 35.93963782696177,
      "grad_norm": 0.9824025630950928,
      "learning_rate": 0.0001281336150518161,
      "loss": 0.2325,
      "step": 17862
    },
    {
      "epoch": 35.94164989939638,
      "grad_norm": 1.0249017477035522,
      "learning_rate": 0.0001281295905020626,
      "loss": 0.2327,
      "step": 17863
    },
    {
      "epoch": 35.943661971830984,
      "grad_norm": 0.9677248597145081,
      "learning_rate": 0.0001281255659523091,
      "loss": 0.2191,
      "step": 17864
    },
    {
      "epoch": 35.945674044265594,
      "grad_norm": 0.9747511148452759,
      "learning_rate": 0.0001281215414025556,
      "loss": 0.2175,
      "step": 17865
    },
    {
      "epoch": 35.947686116700204,
      "grad_norm": 1.0625934600830078,
      "learning_rate": 0.0001281175168528021,
      "loss": 0.2495,
      "step": 17866
    },
    {
      "epoch": 35.94969818913481,
      "grad_norm": 0.9772757887840271,
      "learning_rate": 0.0001281134923030486,
      "loss": 0.2299,
      "step": 17867
    },
    {
      "epoch": 35.95171026156942,
      "grad_norm": 0.978615403175354,
      "learning_rate": 0.0001281094677532951,
      "loss": 0.2324,
      "step": 17868
    },
    {
      "epoch": 35.95372233400403,
      "grad_norm": 1.0808602571487427,
      "learning_rate": 0.0001281054432035416,
      "loss": 0.2237,
      "step": 17869
    },
    {
      "epoch": 35.95573440643863,
      "grad_norm": 1.041951298713684,
      "learning_rate": 0.0001281014186537881,
      "loss": 0.2396,
      "step": 17870
    },
    {
      "epoch": 35.95774647887324,
      "grad_norm": 1.0526676177978516,
      "learning_rate": 0.0001280973941040346,
      "loss": 0.2487,
      "step": 17871
    },
    {
      "epoch": 35.95975855130785,
      "grad_norm": 0.9744648337364197,
      "learning_rate": 0.00012809336955428113,
      "loss": 0.2201,
      "step": 17872
    },
    {
      "epoch": 35.96177062374245,
      "grad_norm": 0.9872030019760132,
      "learning_rate": 0.00012808934500452762,
      "loss": 0.2194,
      "step": 17873
    },
    {
      "epoch": 35.96378269617706,
      "grad_norm": 0.9814032912254333,
      "learning_rate": 0.00012808532045477413,
      "loss": 0.2209,
      "step": 17874
    },
    {
      "epoch": 35.96579476861167,
      "grad_norm": 1.0228455066680908,
      "learning_rate": 0.00012808129590502062,
      "loss": 0.2262,
      "step": 17875
    },
    {
      "epoch": 35.967806841046276,
      "grad_norm": 0.918724775314331,
      "learning_rate": 0.00012807727135526713,
      "loss": 0.2236,
      "step": 17876
    },
    {
      "epoch": 35.969818913480886,
      "grad_norm": 1.0649527311325073,
      "learning_rate": 0.00012807324680551364,
      "loss": 0.2334,
      "step": 17877
    },
    {
      "epoch": 35.971830985915496,
      "grad_norm": 0.9410291314125061,
      "learning_rate": 0.00012806922225576015,
      "loss": 0.2306,
      "step": 17878
    },
    {
      "epoch": 35.9738430583501,
      "grad_norm": 0.9879575371742249,
      "learning_rate": 0.00012806519770600664,
      "loss": 0.2278,
      "step": 17879
    },
    {
      "epoch": 35.97585513078471,
      "grad_norm": 0.9527456164360046,
      "learning_rate": 0.00012806117315625315,
      "loss": 0.2139,
      "step": 17880
    },
    {
      "epoch": 35.97786720321932,
      "grad_norm": 0.9646319150924683,
      "learning_rate": 0.00012805714860649963,
      "loss": 0.2288,
      "step": 17881
    },
    {
      "epoch": 35.97987927565392,
      "grad_norm": 0.9637990593910217,
      "learning_rate": 0.00012805312405674617,
      "loss": 0.2505,
      "step": 17882
    },
    {
      "epoch": 35.98189134808853,
      "grad_norm": 1.045090913772583,
      "learning_rate": 0.00012804909950699266,
      "loss": 0.2215,
      "step": 17883
    },
    {
      "epoch": 35.98390342052314,
      "grad_norm": 0.9665520191192627,
      "learning_rate": 0.00012804507495723917,
      "loss": 0.2258,
      "step": 17884
    },
    {
      "epoch": 35.985915492957744,
      "grad_norm": 0.9697633981704712,
      "learning_rate": 0.00012804105040748566,
      "loss": 0.2239,
      "step": 17885
    },
    {
      "epoch": 35.987927565392354,
      "grad_norm": 0.9839335680007935,
      "learning_rate": 0.00012803702585773217,
      "loss": 0.2152,
      "step": 17886
    },
    {
      "epoch": 35.989939637826964,
      "grad_norm": 0.9827600121498108,
      "learning_rate": 0.00012803300130797868,
      "loss": 0.2248,
      "step": 17887
    },
    {
      "epoch": 35.99195171026157,
      "grad_norm": 1.0216424465179443,
      "learning_rate": 0.0001280289767582252,
      "loss": 0.2424,
      "step": 17888
    },
    {
      "epoch": 35.99396378269618,
      "grad_norm": 0.9382448196411133,
      "learning_rate": 0.00012802495220847168,
      "loss": 0.209,
      "step": 17889
    },
    {
      "epoch": 35.99597585513079,
      "grad_norm": 1.0264779329299927,
      "learning_rate": 0.0001280209276587182,
      "loss": 0.2173,
      "step": 17890
    },
    {
      "epoch": 35.99798792756539,
      "grad_norm": 1.0308477878570557,
      "learning_rate": 0.00012801690310896468,
      "loss": 0.2226,
      "step": 17891
    },
    {
      "epoch": 36.0,
      "grad_norm": 1.0797473192214966,
      "learning_rate": 0.00012801287855921121,
      "loss": 0.2452,
      "step": 17892
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.4942907094955444,
      "eval_runtime": 49.7881,
      "eval_samples_per_second": 19.924,
      "eval_steps_per_second": 2.491,
      "step": 17892
    },
    {
      "epoch": 36.00201207243461,
      "grad_norm": 0.8126774430274963,
      "learning_rate": 0.0001280088540094577,
      "loss": 0.1694,
      "step": 17893
    },
    {
      "epoch": 36.00402414486921,
      "grad_norm": 0.7842492461204529,
      "learning_rate": 0.0001280048294597042,
      "loss": 0.1604,
      "step": 17894
    },
    {
      "epoch": 36.00603621730382,
      "grad_norm": 0.7829332947731018,
      "learning_rate": 0.0001280008049099507,
      "loss": 0.1801,
      "step": 17895
    },
    {
      "epoch": 36.00804828973843,
      "grad_norm": 0.9113848805427551,
      "learning_rate": 0.0001279967803601972,
      "loss": 0.1908,
      "step": 17896
    },
    {
      "epoch": 36.010060362173036,
      "grad_norm": 0.8657130002975464,
      "learning_rate": 0.00012799275581044372,
      "loss": 0.1738,
      "step": 17897
    },
    {
      "epoch": 36.012072434607646,
      "grad_norm": 0.9490077495574951,
      "learning_rate": 0.0001279887312606902,
      "loss": 0.1959,
      "step": 17898
    },
    {
      "epoch": 36.014084507042256,
      "grad_norm": 0.857729434967041,
      "learning_rate": 0.00012798470671093672,
      "loss": 0.1692,
      "step": 17899
    },
    {
      "epoch": 36.01609657947686,
      "grad_norm": 0.8489529490470886,
      "learning_rate": 0.00012798068216118323,
      "loss": 0.1798,
      "step": 17900
    },
    {
      "epoch": 36.01810865191147,
      "grad_norm": 0.8537597060203552,
      "learning_rate": 0.00012797665761142972,
      "loss": 0.1673,
      "step": 17901
    },
    {
      "epoch": 36.02012072434608,
      "grad_norm": 0.8441588878631592,
      "learning_rate": 0.00012797263306167623,
      "loss": 0.1626,
      "step": 17902
    },
    {
      "epoch": 36.02213279678068,
      "grad_norm": 0.9161665439605713,
      "learning_rate": 0.00012796860851192274,
      "loss": 0.2031,
      "step": 17903
    },
    {
      "epoch": 36.02414486921529,
      "grad_norm": 0.881584644317627,
      "learning_rate": 0.00012796458396216923,
      "loss": 0.1753,
      "step": 17904
    },
    {
      "epoch": 36.0261569416499,
      "grad_norm": 0.8410083055496216,
      "learning_rate": 0.00012796055941241574,
      "loss": 0.171,
      "step": 17905
    },
    {
      "epoch": 36.028169014084504,
      "grad_norm": 0.819377064704895,
      "learning_rate": 0.00012795653486266222,
      "loss": 0.1658,
      "step": 17906
    },
    {
      "epoch": 36.030181086519114,
      "grad_norm": 0.8649119734764099,
      "learning_rate": 0.00012795251031290876,
      "loss": 0.171,
      "step": 17907
    },
    {
      "epoch": 36.032193158953724,
      "grad_norm": 0.9417923092842102,
      "learning_rate": 0.00012794848576315525,
      "loss": 0.162,
      "step": 17908
    },
    {
      "epoch": 36.03420523138833,
      "grad_norm": 0.9382818341255188,
      "learning_rate": 0.00012794446121340176,
      "loss": 0.1745,
      "step": 17909
    },
    {
      "epoch": 36.03621730382294,
      "grad_norm": 0.9312732815742493,
      "learning_rate": 0.00012794043666364824,
      "loss": 0.1825,
      "step": 17910
    },
    {
      "epoch": 36.03822937625755,
      "grad_norm": 0.8945172429084778,
      "learning_rate": 0.00012793641211389476,
      "loss": 0.1715,
      "step": 17911
    },
    {
      "epoch": 36.04024144869215,
      "grad_norm": 0.8992677927017212,
      "learning_rate": 0.00012793238756414127,
      "loss": 0.1731,
      "step": 17912
    },
    {
      "epoch": 36.04225352112676,
      "grad_norm": 0.8389179110527039,
      "learning_rate": 0.00012792836301438778,
      "loss": 0.1798,
      "step": 17913
    },
    {
      "epoch": 36.04426559356137,
      "grad_norm": 0.8958104252815247,
      "learning_rate": 0.00012792433846463427,
      "loss": 0.1887,
      "step": 17914
    },
    {
      "epoch": 36.04627766599597,
      "grad_norm": 0.8869601488113403,
      "learning_rate": 0.00012792031391488078,
      "loss": 0.1814,
      "step": 17915
    },
    {
      "epoch": 36.04828973843058,
      "grad_norm": 0.876608669757843,
      "learning_rate": 0.00012791628936512726,
      "loss": 0.1867,
      "step": 17916
    },
    {
      "epoch": 36.05030181086519,
      "grad_norm": 0.8992058038711548,
      "learning_rate": 0.0001279122648153738,
      "loss": 0.1694,
      "step": 17917
    },
    {
      "epoch": 36.052313883299796,
      "grad_norm": 0.8655564785003662,
      "learning_rate": 0.0001279082402656203,
      "loss": 0.1744,
      "step": 17918
    },
    {
      "epoch": 36.054325955734406,
      "grad_norm": 0.9314749836921692,
      "learning_rate": 0.0001279042157158668,
      "loss": 0.1897,
      "step": 17919
    },
    {
      "epoch": 36.056338028169016,
      "grad_norm": 0.8333026170730591,
      "learning_rate": 0.00012790019116611329,
      "loss": 0.1728,
      "step": 17920
    },
    {
      "epoch": 36.05835010060362,
      "grad_norm": 0.88865065574646,
      "learning_rate": 0.0001278961666163598,
      "loss": 0.1691,
      "step": 17921
    },
    {
      "epoch": 36.06036217303823,
      "grad_norm": 0.9114347696304321,
      "learning_rate": 0.0001278921420666063,
      "loss": 0.1741,
      "step": 17922
    },
    {
      "epoch": 36.06237424547284,
      "grad_norm": 0.9631800055503845,
      "learning_rate": 0.00012788811751685282,
      "loss": 0.1657,
      "step": 17923
    },
    {
      "epoch": 36.06438631790744,
      "grad_norm": 0.9149724841117859,
      "learning_rate": 0.0001278840929670993,
      "loss": 0.1864,
      "step": 17924
    },
    {
      "epoch": 36.06639839034205,
      "grad_norm": 0.9006830453872681,
      "learning_rate": 0.00012788006841734582,
      "loss": 0.177,
      "step": 17925
    },
    {
      "epoch": 36.06841046277666,
      "grad_norm": 0.8794630765914917,
      "learning_rate": 0.0001278760438675923,
      "loss": 0.1798,
      "step": 17926
    },
    {
      "epoch": 36.070422535211264,
      "grad_norm": 0.8780504465103149,
      "learning_rate": 0.00012787201931783884,
      "loss": 0.1832,
      "step": 17927
    },
    {
      "epoch": 36.072434607645874,
      "grad_norm": 0.8827845454216003,
      "learning_rate": 0.00012786799476808533,
      "loss": 0.1621,
      "step": 17928
    },
    {
      "epoch": 36.074446680080484,
      "grad_norm": 0.8315541744232178,
      "learning_rate": 0.00012786397021833184,
      "loss": 0.1569,
      "step": 17929
    },
    {
      "epoch": 36.07645875251509,
      "grad_norm": 0.8504188060760498,
      "learning_rate": 0.00012785994566857833,
      "loss": 0.1815,
      "step": 17930
    },
    {
      "epoch": 36.0784708249497,
      "grad_norm": 0.8645377159118652,
      "learning_rate": 0.00012785592111882484,
      "loss": 0.182,
      "step": 17931
    },
    {
      "epoch": 36.08048289738431,
      "grad_norm": 0.8307433128356934,
      "learning_rate": 0.00012785189656907135,
      "loss": 0.1632,
      "step": 17932
    },
    {
      "epoch": 36.08249496981891,
      "grad_norm": 0.9026830792427063,
      "learning_rate": 0.00012784787201931784,
      "loss": 0.1804,
      "step": 17933
    },
    {
      "epoch": 36.08450704225352,
      "grad_norm": 0.8318089246749878,
      "learning_rate": 0.00012784384746956435,
      "loss": 0.1496,
      "step": 17934
    },
    {
      "epoch": 36.08651911468813,
      "grad_norm": 0.840300440788269,
      "learning_rate": 0.00012783982291981086,
      "loss": 0.166,
      "step": 17935
    },
    {
      "epoch": 36.08853118712273,
      "grad_norm": 0.7915306687355042,
      "learning_rate": 0.00012783579837005735,
      "loss": 0.1556,
      "step": 17936
    },
    {
      "epoch": 36.09054325955734,
      "grad_norm": 0.8493560552597046,
      "learning_rate": 0.00012783177382030386,
      "loss": 0.1792,
      "step": 17937
    },
    {
      "epoch": 36.09255533199195,
      "grad_norm": 0.898481011390686,
      "learning_rate": 0.00012782774927055037,
      "loss": 0.1706,
      "step": 17938
    },
    {
      "epoch": 36.094567404426556,
      "grad_norm": 0.8465272784233093,
      "learning_rate": 0.00012782372472079686,
      "loss": 0.1794,
      "step": 17939
    },
    {
      "epoch": 36.096579476861166,
      "grad_norm": 0.7955624461174011,
      "learning_rate": 0.00012781970017104337,
      "loss": 0.152,
      "step": 17940
    },
    {
      "epoch": 36.098591549295776,
      "grad_norm": 0.8783683776855469,
      "learning_rate": 0.00012781567562128985,
      "loss": 0.1909,
      "step": 17941
    },
    {
      "epoch": 36.100603621730386,
      "grad_norm": 0.8823705315589905,
      "learning_rate": 0.0001278116510715364,
      "loss": 0.1785,
      "step": 17942
    },
    {
      "epoch": 36.10261569416499,
      "grad_norm": 0.9132555723190308,
      "learning_rate": 0.00012780762652178288,
      "loss": 0.1998,
      "step": 17943
    },
    {
      "epoch": 36.1046277665996,
      "grad_norm": 0.8610374927520752,
      "learning_rate": 0.0001278036019720294,
      "loss": 0.166,
      "step": 17944
    },
    {
      "epoch": 36.10663983903421,
      "grad_norm": 0.8544468283653259,
      "learning_rate": 0.00012779957742227587,
      "loss": 0.1667,
      "step": 17945
    },
    {
      "epoch": 36.10865191146881,
      "grad_norm": 0.8928558826446533,
      "learning_rate": 0.0001277955528725224,
      "loss": 0.1886,
      "step": 17946
    },
    {
      "epoch": 36.11066398390342,
      "grad_norm": 0.8458271622657776,
      "learning_rate": 0.0001277915283227689,
      "loss": 0.1626,
      "step": 17947
    },
    {
      "epoch": 36.11267605633803,
      "grad_norm": 0.853843629360199,
      "learning_rate": 0.0001277875037730154,
      "loss": 0.1671,
      "step": 17948
    },
    {
      "epoch": 36.114688128772634,
      "grad_norm": 0.8668099045753479,
      "learning_rate": 0.0001277834792232619,
      "loss": 0.1756,
      "step": 17949
    },
    {
      "epoch": 36.116700201207244,
      "grad_norm": 0.9416689276695251,
      "learning_rate": 0.0001277794546735084,
      "loss": 0.1749,
      "step": 17950
    },
    {
      "epoch": 36.118712273641854,
      "grad_norm": 0.9020906686782837,
      "learning_rate": 0.0001277754301237549,
      "loss": 0.1776,
      "step": 17951
    },
    {
      "epoch": 36.12072434607646,
      "grad_norm": 0.8109197020530701,
      "learning_rate": 0.00012777140557400143,
      "loss": 0.1586,
      "step": 17952
    },
    {
      "epoch": 36.12273641851107,
      "grad_norm": 0.9172695875167847,
      "learning_rate": 0.00012776738102424792,
      "loss": 0.1764,
      "step": 17953
    },
    {
      "epoch": 36.12474849094568,
      "grad_norm": 0.862108051776886,
      "learning_rate": 0.00012776335647449443,
      "loss": 0.1619,
      "step": 17954
    },
    {
      "epoch": 36.12676056338028,
      "grad_norm": 0.943254292011261,
      "learning_rate": 0.00012775933192474092,
      "loss": 0.1942,
      "step": 17955
    },
    {
      "epoch": 36.12877263581489,
      "grad_norm": 0.8607880473136902,
      "learning_rate": 0.00012775530737498743,
      "loss": 0.1578,
      "step": 17956
    },
    {
      "epoch": 36.1307847082495,
      "grad_norm": 0.8421945571899414,
      "learning_rate": 0.00012775128282523394,
      "loss": 0.1654,
      "step": 17957
    },
    {
      "epoch": 36.1327967806841,
      "grad_norm": 0.9286727905273438,
      "learning_rate": 0.00012774725827548045,
      "loss": 0.1871,
      "step": 17958
    },
    {
      "epoch": 36.13480885311871,
      "grad_norm": 0.8887239694595337,
      "learning_rate": 0.00012774323372572694,
      "loss": 0.1604,
      "step": 17959
    },
    {
      "epoch": 36.13682092555332,
      "grad_norm": 0.8624873757362366,
      "learning_rate": 0.00012773920917597345,
      "loss": 0.1628,
      "step": 17960
    },
    {
      "epoch": 36.138832997987926,
      "grad_norm": 0.892342209815979,
      "learning_rate": 0.00012773518462621993,
      "loss": 0.1758,
      "step": 17961
    },
    {
      "epoch": 36.140845070422536,
      "grad_norm": 0.8965256810188293,
      "learning_rate": 0.00012773116007646647,
      "loss": 0.1754,
      "step": 17962
    },
    {
      "epoch": 36.142857142857146,
      "grad_norm": 0.8618078827857971,
      "learning_rate": 0.00012772713552671296,
      "loss": 0.2022,
      "step": 17963
    },
    {
      "epoch": 36.14486921529175,
      "grad_norm": 1.0135095119476318,
      "learning_rate": 0.00012772311097695947,
      "loss": 0.181,
      "step": 17964
    },
    {
      "epoch": 36.14688128772636,
      "grad_norm": 0.9207732677459717,
      "learning_rate": 0.00012771908642720596,
      "loss": 0.1829,
      "step": 17965
    },
    {
      "epoch": 36.14889336016097,
      "grad_norm": 0.8489861488342285,
      "learning_rate": 0.00012771506187745247,
      "loss": 0.1661,
      "step": 17966
    },
    {
      "epoch": 36.15090543259557,
      "grad_norm": 0.8546277284622192,
      "learning_rate": 0.00012771103732769898,
      "loss": 0.1709,
      "step": 17967
    },
    {
      "epoch": 36.15291750503018,
      "grad_norm": 0.932617723941803,
      "learning_rate": 0.00012770701277794547,
      "loss": 0.1979,
      "step": 17968
    },
    {
      "epoch": 36.15492957746479,
      "grad_norm": 0.9164701104164124,
      "learning_rate": 0.00012770298822819198,
      "loss": 0.1854,
      "step": 17969
    },
    {
      "epoch": 36.156941649899395,
      "grad_norm": 0.9186447262763977,
      "learning_rate": 0.0001276989636784385,
      "loss": 0.174,
      "step": 17970
    },
    {
      "epoch": 36.158953722334005,
      "grad_norm": 0.8842740654945374,
      "learning_rate": 0.00012769493912868498,
      "loss": 0.1631,
      "step": 17971
    },
    {
      "epoch": 36.160965794768615,
      "grad_norm": 0.8680692315101624,
      "learning_rate": 0.0001276909145789315,
      "loss": 0.1572,
      "step": 17972
    },
    {
      "epoch": 36.16297786720322,
      "grad_norm": 0.8688825964927673,
      "learning_rate": 0.000127686890029178,
      "loss": 0.1646,
      "step": 17973
    },
    {
      "epoch": 36.16498993963783,
      "grad_norm": 0.88252854347229,
      "learning_rate": 0.00012768286547942448,
      "loss": 0.1802,
      "step": 17974
    },
    {
      "epoch": 36.16700201207244,
      "grad_norm": 0.8987217545509338,
      "learning_rate": 0.000127678840929671,
      "loss": 0.1869,
      "step": 17975
    },
    {
      "epoch": 36.16901408450704,
      "grad_norm": 0.8659581542015076,
      "learning_rate": 0.00012767481637991748,
      "loss": 0.1737,
      "step": 17976
    },
    {
      "epoch": 36.17102615694165,
      "grad_norm": 0.861679196357727,
      "learning_rate": 0.00012767079183016402,
      "loss": 0.1611,
      "step": 17977
    },
    {
      "epoch": 36.17303822937626,
      "grad_norm": 0.8979695439338684,
      "learning_rate": 0.0001276667672804105,
      "loss": 0.1722,
      "step": 17978
    },
    {
      "epoch": 36.17505030181086,
      "grad_norm": 0.9291008114814758,
      "learning_rate": 0.00012766274273065702,
      "loss": 0.1739,
      "step": 17979
    },
    {
      "epoch": 36.17706237424547,
      "grad_norm": 0.9004490375518799,
      "learning_rate": 0.0001276587181809035,
      "loss": 0.1837,
      "step": 17980
    },
    {
      "epoch": 36.17907444668008,
      "grad_norm": 0.8552850484848022,
      "learning_rate": 0.00012765469363115002,
      "loss": 0.1796,
      "step": 17981
    },
    {
      "epoch": 36.181086519114686,
      "grad_norm": 0.9052703976631165,
      "learning_rate": 0.00012765066908139653,
      "loss": 0.1741,
      "step": 17982
    },
    {
      "epoch": 36.183098591549296,
      "grad_norm": 0.9243050217628479,
      "learning_rate": 0.00012764664453164304,
      "loss": 0.1929,
      "step": 17983
    },
    {
      "epoch": 36.185110663983906,
      "grad_norm": 0.8982521891593933,
      "learning_rate": 0.00012764261998188953,
      "loss": 0.1878,
      "step": 17984
    },
    {
      "epoch": 36.18712273641851,
      "grad_norm": 0.9441354274749756,
      "learning_rate": 0.00012763859543213604,
      "loss": 0.1892,
      "step": 17985
    },
    {
      "epoch": 36.18913480885312,
      "grad_norm": 0.9326037764549255,
      "learning_rate": 0.00012763457088238252,
      "loss": 0.1855,
      "step": 17986
    },
    {
      "epoch": 36.19114688128773,
      "grad_norm": 0.91374272108078,
      "learning_rate": 0.00012763054633262906,
      "loss": 0.1833,
      "step": 17987
    },
    {
      "epoch": 36.19315895372233,
      "grad_norm": 0.8481480479240417,
      "learning_rate": 0.00012762652178287555,
      "loss": 0.1671,
      "step": 17988
    },
    {
      "epoch": 36.19517102615694,
      "grad_norm": 1.0011751651763916,
      "learning_rate": 0.00012762249723312206,
      "loss": 0.1941,
      "step": 17989
    },
    {
      "epoch": 36.19718309859155,
      "grad_norm": 0.8719525933265686,
      "learning_rate": 0.00012761847268336854,
      "loss": 0.1608,
      "step": 17990
    },
    {
      "epoch": 36.199195171026155,
      "grad_norm": 0.930381715297699,
      "learning_rate": 0.00012761444813361506,
      "loss": 0.179,
      "step": 17991
    },
    {
      "epoch": 36.201207243460765,
      "grad_norm": 0.8847442269325256,
      "learning_rate": 0.00012761042358386157,
      "loss": 0.1753,
      "step": 17992
    },
    {
      "epoch": 36.203219315895375,
      "grad_norm": 0.8850259780883789,
      "learning_rate": 0.00012760639903410808,
      "loss": 0.1843,
      "step": 17993
    },
    {
      "epoch": 36.20523138832998,
      "grad_norm": 0.9234963655471802,
      "learning_rate": 0.00012760237448435457,
      "loss": 0.1825,
      "step": 17994
    },
    {
      "epoch": 36.20724346076459,
      "grad_norm": 0.9559295177459717,
      "learning_rate": 0.00012759834993460108,
      "loss": 0.1836,
      "step": 17995
    },
    {
      "epoch": 36.2092555331992,
      "grad_norm": 0.8848577737808228,
      "learning_rate": 0.00012759432538484756,
      "loss": 0.1625,
      "step": 17996
    },
    {
      "epoch": 36.2112676056338,
      "grad_norm": 0.90949547290802,
      "learning_rate": 0.0001275903008350941,
      "loss": 0.1911,
      "step": 17997
    },
    {
      "epoch": 36.21327967806841,
      "grad_norm": 1.0113710165023804,
      "learning_rate": 0.0001275862762853406,
      "loss": 0.1753,
      "step": 17998
    },
    {
      "epoch": 36.21529175050302,
      "grad_norm": 0.8453514575958252,
      "learning_rate": 0.0001275822517355871,
      "loss": 0.1503,
      "step": 17999
    },
    {
      "epoch": 36.21730382293762,
      "grad_norm": 0.9123189449310303,
      "learning_rate": 0.00012757822718583359,
      "loss": 0.1823,
      "step": 18000
    },
    {
      "epoch": 36.21931589537223,
      "grad_norm": 0.8867148756980896,
      "learning_rate": 0.0001275742026360801,
      "loss": 0.1938,
      "step": 18001
    },
    {
      "epoch": 36.22132796780684,
      "grad_norm": 0.9345693588256836,
      "learning_rate": 0.0001275701780863266,
      "loss": 0.166,
      "step": 18002
    },
    {
      "epoch": 36.223340040241446,
      "grad_norm": 0.8705924153327942,
      "learning_rate": 0.0001275661535365731,
      "loss": 0.1773,
      "step": 18003
    },
    {
      "epoch": 36.225352112676056,
      "grad_norm": 0.9072874784469604,
      "learning_rate": 0.0001275621289868196,
      "loss": 0.1716,
      "step": 18004
    },
    {
      "epoch": 36.227364185110666,
      "grad_norm": 0.8797081112861633,
      "learning_rate": 0.00012755810443706612,
      "loss": 0.1708,
      "step": 18005
    },
    {
      "epoch": 36.22937625754527,
      "grad_norm": 0.9406046271324158,
      "learning_rate": 0.0001275540798873126,
      "loss": 0.1832,
      "step": 18006
    },
    {
      "epoch": 36.23138832997988,
      "grad_norm": 0.9061903357505798,
      "learning_rate": 0.00012755005533755912,
      "loss": 0.1775,
      "step": 18007
    },
    {
      "epoch": 36.23340040241449,
      "grad_norm": 0.9012377858161926,
      "learning_rate": 0.00012754603078780563,
      "loss": 0.1786,
      "step": 18008
    },
    {
      "epoch": 36.23541247484909,
      "grad_norm": 0.8867930769920349,
      "learning_rate": 0.00012754200623805211,
      "loss": 0.1814,
      "step": 18009
    },
    {
      "epoch": 36.2374245472837,
      "grad_norm": 0.9335458874702454,
      "learning_rate": 0.00012753798168829863,
      "loss": 0.1763,
      "step": 18010
    },
    {
      "epoch": 36.23943661971831,
      "grad_norm": 0.9001814723014832,
      "learning_rate": 0.0001275339571385451,
      "loss": 0.1801,
      "step": 18011
    },
    {
      "epoch": 36.241448692152915,
      "grad_norm": 0.9113196730613708,
      "learning_rate": 0.00012752993258879165,
      "loss": 0.1865,
      "step": 18012
    },
    {
      "epoch": 36.243460764587525,
      "grad_norm": 0.9486835598945618,
      "learning_rate": 0.00012752590803903814,
      "loss": 0.182,
      "step": 18013
    },
    {
      "epoch": 36.245472837022135,
      "grad_norm": 0.9557002186775208,
      "learning_rate": 0.00012752188348928465,
      "loss": 0.1788,
      "step": 18014
    },
    {
      "epoch": 36.24748490945674,
      "grad_norm": 0.8609698414802551,
      "learning_rate": 0.00012751785893953113,
      "loss": 0.1756,
      "step": 18015
    },
    {
      "epoch": 36.24949698189135,
      "grad_norm": 0.8517706394195557,
      "learning_rate": 0.00012751383438977765,
      "loss": 0.1777,
      "step": 18016
    },
    {
      "epoch": 36.25150905432596,
      "grad_norm": 0.8922946453094482,
      "learning_rate": 0.00012750980984002416,
      "loss": 0.1921,
      "step": 18017
    },
    {
      "epoch": 36.25352112676056,
      "grad_norm": 0.93561190366745,
      "learning_rate": 0.00012750578529027067,
      "loss": 0.1796,
      "step": 18018
    },
    {
      "epoch": 36.25553319919517,
      "grad_norm": 0.890126645565033,
      "learning_rate": 0.00012750176074051715,
      "loss": 0.1725,
      "step": 18019
    },
    {
      "epoch": 36.25754527162978,
      "grad_norm": 0.8861849308013916,
      "learning_rate": 0.00012749773619076367,
      "loss": 0.1698,
      "step": 18020
    },
    {
      "epoch": 36.25955734406438,
      "grad_norm": 0.9145437479019165,
      "learning_rate": 0.00012749371164101015,
      "loss": 0.1754,
      "step": 18021
    },
    {
      "epoch": 36.26156941649899,
      "grad_norm": 0.9331182241439819,
      "learning_rate": 0.0001274896870912567,
      "loss": 0.1846,
      "step": 18022
    },
    {
      "epoch": 36.2635814889336,
      "grad_norm": 0.944146454334259,
      "learning_rate": 0.00012748566254150318,
      "loss": 0.1838,
      "step": 18023
    },
    {
      "epoch": 36.265593561368206,
      "grad_norm": 0.9895132780075073,
      "learning_rate": 0.0001274816379917497,
      "loss": 0.1926,
      "step": 18024
    },
    {
      "epoch": 36.267605633802816,
      "grad_norm": 0.934337854385376,
      "learning_rate": 0.00012747761344199617,
      "loss": 0.1974,
      "step": 18025
    },
    {
      "epoch": 36.269617706237426,
      "grad_norm": 0.9673876762390137,
      "learning_rate": 0.00012747358889224269,
      "loss": 0.1971,
      "step": 18026
    },
    {
      "epoch": 36.27162977867203,
      "grad_norm": 0.9381449818611145,
      "learning_rate": 0.0001274695643424892,
      "loss": 0.1879,
      "step": 18027
    },
    {
      "epoch": 36.27364185110664,
      "grad_norm": 0.891654372215271,
      "learning_rate": 0.0001274655397927357,
      "loss": 0.1791,
      "step": 18028
    },
    {
      "epoch": 36.27565392354125,
      "grad_norm": 0.9656640887260437,
      "learning_rate": 0.0001274615152429822,
      "loss": 0.207,
      "step": 18029
    },
    {
      "epoch": 36.27766599597585,
      "grad_norm": 0.9432082772254944,
      "learning_rate": 0.0001274574906932287,
      "loss": 0.1945,
      "step": 18030
    },
    {
      "epoch": 36.27967806841046,
      "grad_norm": 0.9769888520240784,
      "learning_rate": 0.0001274534661434752,
      "loss": 0.2012,
      "step": 18031
    },
    {
      "epoch": 36.28169014084507,
      "grad_norm": 0.9251012206077576,
      "learning_rate": 0.00012744944159372173,
      "loss": 0.1956,
      "step": 18032
    },
    {
      "epoch": 36.283702213279675,
      "grad_norm": 0.9952468276023865,
      "learning_rate": 0.00012744541704396822,
      "loss": 0.195,
      "step": 18033
    },
    {
      "epoch": 36.285714285714285,
      "grad_norm": 0.8979164361953735,
      "learning_rate": 0.00012744139249421473,
      "loss": 0.18,
      "step": 18034
    },
    {
      "epoch": 36.287726358148895,
      "grad_norm": 0.9661539793014526,
      "learning_rate": 0.00012743736794446121,
      "loss": 0.2051,
      "step": 18035
    },
    {
      "epoch": 36.2897384305835,
      "grad_norm": 0.9083704352378845,
      "learning_rate": 0.00012743334339470773,
      "loss": 0.1812,
      "step": 18036
    },
    {
      "epoch": 36.29175050301811,
      "grad_norm": 0.944596529006958,
      "learning_rate": 0.00012742931884495424,
      "loss": 0.1767,
      "step": 18037
    },
    {
      "epoch": 36.29376257545272,
      "grad_norm": 0.9206137657165527,
      "learning_rate": 0.00012742529429520072,
      "loss": 0.188,
      "step": 18038
    },
    {
      "epoch": 36.29577464788732,
      "grad_norm": 0.9078260660171509,
      "learning_rate": 0.00012742126974544724,
      "loss": 0.1726,
      "step": 18039
    },
    {
      "epoch": 36.29778672032193,
      "grad_norm": 0.9165664315223694,
      "learning_rate": 0.00012741724519569372,
      "loss": 0.1863,
      "step": 18040
    },
    {
      "epoch": 36.29979879275654,
      "grad_norm": 0.946410596370697,
      "learning_rate": 0.00012741322064594023,
      "loss": 0.1849,
      "step": 18041
    },
    {
      "epoch": 36.30181086519114,
      "grad_norm": 0.9779367446899414,
      "learning_rate": 0.00012740919609618675,
      "loss": 0.192,
      "step": 18042
    },
    {
      "epoch": 36.30382293762575,
      "grad_norm": 0.9781460165977478,
      "learning_rate": 0.00012740517154643326,
      "loss": 0.2048,
      "step": 18043
    },
    {
      "epoch": 36.30583501006036,
      "grad_norm": 0.9573687314987183,
      "learning_rate": 0.00012740114699667974,
      "loss": 0.1867,
      "step": 18044
    },
    {
      "epoch": 36.30784708249497,
      "grad_norm": 0.8320980072021484,
      "learning_rate": 0.00012739712244692626,
      "loss": 0.1663,
      "step": 18045
    },
    {
      "epoch": 36.309859154929576,
      "grad_norm": 0.9201269745826721,
      "learning_rate": 0.00012739309789717274,
      "loss": 0.1942,
      "step": 18046
    },
    {
      "epoch": 36.311871227364186,
      "grad_norm": 0.9370929598808289,
      "learning_rate": 0.00012738907334741928,
      "loss": 0.1993,
      "step": 18047
    },
    {
      "epoch": 36.313883299798796,
      "grad_norm": 0.8971133232116699,
      "learning_rate": 0.00012738504879766577,
      "loss": 0.1724,
      "step": 18048
    },
    {
      "epoch": 36.3158953722334,
      "grad_norm": 0.8862172365188599,
      "learning_rate": 0.00012738102424791228,
      "loss": 0.1815,
      "step": 18049
    },
    {
      "epoch": 36.31790744466801,
      "grad_norm": 1.0184695720672607,
      "learning_rate": 0.00012737699969815876,
      "loss": 0.1708,
      "step": 18050
    },
    {
      "epoch": 36.31991951710262,
      "grad_norm": 0.9234451055526733,
      "learning_rate": 0.00012737297514840527,
      "loss": 0.1768,
      "step": 18051
    },
    {
      "epoch": 36.32193158953722,
      "grad_norm": 0.9377886056900024,
      "learning_rate": 0.0001273689505986518,
      "loss": 0.1749,
      "step": 18052
    },
    {
      "epoch": 36.32394366197183,
      "grad_norm": 0.9711506962776184,
      "learning_rate": 0.0001273649260488983,
      "loss": 0.2059,
      "step": 18053
    },
    {
      "epoch": 36.32595573440644,
      "grad_norm": 0.8856075406074524,
      "learning_rate": 0.00012736090149914478,
      "loss": 0.1734,
      "step": 18054
    },
    {
      "epoch": 36.327967806841045,
      "grad_norm": 0.9445119500160217,
      "learning_rate": 0.0001273568769493913,
      "loss": 0.1822,
      "step": 18055
    },
    {
      "epoch": 36.329979879275655,
      "grad_norm": 0.919122576713562,
      "learning_rate": 0.00012735285239963778,
      "loss": 0.1786,
      "step": 18056
    },
    {
      "epoch": 36.331991951710265,
      "grad_norm": 0.9373558163642883,
      "learning_rate": 0.00012734882784988432,
      "loss": 0.201,
      "step": 18057
    },
    {
      "epoch": 36.33400402414487,
      "grad_norm": 0.946675181388855,
      "learning_rate": 0.0001273448033001308,
      "loss": 0.1953,
      "step": 18058
    },
    {
      "epoch": 36.33601609657948,
      "grad_norm": 0.998955249786377,
      "learning_rate": 0.00012734077875037732,
      "loss": 0.1847,
      "step": 18059
    },
    {
      "epoch": 36.33802816901409,
      "grad_norm": 0.9316725730895996,
      "learning_rate": 0.0001273367542006238,
      "loss": 0.1894,
      "step": 18060
    },
    {
      "epoch": 36.34004024144869,
      "grad_norm": 0.8970484733581543,
      "learning_rate": 0.00012733272965087032,
      "loss": 0.1834,
      "step": 18061
    },
    {
      "epoch": 36.3420523138833,
      "grad_norm": 0.9154061675071716,
      "learning_rate": 0.00012732870510111683,
      "loss": 0.1824,
      "step": 18062
    },
    {
      "epoch": 36.34406438631791,
      "grad_norm": 0.9319074153900146,
      "learning_rate": 0.00012732468055136334,
      "loss": 0.1763,
      "step": 18063
    },
    {
      "epoch": 36.34607645875251,
      "grad_norm": 0.97551029920578,
      "learning_rate": 0.00012732065600160983,
      "loss": 0.1966,
      "step": 18064
    },
    {
      "epoch": 36.34808853118712,
      "grad_norm": 0.9144763946533203,
      "learning_rate": 0.00012731663145185634,
      "loss": 0.1865,
      "step": 18065
    },
    {
      "epoch": 36.35010060362173,
      "grad_norm": 0.8964270353317261,
      "learning_rate": 0.00012731260690210282,
      "loss": 0.1844,
      "step": 18066
    },
    {
      "epoch": 36.352112676056336,
      "grad_norm": 0.9058349132537842,
      "learning_rate": 0.00012730858235234933,
      "loss": 0.1872,
      "step": 18067
    },
    {
      "epoch": 36.354124748490946,
      "grad_norm": 0.9315694570541382,
      "learning_rate": 0.00012730455780259585,
      "loss": 0.1904,
      "step": 18068
    },
    {
      "epoch": 36.356136820925556,
      "grad_norm": 0.9086694121360779,
      "learning_rate": 0.00012730053325284236,
      "loss": 0.1823,
      "step": 18069
    },
    {
      "epoch": 36.35814889336016,
      "grad_norm": 0.9716398119926453,
      "learning_rate": 0.00012729650870308884,
      "loss": 0.1896,
      "step": 18070
    },
    {
      "epoch": 36.36016096579477,
      "grad_norm": 0.9541401267051697,
      "learning_rate": 0.00012729248415333536,
      "loss": 0.1916,
      "step": 18071
    },
    {
      "epoch": 36.36217303822938,
      "grad_norm": 0.9989219903945923,
      "learning_rate": 0.00012728845960358184,
      "loss": 0.2043,
      "step": 18072
    },
    {
      "epoch": 36.36418511066398,
      "grad_norm": 0.8920863270759583,
      "learning_rate": 0.00012728443505382835,
      "loss": 0.1631,
      "step": 18073
    },
    {
      "epoch": 36.36619718309859,
      "grad_norm": 0.8847137689590454,
      "learning_rate": 0.00012728041050407487,
      "loss": 0.1651,
      "step": 18074
    },
    {
      "epoch": 36.3682092555332,
      "grad_norm": 0.9840189814567566,
      "learning_rate": 0.00012727638595432135,
      "loss": 0.1899,
      "step": 18075
    },
    {
      "epoch": 36.370221327967805,
      "grad_norm": 0.918846845626831,
      "learning_rate": 0.00012727236140456786,
      "loss": 0.1813,
      "step": 18076
    },
    {
      "epoch": 36.372233400402415,
      "grad_norm": 0.9295777678489685,
      "learning_rate": 0.00012726833685481438,
      "loss": 0.1914,
      "step": 18077
    },
    {
      "epoch": 36.374245472837025,
      "grad_norm": 0.9776914715766907,
      "learning_rate": 0.0001272643123050609,
      "loss": 0.1839,
      "step": 18078
    },
    {
      "epoch": 36.37625754527163,
      "grad_norm": 0.9225389957427979,
      "learning_rate": 0.00012726028775530737,
      "loss": 0.1903,
      "step": 18079
    },
    {
      "epoch": 36.37826961770624,
      "grad_norm": 1.0399179458618164,
      "learning_rate": 0.00012725626320555389,
      "loss": 0.1946,
      "step": 18080
    },
    {
      "epoch": 36.38028169014085,
      "grad_norm": 0.9143809676170349,
      "learning_rate": 0.00012725223865580037,
      "loss": 0.1935,
      "step": 18081
    },
    {
      "epoch": 36.38229376257545,
      "grad_norm": 0.9911929368972778,
      "learning_rate": 0.00012724821410604688,
      "loss": 0.193,
      "step": 18082
    },
    {
      "epoch": 36.38430583501006,
      "grad_norm": 0.9705150723457336,
      "learning_rate": 0.0001272441895562934,
      "loss": 0.1964,
      "step": 18083
    },
    {
      "epoch": 36.38631790744467,
      "grad_norm": 0.9212258458137512,
      "learning_rate": 0.0001272401650065399,
      "loss": 0.1912,
      "step": 18084
    },
    {
      "epoch": 36.38832997987927,
      "grad_norm": 0.9099524021148682,
      "learning_rate": 0.0001272361404567864,
      "loss": 0.1969,
      "step": 18085
    },
    {
      "epoch": 36.39034205231388,
      "grad_norm": 0.93297278881073,
      "learning_rate": 0.0001272321159070329,
      "loss": 0.1891,
      "step": 18086
    },
    {
      "epoch": 36.39235412474849,
      "grad_norm": 0.9102810025215149,
      "learning_rate": 0.0001272280913572794,
      "loss": 0.1921,
      "step": 18087
    },
    {
      "epoch": 36.394366197183096,
      "grad_norm": 1.0141990184783936,
      "learning_rate": 0.00012722406680752593,
      "loss": 0.2107,
      "step": 18088
    },
    {
      "epoch": 36.396378269617706,
      "grad_norm": 0.9388369917869568,
      "learning_rate": 0.00012722004225777241,
      "loss": 0.1943,
      "step": 18089
    },
    {
      "epoch": 36.398390342052316,
      "grad_norm": 0.9207390546798706,
      "learning_rate": 0.00012721601770801893,
      "loss": 0.1682,
      "step": 18090
    },
    {
      "epoch": 36.40040241448692,
      "grad_norm": 0.9512328505516052,
      "learning_rate": 0.0001272119931582654,
      "loss": 0.191,
      "step": 18091
    },
    {
      "epoch": 36.40241448692153,
      "grad_norm": 0.9487135410308838,
      "learning_rate": 0.00012720796860851192,
      "loss": 0.1839,
      "step": 18092
    },
    {
      "epoch": 36.40442655935614,
      "grad_norm": 0.9024987816810608,
      "learning_rate": 0.00012720394405875844,
      "loss": 0.1674,
      "step": 18093
    },
    {
      "epoch": 36.40643863179074,
      "grad_norm": 0.9691016674041748,
      "learning_rate": 0.00012719991950900495,
      "loss": 0.1974,
      "step": 18094
    },
    {
      "epoch": 36.40845070422535,
      "grad_norm": 1.0343071222305298,
      "learning_rate": 0.00012719589495925143,
      "loss": 0.2076,
      "step": 18095
    },
    {
      "epoch": 36.41046277665996,
      "grad_norm": 0.9235936999320984,
      "learning_rate": 0.00012719187040949795,
      "loss": 0.188,
      "step": 18096
    },
    {
      "epoch": 36.412474849094565,
      "grad_norm": 0.9471498131752014,
      "learning_rate": 0.00012718784585974443,
      "loss": 0.1851,
      "step": 18097
    },
    {
      "epoch": 36.414486921529175,
      "grad_norm": 0.9848262667655945,
      "learning_rate": 0.00012718382130999097,
      "loss": 0.2114,
      "step": 18098
    },
    {
      "epoch": 36.416498993963785,
      "grad_norm": 0.8960721492767334,
      "learning_rate": 0.00012717979676023745,
      "loss": 0.1956,
      "step": 18099
    },
    {
      "epoch": 36.41851106639839,
      "grad_norm": 0.9716250896453857,
      "learning_rate": 0.00012717577221048397,
      "loss": 0.1724,
      "step": 18100
    },
    {
      "epoch": 36.420523138833,
      "grad_norm": 0.9132270812988281,
      "learning_rate": 0.00012717174766073045,
      "loss": 0.1743,
      "step": 18101
    },
    {
      "epoch": 36.42253521126761,
      "grad_norm": 0.9324439764022827,
      "learning_rate": 0.00012716772311097696,
      "loss": 0.1946,
      "step": 18102
    },
    {
      "epoch": 36.42454728370221,
      "grad_norm": 0.9456006288528442,
      "learning_rate": 0.00012716369856122348,
      "loss": 0.1859,
      "step": 18103
    },
    {
      "epoch": 36.42655935613682,
      "grad_norm": 0.9552234411239624,
      "learning_rate": 0.00012715967401147,
      "loss": 0.1911,
      "step": 18104
    },
    {
      "epoch": 36.42857142857143,
      "grad_norm": 0.9564954042434692,
      "learning_rate": 0.00012715564946171647,
      "loss": 0.2045,
      "step": 18105
    },
    {
      "epoch": 36.43058350100603,
      "grad_norm": 0.940375566482544,
      "learning_rate": 0.00012715162491196299,
      "loss": 0.1919,
      "step": 18106
    },
    {
      "epoch": 36.43259557344064,
      "grad_norm": 0.9239351749420166,
      "learning_rate": 0.00012714760036220947,
      "loss": 0.1932,
      "step": 18107
    },
    {
      "epoch": 36.43460764587525,
      "grad_norm": 0.9515926241874695,
      "learning_rate": 0.00012714357581245598,
      "loss": 0.1948,
      "step": 18108
    },
    {
      "epoch": 36.436619718309856,
      "grad_norm": 0.9774152040481567,
      "learning_rate": 0.0001271395512627025,
      "loss": 0.1991,
      "step": 18109
    },
    {
      "epoch": 36.438631790744466,
      "grad_norm": 0.9773488640785217,
      "learning_rate": 0.00012713552671294898,
      "loss": 0.2036,
      "step": 18110
    },
    {
      "epoch": 36.440643863179076,
      "grad_norm": 0.9337737560272217,
      "learning_rate": 0.0001271315021631955,
      "loss": 0.1898,
      "step": 18111
    },
    {
      "epoch": 36.44265593561368,
      "grad_norm": 0.9061763286590576,
      "learning_rate": 0.000127127477613442,
      "loss": 0.1826,
      "step": 18112
    },
    {
      "epoch": 36.44466800804829,
      "grad_norm": 0.952055037021637,
      "learning_rate": 0.00012712345306368852,
      "loss": 0.1977,
      "step": 18113
    },
    {
      "epoch": 36.4466800804829,
      "grad_norm": 0.969789981842041,
      "learning_rate": 0.000127119428513935,
      "loss": 0.1924,
      "step": 18114
    },
    {
      "epoch": 36.4486921529175,
      "grad_norm": 0.9899836778640747,
      "learning_rate": 0.00012711540396418151,
      "loss": 0.1972,
      "step": 18115
    },
    {
      "epoch": 36.45070422535211,
      "grad_norm": 0.9706170558929443,
      "learning_rate": 0.000127111379414428,
      "loss": 0.1915,
      "step": 18116
    },
    {
      "epoch": 36.45271629778672,
      "grad_norm": 0.9480920433998108,
      "learning_rate": 0.0001271073548646745,
      "loss": 0.1948,
      "step": 18117
    },
    {
      "epoch": 36.454728370221325,
      "grad_norm": 0.9538478255271912,
      "learning_rate": 0.00012710333031492102,
      "loss": 0.1851,
      "step": 18118
    },
    {
      "epoch": 36.456740442655935,
      "grad_norm": 0.9920290112495422,
      "learning_rate": 0.00012709930576516754,
      "loss": 0.1989,
      "step": 18119
    },
    {
      "epoch": 36.458752515090545,
      "grad_norm": 1.0181752443313599,
      "learning_rate": 0.00012709528121541402,
      "loss": 0.2073,
      "step": 18120
    },
    {
      "epoch": 36.46076458752515,
      "grad_norm": 0.9308826327323914,
      "learning_rate": 0.00012709125666566053,
      "loss": 0.2001,
      "step": 18121
    },
    {
      "epoch": 36.46277665995976,
      "grad_norm": 0.9590564370155334,
      "learning_rate": 0.00012708723211590702,
      "loss": 0.1969,
      "step": 18122
    },
    {
      "epoch": 36.46478873239437,
      "grad_norm": 0.9059179425239563,
      "learning_rate": 0.00012708320756615356,
      "loss": 0.1879,
      "step": 18123
    },
    {
      "epoch": 36.46680080482897,
      "grad_norm": 0.94820636510849,
      "learning_rate": 0.00012707918301640004,
      "loss": 0.1768,
      "step": 18124
    },
    {
      "epoch": 36.46881287726358,
      "grad_norm": 0.929394006729126,
      "learning_rate": 0.00012707515846664656,
      "loss": 0.1874,
      "step": 18125
    },
    {
      "epoch": 36.47082494969819,
      "grad_norm": 0.9556984901428223,
      "learning_rate": 0.00012707113391689304,
      "loss": 0.195,
      "step": 18126
    },
    {
      "epoch": 36.47283702213279,
      "grad_norm": 0.9580116868019104,
      "learning_rate": 0.00012706710936713955,
      "loss": 0.1945,
      "step": 18127
    },
    {
      "epoch": 36.4748490945674,
      "grad_norm": 0.9992562532424927,
      "learning_rate": 0.00012706308481738607,
      "loss": 0.2004,
      "step": 18128
    },
    {
      "epoch": 36.47686116700201,
      "grad_norm": 0.9808927774429321,
      "learning_rate": 0.00012705906026763258,
      "loss": 0.1958,
      "step": 18129
    },
    {
      "epoch": 36.478873239436616,
      "grad_norm": 0.9187141060829163,
      "learning_rate": 0.00012705503571787906,
      "loss": 0.1893,
      "step": 18130
    },
    {
      "epoch": 36.480885311871226,
      "grad_norm": 0.9474038481712341,
      "learning_rate": 0.00012705101116812557,
      "loss": 0.205,
      "step": 18131
    },
    {
      "epoch": 36.482897384305836,
      "grad_norm": 0.9549490809440613,
      "learning_rate": 0.00012704698661837206,
      "loss": 0.1983,
      "step": 18132
    },
    {
      "epoch": 36.48490945674044,
      "grad_norm": 0.9191516041755676,
      "learning_rate": 0.0001270429620686186,
      "loss": 0.1932,
      "step": 18133
    },
    {
      "epoch": 36.48692152917505,
      "grad_norm": 0.9366602897644043,
      "learning_rate": 0.00012703893751886508,
      "loss": 0.1857,
      "step": 18134
    },
    {
      "epoch": 36.48893360160966,
      "grad_norm": 0.9630454778671265,
      "learning_rate": 0.0001270349129691116,
      "loss": 0.201,
      "step": 18135
    },
    {
      "epoch": 36.49094567404426,
      "grad_norm": 0.9692327976226807,
      "learning_rate": 0.00012703088841935808,
      "loss": 0.2133,
      "step": 18136
    },
    {
      "epoch": 36.49295774647887,
      "grad_norm": 0.9948349595069885,
      "learning_rate": 0.0001270268638696046,
      "loss": 0.1956,
      "step": 18137
    },
    {
      "epoch": 36.49496981891348,
      "grad_norm": 1.0187040567398071,
      "learning_rate": 0.0001270228393198511,
      "loss": 0.2037,
      "step": 18138
    },
    {
      "epoch": 36.496981891348085,
      "grad_norm": 0.937089741230011,
      "learning_rate": 0.00012701881477009762,
      "loss": 0.1969,
      "step": 18139
    },
    {
      "epoch": 36.498993963782695,
      "grad_norm": 0.9198726415634155,
      "learning_rate": 0.0001270147902203441,
      "loss": 0.2106,
      "step": 18140
    },
    {
      "epoch": 36.501006036217305,
      "grad_norm": 0.9951912760734558,
      "learning_rate": 0.00012701076567059062,
      "loss": 0.2116,
      "step": 18141
    },
    {
      "epoch": 36.503018108651915,
      "grad_norm": 0.9783907532691956,
      "learning_rate": 0.0001270067411208371,
      "loss": 0.2002,
      "step": 18142
    },
    {
      "epoch": 36.50503018108652,
      "grad_norm": 1.0418894290924072,
      "learning_rate": 0.0001270027165710836,
      "loss": 0.2093,
      "step": 18143
    },
    {
      "epoch": 36.50704225352113,
      "grad_norm": 0.9655514359474182,
      "learning_rate": 0.00012699869202133012,
      "loss": 0.1838,
      "step": 18144
    },
    {
      "epoch": 36.50905432595574,
      "grad_norm": 0.9910419583320618,
      "learning_rate": 0.0001269946674715766,
      "loss": 0.2003,
      "step": 18145
    },
    {
      "epoch": 36.51106639839034,
      "grad_norm": 0.9608378410339355,
      "learning_rate": 0.00012699064292182312,
      "loss": 0.1933,
      "step": 18146
    },
    {
      "epoch": 36.51307847082495,
      "grad_norm": 0.9824869632720947,
      "learning_rate": 0.00012698661837206963,
      "loss": 0.2012,
      "step": 18147
    },
    {
      "epoch": 36.51509054325956,
      "grad_norm": 0.951682448387146,
      "learning_rate": 0.00012698259382231615,
      "loss": 0.1903,
      "step": 18148
    },
    {
      "epoch": 36.517102615694164,
      "grad_norm": 0.937344491481781,
      "learning_rate": 0.00012697856927256263,
      "loss": 0.1876,
      "step": 18149
    },
    {
      "epoch": 36.519114688128774,
      "grad_norm": 0.9997593760490417,
      "learning_rate": 0.00012697454472280914,
      "loss": 0.1896,
      "step": 18150
    },
    {
      "epoch": 36.521126760563384,
      "grad_norm": 1.003434658050537,
      "learning_rate": 0.00012697052017305563,
      "loss": 0.2135,
      "step": 18151
    },
    {
      "epoch": 36.52313883299799,
      "grad_norm": 0.9878127574920654,
      "learning_rate": 0.00012696649562330214,
      "loss": 0.21,
      "step": 18152
    },
    {
      "epoch": 36.5251509054326,
      "grad_norm": 0.9543808102607727,
      "learning_rate": 0.00012696247107354865,
      "loss": 0.1914,
      "step": 18153
    },
    {
      "epoch": 36.52716297786721,
      "grad_norm": 0.9572131037712097,
      "learning_rate": 0.00012695844652379517,
      "loss": 0.2031,
      "step": 18154
    },
    {
      "epoch": 36.52917505030181,
      "grad_norm": 0.9812753200531006,
      "learning_rate": 0.00012695442197404165,
      "loss": 0.21,
      "step": 18155
    },
    {
      "epoch": 36.53118712273642,
      "grad_norm": 0.9593605399131775,
      "learning_rate": 0.00012695039742428816,
      "loss": 0.2062,
      "step": 18156
    },
    {
      "epoch": 36.53319919517103,
      "grad_norm": 0.9584086537361145,
      "learning_rate": 0.00012694637287453465,
      "loss": 0.2011,
      "step": 18157
    },
    {
      "epoch": 36.53521126760563,
      "grad_norm": 0.9600951075553894,
      "learning_rate": 0.0001269423483247812,
      "loss": 0.1938,
      "step": 18158
    },
    {
      "epoch": 36.53722334004024,
      "grad_norm": 0.9851243495941162,
      "learning_rate": 0.00012693832377502767,
      "loss": 0.204,
      "step": 18159
    },
    {
      "epoch": 36.53923541247485,
      "grad_norm": 0.9743529558181763,
      "learning_rate": 0.00012693429922527418,
      "loss": 0.1882,
      "step": 18160
    },
    {
      "epoch": 36.541247484909455,
      "grad_norm": 1.0733543634414673,
      "learning_rate": 0.00012693027467552067,
      "loss": 0.211,
      "step": 18161
    },
    {
      "epoch": 36.543259557344065,
      "grad_norm": 0.9541441798210144,
      "learning_rate": 0.00012692625012576718,
      "loss": 0.1974,
      "step": 18162
    },
    {
      "epoch": 36.545271629778675,
      "grad_norm": 0.9498003721237183,
      "learning_rate": 0.0001269222255760137,
      "loss": 0.1972,
      "step": 18163
    },
    {
      "epoch": 36.54728370221328,
      "grad_norm": 0.9603949785232544,
      "learning_rate": 0.0001269182010262602,
      "loss": 0.2011,
      "step": 18164
    },
    {
      "epoch": 36.54929577464789,
      "grad_norm": 0.9953989386558533,
      "learning_rate": 0.0001269141764765067,
      "loss": 0.2075,
      "step": 18165
    },
    {
      "epoch": 36.5513078470825,
      "grad_norm": 1.0294623374938965,
      "learning_rate": 0.0001269101519267532,
      "loss": 0.2061,
      "step": 18166
    },
    {
      "epoch": 36.5533199195171,
      "grad_norm": 0.9438430070877075,
      "learning_rate": 0.0001269061273769997,
      "loss": 0.1917,
      "step": 18167
    },
    {
      "epoch": 36.55533199195171,
      "grad_norm": 1.0249767303466797,
      "learning_rate": 0.00012690210282724623,
      "loss": 0.209,
      "step": 18168
    },
    {
      "epoch": 36.55734406438632,
      "grad_norm": 0.9582273364067078,
      "learning_rate": 0.00012689807827749271,
      "loss": 0.2008,
      "step": 18169
    },
    {
      "epoch": 36.559356136820924,
      "grad_norm": 0.9353606700897217,
      "learning_rate": 0.00012689405372773923,
      "loss": 0.2019,
      "step": 18170
    },
    {
      "epoch": 36.561368209255534,
      "grad_norm": 0.9487276077270508,
      "learning_rate": 0.0001268900291779857,
      "loss": 0.194,
      "step": 18171
    },
    {
      "epoch": 36.563380281690144,
      "grad_norm": 0.9671422839164734,
      "learning_rate": 0.00012688600462823222,
      "loss": 0.2122,
      "step": 18172
    },
    {
      "epoch": 36.56539235412475,
      "grad_norm": 1.036239504814148,
      "learning_rate": 0.00012688198007847874,
      "loss": 0.2024,
      "step": 18173
    },
    {
      "epoch": 36.56740442655936,
      "grad_norm": 0.9215076565742493,
      "learning_rate": 0.00012687795552872525,
      "loss": 0.2155,
      "step": 18174
    },
    {
      "epoch": 36.56941649899397,
      "grad_norm": 0.9446895718574524,
      "learning_rate": 0.00012687393097897173,
      "loss": 0.2227,
      "step": 18175
    },
    {
      "epoch": 36.57142857142857,
      "grad_norm": 0.9616670608520508,
      "learning_rate": 0.00012686990642921824,
      "loss": 0.2106,
      "step": 18176
    },
    {
      "epoch": 36.57344064386318,
      "grad_norm": 1.0089762210845947,
      "learning_rate": 0.00012686588187946473,
      "loss": 0.1985,
      "step": 18177
    },
    {
      "epoch": 36.57545271629779,
      "grad_norm": 0.9476690292358398,
      "learning_rate": 0.00012686185732971124,
      "loss": 0.2183,
      "step": 18178
    },
    {
      "epoch": 36.57746478873239,
      "grad_norm": 0.959789514541626,
      "learning_rate": 0.00012685783277995775,
      "loss": 0.2121,
      "step": 18179
    },
    {
      "epoch": 36.579476861167,
      "grad_norm": 0.9965041279792786,
      "learning_rate": 0.00012685380823020424,
      "loss": 0.2101,
      "step": 18180
    },
    {
      "epoch": 36.58148893360161,
      "grad_norm": 0.9931780099868774,
      "learning_rate": 0.00012684978368045075,
      "loss": 0.1906,
      "step": 18181
    },
    {
      "epoch": 36.583501006036215,
      "grad_norm": 1.054878830909729,
      "learning_rate": 0.00012684575913069724,
      "loss": 0.2206,
      "step": 18182
    },
    {
      "epoch": 36.585513078470825,
      "grad_norm": 0.9532244205474854,
      "learning_rate": 0.00012684173458094378,
      "loss": 0.1979,
      "step": 18183
    },
    {
      "epoch": 36.587525150905435,
      "grad_norm": 0.9471428990364075,
      "learning_rate": 0.00012683771003119026,
      "loss": 0.2023,
      "step": 18184
    },
    {
      "epoch": 36.58953722334004,
      "grad_norm": 0.912762463092804,
      "learning_rate": 0.00012683368548143677,
      "loss": 0.1983,
      "step": 18185
    },
    {
      "epoch": 36.59154929577465,
      "grad_norm": 0.9942700862884521,
      "learning_rate": 0.00012682966093168326,
      "loss": 0.2035,
      "step": 18186
    },
    {
      "epoch": 36.59356136820926,
      "grad_norm": 0.979346752166748,
      "learning_rate": 0.00012682563638192977,
      "loss": 0.2245,
      "step": 18187
    },
    {
      "epoch": 36.59557344064386,
      "grad_norm": 0.9215140342712402,
      "learning_rate": 0.00012682161183217628,
      "loss": 0.197,
      "step": 18188
    },
    {
      "epoch": 36.59758551307847,
      "grad_norm": 0.9760607481002808,
      "learning_rate": 0.0001268175872824228,
      "loss": 0.2122,
      "step": 18189
    },
    {
      "epoch": 36.59959758551308,
      "grad_norm": 0.9791921973228455,
      "learning_rate": 0.00012681356273266928,
      "loss": 0.2055,
      "step": 18190
    },
    {
      "epoch": 36.601609657947684,
      "grad_norm": 0.9557631611824036,
      "learning_rate": 0.0001268095381829158,
      "loss": 0.1989,
      "step": 18191
    },
    {
      "epoch": 36.603621730382294,
      "grad_norm": 1.0526288747787476,
      "learning_rate": 0.00012680551363316228,
      "loss": 0.2051,
      "step": 18192
    },
    {
      "epoch": 36.605633802816904,
      "grad_norm": 0.9639778137207031,
      "learning_rate": 0.00012680148908340882,
      "loss": 0.1983,
      "step": 18193
    },
    {
      "epoch": 36.60764587525151,
      "grad_norm": 0.994875431060791,
      "learning_rate": 0.0001267974645336553,
      "loss": 0.1996,
      "step": 18194
    },
    {
      "epoch": 36.60965794768612,
      "grad_norm": 0.9711788296699524,
      "learning_rate": 0.00012679343998390181,
      "loss": 0.2046,
      "step": 18195
    },
    {
      "epoch": 36.61167002012073,
      "grad_norm": 1.0154591798782349,
      "learning_rate": 0.0001267894154341483,
      "loss": 0.1891,
      "step": 18196
    },
    {
      "epoch": 36.61368209255533,
      "grad_norm": 0.9755887389183044,
      "learning_rate": 0.0001267853908843948,
      "loss": 0.2193,
      "step": 18197
    },
    {
      "epoch": 36.61569416498994,
      "grad_norm": 0.9271289110183716,
      "learning_rate": 0.00012678136633464132,
      "loss": 0.2195,
      "step": 18198
    },
    {
      "epoch": 36.61770623742455,
      "grad_norm": 1.0045348405838013,
      "learning_rate": 0.00012677734178488784,
      "loss": 0.2339,
      "step": 18199
    },
    {
      "epoch": 36.61971830985915,
      "grad_norm": 0.969010055065155,
      "learning_rate": 0.00012677331723513432,
      "loss": 0.2052,
      "step": 18200
    },
    {
      "epoch": 36.62173038229376,
      "grad_norm": 1.0597710609436035,
      "learning_rate": 0.00012676929268538083,
      "loss": 0.2232,
      "step": 18201
    },
    {
      "epoch": 36.62374245472837,
      "grad_norm": 0.9704596996307373,
      "learning_rate": 0.00012676526813562732,
      "loss": 0.2031,
      "step": 18202
    },
    {
      "epoch": 36.625754527162975,
      "grad_norm": 1.0043962001800537,
      "learning_rate": 0.00012676124358587386,
      "loss": 0.2143,
      "step": 18203
    },
    {
      "epoch": 36.627766599597585,
      "grad_norm": 1.005394697189331,
      "learning_rate": 0.00012675721903612034,
      "loss": 0.2067,
      "step": 18204
    },
    {
      "epoch": 36.629778672032195,
      "grad_norm": 0.955666184425354,
      "learning_rate": 0.00012675319448636686,
      "loss": 0.1989,
      "step": 18205
    },
    {
      "epoch": 36.6317907444668,
      "grad_norm": 0.9847819209098816,
      "learning_rate": 0.00012674916993661334,
      "loss": 0.1995,
      "step": 18206
    },
    {
      "epoch": 36.63380281690141,
      "grad_norm": 0.9769821166992188,
      "learning_rate": 0.00012674514538685985,
      "loss": 0.2093,
      "step": 18207
    },
    {
      "epoch": 36.63581488933602,
      "grad_norm": 1.064249873161316,
      "learning_rate": 0.00012674112083710636,
      "loss": 0.2148,
      "step": 18208
    },
    {
      "epoch": 36.63782696177062,
      "grad_norm": 1.0128251314163208,
      "learning_rate": 0.00012673709628735285,
      "loss": 0.2175,
      "step": 18209
    },
    {
      "epoch": 36.63983903420523,
      "grad_norm": 0.9352068305015564,
      "learning_rate": 0.00012673307173759936,
      "loss": 0.1723,
      "step": 18210
    },
    {
      "epoch": 36.64185110663984,
      "grad_norm": 1.0446336269378662,
      "learning_rate": 0.00012672904718784587,
      "loss": 0.2016,
      "step": 18211
    },
    {
      "epoch": 36.643863179074444,
      "grad_norm": 1.0328372716903687,
      "learning_rate": 0.00012672502263809236,
      "loss": 0.1977,
      "step": 18212
    },
    {
      "epoch": 36.645875251509054,
      "grad_norm": 0.9499687552452087,
      "learning_rate": 0.00012672099808833887,
      "loss": 0.1985,
      "step": 18213
    },
    {
      "epoch": 36.647887323943664,
      "grad_norm": 0.9767071008682251,
      "learning_rate": 0.00012671697353858538,
      "loss": 0.2006,
      "step": 18214
    },
    {
      "epoch": 36.64989939637827,
      "grad_norm": 0.9924579858779907,
      "learning_rate": 0.00012671294898883187,
      "loss": 0.2052,
      "step": 18215
    },
    {
      "epoch": 36.65191146881288,
      "grad_norm": 0.9611033797264099,
      "learning_rate": 0.00012670892443907838,
      "loss": 0.2167,
      "step": 18216
    },
    {
      "epoch": 36.65392354124749,
      "grad_norm": 0.9263498187065125,
      "learning_rate": 0.00012670489988932487,
      "loss": 0.2089,
      "step": 18217
    },
    {
      "epoch": 36.65593561368209,
      "grad_norm": 1.0115450620651245,
      "learning_rate": 0.0001267008753395714,
      "loss": 0.2217,
      "step": 18218
    },
    {
      "epoch": 36.6579476861167,
      "grad_norm": 1.0114012956619263,
      "learning_rate": 0.0001266968507898179,
      "loss": 0.2019,
      "step": 18219
    },
    {
      "epoch": 36.65995975855131,
      "grad_norm": 0.943490743637085,
      "learning_rate": 0.0001266928262400644,
      "loss": 0.1923,
      "step": 18220
    },
    {
      "epoch": 36.66197183098591,
      "grad_norm": 1.028341293334961,
      "learning_rate": 0.0001266888016903109,
      "loss": 0.2122,
      "step": 18221
    },
    {
      "epoch": 36.66398390342052,
      "grad_norm": 0.9643837213516235,
      "learning_rate": 0.0001266847771405574,
      "loss": 0.2019,
      "step": 18222
    },
    {
      "epoch": 36.66599597585513,
      "grad_norm": 0.993223249912262,
      "learning_rate": 0.0001266807525908039,
      "loss": 0.2213,
      "step": 18223
    },
    {
      "epoch": 36.668008048289735,
      "grad_norm": 0.983296811580658,
      "learning_rate": 0.00012667672804105042,
      "loss": 0.2032,
      "step": 18224
    },
    {
      "epoch": 36.670020120724345,
      "grad_norm": 0.9589024782180786,
      "learning_rate": 0.0001266727034912969,
      "loss": 0.2175,
      "step": 18225
    },
    {
      "epoch": 36.672032193158955,
      "grad_norm": 0.9532794952392578,
      "learning_rate": 0.00012666867894154342,
      "loss": 0.2242,
      "step": 18226
    },
    {
      "epoch": 36.67404426559356,
      "grad_norm": 1.0666401386260986,
      "learning_rate": 0.0001266646543917899,
      "loss": 0.2181,
      "step": 18227
    },
    {
      "epoch": 36.67605633802817,
      "grad_norm": 0.9751886129379272,
      "learning_rate": 0.00012666062984203645,
      "loss": 0.2147,
      "step": 18228
    },
    {
      "epoch": 36.67806841046278,
      "grad_norm": 1.0830801725387573,
      "learning_rate": 0.00012665660529228293,
      "loss": 0.2121,
      "step": 18229
    },
    {
      "epoch": 36.68008048289738,
      "grad_norm": 0.9994804263114929,
      "learning_rate": 0.00012665258074252944,
      "loss": 0.2056,
      "step": 18230
    },
    {
      "epoch": 36.68209255533199,
      "grad_norm": 0.9850150346755981,
      "learning_rate": 0.00012664855619277593,
      "loss": 0.1967,
      "step": 18231
    },
    {
      "epoch": 36.6841046277666,
      "grad_norm": 0.9907262325286865,
      "learning_rate": 0.00012664453164302244,
      "loss": 0.2056,
      "step": 18232
    },
    {
      "epoch": 36.686116700201204,
      "grad_norm": 0.9948329925537109,
      "learning_rate": 0.00012664050709326895,
      "loss": 0.1987,
      "step": 18233
    },
    {
      "epoch": 36.688128772635814,
      "grad_norm": 1.0480690002441406,
      "learning_rate": 0.00012663648254351547,
      "loss": 0.2037,
      "step": 18234
    },
    {
      "epoch": 36.690140845070424,
      "grad_norm": 1.0406246185302734,
      "learning_rate": 0.00012663245799376195,
      "loss": 0.2082,
      "step": 18235
    },
    {
      "epoch": 36.69215291750503,
      "grad_norm": 0.9733124375343323,
      "learning_rate": 0.00012662843344400846,
      "loss": 0.2069,
      "step": 18236
    },
    {
      "epoch": 36.69416498993964,
      "grad_norm": 1.0176182985305786,
      "learning_rate": 0.00012662440889425495,
      "loss": 0.2228,
      "step": 18237
    },
    {
      "epoch": 36.69617706237425,
      "grad_norm": 0.99181067943573,
      "learning_rate": 0.0001266203843445015,
      "loss": 0.216,
      "step": 18238
    },
    {
      "epoch": 36.69818913480886,
      "grad_norm": 0.9754212498664856,
      "learning_rate": 0.00012661635979474797,
      "loss": 0.2046,
      "step": 18239
    },
    {
      "epoch": 36.70020120724346,
      "grad_norm": 1.0104193687438965,
      "learning_rate": 0.00012661233524499448,
      "loss": 0.1996,
      "step": 18240
    },
    {
      "epoch": 36.70221327967807,
      "grad_norm": 0.9741945862770081,
      "learning_rate": 0.00012660831069524097,
      "loss": 0.2072,
      "step": 18241
    },
    {
      "epoch": 36.70422535211267,
      "grad_norm": 0.9250195026397705,
      "learning_rate": 0.00012660428614548748,
      "loss": 0.1849,
      "step": 18242
    },
    {
      "epoch": 36.70623742454728,
      "grad_norm": 0.9708171486854553,
      "learning_rate": 0.000126600261595734,
      "loss": 0.2178,
      "step": 18243
    },
    {
      "epoch": 36.70824949698189,
      "grad_norm": 1.0729436874389648,
      "learning_rate": 0.00012659623704598048,
      "loss": 0.2394,
      "step": 18244
    },
    {
      "epoch": 36.7102615694165,
      "grad_norm": 0.9731975197792053,
      "learning_rate": 0.000126592212496227,
      "loss": 0.2012,
      "step": 18245
    },
    {
      "epoch": 36.712273641851105,
      "grad_norm": 1.001131534576416,
      "learning_rate": 0.0001265881879464735,
      "loss": 0.2112,
      "step": 18246
    },
    {
      "epoch": 36.714285714285715,
      "grad_norm": 1.1223989725112915,
      "learning_rate": 0.00012658416339672,
      "loss": 0.2184,
      "step": 18247
    },
    {
      "epoch": 36.716297786720325,
      "grad_norm": 1.0311471223831177,
      "learning_rate": 0.0001265801388469665,
      "loss": 0.2009,
      "step": 18248
    },
    {
      "epoch": 36.71830985915493,
      "grad_norm": 1.0743945837020874,
      "learning_rate": 0.000126576114297213,
      "loss": 0.2119,
      "step": 18249
    },
    {
      "epoch": 36.72032193158954,
      "grad_norm": 0.9767441153526306,
      "learning_rate": 0.0001265720897474595,
      "loss": 0.2141,
      "step": 18250
    },
    {
      "epoch": 36.72233400402415,
      "grad_norm": 0.9220287799835205,
      "learning_rate": 0.000126568065197706,
      "loss": 0.1885,
      "step": 18251
    },
    {
      "epoch": 36.72434607645875,
      "grad_norm": 0.9386100769042969,
      "learning_rate": 0.0001265640406479525,
      "loss": 0.1987,
      "step": 18252
    },
    {
      "epoch": 36.72635814889336,
      "grad_norm": 0.987723708152771,
      "learning_rate": 0.00012656001609819904,
      "loss": 0.2165,
      "step": 18253
    },
    {
      "epoch": 36.72837022132797,
      "grad_norm": 0.9446619749069214,
      "learning_rate": 0.00012655599154844552,
      "loss": 0.1953,
      "step": 18254
    },
    {
      "epoch": 36.730382293762574,
      "grad_norm": 1.0474302768707275,
      "learning_rate": 0.00012655196699869203,
      "loss": 0.2224,
      "step": 18255
    },
    {
      "epoch": 36.732394366197184,
      "grad_norm": 1.0012093782424927,
      "learning_rate": 0.00012654794244893852,
      "loss": 0.2023,
      "step": 18256
    },
    {
      "epoch": 36.734406438631794,
      "grad_norm": 0.968826174736023,
      "learning_rate": 0.00012654391789918503,
      "loss": 0.2193,
      "step": 18257
    },
    {
      "epoch": 36.7364185110664,
      "grad_norm": 0.9795320630073547,
      "learning_rate": 0.00012653989334943154,
      "loss": 0.1953,
      "step": 18258
    },
    {
      "epoch": 36.73843058350101,
      "grad_norm": 1.0173650979995728,
      "learning_rate": 0.00012653586879967805,
      "loss": 0.218,
      "step": 18259
    },
    {
      "epoch": 36.74044265593562,
      "grad_norm": 0.9678876996040344,
      "learning_rate": 0.00012653184424992454,
      "loss": 0.2105,
      "step": 18260
    },
    {
      "epoch": 36.74245472837022,
      "grad_norm": 1.0780571699142456,
      "learning_rate": 0.00012652781970017105,
      "loss": 0.2144,
      "step": 18261
    },
    {
      "epoch": 36.74446680080483,
      "grad_norm": 0.9393901228904724,
      "learning_rate": 0.00012652379515041754,
      "loss": 0.1959,
      "step": 18262
    },
    {
      "epoch": 36.74647887323944,
      "grad_norm": 1.0246740579605103,
      "learning_rate": 0.00012651977060066408,
      "loss": 0.2136,
      "step": 18263
    },
    {
      "epoch": 36.74849094567404,
      "grad_norm": 1.1323432922363281,
      "learning_rate": 0.00012651574605091056,
      "loss": 0.2272,
      "step": 18264
    },
    {
      "epoch": 36.75050301810865,
      "grad_norm": 1.0395269393920898,
      "learning_rate": 0.00012651172150115707,
      "loss": 0.2127,
      "step": 18265
    },
    {
      "epoch": 36.75251509054326,
      "grad_norm": 0.9992552399635315,
      "learning_rate": 0.00012650769695140356,
      "loss": 0.208,
      "step": 18266
    },
    {
      "epoch": 36.754527162977865,
      "grad_norm": 0.9491175413131714,
      "learning_rate": 0.00012650367240165007,
      "loss": 0.1994,
      "step": 18267
    },
    {
      "epoch": 36.756539235412475,
      "grad_norm": 0.967014729976654,
      "learning_rate": 0.00012649964785189658,
      "loss": 0.1992,
      "step": 18268
    },
    {
      "epoch": 36.758551307847085,
      "grad_norm": 0.9822556972503662,
      "learning_rate": 0.0001264956233021431,
      "loss": 0.1962,
      "step": 18269
    },
    {
      "epoch": 36.76056338028169,
      "grad_norm": 0.9628927111625671,
      "learning_rate": 0.00012649159875238958,
      "loss": 0.2021,
      "step": 18270
    },
    {
      "epoch": 36.7625754527163,
      "grad_norm": 1.0565305948257446,
      "learning_rate": 0.0001264875742026361,
      "loss": 0.2379,
      "step": 18271
    },
    {
      "epoch": 36.76458752515091,
      "grad_norm": 1.0523526668548584,
      "learning_rate": 0.00012648354965288258,
      "loss": 0.2151,
      "step": 18272
    },
    {
      "epoch": 36.76659959758551,
      "grad_norm": 0.9298508167266846,
      "learning_rate": 0.00012647952510312912,
      "loss": 0.2014,
      "step": 18273
    },
    {
      "epoch": 36.76861167002012,
      "grad_norm": 1.0110732316970825,
      "learning_rate": 0.0001264755005533756,
      "loss": 0.2084,
      "step": 18274
    },
    {
      "epoch": 36.77062374245473,
      "grad_norm": 1.0565299987792969,
      "learning_rate": 0.00012647147600362211,
      "loss": 0.2018,
      "step": 18275
    },
    {
      "epoch": 36.772635814889334,
      "grad_norm": 1.0433274507522583,
      "learning_rate": 0.0001264674514538686,
      "loss": 0.2178,
      "step": 18276
    },
    {
      "epoch": 36.774647887323944,
      "grad_norm": 0.9808580279350281,
      "learning_rate": 0.0001264634269041151,
      "loss": 0.2146,
      "step": 18277
    },
    {
      "epoch": 36.776659959758554,
      "grad_norm": 1.0819215774536133,
      "learning_rate": 0.00012645940235436162,
      "loss": 0.2215,
      "step": 18278
    },
    {
      "epoch": 36.77867203219316,
      "grad_norm": 0.9329887628555298,
      "learning_rate": 0.0001264553778046081,
      "loss": 0.1923,
      "step": 18279
    },
    {
      "epoch": 36.78068410462777,
      "grad_norm": 1.0114803314208984,
      "learning_rate": 0.00012645135325485462,
      "loss": 0.2219,
      "step": 18280
    },
    {
      "epoch": 36.78269617706238,
      "grad_norm": 1.0033632516860962,
      "learning_rate": 0.00012644732870510113,
      "loss": 0.1975,
      "step": 18281
    },
    {
      "epoch": 36.78470824949698,
      "grad_norm": 0.9951142072677612,
      "learning_rate": 0.00012644330415534762,
      "loss": 0.2217,
      "step": 18282
    },
    {
      "epoch": 36.78672032193159,
      "grad_norm": 0.9562411308288574,
      "learning_rate": 0.00012643927960559413,
      "loss": 0.1804,
      "step": 18283
    },
    {
      "epoch": 36.7887323943662,
      "grad_norm": 0.9767035841941833,
      "learning_rate": 0.00012643525505584064,
      "loss": 0.2059,
      "step": 18284
    },
    {
      "epoch": 36.7907444668008,
      "grad_norm": 1.0465302467346191,
      "learning_rate": 0.00012643123050608713,
      "loss": 0.2115,
      "step": 18285
    },
    {
      "epoch": 36.79275653923541,
      "grad_norm": 0.9226076602935791,
      "learning_rate": 0.00012642720595633364,
      "loss": 0.1888,
      "step": 18286
    },
    {
      "epoch": 36.79476861167002,
      "grad_norm": 1.102217197418213,
      "learning_rate": 0.00012642318140658013,
      "loss": 0.2335,
      "step": 18287
    },
    {
      "epoch": 36.796780684104625,
      "grad_norm": 0.9393630623817444,
      "learning_rate": 0.00012641915685682666,
      "loss": 0.1988,
      "step": 18288
    },
    {
      "epoch": 36.798792756539235,
      "grad_norm": 1.061874508857727,
      "learning_rate": 0.00012641513230707315,
      "loss": 0.2039,
      "step": 18289
    },
    {
      "epoch": 36.800804828973845,
      "grad_norm": 0.9576920866966248,
      "learning_rate": 0.00012641110775731966,
      "loss": 0.2151,
      "step": 18290
    },
    {
      "epoch": 36.80281690140845,
      "grad_norm": 0.9441865086555481,
      "learning_rate": 0.00012640708320756615,
      "loss": 0.2024,
      "step": 18291
    },
    {
      "epoch": 36.80482897384306,
      "grad_norm": 0.9790614247322083,
      "learning_rate": 0.00012640305865781266,
      "loss": 0.2148,
      "step": 18292
    },
    {
      "epoch": 36.80684104627767,
      "grad_norm": 1.0850944519042969,
      "learning_rate": 0.00012639903410805917,
      "loss": 0.2259,
      "step": 18293
    },
    {
      "epoch": 36.80885311871227,
      "grad_norm": 1.0192198753356934,
      "learning_rate": 0.00012639500955830568,
      "loss": 0.2252,
      "step": 18294
    },
    {
      "epoch": 36.81086519114688,
      "grad_norm": 0.96152263879776,
      "learning_rate": 0.00012639098500855217,
      "loss": 0.2257,
      "step": 18295
    },
    {
      "epoch": 36.81287726358149,
      "grad_norm": 0.9418460726737976,
      "learning_rate": 0.00012638696045879868,
      "loss": 0.2176,
      "step": 18296
    },
    {
      "epoch": 36.814889336016094,
      "grad_norm": 0.9192323684692383,
      "learning_rate": 0.00012638293590904517,
      "loss": 0.2057,
      "step": 18297
    },
    {
      "epoch": 36.816901408450704,
      "grad_norm": 0.9411118626594543,
      "learning_rate": 0.0001263789113592917,
      "loss": 0.197,
      "step": 18298
    },
    {
      "epoch": 36.818913480885314,
      "grad_norm": 0.9815930724143982,
      "learning_rate": 0.0001263748868095382,
      "loss": 0.1916,
      "step": 18299
    },
    {
      "epoch": 36.82092555331992,
      "grad_norm": 0.9873383641242981,
      "learning_rate": 0.0001263708622597847,
      "loss": 0.2089,
      "step": 18300
    },
    {
      "epoch": 36.82293762575453,
      "grad_norm": 1.0000275373458862,
      "learning_rate": 0.0001263668377100312,
      "loss": 0.2231,
      "step": 18301
    },
    {
      "epoch": 36.82494969818914,
      "grad_norm": 1.0058696269989014,
      "learning_rate": 0.0001263628131602777,
      "loss": 0.2163,
      "step": 18302
    },
    {
      "epoch": 36.82696177062374,
      "grad_norm": 1.0311259031295776,
      "learning_rate": 0.0001263587886105242,
      "loss": 0.2152,
      "step": 18303
    },
    {
      "epoch": 36.82897384305835,
      "grad_norm": 1.0575525760650635,
      "learning_rate": 0.00012635476406077072,
      "loss": 0.2183,
      "step": 18304
    },
    {
      "epoch": 36.83098591549296,
      "grad_norm": 0.9946471452713013,
      "learning_rate": 0.0001263507395110172,
      "loss": 0.2112,
      "step": 18305
    },
    {
      "epoch": 36.83299798792756,
      "grad_norm": 0.9659170508384705,
      "learning_rate": 0.00012634671496126372,
      "loss": 0.2136,
      "step": 18306
    },
    {
      "epoch": 36.83501006036217,
      "grad_norm": 1.020714521408081,
      "learning_rate": 0.0001263426904115102,
      "loss": 0.2406,
      "step": 18307
    },
    {
      "epoch": 36.83702213279678,
      "grad_norm": 0.9763526916503906,
      "learning_rate": 0.00012633866586175672,
      "loss": 0.1993,
      "step": 18308
    },
    {
      "epoch": 36.839034205231385,
      "grad_norm": 1.0312633514404297,
      "learning_rate": 0.00012633464131200323,
      "loss": 0.2048,
      "step": 18309
    },
    {
      "epoch": 36.841046277665995,
      "grad_norm": 1.0110026597976685,
      "learning_rate": 0.00012633061676224974,
      "loss": 0.2259,
      "step": 18310
    },
    {
      "epoch": 36.843058350100605,
      "grad_norm": 0.9618978500366211,
      "learning_rate": 0.00012632659221249623,
      "loss": 0.2027,
      "step": 18311
    },
    {
      "epoch": 36.84507042253521,
      "grad_norm": 1.0133854150772095,
      "learning_rate": 0.00012632256766274274,
      "loss": 0.2331,
      "step": 18312
    },
    {
      "epoch": 36.84708249496982,
      "grad_norm": 1.0757880210876465,
      "learning_rate": 0.00012631854311298923,
      "loss": 0.2095,
      "step": 18313
    },
    {
      "epoch": 36.84909456740443,
      "grad_norm": 0.9277114272117615,
      "learning_rate": 0.00012631451856323574,
      "loss": 0.2047,
      "step": 18314
    },
    {
      "epoch": 36.85110663983903,
      "grad_norm": 1.0467908382415771,
      "learning_rate": 0.00012631049401348225,
      "loss": 0.224,
      "step": 18315
    },
    {
      "epoch": 36.85311871227364,
      "grad_norm": 1.049609899520874,
      "learning_rate": 0.00012630646946372876,
      "loss": 0.241,
      "step": 18316
    },
    {
      "epoch": 36.85513078470825,
      "grad_norm": 0.9353832602500916,
      "learning_rate": 0.00012630244491397525,
      "loss": 0.1993,
      "step": 18317
    },
    {
      "epoch": 36.857142857142854,
      "grad_norm": 0.9703640341758728,
      "learning_rate": 0.00012629842036422176,
      "loss": 0.2229,
      "step": 18318
    },
    {
      "epoch": 36.859154929577464,
      "grad_norm": 0.956160843372345,
      "learning_rate": 0.00012629439581446827,
      "loss": 0.2101,
      "step": 18319
    },
    {
      "epoch": 36.861167002012074,
      "grad_norm": 1.0185288190841675,
      "learning_rate": 0.00012629037126471476,
      "loss": 0.2118,
      "step": 18320
    },
    {
      "epoch": 36.86317907444668,
      "grad_norm": 1.0069156885147095,
      "learning_rate": 0.00012628634671496127,
      "loss": 0.2103,
      "step": 18321
    },
    {
      "epoch": 36.86519114688129,
      "grad_norm": 0.944003164768219,
      "learning_rate": 0.00012628232216520775,
      "loss": 0.2025,
      "step": 18322
    },
    {
      "epoch": 36.8672032193159,
      "grad_norm": 0.96849524974823,
      "learning_rate": 0.00012627829761545427,
      "loss": 0.1913,
      "step": 18323
    },
    {
      "epoch": 36.8692152917505,
      "grad_norm": 1.0455573797225952,
      "learning_rate": 0.00012627427306570078,
      "loss": 0.217,
      "step": 18324
    },
    {
      "epoch": 36.87122736418511,
      "grad_norm": 0.9791860580444336,
      "learning_rate": 0.0001262702485159473,
      "loss": 0.2138,
      "step": 18325
    },
    {
      "epoch": 36.87323943661972,
      "grad_norm": 1.0091655254364014,
      "learning_rate": 0.00012626622396619378,
      "loss": 0.2121,
      "step": 18326
    },
    {
      "epoch": 36.87525150905432,
      "grad_norm": 1.0190621614456177,
      "learning_rate": 0.0001262621994164403,
      "loss": 0.2262,
      "step": 18327
    },
    {
      "epoch": 36.87726358148893,
      "grad_norm": 1.0433740615844727,
      "learning_rate": 0.00012625817486668677,
      "loss": 0.2184,
      "step": 18328
    },
    {
      "epoch": 36.87927565392354,
      "grad_norm": 0.9781575202941895,
      "learning_rate": 0.0001262541503169333,
      "loss": 0.1959,
      "step": 18329
    },
    {
      "epoch": 36.881287726358146,
      "grad_norm": 0.9469864964485168,
      "learning_rate": 0.0001262501257671798,
      "loss": 0.2078,
      "step": 18330
    },
    {
      "epoch": 36.883299798792756,
      "grad_norm": 1.081930160522461,
      "learning_rate": 0.0001262461012174263,
      "loss": 0.2154,
      "step": 18331
    },
    {
      "epoch": 36.885311871227366,
      "grad_norm": 1.0048164129257202,
      "learning_rate": 0.0001262420766676728,
      "loss": 0.2054,
      "step": 18332
    },
    {
      "epoch": 36.88732394366197,
      "grad_norm": 1.014840006828308,
      "learning_rate": 0.0001262380521179193,
      "loss": 0.2289,
      "step": 18333
    },
    {
      "epoch": 36.88933601609658,
      "grad_norm": 1.1034488677978516,
      "learning_rate": 0.00012623402756816582,
      "loss": 0.2386,
      "step": 18334
    },
    {
      "epoch": 36.89134808853119,
      "grad_norm": 0.9763128161430359,
      "learning_rate": 0.00012623000301841233,
      "loss": 0.2235,
      "step": 18335
    },
    {
      "epoch": 36.89336016096579,
      "grad_norm": 1.0123258829116821,
      "learning_rate": 0.00012622597846865882,
      "loss": 0.2288,
      "step": 18336
    },
    {
      "epoch": 36.8953722334004,
      "grad_norm": 1.0147510766983032,
      "learning_rate": 0.00012622195391890533,
      "loss": 0.2266,
      "step": 18337
    },
    {
      "epoch": 36.89738430583501,
      "grad_norm": 1.0136818885803223,
      "learning_rate": 0.00012621792936915181,
      "loss": 0.2153,
      "step": 18338
    },
    {
      "epoch": 36.899396378269614,
      "grad_norm": 1.0461372137069702,
      "learning_rate": 0.00012621390481939835,
      "loss": 0.2216,
      "step": 18339
    },
    {
      "epoch": 36.901408450704224,
      "grad_norm": 0.9627413749694824,
      "learning_rate": 0.00012620988026964484,
      "loss": 0.2099,
      "step": 18340
    },
    {
      "epoch": 36.903420523138834,
      "grad_norm": 1.0454868078231812,
      "learning_rate": 0.00012620585571989135,
      "loss": 0.216,
      "step": 18341
    },
    {
      "epoch": 36.905432595573444,
      "grad_norm": 1.0438610315322876,
      "learning_rate": 0.00012620183117013784,
      "loss": 0.2109,
      "step": 18342
    },
    {
      "epoch": 36.90744466800805,
      "grad_norm": 0.9787282347679138,
      "learning_rate": 0.00012619780662038435,
      "loss": 0.2191,
      "step": 18343
    },
    {
      "epoch": 36.90945674044266,
      "grad_norm": 0.9595913887023926,
      "learning_rate": 0.00012619378207063086,
      "loss": 0.2148,
      "step": 18344
    },
    {
      "epoch": 36.91146881287727,
      "grad_norm": 0.9344608187675476,
      "learning_rate": 0.00012618975752087737,
      "loss": 0.2031,
      "step": 18345
    },
    {
      "epoch": 36.91348088531187,
      "grad_norm": 0.9268823266029358,
      "learning_rate": 0.00012618573297112386,
      "loss": 0.2097,
      "step": 18346
    },
    {
      "epoch": 36.91549295774648,
      "grad_norm": 0.980080783367157,
      "learning_rate": 0.00012618170842137037,
      "loss": 0.2159,
      "step": 18347
    },
    {
      "epoch": 36.91750503018109,
      "grad_norm": 1.0511716604232788,
      "learning_rate": 0.00012617768387161686,
      "loss": 0.2305,
      "step": 18348
    },
    {
      "epoch": 36.91951710261569,
      "grad_norm": 1.0161038637161255,
      "learning_rate": 0.00012617365932186337,
      "loss": 0.2164,
      "step": 18349
    },
    {
      "epoch": 36.9215291750503,
      "grad_norm": 1.0546095371246338,
      "learning_rate": 0.00012616963477210988,
      "loss": 0.2023,
      "step": 18350
    },
    {
      "epoch": 36.92354124748491,
      "grad_norm": 0.9776219725608826,
      "learning_rate": 0.00012616561022235636,
      "loss": 0.2029,
      "step": 18351
    },
    {
      "epoch": 36.925553319919516,
      "grad_norm": 0.9574097990989685,
      "learning_rate": 0.00012616158567260288,
      "loss": 0.2064,
      "step": 18352
    },
    {
      "epoch": 36.927565392354126,
      "grad_norm": 0.9682729244232178,
      "learning_rate": 0.0001261575611228494,
      "loss": 0.2166,
      "step": 18353
    },
    {
      "epoch": 36.929577464788736,
      "grad_norm": 1.0311156511306763,
      "learning_rate": 0.0001261535365730959,
      "loss": 0.229,
      "step": 18354
    },
    {
      "epoch": 36.93158953722334,
      "grad_norm": 0.9810583591461182,
      "learning_rate": 0.0001261495120233424,
      "loss": 0.2018,
      "step": 18355
    },
    {
      "epoch": 36.93360160965795,
      "grad_norm": 0.9264811277389526,
      "learning_rate": 0.0001261454874735889,
      "loss": 0.1984,
      "step": 18356
    },
    {
      "epoch": 36.93561368209256,
      "grad_norm": 1.0042215585708618,
      "learning_rate": 0.00012614146292383538,
      "loss": 0.213,
      "step": 18357
    },
    {
      "epoch": 36.93762575452716,
      "grad_norm": 0.9650513529777527,
      "learning_rate": 0.0001261374383740819,
      "loss": 0.2046,
      "step": 18358
    },
    {
      "epoch": 36.93963782696177,
      "grad_norm": 1.055446982383728,
      "learning_rate": 0.0001261334138243284,
      "loss": 0.2393,
      "step": 18359
    },
    {
      "epoch": 36.94164989939638,
      "grad_norm": 1.0224701166152954,
      "learning_rate": 0.00012612938927457492,
      "loss": 0.207,
      "step": 18360
    },
    {
      "epoch": 36.943661971830984,
      "grad_norm": 1.036776065826416,
      "learning_rate": 0.0001261253647248214,
      "loss": 0.2331,
      "step": 18361
    },
    {
      "epoch": 36.945674044265594,
      "grad_norm": 1.0283757448196411,
      "learning_rate": 0.00012612134017506792,
      "loss": 0.2279,
      "step": 18362
    },
    {
      "epoch": 36.947686116700204,
      "grad_norm": 0.9639893770217896,
      "learning_rate": 0.0001261173156253144,
      "loss": 0.196,
      "step": 18363
    },
    {
      "epoch": 36.94969818913481,
      "grad_norm": 0.9489967226982117,
      "learning_rate": 0.00012611329107556094,
      "loss": 0.2041,
      "step": 18364
    },
    {
      "epoch": 36.95171026156942,
      "grad_norm": 1.0424124002456665,
      "learning_rate": 0.00012610926652580743,
      "loss": 0.2309,
      "step": 18365
    },
    {
      "epoch": 36.95372233400403,
      "grad_norm": 1.002647042274475,
      "learning_rate": 0.00012610524197605394,
      "loss": 0.2157,
      "step": 18366
    },
    {
      "epoch": 36.95573440643863,
      "grad_norm": 1.003139615058899,
      "learning_rate": 0.00012610121742630042,
      "loss": 0.2178,
      "step": 18367
    },
    {
      "epoch": 36.95774647887324,
      "grad_norm": 0.996152937412262,
      "learning_rate": 0.00012609719287654694,
      "loss": 0.198,
      "step": 18368
    },
    {
      "epoch": 36.95975855130785,
      "grad_norm": 0.975715160369873,
      "learning_rate": 0.00012609316832679345,
      "loss": 0.2121,
      "step": 18369
    },
    {
      "epoch": 36.96177062374245,
      "grad_norm": 1.0193061828613281,
      "learning_rate": 0.00012608914377703996,
      "loss": 0.2163,
      "step": 18370
    },
    {
      "epoch": 36.96378269617706,
      "grad_norm": 0.9790644645690918,
      "learning_rate": 0.00012608511922728645,
      "loss": 0.2217,
      "step": 18371
    },
    {
      "epoch": 36.96579476861167,
      "grad_norm": 1.0701549053192139,
      "learning_rate": 0.00012608109467753296,
      "loss": 0.2233,
      "step": 18372
    },
    {
      "epoch": 36.967806841046276,
      "grad_norm": 0.9674647450447083,
      "learning_rate": 0.00012607707012777944,
      "loss": 0.2178,
      "step": 18373
    },
    {
      "epoch": 36.969818913480886,
      "grad_norm": 1.0325413942337036,
      "learning_rate": 0.00012607304557802598,
      "loss": 0.2123,
      "step": 18374
    },
    {
      "epoch": 36.971830985915496,
      "grad_norm": 1.0088515281677246,
      "learning_rate": 0.00012606902102827247,
      "loss": 0.207,
      "step": 18375
    },
    {
      "epoch": 36.9738430583501,
      "grad_norm": 0.9971997141838074,
      "learning_rate": 0.00012606499647851898,
      "loss": 0.2331,
      "step": 18376
    },
    {
      "epoch": 36.97585513078471,
      "grad_norm": 1.0240976810455322,
      "learning_rate": 0.00012606097192876547,
      "loss": 0.2045,
      "step": 18377
    },
    {
      "epoch": 36.97786720321932,
      "grad_norm": 1.073020577430725,
      "learning_rate": 0.00012605694737901198,
      "loss": 0.2217,
      "step": 18378
    },
    {
      "epoch": 36.97987927565392,
      "grad_norm": 0.9965285062789917,
      "learning_rate": 0.0001260529228292585,
      "loss": 0.2249,
      "step": 18379
    },
    {
      "epoch": 36.98189134808853,
      "grad_norm": 1.070623755455017,
      "learning_rate": 0.000126048898279505,
      "loss": 0.219,
      "step": 18380
    },
    {
      "epoch": 36.98390342052314,
      "grad_norm": 0.9810028076171875,
      "learning_rate": 0.0001260448737297515,
      "loss": 0.21,
      "step": 18381
    },
    {
      "epoch": 36.985915492957744,
      "grad_norm": 0.9903735518455505,
      "learning_rate": 0.000126040849179998,
      "loss": 0.2084,
      "step": 18382
    },
    {
      "epoch": 36.987927565392354,
      "grad_norm": 1.0268564224243164,
      "learning_rate": 0.00012603682463024448,
      "loss": 0.2213,
      "step": 18383
    },
    {
      "epoch": 36.989939637826964,
      "grad_norm": 1.0063631534576416,
      "learning_rate": 0.000126032800080491,
      "loss": 0.2139,
      "step": 18384
    },
    {
      "epoch": 36.99195171026157,
      "grad_norm": 1.0084538459777832,
      "learning_rate": 0.0001260287755307375,
      "loss": 0.2226,
      "step": 18385
    },
    {
      "epoch": 36.99396378269618,
      "grad_norm": 1.0099685192108154,
      "learning_rate": 0.000126024750980984,
      "loss": 0.2244,
      "step": 18386
    },
    {
      "epoch": 36.99597585513079,
      "grad_norm": 1.0020127296447754,
      "learning_rate": 0.0001260207264312305,
      "loss": 0.2112,
      "step": 18387
    },
    {
      "epoch": 36.99798792756539,
      "grad_norm": 1.0339351892471313,
      "learning_rate": 0.00012601670188147702,
      "loss": 0.2357,
      "step": 18388
    },
    {
      "epoch": 37.0,
      "grad_norm": 1.0529699325561523,
      "learning_rate": 0.00012601267733172353,
      "loss": 0.221,
      "step": 18389
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.5326262712478638,
      "eval_runtime": 49.7968,
      "eval_samples_per_second": 19.921,
      "eval_steps_per_second": 2.49,
      "step": 18389
    },
    {
      "epoch": 37.00201207243461,
      "grad_norm": 0.8140925168991089,
      "learning_rate": 0.00012600865278197002,
      "loss": 0.1815,
      "step": 18390
    },
    {
      "epoch": 37.00402414486921,
      "grad_norm": 0.824164867401123,
      "learning_rate": 0.00012600462823221653,
      "loss": 0.1704,
      "step": 18391
    },
    {
      "epoch": 37.00603621730382,
      "grad_norm": 0.8216022253036499,
      "learning_rate": 0.000126000603682463,
      "loss": 0.1631,
      "step": 18392
    },
    {
      "epoch": 37.00804828973843,
      "grad_norm": 0.8881436586380005,
      "learning_rate": 0.00012599657913270953,
      "loss": 0.1731,
      "step": 18393
    },
    {
      "epoch": 37.010060362173036,
      "grad_norm": 0.9634934067726135,
      "learning_rate": 0.00012599255458295604,
      "loss": 0.1768,
      "step": 18394
    },
    {
      "epoch": 37.012072434607646,
      "grad_norm": 0.9337162375450134,
      "learning_rate": 0.00012598853003320255,
      "loss": 0.1658,
      "step": 18395
    },
    {
      "epoch": 37.014084507042256,
      "grad_norm": 0.892336368560791,
      "learning_rate": 0.00012598450548344904,
      "loss": 0.1665,
      "step": 18396
    },
    {
      "epoch": 37.01609657947686,
      "grad_norm": 0.8509668111801147,
      "learning_rate": 0.00012598048093369555,
      "loss": 0.1582,
      "step": 18397
    },
    {
      "epoch": 37.01810865191147,
      "grad_norm": 0.8018302321434021,
      "learning_rate": 0.00012597645638394203,
      "loss": 0.1489,
      "step": 18398
    },
    {
      "epoch": 37.02012072434608,
      "grad_norm": 0.8839932680130005,
      "learning_rate": 0.00012597243183418857,
      "loss": 0.1717,
      "step": 18399
    },
    {
      "epoch": 37.02213279678068,
      "grad_norm": 0.8760423064231873,
      "learning_rate": 0.00012596840728443506,
      "loss": 0.1649,
      "step": 18400
    },
    {
      "epoch": 37.02414486921529,
      "grad_norm": 0.8756305575370789,
      "learning_rate": 0.00012596438273468157,
      "loss": 0.1718,
      "step": 18401
    },
    {
      "epoch": 37.0261569416499,
      "grad_norm": 0.8443242311477661,
      "learning_rate": 0.00012596035818492805,
      "loss": 0.1646,
      "step": 18402
    },
    {
      "epoch": 37.028169014084504,
      "grad_norm": 0.8123610615730286,
      "learning_rate": 0.00012595633363517457,
      "loss": 0.1682,
      "step": 18403
    },
    {
      "epoch": 37.030181086519114,
      "grad_norm": 0.8745318651199341,
      "learning_rate": 0.00012595230908542108,
      "loss": 0.1613,
      "step": 18404
    },
    {
      "epoch": 37.032193158953724,
      "grad_norm": 0.9044844508171082,
      "learning_rate": 0.0001259482845356676,
      "loss": 0.1652,
      "step": 18405
    },
    {
      "epoch": 37.03420523138833,
      "grad_norm": 0.8892126083374023,
      "learning_rate": 0.00012594425998591408,
      "loss": 0.1621,
      "step": 18406
    },
    {
      "epoch": 37.03621730382294,
      "grad_norm": 0.9067818522453308,
      "learning_rate": 0.0001259402354361606,
      "loss": 0.1737,
      "step": 18407
    },
    {
      "epoch": 37.03822937625755,
      "grad_norm": 0.8935151100158691,
      "learning_rate": 0.00012593621088640707,
      "loss": 0.1574,
      "step": 18408
    },
    {
      "epoch": 37.04024144869215,
      "grad_norm": 0.9377645254135132,
      "learning_rate": 0.0001259321863366536,
      "loss": 0.1735,
      "step": 18409
    },
    {
      "epoch": 37.04225352112676,
      "grad_norm": 0.9486944079399109,
      "learning_rate": 0.0001259281617869001,
      "loss": 0.1787,
      "step": 18410
    },
    {
      "epoch": 37.04426559356137,
      "grad_norm": 0.8739142417907715,
      "learning_rate": 0.0001259241372371466,
      "loss": 0.1677,
      "step": 18411
    },
    {
      "epoch": 37.04627766599597,
      "grad_norm": 0.8380693197250366,
      "learning_rate": 0.0001259201126873931,
      "loss": 0.1617,
      "step": 18412
    },
    {
      "epoch": 37.04828973843058,
      "grad_norm": 0.9122408628463745,
      "learning_rate": 0.0001259160881376396,
      "loss": 0.1723,
      "step": 18413
    },
    {
      "epoch": 37.05030181086519,
      "grad_norm": 0.8422020077705383,
      "learning_rate": 0.00012591206358788612,
      "loss": 0.1584,
      "step": 18414
    },
    {
      "epoch": 37.052313883299796,
      "grad_norm": 0.8371554613113403,
      "learning_rate": 0.00012590803903813263,
      "loss": 0.1612,
      "step": 18415
    },
    {
      "epoch": 37.054325955734406,
      "grad_norm": 0.953912079334259,
      "learning_rate": 0.00012590401448837912,
      "loss": 0.1672,
      "step": 18416
    },
    {
      "epoch": 37.056338028169016,
      "grad_norm": 0.8434497714042664,
      "learning_rate": 0.00012589998993862563,
      "loss": 0.1673,
      "step": 18417
    },
    {
      "epoch": 37.05835010060362,
      "grad_norm": 0.8518991470336914,
      "learning_rate": 0.00012589596538887211,
      "loss": 0.1658,
      "step": 18418
    },
    {
      "epoch": 37.06036217303823,
      "grad_norm": 0.8766126036643982,
      "learning_rate": 0.00012589194083911863,
      "loss": 0.156,
      "step": 18419
    },
    {
      "epoch": 37.06237424547284,
      "grad_norm": 0.8910407423973083,
      "learning_rate": 0.00012588791628936514,
      "loss": 0.1724,
      "step": 18420
    },
    {
      "epoch": 37.06438631790744,
      "grad_norm": 1.0082663297653198,
      "learning_rate": 0.00012588389173961162,
      "loss": 0.1897,
      "step": 18421
    },
    {
      "epoch": 37.06639839034205,
      "grad_norm": 0.8651750683784485,
      "learning_rate": 0.00012587986718985814,
      "loss": 0.1679,
      "step": 18422
    },
    {
      "epoch": 37.06841046277666,
      "grad_norm": 0.849460244178772,
      "learning_rate": 0.00012587584264010465,
      "loss": 0.1583,
      "step": 18423
    },
    {
      "epoch": 37.070422535211264,
      "grad_norm": 0.9092452526092529,
      "learning_rate": 0.00012587181809035116,
      "loss": 0.1636,
      "step": 18424
    },
    {
      "epoch": 37.072434607645874,
      "grad_norm": 0.874008297920227,
      "learning_rate": 0.00012586779354059765,
      "loss": 0.1658,
      "step": 18425
    },
    {
      "epoch": 37.074446680080484,
      "grad_norm": 0.8242462277412415,
      "learning_rate": 0.00012586376899084416,
      "loss": 0.167,
      "step": 18426
    },
    {
      "epoch": 37.07645875251509,
      "grad_norm": 0.8735817074775696,
      "learning_rate": 0.00012585974444109064,
      "loss": 0.1684,
      "step": 18427
    },
    {
      "epoch": 37.0784708249497,
      "grad_norm": 0.817645251750946,
      "learning_rate": 0.00012585571989133716,
      "loss": 0.1486,
      "step": 18428
    },
    {
      "epoch": 37.08048289738431,
      "grad_norm": 0.9146503806114197,
      "learning_rate": 0.00012585169534158367,
      "loss": 0.1692,
      "step": 18429
    },
    {
      "epoch": 37.08249496981891,
      "grad_norm": 0.863140881061554,
      "learning_rate": 0.00012584767079183018,
      "loss": 0.1594,
      "step": 18430
    },
    {
      "epoch": 37.08450704225352,
      "grad_norm": 0.8753390908241272,
      "learning_rate": 0.00012584364624207666,
      "loss": 0.1565,
      "step": 18431
    },
    {
      "epoch": 37.08651911468813,
      "grad_norm": 0.9449566006660461,
      "learning_rate": 0.00012583962169232318,
      "loss": 0.173,
      "step": 18432
    },
    {
      "epoch": 37.08853118712273,
      "grad_norm": 0.8838595747947693,
      "learning_rate": 0.00012583559714256966,
      "loss": 0.1662,
      "step": 18433
    },
    {
      "epoch": 37.09054325955734,
      "grad_norm": 0.9550409317016602,
      "learning_rate": 0.0001258315725928162,
      "loss": 0.1754,
      "step": 18434
    },
    {
      "epoch": 37.09255533199195,
      "grad_norm": 0.877804696559906,
      "learning_rate": 0.00012582754804306269,
      "loss": 0.1659,
      "step": 18435
    },
    {
      "epoch": 37.094567404426556,
      "grad_norm": 0.9007775187492371,
      "learning_rate": 0.0001258235234933092,
      "loss": 0.1757,
      "step": 18436
    },
    {
      "epoch": 37.096579476861166,
      "grad_norm": 0.87244713306427,
      "learning_rate": 0.00012581949894355568,
      "loss": 0.1555,
      "step": 18437
    },
    {
      "epoch": 37.098591549295776,
      "grad_norm": 0.797669529914856,
      "learning_rate": 0.0001258154743938022,
      "loss": 0.1505,
      "step": 18438
    },
    {
      "epoch": 37.100603621730386,
      "grad_norm": 0.8579545617103577,
      "learning_rate": 0.0001258114498440487,
      "loss": 0.1592,
      "step": 18439
    },
    {
      "epoch": 37.10261569416499,
      "grad_norm": 0.946014940738678,
      "learning_rate": 0.00012580742529429522,
      "loss": 0.175,
      "step": 18440
    },
    {
      "epoch": 37.1046277665996,
      "grad_norm": 0.8992966413497925,
      "learning_rate": 0.0001258034007445417,
      "loss": 0.1612,
      "step": 18441
    },
    {
      "epoch": 37.10663983903421,
      "grad_norm": 0.8352088928222656,
      "learning_rate": 0.00012579937619478822,
      "loss": 0.1489,
      "step": 18442
    },
    {
      "epoch": 37.10865191146881,
      "grad_norm": 0.8331469893455505,
      "learning_rate": 0.0001257953516450347,
      "loss": 0.1502,
      "step": 18443
    },
    {
      "epoch": 37.11066398390342,
      "grad_norm": 0.8733432292938232,
      "learning_rate": 0.00012579132709528124,
      "loss": 0.1753,
      "step": 18444
    },
    {
      "epoch": 37.11267605633803,
      "grad_norm": 0.8613178730010986,
      "learning_rate": 0.00012578730254552773,
      "loss": 0.1728,
      "step": 18445
    },
    {
      "epoch": 37.114688128772634,
      "grad_norm": 0.9326547384262085,
      "learning_rate": 0.00012578327799577424,
      "loss": 0.1672,
      "step": 18446
    },
    {
      "epoch": 37.116700201207244,
      "grad_norm": 0.8310763239860535,
      "learning_rate": 0.00012577925344602072,
      "loss": 0.1631,
      "step": 18447
    },
    {
      "epoch": 37.118712273641854,
      "grad_norm": 0.8573911786079407,
      "learning_rate": 0.00012577522889626724,
      "loss": 0.1565,
      "step": 18448
    },
    {
      "epoch": 37.12072434607646,
      "grad_norm": 0.9021256566047668,
      "learning_rate": 0.00012577120434651375,
      "loss": 0.1738,
      "step": 18449
    },
    {
      "epoch": 37.12273641851107,
      "grad_norm": 0.8511332273483276,
      "learning_rate": 0.00012576717979676026,
      "loss": 0.1502,
      "step": 18450
    },
    {
      "epoch": 37.12474849094568,
      "grad_norm": 0.9184112548828125,
      "learning_rate": 0.00012576315524700675,
      "loss": 0.1621,
      "step": 18451
    },
    {
      "epoch": 37.12676056338028,
      "grad_norm": 0.878614068031311,
      "learning_rate": 0.00012575913069725326,
      "loss": 0.157,
      "step": 18452
    },
    {
      "epoch": 37.12877263581489,
      "grad_norm": 0.9022209048271179,
      "learning_rate": 0.00012575510614749974,
      "loss": 0.1775,
      "step": 18453
    },
    {
      "epoch": 37.1307847082495,
      "grad_norm": 0.8789007663726807,
      "learning_rate": 0.00012575108159774626,
      "loss": 0.1563,
      "step": 18454
    },
    {
      "epoch": 37.1327967806841,
      "grad_norm": 0.8721303343772888,
      "learning_rate": 0.00012574705704799277,
      "loss": 0.1543,
      "step": 18455
    },
    {
      "epoch": 37.13480885311871,
      "grad_norm": 0.9079372882843018,
      "learning_rate": 0.00012574303249823925,
      "loss": 0.1761,
      "step": 18456
    },
    {
      "epoch": 37.13682092555332,
      "grad_norm": 0.8938468098640442,
      "learning_rate": 0.00012573900794848577,
      "loss": 0.1669,
      "step": 18457
    },
    {
      "epoch": 37.138832997987926,
      "grad_norm": 0.8944740295410156,
      "learning_rate": 0.00012573498339873228,
      "loss": 0.1689,
      "step": 18458
    },
    {
      "epoch": 37.140845070422536,
      "grad_norm": 0.8759877681732178,
      "learning_rate": 0.0001257309588489788,
      "loss": 0.1631,
      "step": 18459
    },
    {
      "epoch": 37.142857142857146,
      "grad_norm": 0.839120626449585,
      "learning_rate": 0.00012572693429922528,
      "loss": 0.1629,
      "step": 18460
    },
    {
      "epoch": 37.14486921529175,
      "grad_norm": 0.8604263663291931,
      "learning_rate": 0.0001257229097494718,
      "loss": 0.1735,
      "step": 18461
    },
    {
      "epoch": 37.14688128772636,
      "grad_norm": 0.9384557604789734,
      "learning_rate": 0.00012571888519971827,
      "loss": 0.1656,
      "step": 18462
    },
    {
      "epoch": 37.14889336016097,
      "grad_norm": 0.9525998830795288,
      "learning_rate": 0.00012571486064996478,
      "loss": 0.1912,
      "step": 18463
    },
    {
      "epoch": 37.15090543259557,
      "grad_norm": 0.9941838383674622,
      "learning_rate": 0.0001257108361002113,
      "loss": 0.1699,
      "step": 18464
    },
    {
      "epoch": 37.15291750503018,
      "grad_norm": 0.9046655297279358,
      "learning_rate": 0.0001257068115504578,
      "loss": 0.1647,
      "step": 18465
    },
    {
      "epoch": 37.15492957746479,
      "grad_norm": 0.9020455479621887,
      "learning_rate": 0.0001257027870007043,
      "loss": 0.1747,
      "step": 18466
    },
    {
      "epoch": 37.156941649899395,
      "grad_norm": 0.9003085494041443,
      "learning_rate": 0.0001256987624509508,
      "loss": 0.1865,
      "step": 18467
    },
    {
      "epoch": 37.158953722334005,
      "grad_norm": 0.8970436453819275,
      "learning_rate": 0.0001256947379011973,
      "loss": 0.1736,
      "step": 18468
    },
    {
      "epoch": 37.160965794768615,
      "grad_norm": 0.9706458449363708,
      "learning_rate": 0.00012569071335144383,
      "loss": 0.1819,
      "step": 18469
    },
    {
      "epoch": 37.16297786720322,
      "grad_norm": 0.9231985807418823,
      "learning_rate": 0.00012568668880169032,
      "loss": 0.1688,
      "step": 18470
    },
    {
      "epoch": 37.16498993963783,
      "grad_norm": 0.8549240231513977,
      "learning_rate": 0.00012568266425193683,
      "loss": 0.1668,
      "step": 18471
    },
    {
      "epoch": 37.16700201207244,
      "grad_norm": 0.859927237033844,
      "learning_rate": 0.0001256786397021833,
      "loss": 0.1629,
      "step": 18472
    },
    {
      "epoch": 37.16901408450704,
      "grad_norm": 0.8620539307594299,
      "learning_rate": 0.00012567461515242983,
      "loss": 0.1794,
      "step": 18473
    },
    {
      "epoch": 37.17102615694165,
      "grad_norm": 0.9114037156105042,
      "learning_rate": 0.00012567059060267634,
      "loss": 0.1723,
      "step": 18474
    },
    {
      "epoch": 37.17303822937626,
      "grad_norm": 0.9009431600570679,
      "learning_rate": 0.00012566656605292285,
      "loss": 0.1743,
      "step": 18475
    },
    {
      "epoch": 37.17505030181086,
      "grad_norm": 0.9081478714942932,
      "learning_rate": 0.00012566254150316933,
      "loss": 0.1666,
      "step": 18476
    },
    {
      "epoch": 37.17706237424547,
      "grad_norm": 0.8954571485519409,
      "learning_rate": 0.00012565851695341585,
      "loss": 0.1648,
      "step": 18477
    },
    {
      "epoch": 37.17907444668008,
      "grad_norm": 0.9157082438468933,
      "learning_rate": 0.00012565449240366233,
      "loss": 0.1823,
      "step": 18478
    },
    {
      "epoch": 37.181086519114686,
      "grad_norm": 0.8784863352775574,
      "learning_rate": 0.00012565046785390887,
      "loss": 0.171,
      "step": 18479
    },
    {
      "epoch": 37.183098591549296,
      "grad_norm": 0.9321876168251038,
      "learning_rate": 0.00012564644330415536,
      "loss": 0.1694,
      "step": 18480
    },
    {
      "epoch": 37.185110663983906,
      "grad_norm": 0.9122477173805237,
      "learning_rate": 0.00012564241875440187,
      "loss": 0.1745,
      "step": 18481
    },
    {
      "epoch": 37.18712273641851,
      "grad_norm": 0.9609702229499817,
      "learning_rate": 0.00012563839420464835,
      "loss": 0.1624,
      "step": 18482
    },
    {
      "epoch": 37.18913480885312,
      "grad_norm": 0.8722653985023499,
      "learning_rate": 0.00012563436965489487,
      "loss": 0.1709,
      "step": 18483
    },
    {
      "epoch": 37.19114688128773,
      "grad_norm": 0.8523507714271545,
      "learning_rate": 0.00012563034510514138,
      "loss": 0.1622,
      "step": 18484
    },
    {
      "epoch": 37.19315895372233,
      "grad_norm": 0.8799338936805725,
      "learning_rate": 0.0001256263205553879,
      "loss": 0.1558,
      "step": 18485
    },
    {
      "epoch": 37.19517102615694,
      "grad_norm": 0.9324235320091248,
      "learning_rate": 0.00012562229600563438,
      "loss": 0.1816,
      "step": 18486
    },
    {
      "epoch": 37.19718309859155,
      "grad_norm": 0.9137318134307861,
      "learning_rate": 0.0001256182714558809,
      "loss": 0.1725,
      "step": 18487
    },
    {
      "epoch": 37.199195171026155,
      "grad_norm": 0.9142553806304932,
      "learning_rate": 0.00012561424690612737,
      "loss": 0.1658,
      "step": 18488
    },
    {
      "epoch": 37.201207243460765,
      "grad_norm": 0.9123973250389099,
      "learning_rate": 0.00012561022235637389,
      "loss": 0.1798,
      "step": 18489
    },
    {
      "epoch": 37.203219315895375,
      "grad_norm": 0.9636388421058655,
      "learning_rate": 0.0001256061978066204,
      "loss": 0.1752,
      "step": 18490
    },
    {
      "epoch": 37.20523138832998,
      "grad_norm": 0.8717129230499268,
      "learning_rate": 0.00012560217325686688,
      "loss": 0.1665,
      "step": 18491
    },
    {
      "epoch": 37.20724346076459,
      "grad_norm": 0.8872008919715881,
      "learning_rate": 0.0001255981487071134,
      "loss": 0.1696,
      "step": 18492
    },
    {
      "epoch": 37.2092555331992,
      "grad_norm": 0.9064809083938599,
      "learning_rate": 0.0001255941241573599,
      "loss": 0.1729,
      "step": 18493
    },
    {
      "epoch": 37.2112676056338,
      "grad_norm": 0.9342600703239441,
      "learning_rate": 0.00012559009960760642,
      "loss": 0.177,
      "step": 18494
    },
    {
      "epoch": 37.21327967806841,
      "grad_norm": 0.9117153286933899,
      "learning_rate": 0.0001255860750578529,
      "loss": 0.1667,
      "step": 18495
    },
    {
      "epoch": 37.21529175050302,
      "grad_norm": 0.9800575971603394,
      "learning_rate": 0.00012558205050809942,
      "loss": 0.1854,
      "step": 18496
    },
    {
      "epoch": 37.21730382293762,
      "grad_norm": 0.9825690388679504,
      "learning_rate": 0.0001255780259583459,
      "loss": 0.1818,
      "step": 18497
    },
    {
      "epoch": 37.21931589537223,
      "grad_norm": 0.9015412330627441,
      "learning_rate": 0.00012557400140859241,
      "loss": 0.1802,
      "step": 18498
    },
    {
      "epoch": 37.22132796780684,
      "grad_norm": 0.9095960855484009,
      "learning_rate": 0.00012556997685883893,
      "loss": 0.1621,
      "step": 18499
    },
    {
      "epoch": 37.223340040241446,
      "grad_norm": 0.9362935423851013,
      "learning_rate": 0.00012556595230908544,
      "loss": 0.1702,
      "step": 18500
    },
    {
      "epoch": 37.225352112676056,
      "grad_norm": 0.9048277735710144,
      "learning_rate": 0.00012556192775933192,
      "loss": 0.1709,
      "step": 18501
    },
    {
      "epoch": 37.227364185110666,
      "grad_norm": 0.9777051210403442,
      "learning_rate": 0.00012555790320957844,
      "loss": 0.1696,
      "step": 18502
    },
    {
      "epoch": 37.22937625754527,
      "grad_norm": 0.904170572757721,
      "learning_rate": 0.00012555387865982492,
      "loss": 0.1669,
      "step": 18503
    },
    {
      "epoch": 37.23138832997988,
      "grad_norm": 0.9107404947280884,
      "learning_rate": 0.00012554985411007146,
      "loss": 0.1601,
      "step": 18504
    },
    {
      "epoch": 37.23340040241449,
      "grad_norm": 1.025416612625122,
      "learning_rate": 0.00012554582956031795,
      "loss": 0.1915,
      "step": 18505
    },
    {
      "epoch": 37.23541247484909,
      "grad_norm": 0.8767669796943665,
      "learning_rate": 0.00012554180501056446,
      "loss": 0.172,
      "step": 18506
    },
    {
      "epoch": 37.2374245472837,
      "grad_norm": 0.9515784382820129,
      "learning_rate": 0.00012553778046081094,
      "loss": 0.1623,
      "step": 18507
    },
    {
      "epoch": 37.23943661971831,
      "grad_norm": 0.898381769657135,
      "learning_rate": 0.00012553375591105745,
      "loss": 0.1684,
      "step": 18508
    },
    {
      "epoch": 37.241448692152915,
      "grad_norm": 0.9183069467544556,
      "learning_rate": 0.00012552973136130397,
      "loss": 0.1713,
      "step": 18509
    },
    {
      "epoch": 37.243460764587525,
      "grad_norm": 1.0055389404296875,
      "learning_rate": 0.00012552570681155048,
      "loss": 0.1714,
      "step": 18510
    },
    {
      "epoch": 37.245472837022135,
      "grad_norm": 0.9259359240531921,
      "learning_rate": 0.00012552168226179696,
      "loss": 0.1892,
      "step": 18511
    },
    {
      "epoch": 37.24748490945674,
      "grad_norm": 0.9522360563278198,
      "learning_rate": 0.00012551765771204348,
      "loss": 0.1875,
      "step": 18512
    },
    {
      "epoch": 37.24949698189135,
      "grad_norm": 0.9285995364189148,
      "learning_rate": 0.00012551363316228996,
      "loss": 0.1741,
      "step": 18513
    },
    {
      "epoch": 37.25150905432596,
      "grad_norm": 0.954584002494812,
      "learning_rate": 0.0001255096086125365,
      "loss": 0.1953,
      "step": 18514
    },
    {
      "epoch": 37.25352112676056,
      "grad_norm": 1.0161017179489136,
      "learning_rate": 0.00012550558406278299,
      "loss": 0.1833,
      "step": 18515
    },
    {
      "epoch": 37.25553319919517,
      "grad_norm": 1.0050407648086548,
      "learning_rate": 0.0001255015595130295,
      "loss": 0.1772,
      "step": 18516
    },
    {
      "epoch": 37.25754527162978,
      "grad_norm": 0.9004697203636169,
      "learning_rate": 0.00012549753496327598,
      "loss": 0.1819,
      "step": 18517
    },
    {
      "epoch": 37.25955734406438,
      "grad_norm": 0.8524003624916077,
      "learning_rate": 0.0001254935104135225,
      "loss": 0.1769,
      "step": 18518
    },
    {
      "epoch": 37.26156941649899,
      "grad_norm": 0.8834688067436218,
      "learning_rate": 0.000125489485863769,
      "loss": 0.1717,
      "step": 18519
    },
    {
      "epoch": 37.2635814889336,
      "grad_norm": 0.9176859855651855,
      "learning_rate": 0.00012548546131401552,
      "loss": 0.1715,
      "step": 18520
    },
    {
      "epoch": 37.265593561368206,
      "grad_norm": 0.9020726680755615,
      "learning_rate": 0.000125481436764262,
      "loss": 0.1764,
      "step": 18521
    },
    {
      "epoch": 37.267605633802816,
      "grad_norm": 1.0196075439453125,
      "learning_rate": 0.00012547741221450852,
      "loss": 0.1763,
      "step": 18522
    },
    {
      "epoch": 37.269617706237426,
      "grad_norm": 0.9730384945869446,
      "learning_rate": 0.000125473387664755,
      "loss": 0.1673,
      "step": 18523
    },
    {
      "epoch": 37.27162977867203,
      "grad_norm": 0.942307710647583,
      "learning_rate": 0.00012546936311500151,
      "loss": 0.1916,
      "step": 18524
    },
    {
      "epoch": 37.27364185110664,
      "grad_norm": 0.9474523663520813,
      "learning_rate": 0.00012546533856524803,
      "loss": 0.1876,
      "step": 18525
    },
    {
      "epoch": 37.27565392354125,
      "grad_norm": 0.9518945813179016,
      "learning_rate": 0.0001254613140154945,
      "loss": 0.1641,
      "step": 18526
    },
    {
      "epoch": 37.27766599597585,
      "grad_norm": 0.9808030128479004,
      "learning_rate": 0.00012545728946574102,
      "loss": 0.1838,
      "step": 18527
    },
    {
      "epoch": 37.27967806841046,
      "grad_norm": 0.941620409488678,
      "learning_rate": 0.0001254532649159875,
      "loss": 0.1833,
      "step": 18528
    },
    {
      "epoch": 37.28169014084507,
      "grad_norm": 0.8839031457901001,
      "learning_rate": 0.00012544924036623405,
      "loss": 0.1749,
      "step": 18529
    },
    {
      "epoch": 37.283702213279675,
      "grad_norm": 0.8955935835838318,
      "learning_rate": 0.00012544521581648053,
      "loss": 0.1728,
      "step": 18530
    },
    {
      "epoch": 37.285714285714285,
      "grad_norm": 0.9166145324707031,
      "learning_rate": 0.00012544119126672705,
      "loss": 0.1627,
      "step": 18531
    },
    {
      "epoch": 37.287726358148895,
      "grad_norm": 0.9234635829925537,
      "learning_rate": 0.00012543716671697353,
      "loss": 0.1804,
      "step": 18532
    },
    {
      "epoch": 37.2897384305835,
      "grad_norm": 0.891441822052002,
      "learning_rate": 0.00012543314216722004,
      "loss": 0.1741,
      "step": 18533
    },
    {
      "epoch": 37.29175050301811,
      "grad_norm": 0.9206193089485168,
      "learning_rate": 0.00012542911761746656,
      "loss": 0.1658,
      "step": 18534
    },
    {
      "epoch": 37.29376257545272,
      "grad_norm": 0.9141727685928345,
      "learning_rate": 0.00012542509306771307,
      "loss": 0.171,
      "step": 18535
    },
    {
      "epoch": 37.29577464788732,
      "grad_norm": 0.9621029496192932,
      "learning_rate": 0.00012542106851795955,
      "loss": 0.1863,
      "step": 18536
    },
    {
      "epoch": 37.29778672032193,
      "grad_norm": 0.9460443258285522,
      "learning_rate": 0.00012541704396820607,
      "loss": 0.186,
      "step": 18537
    },
    {
      "epoch": 37.29979879275654,
      "grad_norm": 0.8916741609573364,
      "learning_rate": 0.00012541301941845255,
      "loss": 0.1666,
      "step": 18538
    },
    {
      "epoch": 37.30181086519114,
      "grad_norm": 0.9761340022087097,
      "learning_rate": 0.0001254089948686991,
      "loss": 0.1663,
      "step": 18539
    },
    {
      "epoch": 37.30382293762575,
      "grad_norm": 0.9478347897529602,
      "learning_rate": 0.00012540497031894557,
      "loss": 0.1872,
      "step": 18540
    },
    {
      "epoch": 37.30583501006036,
      "grad_norm": 0.9981075525283813,
      "learning_rate": 0.0001254009457691921,
      "loss": 0.197,
      "step": 18541
    },
    {
      "epoch": 37.30784708249497,
      "grad_norm": 0.8830950856208801,
      "learning_rate": 0.00012539692121943857,
      "loss": 0.1702,
      "step": 18542
    },
    {
      "epoch": 37.309859154929576,
      "grad_norm": 0.9461923837661743,
      "learning_rate": 0.00012539289666968508,
      "loss": 0.174,
      "step": 18543
    },
    {
      "epoch": 37.311871227364186,
      "grad_norm": 1.0066641569137573,
      "learning_rate": 0.0001253888721199316,
      "loss": 0.1778,
      "step": 18544
    },
    {
      "epoch": 37.313883299798796,
      "grad_norm": 0.9932113885879517,
      "learning_rate": 0.0001253848475701781,
      "loss": 0.1889,
      "step": 18545
    },
    {
      "epoch": 37.3158953722334,
      "grad_norm": 0.9677610397338867,
      "learning_rate": 0.0001253808230204246,
      "loss": 0.185,
      "step": 18546
    },
    {
      "epoch": 37.31790744466801,
      "grad_norm": 0.9415224194526672,
      "learning_rate": 0.0001253767984706711,
      "loss": 0.1712,
      "step": 18547
    },
    {
      "epoch": 37.31991951710262,
      "grad_norm": 0.9819669127464294,
      "learning_rate": 0.0001253727739209176,
      "loss": 0.18,
      "step": 18548
    },
    {
      "epoch": 37.32193158953722,
      "grad_norm": 0.9933301210403442,
      "learning_rate": 0.00012536874937116413,
      "loss": 0.1755,
      "step": 18549
    },
    {
      "epoch": 37.32394366197183,
      "grad_norm": 1.0320323705673218,
      "learning_rate": 0.00012536472482141062,
      "loss": 0.1829,
      "step": 18550
    },
    {
      "epoch": 37.32595573440644,
      "grad_norm": 0.9941452145576477,
      "learning_rate": 0.00012536070027165713,
      "loss": 0.1844,
      "step": 18551
    },
    {
      "epoch": 37.327967806841045,
      "grad_norm": 0.9293085336685181,
      "learning_rate": 0.0001253566757219036,
      "loss": 0.1841,
      "step": 18552
    },
    {
      "epoch": 37.329979879275655,
      "grad_norm": 1.011027455329895,
      "learning_rate": 0.00012535265117215013,
      "loss": 0.194,
      "step": 18553
    },
    {
      "epoch": 37.331991951710265,
      "grad_norm": 0.8764998912811279,
      "learning_rate": 0.0001253486266223966,
      "loss": 0.1714,
      "step": 18554
    },
    {
      "epoch": 37.33400402414487,
      "grad_norm": 0.9666465520858765,
      "learning_rate": 0.00012534460207264312,
      "loss": 0.188,
      "step": 18555
    },
    {
      "epoch": 37.33601609657948,
      "grad_norm": 0.8936490416526794,
      "learning_rate": 0.00012534057752288963,
      "loss": 0.1613,
      "step": 18556
    },
    {
      "epoch": 37.33802816901409,
      "grad_norm": 0.9089828729629517,
      "learning_rate": 0.00012533655297313615,
      "loss": 0.1799,
      "step": 18557
    },
    {
      "epoch": 37.34004024144869,
      "grad_norm": 1.033997654914856,
      "learning_rate": 0.00012533252842338263,
      "loss": 0.1819,
      "step": 18558
    },
    {
      "epoch": 37.3420523138833,
      "grad_norm": 1.04246985912323,
      "learning_rate": 0.00012532850387362914,
      "loss": 0.1801,
      "step": 18559
    },
    {
      "epoch": 37.34406438631791,
      "grad_norm": 0.9583633542060852,
      "learning_rate": 0.00012532447932387566,
      "loss": 0.1754,
      "step": 18560
    },
    {
      "epoch": 37.34607645875251,
      "grad_norm": 0.9575097560882568,
      "learning_rate": 0.00012532045477412214,
      "loss": 0.1847,
      "step": 18561
    },
    {
      "epoch": 37.34808853118712,
      "grad_norm": 0.9459134340286255,
      "learning_rate": 0.00012531643022436865,
      "loss": 0.1942,
      "step": 18562
    },
    {
      "epoch": 37.35010060362173,
      "grad_norm": 0.9804810285568237,
      "learning_rate": 0.00012531240567461514,
      "loss": 0.1827,
      "step": 18563
    },
    {
      "epoch": 37.352112676056336,
      "grad_norm": 0.9050053954124451,
      "learning_rate": 0.00012530838112486165,
      "loss": 0.1798,
      "step": 18564
    },
    {
      "epoch": 37.354124748490946,
      "grad_norm": 0.9264068603515625,
      "learning_rate": 0.00012530435657510816,
      "loss": 0.1883,
      "step": 18565
    },
    {
      "epoch": 37.356136820925556,
      "grad_norm": 1.0202537775039673,
      "learning_rate": 0.00012530033202535468,
      "loss": 0.1905,
      "step": 18566
    },
    {
      "epoch": 37.35814889336016,
      "grad_norm": 0.9329847693443298,
      "learning_rate": 0.00012529630747560116,
      "loss": 0.1709,
      "step": 18567
    },
    {
      "epoch": 37.36016096579477,
      "grad_norm": 0.9838921427726746,
      "learning_rate": 0.00012529228292584767,
      "loss": 0.1706,
      "step": 18568
    },
    {
      "epoch": 37.36217303822938,
      "grad_norm": 0.9029723405838013,
      "learning_rate": 0.00012528825837609416,
      "loss": 0.1713,
      "step": 18569
    },
    {
      "epoch": 37.36418511066398,
      "grad_norm": 0.988153338432312,
      "learning_rate": 0.0001252842338263407,
      "loss": 0.1842,
      "step": 18570
    },
    {
      "epoch": 37.36619718309859,
      "grad_norm": 0.9588958024978638,
      "learning_rate": 0.00012528020927658718,
      "loss": 0.1672,
      "step": 18571
    },
    {
      "epoch": 37.3682092555332,
      "grad_norm": 0.9942059516906738,
      "learning_rate": 0.0001252761847268337,
      "loss": 0.2,
      "step": 18572
    },
    {
      "epoch": 37.370221327967805,
      "grad_norm": 0.939456045627594,
      "learning_rate": 0.00012527216017708018,
      "loss": 0.1908,
      "step": 18573
    },
    {
      "epoch": 37.372233400402415,
      "grad_norm": 0.9397087693214417,
      "learning_rate": 0.0001252681356273267,
      "loss": 0.1882,
      "step": 18574
    },
    {
      "epoch": 37.374245472837025,
      "grad_norm": 0.9558388590812683,
      "learning_rate": 0.0001252641110775732,
      "loss": 0.1826,
      "step": 18575
    },
    {
      "epoch": 37.37625754527163,
      "grad_norm": 0.9226650595664978,
      "learning_rate": 0.00012526008652781972,
      "loss": 0.1905,
      "step": 18576
    },
    {
      "epoch": 37.37826961770624,
      "grad_norm": 0.9764747023582458,
      "learning_rate": 0.0001252560619780662,
      "loss": 0.1735,
      "step": 18577
    },
    {
      "epoch": 37.38028169014085,
      "grad_norm": 1.0615544319152832,
      "learning_rate": 0.00012525203742831271,
      "loss": 0.2134,
      "step": 18578
    },
    {
      "epoch": 37.38229376257545,
      "grad_norm": 0.9485327005386353,
      "learning_rate": 0.0001252480128785592,
      "loss": 0.1899,
      "step": 18579
    },
    {
      "epoch": 37.38430583501006,
      "grad_norm": 0.9212474822998047,
      "learning_rate": 0.00012524398832880574,
      "loss": 0.171,
      "step": 18580
    },
    {
      "epoch": 37.38631790744467,
      "grad_norm": 0.9913187026977539,
      "learning_rate": 0.00012523996377905222,
      "loss": 0.2007,
      "step": 18581
    },
    {
      "epoch": 37.38832997987927,
      "grad_norm": 0.9574683904647827,
      "learning_rate": 0.00012523593922929874,
      "loss": 0.1846,
      "step": 18582
    },
    {
      "epoch": 37.39034205231388,
      "grad_norm": 0.9071332812309265,
      "learning_rate": 0.00012523191467954522,
      "loss": 0.1721,
      "step": 18583
    },
    {
      "epoch": 37.39235412474849,
      "grad_norm": 0.9258034229278564,
      "learning_rate": 0.00012522789012979173,
      "loss": 0.175,
      "step": 18584
    },
    {
      "epoch": 37.394366197183096,
      "grad_norm": 0.970367431640625,
      "learning_rate": 0.00012522386558003825,
      "loss": 0.1741,
      "step": 18585
    },
    {
      "epoch": 37.396378269617706,
      "grad_norm": 0.9754490852355957,
      "learning_rate": 0.00012521984103028476,
      "loss": 0.1901,
      "step": 18586
    },
    {
      "epoch": 37.398390342052316,
      "grad_norm": 0.9032459259033203,
      "learning_rate": 0.00012521581648053124,
      "loss": 0.163,
      "step": 18587
    },
    {
      "epoch": 37.40040241448692,
      "grad_norm": 0.983982503414154,
      "learning_rate": 0.00012521179193077775,
      "loss": 0.1849,
      "step": 18588
    },
    {
      "epoch": 37.40241448692153,
      "grad_norm": 0.8984010815620422,
      "learning_rate": 0.00012520776738102424,
      "loss": 0.1806,
      "step": 18589
    },
    {
      "epoch": 37.40442655935614,
      "grad_norm": 0.9737077951431274,
      "learning_rate": 0.00012520374283127075,
      "loss": 0.1904,
      "step": 18590
    },
    {
      "epoch": 37.40643863179074,
      "grad_norm": 0.9672748446464539,
      "learning_rate": 0.00012519971828151726,
      "loss": 0.1986,
      "step": 18591
    },
    {
      "epoch": 37.40845070422535,
      "grad_norm": 1.00602388381958,
      "learning_rate": 0.00012519569373176378,
      "loss": 0.1851,
      "step": 18592
    },
    {
      "epoch": 37.41046277665996,
      "grad_norm": 1.0290677547454834,
      "learning_rate": 0.00012519166918201026,
      "loss": 0.193,
      "step": 18593
    },
    {
      "epoch": 37.412474849094565,
      "grad_norm": 0.9519706964492798,
      "learning_rate": 0.00012518764463225677,
      "loss": 0.1744,
      "step": 18594
    },
    {
      "epoch": 37.414486921529175,
      "grad_norm": 0.9406237006187439,
      "learning_rate": 0.00012518362008250329,
      "loss": 0.1695,
      "step": 18595
    },
    {
      "epoch": 37.416498993963785,
      "grad_norm": 0.9871858954429626,
      "learning_rate": 0.00012517959553274977,
      "loss": 0.2028,
      "step": 18596
    },
    {
      "epoch": 37.41851106639839,
      "grad_norm": 0.9837949872016907,
      "learning_rate": 0.00012517557098299628,
      "loss": 0.1948,
      "step": 18597
    },
    {
      "epoch": 37.420523138833,
      "grad_norm": 1.0019794702529907,
      "learning_rate": 0.00012517154643324277,
      "loss": 0.1837,
      "step": 18598
    },
    {
      "epoch": 37.42253521126761,
      "grad_norm": 0.9532936811447144,
      "learning_rate": 0.00012516752188348928,
      "loss": 0.1822,
      "step": 18599
    },
    {
      "epoch": 37.42454728370221,
      "grad_norm": 0.9751289486885071,
      "learning_rate": 0.0001251634973337358,
      "loss": 0.1937,
      "step": 18600
    },
    {
      "epoch": 37.42655935613682,
      "grad_norm": 0.9259531497955322,
      "learning_rate": 0.0001251594727839823,
      "loss": 0.1929,
      "step": 18601
    },
    {
      "epoch": 37.42857142857143,
      "grad_norm": 0.9743415117263794,
      "learning_rate": 0.0001251554482342288,
      "loss": 0.1867,
      "step": 18602
    },
    {
      "epoch": 37.43058350100603,
      "grad_norm": 1.0015579462051392,
      "learning_rate": 0.0001251514236844753,
      "loss": 0.1886,
      "step": 18603
    },
    {
      "epoch": 37.43259557344064,
      "grad_norm": 0.99900883436203,
      "learning_rate": 0.0001251473991347218,
      "loss": 0.1828,
      "step": 18604
    },
    {
      "epoch": 37.43460764587525,
      "grad_norm": 0.920725405216217,
      "learning_rate": 0.00012514337458496833,
      "loss": 0.1839,
      "step": 18605
    },
    {
      "epoch": 37.436619718309856,
      "grad_norm": 1.0374231338500977,
      "learning_rate": 0.0001251393500352148,
      "loss": 0.1837,
      "step": 18606
    },
    {
      "epoch": 37.438631790744466,
      "grad_norm": 0.9446293711662292,
      "learning_rate": 0.00012513532548546132,
      "loss": 0.2017,
      "step": 18607
    },
    {
      "epoch": 37.440643863179076,
      "grad_norm": 1.0021873712539673,
      "learning_rate": 0.0001251313009357078,
      "loss": 0.2009,
      "step": 18608
    },
    {
      "epoch": 37.44265593561368,
      "grad_norm": 0.9493420720100403,
      "learning_rate": 0.00012512727638595432,
      "loss": 0.1843,
      "step": 18609
    },
    {
      "epoch": 37.44466800804829,
      "grad_norm": 0.9250036478042603,
      "learning_rate": 0.00012512325183620083,
      "loss": 0.1714,
      "step": 18610
    },
    {
      "epoch": 37.4466800804829,
      "grad_norm": 0.9736072421073914,
      "learning_rate": 0.00012511922728644735,
      "loss": 0.1851,
      "step": 18611
    },
    {
      "epoch": 37.4486921529175,
      "grad_norm": 0.988094687461853,
      "learning_rate": 0.00012511520273669383,
      "loss": 0.193,
      "step": 18612
    },
    {
      "epoch": 37.45070422535211,
      "grad_norm": 1.0095258951187134,
      "learning_rate": 0.00012511117818694034,
      "loss": 0.1772,
      "step": 18613
    },
    {
      "epoch": 37.45271629778672,
      "grad_norm": 0.9316443800926208,
      "learning_rate": 0.00012510715363718683,
      "loss": 0.1799,
      "step": 18614
    },
    {
      "epoch": 37.454728370221325,
      "grad_norm": 0.9912909269332886,
      "learning_rate": 0.00012510312908743337,
      "loss": 0.1966,
      "step": 18615
    },
    {
      "epoch": 37.456740442655935,
      "grad_norm": 0.9395609498023987,
      "learning_rate": 0.00012509910453767985,
      "loss": 0.1792,
      "step": 18616
    },
    {
      "epoch": 37.458752515090545,
      "grad_norm": 0.98108971118927,
      "learning_rate": 0.00012509507998792636,
      "loss": 0.1851,
      "step": 18617
    },
    {
      "epoch": 37.46076458752515,
      "grad_norm": 0.9990902543067932,
      "learning_rate": 0.00012509105543817285,
      "loss": 0.1974,
      "step": 18618
    },
    {
      "epoch": 37.46277665995976,
      "grad_norm": 0.9483550190925598,
      "learning_rate": 0.00012508703088841936,
      "loss": 0.1879,
      "step": 18619
    },
    {
      "epoch": 37.46478873239437,
      "grad_norm": 1.0427172183990479,
      "learning_rate": 0.00012508300633866587,
      "loss": 0.2,
      "step": 18620
    },
    {
      "epoch": 37.46680080482897,
      "grad_norm": 0.9484179019927979,
      "learning_rate": 0.0001250789817889124,
      "loss": 0.1957,
      "step": 18621
    },
    {
      "epoch": 37.46881287726358,
      "grad_norm": 0.9747930765151978,
      "learning_rate": 0.00012507495723915887,
      "loss": 0.2039,
      "step": 18622
    },
    {
      "epoch": 37.47082494969819,
      "grad_norm": 0.9391992688179016,
      "learning_rate": 0.00012507093268940538,
      "loss": 0.1902,
      "step": 18623
    },
    {
      "epoch": 37.47283702213279,
      "grad_norm": 0.9120648503303528,
      "learning_rate": 0.00012506690813965187,
      "loss": 0.1699,
      "step": 18624
    },
    {
      "epoch": 37.4748490945674,
      "grad_norm": 0.9448211789131165,
      "learning_rate": 0.00012506288358989838,
      "loss": 0.1846,
      "step": 18625
    },
    {
      "epoch": 37.47686116700201,
      "grad_norm": 0.9207527041435242,
      "learning_rate": 0.0001250588590401449,
      "loss": 0.1739,
      "step": 18626
    },
    {
      "epoch": 37.478873239436616,
      "grad_norm": 0.9307020306587219,
      "learning_rate": 0.0001250548344903914,
      "loss": 0.1814,
      "step": 18627
    },
    {
      "epoch": 37.480885311871226,
      "grad_norm": 1.001327395439148,
      "learning_rate": 0.0001250508099406379,
      "loss": 0.196,
      "step": 18628
    },
    {
      "epoch": 37.482897384305836,
      "grad_norm": 1.0273020267486572,
      "learning_rate": 0.0001250467853908844,
      "loss": 0.1995,
      "step": 18629
    },
    {
      "epoch": 37.48490945674044,
      "grad_norm": 0.9585978388786316,
      "learning_rate": 0.00012504276084113092,
      "loss": 0.1877,
      "step": 18630
    },
    {
      "epoch": 37.48692152917505,
      "grad_norm": 0.9689578413963318,
      "learning_rate": 0.0001250387362913774,
      "loss": 0.1878,
      "step": 18631
    },
    {
      "epoch": 37.48893360160966,
      "grad_norm": 0.9305312037467957,
      "learning_rate": 0.0001250347117416239,
      "loss": 0.172,
      "step": 18632
    },
    {
      "epoch": 37.49094567404426,
      "grad_norm": 0.9814797043800354,
      "learning_rate": 0.0001250306871918704,
      "loss": 0.1946,
      "step": 18633
    },
    {
      "epoch": 37.49295774647887,
      "grad_norm": 0.9250637888908386,
      "learning_rate": 0.0001250266626421169,
      "loss": 0.1819,
      "step": 18634
    },
    {
      "epoch": 37.49496981891348,
      "grad_norm": 0.9889506697654724,
      "learning_rate": 0.00012502263809236342,
      "loss": 0.1886,
      "step": 18635
    },
    {
      "epoch": 37.496981891348085,
      "grad_norm": 0.9929643869400024,
      "learning_rate": 0.00012501861354260993,
      "loss": 0.1928,
      "step": 18636
    },
    {
      "epoch": 37.498993963782695,
      "grad_norm": 1.0319058895111084,
      "learning_rate": 0.00012501458899285642,
      "loss": 0.2098,
      "step": 18637
    },
    {
      "epoch": 37.501006036217305,
      "grad_norm": 0.9628491401672363,
      "learning_rate": 0.00012501056444310293,
      "loss": 0.1903,
      "step": 18638
    },
    {
      "epoch": 37.503018108651915,
      "grad_norm": 1.0034085512161255,
      "learning_rate": 0.00012500653989334942,
      "loss": 0.193,
      "step": 18639
    },
    {
      "epoch": 37.50503018108652,
      "grad_norm": 0.9756157398223877,
      "learning_rate": 0.00012500251534359596,
      "loss": 0.1905,
      "step": 18640
    },
    {
      "epoch": 37.50704225352113,
      "grad_norm": 0.959904670715332,
      "learning_rate": 0.00012499849079384244,
      "loss": 0.1772,
      "step": 18641
    },
    {
      "epoch": 37.50905432595574,
      "grad_norm": 1.015830636024475,
      "learning_rate": 0.00012499446624408895,
      "loss": 0.1955,
      "step": 18642
    },
    {
      "epoch": 37.51106639839034,
      "grad_norm": 0.9540894031524658,
      "learning_rate": 0.00012499044169433544,
      "loss": 0.2042,
      "step": 18643
    },
    {
      "epoch": 37.51307847082495,
      "grad_norm": 0.9206341505050659,
      "learning_rate": 0.00012498641714458195,
      "loss": 0.1799,
      "step": 18644
    },
    {
      "epoch": 37.51509054325956,
      "grad_norm": 1.0266607999801636,
      "learning_rate": 0.00012498239259482846,
      "loss": 0.2023,
      "step": 18645
    },
    {
      "epoch": 37.517102615694164,
      "grad_norm": 0.9413984417915344,
      "learning_rate": 0.00012497836804507498,
      "loss": 0.1756,
      "step": 18646
    },
    {
      "epoch": 37.519114688128774,
      "grad_norm": 1.0432630777359009,
      "learning_rate": 0.00012497434349532146,
      "loss": 0.2025,
      "step": 18647
    },
    {
      "epoch": 37.521126760563384,
      "grad_norm": 1.0565361976623535,
      "learning_rate": 0.00012497031894556797,
      "loss": 0.2164,
      "step": 18648
    },
    {
      "epoch": 37.52313883299799,
      "grad_norm": 1.0759642124176025,
      "learning_rate": 0.00012496629439581446,
      "loss": 0.206,
      "step": 18649
    },
    {
      "epoch": 37.5251509054326,
      "grad_norm": 1.0268691778182983,
      "learning_rate": 0.000124962269846061,
      "loss": 0.213,
      "step": 18650
    },
    {
      "epoch": 37.52716297786721,
      "grad_norm": 0.9805518984794617,
      "learning_rate": 0.00012495824529630748,
      "loss": 0.1942,
      "step": 18651
    },
    {
      "epoch": 37.52917505030181,
      "grad_norm": 0.9409493803977966,
      "learning_rate": 0.000124954220746554,
      "loss": 0.1947,
      "step": 18652
    },
    {
      "epoch": 37.53118712273642,
      "grad_norm": 1.001976490020752,
      "learning_rate": 0.00012495019619680048,
      "loss": 0.1931,
      "step": 18653
    },
    {
      "epoch": 37.53319919517103,
      "grad_norm": 0.9936267137527466,
      "learning_rate": 0.000124946171647047,
      "loss": 0.1829,
      "step": 18654
    },
    {
      "epoch": 37.53521126760563,
      "grad_norm": 0.9612223505973816,
      "learning_rate": 0.0001249421470972935,
      "loss": 0.1933,
      "step": 18655
    },
    {
      "epoch": 37.53722334004024,
      "grad_norm": 1.0429314374923706,
      "learning_rate": 0.00012493812254754002,
      "loss": 0.2063,
      "step": 18656
    },
    {
      "epoch": 37.53923541247485,
      "grad_norm": 0.9829413890838623,
      "learning_rate": 0.0001249340979977865,
      "loss": 0.1746,
      "step": 18657
    },
    {
      "epoch": 37.541247484909455,
      "grad_norm": 0.9717487692832947,
      "learning_rate": 0.000124930073448033,
      "loss": 0.177,
      "step": 18658
    },
    {
      "epoch": 37.543259557344065,
      "grad_norm": 1.0581789016723633,
      "learning_rate": 0.0001249260488982795,
      "loss": 0.1936,
      "step": 18659
    },
    {
      "epoch": 37.545271629778675,
      "grad_norm": 1.0334913730621338,
      "learning_rate": 0.000124922024348526,
      "loss": 0.2034,
      "step": 18660
    },
    {
      "epoch": 37.54728370221328,
      "grad_norm": 0.8945065140724182,
      "learning_rate": 0.00012491799979877252,
      "loss": 0.1824,
      "step": 18661
    },
    {
      "epoch": 37.54929577464789,
      "grad_norm": 0.9751197695732117,
      "learning_rate": 0.00012491397524901904,
      "loss": 0.1787,
      "step": 18662
    },
    {
      "epoch": 37.5513078470825,
      "grad_norm": 1.0180073976516724,
      "learning_rate": 0.00012490995069926552,
      "loss": 0.2065,
      "step": 18663
    },
    {
      "epoch": 37.5533199195171,
      "grad_norm": 0.990096390247345,
      "learning_rate": 0.00012490592614951203,
      "loss": 0.191,
      "step": 18664
    },
    {
      "epoch": 37.55533199195171,
      "grad_norm": 1.006993055343628,
      "learning_rate": 0.00012490190159975854,
      "loss": 0.1927,
      "step": 18665
    },
    {
      "epoch": 37.55734406438632,
      "grad_norm": 0.9371325969696045,
      "learning_rate": 0.00012489787705000503,
      "loss": 0.1809,
      "step": 18666
    },
    {
      "epoch": 37.559356136820924,
      "grad_norm": 0.9702998399734497,
      "learning_rate": 0.00012489385250025154,
      "loss": 0.2002,
      "step": 18667
    },
    {
      "epoch": 37.561368209255534,
      "grad_norm": 1.1020739078521729,
      "learning_rate": 0.00012488982795049803,
      "loss": 0.1893,
      "step": 18668
    },
    {
      "epoch": 37.563380281690144,
      "grad_norm": 1.0076910257339478,
      "learning_rate": 0.00012488580340074454,
      "loss": 0.1949,
      "step": 18669
    },
    {
      "epoch": 37.56539235412475,
      "grad_norm": 0.9544364809989929,
      "learning_rate": 0.00012488177885099105,
      "loss": 0.1889,
      "step": 18670
    },
    {
      "epoch": 37.56740442655936,
      "grad_norm": 0.9893507957458496,
      "learning_rate": 0.00012487775430123756,
      "loss": 0.1835,
      "step": 18671
    },
    {
      "epoch": 37.56941649899397,
      "grad_norm": 0.9376275539398193,
      "learning_rate": 0.00012487372975148405,
      "loss": 0.1969,
      "step": 18672
    },
    {
      "epoch": 37.57142857142857,
      "grad_norm": 0.9725013375282288,
      "learning_rate": 0.00012486970520173056,
      "loss": 0.1867,
      "step": 18673
    },
    {
      "epoch": 37.57344064386318,
      "grad_norm": 1.0267765522003174,
      "learning_rate": 0.00012486568065197705,
      "loss": 0.2052,
      "step": 18674
    },
    {
      "epoch": 37.57545271629779,
      "grad_norm": 0.9651365280151367,
      "learning_rate": 0.00012486165610222359,
      "loss": 0.2019,
      "step": 18675
    },
    {
      "epoch": 37.57746478873239,
      "grad_norm": 0.9688554406166077,
      "learning_rate": 0.00012485763155247007,
      "loss": 0.1915,
      "step": 18676
    },
    {
      "epoch": 37.579476861167,
      "grad_norm": 0.9505980014801025,
      "learning_rate": 0.00012485360700271658,
      "loss": 0.1791,
      "step": 18677
    },
    {
      "epoch": 37.58148893360161,
      "grad_norm": 1.0131428241729736,
      "learning_rate": 0.00012484958245296307,
      "loss": 0.217,
      "step": 18678
    },
    {
      "epoch": 37.583501006036215,
      "grad_norm": 0.98321133852005,
      "learning_rate": 0.00012484555790320958,
      "loss": 0.1972,
      "step": 18679
    },
    {
      "epoch": 37.585513078470825,
      "grad_norm": 1.019141435623169,
      "learning_rate": 0.0001248415333534561,
      "loss": 0.1893,
      "step": 18680
    },
    {
      "epoch": 37.587525150905435,
      "grad_norm": 0.9531320929527283,
      "learning_rate": 0.0001248375088037026,
      "loss": 0.187,
      "step": 18681
    },
    {
      "epoch": 37.58953722334004,
      "grad_norm": 1.0356597900390625,
      "learning_rate": 0.0001248334842539491,
      "loss": 0.2052,
      "step": 18682
    },
    {
      "epoch": 37.59154929577465,
      "grad_norm": 0.9702355265617371,
      "learning_rate": 0.0001248294597041956,
      "loss": 0.194,
      "step": 18683
    },
    {
      "epoch": 37.59356136820926,
      "grad_norm": 1.04108464717865,
      "learning_rate": 0.0001248254351544421,
      "loss": 0.2058,
      "step": 18684
    },
    {
      "epoch": 37.59557344064386,
      "grad_norm": 0.9659821391105652,
      "learning_rate": 0.00012482141060468863,
      "loss": 0.1952,
      "step": 18685
    },
    {
      "epoch": 37.59758551307847,
      "grad_norm": 0.9674012660980225,
      "learning_rate": 0.0001248173860549351,
      "loss": 0.1793,
      "step": 18686
    },
    {
      "epoch": 37.59959758551308,
      "grad_norm": 0.8979398012161255,
      "learning_rate": 0.00012481336150518162,
      "loss": 0.1801,
      "step": 18687
    },
    {
      "epoch": 37.601609657947684,
      "grad_norm": 0.9312453866004944,
      "learning_rate": 0.0001248093369554281,
      "loss": 0.1695,
      "step": 18688
    },
    {
      "epoch": 37.603621730382294,
      "grad_norm": 1.0062711238861084,
      "learning_rate": 0.00012480531240567462,
      "loss": 0.2031,
      "step": 18689
    },
    {
      "epoch": 37.605633802816904,
      "grad_norm": 0.994915783405304,
      "learning_rate": 0.00012480128785592113,
      "loss": 0.1975,
      "step": 18690
    },
    {
      "epoch": 37.60764587525151,
      "grad_norm": 0.9320778250694275,
      "learning_rate": 0.00012479726330616765,
      "loss": 0.1809,
      "step": 18691
    },
    {
      "epoch": 37.60965794768612,
      "grad_norm": 1.0305967330932617,
      "learning_rate": 0.00012479323875641413,
      "loss": 0.2097,
      "step": 18692
    },
    {
      "epoch": 37.61167002012073,
      "grad_norm": 0.9266394376754761,
      "learning_rate": 0.00012478921420666064,
      "loss": 0.1862,
      "step": 18693
    },
    {
      "epoch": 37.61368209255533,
      "grad_norm": 0.9409215450286865,
      "learning_rate": 0.00012478518965690713,
      "loss": 0.1918,
      "step": 18694
    },
    {
      "epoch": 37.61569416498994,
      "grad_norm": 0.9222713708877563,
      "learning_rate": 0.00012478116510715364,
      "loss": 0.1866,
      "step": 18695
    },
    {
      "epoch": 37.61770623742455,
      "grad_norm": 0.9987902641296387,
      "learning_rate": 0.00012477714055740015,
      "loss": 0.1884,
      "step": 18696
    },
    {
      "epoch": 37.61971830985915,
      "grad_norm": 1.0428599119186401,
      "learning_rate": 0.00012477311600764664,
      "loss": 0.2015,
      "step": 18697
    },
    {
      "epoch": 37.62173038229376,
      "grad_norm": 0.928967297077179,
      "learning_rate": 0.00012476909145789315,
      "loss": 0.1822,
      "step": 18698
    },
    {
      "epoch": 37.62374245472837,
      "grad_norm": 1.0058348178863525,
      "learning_rate": 0.00012476506690813966,
      "loss": 0.2069,
      "step": 18699
    },
    {
      "epoch": 37.625754527162975,
      "grad_norm": 0.9836833477020264,
      "learning_rate": 0.00012476104235838617,
      "loss": 0.192,
      "step": 18700
    },
    {
      "epoch": 37.627766599597585,
      "grad_norm": 0.9741608500480652,
      "learning_rate": 0.00012475701780863266,
      "loss": 0.2015,
      "step": 18701
    },
    {
      "epoch": 37.629778672032195,
      "grad_norm": 1.0209238529205322,
      "learning_rate": 0.00012475299325887917,
      "loss": 0.2059,
      "step": 18702
    },
    {
      "epoch": 37.6317907444668,
      "grad_norm": 0.9788674712181091,
      "learning_rate": 0.00012474896870912566,
      "loss": 0.2009,
      "step": 18703
    },
    {
      "epoch": 37.63380281690141,
      "grad_norm": 1.072715163230896,
      "learning_rate": 0.00012474494415937217,
      "loss": 0.2167,
      "step": 18704
    },
    {
      "epoch": 37.63581488933602,
      "grad_norm": 0.9691451787948608,
      "learning_rate": 0.00012474091960961868,
      "loss": 0.1834,
      "step": 18705
    },
    {
      "epoch": 37.63782696177062,
      "grad_norm": 0.960024356842041,
      "learning_rate": 0.0001247368950598652,
      "loss": 0.2042,
      "step": 18706
    },
    {
      "epoch": 37.63983903420523,
      "grad_norm": 0.9519059658050537,
      "learning_rate": 0.00012473287051011168,
      "loss": 0.1937,
      "step": 18707
    },
    {
      "epoch": 37.64185110663984,
      "grad_norm": 0.9150387644767761,
      "learning_rate": 0.0001247288459603582,
      "loss": 0.1911,
      "step": 18708
    },
    {
      "epoch": 37.643863179074444,
      "grad_norm": 1.086171269416809,
      "learning_rate": 0.00012472482141060468,
      "loss": 0.2228,
      "step": 18709
    },
    {
      "epoch": 37.645875251509054,
      "grad_norm": 0.9638859629631042,
      "learning_rate": 0.00012472079686085122,
      "loss": 0.1894,
      "step": 18710
    },
    {
      "epoch": 37.647887323943664,
      "grad_norm": 1.0016860961914062,
      "learning_rate": 0.0001247167723110977,
      "loss": 0.2003,
      "step": 18711
    },
    {
      "epoch": 37.64989939637827,
      "grad_norm": 1.0067169666290283,
      "learning_rate": 0.0001247127477613442,
      "loss": 0.2132,
      "step": 18712
    },
    {
      "epoch": 37.65191146881288,
      "grad_norm": 1.0239840745925903,
      "learning_rate": 0.0001247087232115907,
      "loss": 0.1992,
      "step": 18713
    },
    {
      "epoch": 37.65392354124749,
      "grad_norm": 1.0253870487213135,
      "learning_rate": 0.0001247046986618372,
      "loss": 0.2057,
      "step": 18714
    },
    {
      "epoch": 37.65593561368209,
      "grad_norm": 1.03052818775177,
      "learning_rate": 0.00012470067411208372,
      "loss": 0.1888,
      "step": 18715
    },
    {
      "epoch": 37.6579476861167,
      "grad_norm": 1.0124143362045288,
      "learning_rate": 0.00012469664956233023,
      "loss": 0.1953,
      "step": 18716
    },
    {
      "epoch": 37.65995975855131,
      "grad_norm": 0.9689235687255859,
      "learning_rate": 0.00012469262501257672,
      "loss": 0.1839,
      "step": 18717
    },
    {
      "epoch": 37.66197183098591,
      "grad_norm": 0.9918740391731262,
      "learning_rate": 0.00012468860046282323,
      "loss": 0.2049,
      "step": 18718
    },
    {
      "epoch": 37.66398390342052,
      "grad_norm": 0.9910595417022705,
      "learning_rate": 0.00012468457591306972,
      "loss": 0.2098,
      "step": 18719
    },
    {
      "epoch": 37.66599597585513,
      "grad_norm": 0.9778704047203064,
      "learning_rate": 0.00012468055136331626,
      "loss": 0.1896,
      "step": 18720
    },
    {
      "epoch": 37.668008048289735,
      "grad_norm": 1.0301793813705444,
      "learning_rate": 0.00012467652681356274,
      "loss": 0.1932,
      "step": 18721
    },
    {
      "epoch": 37.670020120724345,
      "grad_norm": 1.0044244527816772,
      "learning_rate": 0.00012467250226380925,
      "loss": 0.2022,
      "step": 18722
    },
    {
      "epoch": 37.672032193158955,
      "grad_norm": 1.0175788402557373,
      "learning_rate": 0.00012466847771405574,
      "loss": 0.1945,
      "step": 18723
    },
    {
      "epoch": 37.67404426559356,
      "grad_norm": 0.9716120362281799,
      "learning_rate": 0.00012466445316430225,
      "loss": 0.191,
      "step": 18724
    },
    {
      "epoch": 37.67605633802817,
      "grad_norm": 0.993572473526001,
      "learning_rate": 0.00012466042861454876,
      "loss": 0.2051,
      "step": 18725
    },
    {
      "epoch": 37.67806841046278,
      "grad_norm": 1.05338716506958,
      "learning_rate": 0.00012465640406479527,
      "loss": 0.2108,
      "step": 18726
    },
    {
      "epoch": 37.68008048289738,
      "grad_norm": 1.138352632522583,
      "learning_rate": 0.00012465237951504176,
      "loss": 0.2055,
      "step": 18727
    },
    {
      "epoch": 37.68209255533199,
      "grad_norm": 0.9915666580200195,
      "learning_rate": 0.00012464835496528827,
      "loss": 0.1837,
      "step": 18728
    },
    {
      "epoch": 37.6841046277666,
      "grad_norm": 0.9815518260002136,
      "learning_rate": 0.00012464433041553476,
      "loss": 0.1968,
      "step": 18729
    },
    {
      "epoch": 37.686116700201204,
      "grad_norm": 1.0769567489624023,
      "learning_rate": 0.00012464030586578127,
      "loss": 0.2193,
      "step": 18730
    },
    {
      "epoch": 37.688128772635814,
      "grad_norm": 0.984286367893219,
      "learning_rate": 0.00012463628131602778,
      "loss": 0.197,
      "step": 18731
    },
    {
      "epoch": 37.690140845070424,
      "grad_norm": 0.9196310639381409,
      "learning_rate": 0.00012463225676627427,
      "loss": 0.1767,
      "step": 18732
    },
    {
      "epoch": 37.69215291750503,
      "grad_norm": 0.9645223617553711,
      "learning_rate": 0.00012462823221652078,
      "loss": 0.2018,
      "step": 18733
    },
    {
      "epoch": 37.69416498993964,
      "grad_norm": 0.9913174510002136,
      "learning_rate": 0.0001246242076667673,
      "loss": 0.1936,
      "step": 18734
    },
    {
      "epoch": 37.69617706237425,
      "grad_norm": 0.9271821975708008,
      "learning_rate": 0.0001246201831170138,
      "loss": 0.2057,
      "step": 18735
    },
    {
      "epoch": 37.69818913480886,
      "grad_norm": 0.9879818558692932,
      "learning_rate": 0.0001246161585672603,
      "loss": 0.2146,
      "step": 18736
    },
    {
      "epoch": 37.70020120724346,
      "grad_norm": 1.048202395439148,
      "learning_rate": 0.0001246121340175068,
      "loss": 0.1952,
      "step": 18737
    },
    {
      "epoch": 37.70221327967807,
      "grad_norm": 1.0009160041809082,
      "learning_rate": 0.00012460810946775329,
      "loss": 0.1892,
      "step": 18738
    },
    {
      "epoch": 37.70422535211267,
      "grad_norm": 1.012712836265564,
      "learning_rate": 0.0001246040849179998,
      "loss": 0.2037,
      "step": 18739
    },
    {
      "epoch": 37.70623742454728,
      "grad_norm": 1.0043445825576782,
      "learning_rate": 0.0001246000603682463,
      "loss": 0.1944,
      "step": 18740
    },
    {
      "epoch": 37.70824949698189,
      "grad_norm": 1.027647614479065,
      "learning_rate": 0.00012459603581849282,
      "loss": 0.1859,
      "step": 18741
    },
    {
      "epoch": 37.7102615694165,
      "grad_norm": 0.9793362617492676,
      "learning_rate": 0.0001245920112687393,
      "loss": 0.2052,
      "step": 18742
    },
    {
      "epoch": 37.712273641851105,
      "grad_norm": 1.0337917804718018,
      "learning_rate": 0.00012458798671898582,
      "loss": 0.2088,
      "step": 18743
    },
    {
      "epoch": 37.714285714285715,
      "grad_norm": 1.0332236289978027,
      "learning_rate": 0.0001245839621692323,
      "loss": 0.2036,
      "step": 18744
    },
    {
      "epoch": 37.716297786720325,
      "grad_norm": 0.9260103106498718,
      "learning_rate": 0.00012457993761947884,
      "loss": 0.1806,
      "step": 18745
    },
    {
      "epoch": 37.71830985915493,
      "grad_norm": 0.9773954153060913,
      "learning_rate": 0.00012457591306972533,
      "loss": 0.1872,
      "step": 18746
    },
    {
      "epoch": 37.72032193158954,
      "grad_norm": 1.0132837295532227,
      "learning_rate": 0.00012457188851997184,
      "loss": 0.1995,
      "step": 18747
    },
    {
      "epoch": 37.72233400402415,
      "grad_norm": 0.9413421750068665,
      "learning_rate": 0.00012456786397021833,
      "loss": 0.1861,
      "step": 18748
    },
    {
      "epoch": 37.72434607645875,
      "grad_norm": 1.0207880735397339,
      "learning_rate": 0.00012456383942046484,
      "loss": 0.1833,
      "step": 18749
    },
    {
      "epoch": 37.72635814889336,
      "grad_norm": 1.0094538927078247,
      "learning_rate": 0.00012455981487071135,
      "loss": 0.2044,
      "step": 18750
    },
    {
      "epoch": 37.72837022132797,
      "grad_norm": 1.0237056016921997,
      "learning_rate": 0.00012455579032095786,
      "loss": 0.2095,
      "step": 18751
    },
    {
      "epoch": 37.730382293762574,
      "grad_norm": 1.0192720890045166,
      "learning_rate": 0.00012455176577120435,
      "loss": 0.2057,
      "step": 18752
    },
    {
      "epoch": 37.732394366197184,
      "grad_norm": 0.9753247499465942,
      "learning_rate": 0.00012454774122145086,
      "loss": 0.1945,
      "step": 18753
    },
    {
      "epoch": 37.734406438631794,
      "grad_norm": 0.9889864921569824,
      "learning_rate": 0.00012454371667169735,
      "loss": 0.2058,
      "step": 18754
    },
    {
      "epoch": 37.7364185110664,
      "grad_norm": 0.976600170135498,
      "learning_rate": 0.00012453969212194389,
      "loss": 0.1974,
      "step": 18755
    },
    {
      "epoch": 37.73843058350101,
      "grad_norm": 0.945224404335022,
      "learning_rate": 0.00012453566757219037,
      "loss": 0.1819,
      "step": 18756
    },
    {
      "epoch": 37.74044265593562,
      "grad_norm": 0.95907062292099,
      "learning_rate": 0.00012453164302243688,
      "loss": 0.1897,
      "step": 18757
    },
    {
      "epoch": 37.74245472837022,
      "grad_norm": 1.0268685817718506,
      "learning_rate": 0.00012452761847268337,
      "loss": 0.2119,
      "step": 18758
    },
    {
      "epoch": 37.74446680080483,
      "grad_norm": 0.9328367710113525,
      "learning_rate": 0.00012452359392292988,
      "loss": 0.1921,
      "step": 18759
    },
    {
      "epoch": 37.74647887323944,
      "grad_norm": 1.0199397802352905,
      "learning_rate": 0.0001245195693731764,
      "loss": 0.2009,
      "step": 18760
    },
    {
      "epoch": 37.74849094567404,
      "grad_norm": 0.9903268814086914,
      "learning_rate": 0.0001245155448234229,
      "loss": 0.1791,
      "step": 18761
    },
    {
      "epoch": 37.75050301810865,
      "grad_norm": 0.9700508117675781,
      "learning_rate": 0.0001245115202736694,
      "loss": 0.2081,
      "step": 18762
    },
    {
      "epoch": 37.75251509054326,
      "grad_norm": 0.978265106678009,
      "learning_rate": 0.0001245074957239159,
      "loss": 0.1957,
      "step": 18763
    },
    {
      "epoch": 37.754527162977865,
      "grad_norm": 0.9596468210220337,
      "learning_rate": 0.0001245034711741624,
      "loss": 0.1979,
      "step": 18764
    },
    {
      "epoch": 37.756539235412475,
      "grad_norm": 1.0332931280136108,
      "learning_rate": 0.0001244994466244089,
      "loss": 0.1908,
      "step": 18765
    },
    {
      "epoch": 37.758551307847085,
      "grad_norm": 1.0488362312316895,
      "learning_rate": 0.0001244954220746554,
      "loss": 0.1818,
      "step": 18766
    },
    {
      "epoch": 37.76056338028169,
      "grad_norm": 1.003169059753418,
      "learning_rate": 0.0001244913975249019,
      "loss": 0.21,
      "step": 18767
    },
    {
      "epoch": 37.7625754527163,
      "grad_norm": 1.089086651802063,
      "learning_rate": 0.0001244873729751484,
      "loss": 0.2242,
      "step": 18768
    },
    {
      "epoch": 37.76458752515091,
      "grad_norm": 1.0271373987197876,
      "learning_rate": 0.00012448334842539492,
      "loss": 0.199,
      "step": 18769
    },
    {
      "epoch": 37.76659959758551,
      "grad_norm": 1.0490856170654297,
      "learning_rate": 0.00012447932387564143,
      "loss": 0.2152,
      "step": 18770
    },
    {
      "epoch": 37.76861167002012,
      "grad_norm": 1.0444309711456299,
      "learning_rate": 0.00012447529932588792,
      "loss": 0.2102,
      "step": 18771
    },
    {
      "epoch": 37.77062374245473,
      "grad_norm": 0.9992513060569763,
      "learning_rate": 0.00012447127477613443,
      "loss": 0.1985,
      "step": 18772
    },
    {
      "epoch": 37.772635814889334,
      "grad_norm": 1.0009794235229492,
      "learning_rate": 0.00012446725022638092,
      "loss": 0.2004,
      "step": 18773
    },
    {
      "epoch": 37.774647887323944,
      "grad_norm": 0.9842468500137329,
      "learning_rate": 0.00012446322567662743,
      "loss": 0.203,
      "step": 18774
    },
    {
      "epoch": 37.776659959758554,
      "grad_norm": 1.0520217418670654,
      "learning_rate": 0.00012445920112687394,
      "loss": 0.2254,
      "step": 18775
    },
    {
      "epoch": 37.77867203219316,
      "grad_norm": 0.9968835711479187,
      "learning_rate": 0.00012445517657712045,
      "loss": 0.1979,
      "step": 18776
    },
    {
      "epoch": 37.78068410462777,
      "grad_norm": 1.0385137796401978,
      "learning_rate": 0.00012445115202736694,
      "loss": 0.2114,
      "step": 18777
    },
    {
      "epoch": 37.78269617706238,
      "grad_norm": 0.9916641712188721,
      "learning_rate": 0.00012444712747761345,
      "loss": 0.2109,
      "step": 18778
    },
    {
      "epoch": 37.78470824949698,
      "grad_norm": 1.0421867370605469,
      "learning_rate": 0.00012444310292785993,
      "loss": 0.2223,
      "step": 18779
    },
    {
      "epoch": 37.78672032193159,
      "grad_norm": 1.0669336318969727,
      "learning_rate": 0.00012443907837810647,
      "loss": 0.217,
      "step": 18780
    },
    {
      "epoch": 37.7887323943662,
      "grad_norm": 0.9916324615478516,
      "learning_rate": 0.00012443505382835296,
      "loss": 0.2065,
      "step": 18781
    },
    {
      "epoch": 37.7907444668008,
      "grad_norm": 1.0653539896011353,
      "learning_rate": 0.00012443102927859947,
      "loss": 0.1917,
      "step": 18782
    },
    {
      "epoch": 37.79275653923541,
      "grad_norm": 1.0288281440734863,
      "learning_rate": 0.00012442700472884596,
      "loss": 0.2093,
      "step": 18783
    },
    {
      "epoch": 37.79476861167002,
      "grad_norm": 1.0213098526000977,
      "learning_rate": 0.00012442298017909247,
      "loss": 0.1977,
      "step": 18784
    },
    {
      "epoch": 37.796780684104625,
      "grad_norm": 0.9633578062057495,
      "learning_rate": 0.00012441895562933898,
      "loss": 0.2024,
      "step": 18785
    },
    {
      "epoch": 37.798792756539235,
      "grad_norm": 0.9783235192298889,
      "learning_rate": 0.0001244149310795855,
      "loss": 0.2041,
      "step": 18786
    },
    {
      "epoch": 37.800804828973845,
      "grad_norm": 0.9678493142127991,
      "learning_rate": 0.00012441090652983198,
      "loss": 0.1978,
      "step": 18787
    },
    {
      "epoch": 37.80281690140845,
      "grad_norm": 0.9845783114433289,
      "learning_rate": 0.0001244068819800785,
      "loss": 0.2015,
      "step": 18788
    },
    {
      "epoch": 37.80482897384306,
      "grad_norm": 1.0306718349456787,
      "learning_rate": 0.00012440285743032498,
      "loss": 0.205,
      "step": 18789
    },
    {
      "epoch": 37.80684104627767,
      "grad_norm": 1.033908724784851,
      "learning_rate": 0.00012439883288057151,
      "loss": 0.2021,
      "step": 18790
    },
    {
      "epoch": 37.80885311871227,
      "grad_norm": 0.9822037816047668,
      "learning_rate": 0.000124394808330818,
      "loss": 0.2082,
      "step": 18791
    },
    {
      "epoch": 37.81086519114688,
      "grad_norm": 0.975780725479126,
      "learning_rate": 0.0001243907837810645,
      "loss": 0.2068,
      "step": 18792
    },
    {
      "epoch": 37.81287726358149,
      "grad_norm": 0.9566493630409241,
      "learning_rate": 0.000124386759231311,
      "loss": 0.2047,
      "step": 18793
    },
    {
      "epoch": 37.814889336016094,
      "grad_norm": 0.967411458492279,
      "learning_rate": 0.0001243827346815575,
      "loss": 0.194,
      "step": 18794
    },
    {
      "epoch": 37.816901408450704,
      "grad_norm": 1.050679087638855,
      "learning_rate": 0.000124378710131804,
      "loss": 0.2101,
      "step": 18795
    },
    {
      "epoch": 37.818913480885314,
      "grad_norm": 0.9823094606399536,
      "learning_rate": 0.00012437468558205053,
      "loss": 0.2046,
      "step": 18796
    },
    {
      "epoch": 37.82092555331992,
      "grad_norm": 0.9793132543563843,
      "learning_rate": 0.00012437066103229702,
      "loss": 0.2067,
      "step": 18797
    },
    {
      "epoch": 37.82293762575453,
      "grad_norm": 1.0101310014724731,
      "learning_rate": 0.00012436663648254353,
      "loss": 0.1921,
      "step": 18798
    },
    {
      "epoch": 37.82494969818914,
      "grad_norm": 0.9832582473754883,
      "learning_rate": 0.00012436261193279002,
      "loss": 0.1965,
      "step": 18799
    },
    {
      "epoch": 37.82696177062374,
      "grad_norm": 0.9726905226707458,
      "learning_rate": 0.00012435858738303653,
      "loss": 0.2073,
      "step": 18800
    },
    {
      "epoch": 37.82897384305835,
      "grad_norm": 1.0285634994506836,
      "learning_rate": 0.00012435456283328304,
      "loss": 0.206,
      "step": 18801
    },
    {
      "epoch": 37.83098591549296,
      "grad_norm": 0.9907756447792053,
      "learning_rate": 0.00012435053828352953,
      "loss": 0.1925,
      "step": 18802
    },
    {
      "epoch": 37.83299798792756,
      "grad_norm": 0.9396780729293823,
      "learning_rate": 0.00012434651373377604,
      "loss": 0.1939,
      "step": 18803
    },
    {
      "epoch": 37.83501006036217,
      "grad_norm": 0.9924782514572144,
      "learning_rate": 0.00012434248918402255,
      "loss": 0.2047,
      "step": 18804
    },
    {
      "epoch": 37.83702213279678,
      "grad_norm": 1.022229790687561,
      "learning_rate": 0.00012433846463426904,
      "loss": 0.1853,
      "step": 18805
    },
    {
      "epoch": 37.839034205231385,
      "grad_norm": 0.9527320265769958,
      "learning_rate": 0.00012433444008451555,
      "loss": 0.1924,
      "step": 18806
    },
    {
      "epoch": 37.841046277665995,
      "grad_norm": 0.9902905821800232,
      "learning_rate": 0.00012433041553476206,
      "loss": 0.197,
      "step": 18807
    },
    {
      "epoch": 37.843058350100605,
      "grad_norm": 1.0373823642730713,
      "learning_rate": 0.00012432639098500854,
      "loss": 0.2128,
      "step": 18808
    },
    {
      "epoch": 37.84507042253521,
      "grad_norm": 0.9363898038864136,
      "learning_rate": 0.00012432236643525506,
      "loss": 0.1869,
      "step": 18809
    },
    {
      "epoch": 37.84708249496982,
      "grad_norm": 1.0512166023254395,
      "learning_rate": 0.00012431834188550154,
      "loss": 0.205,
      "step": 18810
    },
    {
      "epoch": 37.84909456740443,
      "grad_norm": 1.0602039098739624,
      "learning_rate": 0.00012431431733574808,
      "loss": 0.1966,
      "step": 18811
    },
    {
      "epoch": 37.85110663983903,
      "grad_norm": 1.0568853616714478,
      "learning_rate": 0.00012431029278599457,
      "loss": 0.2049,
      "step": 18812
    },
    {
      "epoch": 37.85311871227364,
      "grad_norm": 0.9778909683227539,
      "learning_rate": 0.00012430626823624108,
      "loss": 0.1951,
      "step": 18813
    },
    {
      "epoch": 37.85513078470825,
      "grad_norm": 0.9976051449775696,
      "learning_rate": 0.00012430224368648756,
      "loss": 0.193,
      "step": 18814
    },
    {
      "epoch": 37.857142857142854,
      "grad_norm": 1.0434858798980713,
      "learning_rate": 0.00012429821913673408,
      "loss": 0.2146,
      "step": 18815
    },
    {
      "epoch": 37.859154929577464,
      "grad_norm": 1.076366901397705,
      "learning_rate": 0.0001242941945869806,
      "loss": 0.2118,
      "step": 18816
    },
    {
      "epoch": 37.861167002012074,
      "grad_norm": 1.0298550128936768,
      "learning_rate": 0.0001242901700372271,
      "loss": 0.2075,
      "step": 18817
    },
    {
      "epoch": 37.86317907444668,
      "grad_norm": 1.031123399734497,
      "learning_rate": 0.00012428614548747359,
      "loss": 0.2222,
      "step": 18818
    },
    {
      "epoch": 37.86519114688129,
      "grad_norm": 1.07411527633667,
      "learning_rate": 0.0001242821209377201,
      "loss": 0.2247,
      "step": 18819
    },
    {
      "epoch": 37.8672032193159,
      "grad_norm": 0.9611954689025879,
      "learning_rate": 0.00012427809638796658,
      "loss": 0.1897,
      "step": 18820
    },
    {
      "epoch": 37.8692152917505,
      "grad_norm": 0.9598423838615417,
      "learning_rate": 0.00012427407183821312,
      "loss": 0.2049,
      "step": 18821
    },
    {
      "epoch": 37.87122736418511,
      "grad_norm": 1.0136158466339111,
      "learning_rate": 0.0001242700472884596,
      "loss": 0.1907,
      "step": 18822
    },
    {
      "epoch": 37.87323943661972,
      "grad_norm": 1.0119857788085938,
      "learning_rate": 0.00012426602273870612,
      "loss": 0.2061,
      "step": 18823
    },
    {
      "epoch": 37.87525150905432,
      "grad_norm": 1.009973168373108,
      "learning_rate": 0.0001242619981889526,
      "loss": 0.194,
      "step": 18824
    },
    {
      "epoch": 37.87726358148893,
      "grad_norm": 1.004509449005127,
      "learning_rate": 0.00012425797363919912,
      "loss": 0.2216,
      "step": 18825
    },
    {
      "epoch": 37.87927565392354,
      "grad_norm": 1.0175096988677979,
      "learning_rate": 0.00012425394908944563,
      "loss": 0.2095,
      "step": 18826
    },
    {
      "epoch": 37.881287726358146,
      "grad_norm": 1.0227023363113403,
      "learning_rate": 0.00012424992453969214,
      "loss": 0.2126,
      "step": 18827
    },
    {
      "epoch": 37.883299798792756,
      "grad_norm": 1.0008801221847534,
      "learning_rate": 0.00012424589998993863,
      "loss": 0.2042,
      "step": 18828
    },
    {
      "epoch": 37.885311871227366,
      "grad_norm": 1.0379854440689087,
      "learning_rate": 0.00012424187544018514,
      "loss": 0.1978,
      "step": 18829
    },
    {
      "epoch": 37.88732394366197,
      "grad_norm": 1.0009410381317139,
      "learning_rate": 0.00012423785089043162,
      "loss": 0.2013,
      "step": 18830
    },
    {
      "epoch": 37.88933601609658,
      "grad_norm": 0.9555762410163879,
      "learning_rate": 0.00012423382634067816,
      "loss": 0.2046,
      "step": 18831
    },
    {
      "epoch": 37.89134808853119,
      "grad_norm": 0.9875212907791138,
      "learning_rate": 0.00012422980179092465,
      "loss": 0.1918,
      "step": 18832
    },
    {
      "epoch": 37.89336016096579,
      "grad_norm": 1.0184205770492554,
      "learning_rate": 0.00012422577724117116,
      "loss": 0.1938,
      "step": 18833
    },
    {
      "epoch": 37.8953722334004,
      "grad_norm": 1.0218682289123535,
      "learning_rate": 0.00012422175269141765,
      "loss": 0.2103,
      "step": 18834
    },
    {
      "epoch": 37.89738430583501,
      "grad_norm": 1.0172717571258545,
      "learning_rate": 0.00012421772814166416,
      "loss": 0.2082,
      "step": 18835
    },
    {
      "epoch": 37.899396378269614,
      "grad_norm": 0.990729570388794,
      "learning_rate": 0.00012421370359191067,
      "loss": 0.1858,
      "step": 18836
    },
    {
      "epoch": 37.901408450704224,
      "grad_norm": 1.0510660409927368,
      "learning_rate": 0.00012420967904215716,
      "loss": 0.2212,
      "step": 18837
    },
    {
      "epoch": 37.903420523138834,
      "grad_norm": 0.9904453754425049,
      "learning_rate": 0.00012420565449240367,
      "loss": 0.2023,
      "step": 18838
    },
    {
      "epoch": 37.905432595573444,
      "grad_norm": 0.9938942790031433,
      "learning_rate": 0.00012420162994265015,
      "loss": 0.1893,
      "step": 18839
    },
    {
      "epoch": 37.90744466800805,
      "grad_norm": 0.9995278716087341,
      "learning_rate": 0.00012419760539289666,
      "loss": 0.2021,
      "step": 18840
    },
    {
      "epoch": 37.90945674044266,
      "grad_norm": 0.9627891182899475,
      "learning_rate": 0.00012419358084314318,
      "loss": 0.1925,
      "step": 18841
    },
    {
      "epoch": 37.91146881287727,
      "grad_norm": 1.0172773599624634,
      "learning_rate": 0.0001241895562933897,
      "loss": 0.1961,
      "step": 18842
    },
    {
      "epoch": 37.91348088531187,
      "grad_norm": 1.0490169525146484,
      "learning_rate": 0.00012418553174363617,
      "loss": 0.2096,
      "step": 18843
    },
    {
      "epoch": 37.91549295774648,
      "grad_norm": 1.0423117876052856,
      "learning_rate": 0.0001241815071938827,
      "loss": 0.2182,
      "step": 18844
    },
    {
      "epoch": 37.91750503018109,
      "grad_norm": 1.0526103973388672,
      "learning_rate": 0.00012417748264412917,
      "loss": 0.2293,
      "step": 18845
    },
    {
      "epoch": 37.91951710261569,
      "grad_norm": 1.045597791671753,
      "learning_rate": 0.0001241734580943757,
      "loss": 0.1921,
      "step": 18846
    },
    {
      "epoch": 37.9215291750503,
      "grad_norm": 1.1771156787872314,
      "learning_rate": 0.0001241694335446222,
      "loss": 0.2269,
      "step": 18847
    },
    {
      "epoch": 37.92354124748491,
      "grad_norm": 1.0168063640594482,
      "learning_rate": 0.0001241654089948687,
      "loss": 0.207,
      "step": 18848
    },
    {
      "epoch": 37.925553319919516,
      "grad_norm": 1.0910556316375732,
      "learning_rate": 0.0001241613844451152,
      "loss": 0.2121,
      "step": 18849
    },
    {
      "epoch": 37.927565392354126,
      "grad_norm": 1.0729655027389526,
      "learning_rate": 0.0001241573598953617,
      "loss": 0.2076,
      "step": 18850
    },
    {
      "epoch": 37.929577464788736,
      "grad_norm": 0.9993253946304321,
      "learning_rate": 0.00012415333534560822,
      "loss": 0.1959,
      "step": 18851
    },
    {
      "epoch": 37.93158953722334,
      "grad_norm": 1.04862380027771,
      "learning_rate": 0.00012414931079585473,
      "loss": 0.2067,
      "step": 18852
    },
    {
      "epoch": 37.93360160965795,
      "grad_norm": 1.0633972883224487,
      "learning_rate": 0.00012414528624610122,
      "loss": 0.2115,
      "step": 18853
    },
    {
      "epoch": 37.93561368209256,
      "grad_norm": 0.9601620435714722,
      "learning_rate": 0.00012414126169634773,
      "loss": 0.2022,
      "step": 18854
    },
    {
      "epoch": 37.93762575452716,
      "grad_norm": 1.069239854812622,
      "learning_rate": 0.0001241372371465942,
      "loss": 0.2322,
      "step": 18855
    },
    {
      "epoch": 37.93963782696177,
      "grad_norm": 1.0379290580749512,
      "learning_rate": 0.00012413321259684075,
      "loss": 0.2093,
      "step": 18856
    },
    {
      "epoch": 37.94164989939638,
      "grad_norm": 1.0153369903564453,
      "learning_rate": 0.00012412918804708724,
      "loss": 0.2161,
      "step": 18857
    },
    {
      "epoch": 37.943661971830984,
      "grad_norm": 1.0459563732147217,
      "learning_rate": 0.00012412516349733375,
      "loss": 0.2073,
      "step": 18858
    },
    {
      "epoch": 37.945674044265594,
      "grad_norm": 0.9883747696876526,
      "learning_rate": 0.00012412113894758023,
      "loss": 0.2088,
      "step": 18859
    },
    {
      "epoch": 37.947686116700204,
      "grad_norm": 0.9962043762207031,
      "learning_rate": 0.00012411711439782675,
      "loss": 0.1986,
      "step": 18860
    },
    {
      "epoch": 37.94969818913481,
      "grad_norm": 1.0619587898254395,
      "learning_rate": 0.00012411308984807326,
      "loss": 0.2054,
      "step": 18861
    },
    {
      "epoch": 37.95171026156942,
      "grad_norm": 1.0502792596817017,
      "learning_rate": 0.00012410906529831977,
      "loss": 0.2223,
      "step": 18862
    },
    {
      "epoch": 37.95372233400403,
      "grad_norm": 0.9945694208145142,
      "learning_rate": 0.00012410504074856626,
      "loss": 0.2157,
      "step": 18863
    },
    {
      "epoch": 37.95573440643863,
      "grad_norm": 1.0233036279678345,
      "learning_rate": 0.00012410101619881277,
      "loss": 0.2176,
      "step": 18864
    },
    {
      "epoch": 37.95774647887324,
      "grad_norm": 1.0364351272583008,
      "learning_rate": 0.00012409699164905925,
      "loss": 0.216,
      "step": 18865
    },
    {
      "epoch": 37.95975855130785,
      "grad_norm": 1.0978087186813354,
      "learning_rate": 0.00012409296709930577,
      "loss": 0.2388,
      "step": 18866
    },
    {
      "epoch": 37.96177062374245,
      "grad_norm": 1.0210683345794678,
      "learning_rate": 0.00012408894254955228,
      "loss": 0.2149,
      "step": 18867
    },
    {
      "epoch": 37.96378269617706,
      "grad_norm": 1.0583957433700562,
      "learning_rate": 0.0001240849179997988,
      "loss": 0.2247,
      "step": 18868
    },
    {
      "epoch": 37.96579476861167,
      "grad_norm": 1.000852108001709,
      "learning_rate": 0.00012408089345004528,
      "loss": 0.2129,
      "step": 18869
    },
    {
      "epoch": 37.967806841046276,
      "grad_norm": 1.0312097072601318,
      "learning_rate": 0.0001240768689002918,
      "loss": 0.2203,
      "step": 18870
    },
    {
      "epoch": 37.969818913480886,
      "grad_norm": 0.979966402053833,
      "learning_rate": 0.0001240728443505383,
      "loss": 0.2067,
      "step": 18871
    },
    {
      "epoch": 37.971830985915496,
      "grad_norm": 1.0109388828277588,
      "learning_rate": 0.00012406881980078478,
      "loss": 0.2275,
      "step": 18872
    },
    {
      "epoch": 37.9738430583501,
      "grad_norm": 1.0045976638793945,
      "learning_rate": 0.0001240647952510313,
      "loss": 0.2018,
      "step": 18873
    },
    {
      "epoch": 37.97585513078471,
      "grad_norm": 1.0200669765472412,
      "learning_rate": 0.00012406077070127778,
      "loss": 0.2243,
      "step": 18874
    },
    {
      "epoch": 37.97786720321932,
      "grad_norm": 1.0171586275100708,
      "learning_rate": 0.0001240567461515243,
      "loss": 0.2101,
      "step": 18875
    },
    {
      "epoch": 37.97987927565392,
      "grad_norm": 1.0272934436798096,
      "learning_rate": 0.0001240527216017708,
      "loss": 0.2014,
      "step": 18876
    },
    {
      "epoch": 37.98189134808853,
      "grad_norm": 1.015526533126831,
      "learning_rate": 0.00012404869705201732,
      "loss": 0.2057,
      "step": 18877
    },
    {
      "epoch": 37.98390342052314,
      "grad_norm": 1.0222402811050415,
      "learning_rate": 0.0001240446725022638,
      "loss": 0.2138,
      "step": 18878
    },
    {
      "epoch": 37.985915492957744,
      "grad_norm": 1.080223560333252,
      "learning_rate": 0.00012404064795251032,
      "loss": 0.2276,
      "step": 18879
    },
    {
      "epoch": 37.987927565392354,
      "grad_norm": 1.065843939781189,
      "learning_rate": 0.0001240366234027568,
      "loss": 0.2292,
      "step": 18880
    },
    {
      "epoch": 37.989939637826964,
      "grad_norm": 1.0568161010742188,
      "learning_rate": 0.00012403259885300334,
      "loss": 0.2226,
      "step": 18881
    },
    {
      "epoch": 37.99195171026157,
      "grad_norm": 0.9872822165489197,
      "learning_rate": 0.00012402857430324983,
      "loss": 0.2079,
      "step": 18882
    },
    {
      "epoch": 37.99396378269618,
      "grad_norm": 1.0017911195755005,
      "learning_rate": 0.00012402454975349634,
      "loss": 0.1934,
      "step": 18883
    },
    {
      "epoch": 37.99597585513079,
      "grad_norm": 0.9428618550300598,
      "learning_rate": 0.00012402052520374282,
      "loss": 0.2051,
      "step": 18884
    },
    {
      "epoch": 37.99798792756539,
      "grad_norm": 1.0901230573654175,
      "learning_rate": 0.00012401650065398934,
      "loss": 0.2286,
      "step": 18885
    },
    {
      "epoch": 38.0,
      "grad_norm": 1.0656052827835083,
      "learning_rate": 0.00012401247610423585,
      "loss": 0.2186,
      "step": 18886
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.5798417329788208,
      "eval_runtime": 49.8369,
      "eval_samples_per_second": 19.905,
      "eval_steps_per_second": 2.488,
      "step": 18886
    },
    {
      "epoch": 38.00201207243461,
      "grad_norm": 0.8313473463058472,
      "learning_rate": 0.00012400845155448236,
      "loss": 0.1595,
      "step": 18887
    },
    {
      "epoch": 38.00402414486921,
      "grad_norm": 0.8997723460197449,
      "learning_rate": 0.00012400442700472884,
      "loss": 0.1628,
      "step": 18888
    },
    {
      "epoch": 38.00603621730382,
      "grad_norm": 0.8805976510047913,
      "learning_rate": 0.00012400040245497536,
      "loss": 0.1564,
      "step": 18889
    },
    {
      "epoch": 38.00804828973843,
      "grad_norm": 0.840149998664856,
      "learning_rate": 0.00012399637790522184,
      "loss": 0.151,
      "step": 18890
    },
    {
      "epoch": 38.010060362173036,
      "grad_norm": 0.8855867981910706,
      "learning_rate": 0.00012399235335546838,
      "loss": 0.1567,
      "step": 18891
    },
    {
      "epoch": 38.012072434607646,
      "grad_norm": 0.9395853281021118,
      "learning_rate": 0.00012398832880571487,
      "loss": 0.1607,
      "step": 18892
    },
    {
      "epoch": 38.014084507042256,
      "grad_norm": 0.8554180860519409,
      "learning_rate": 0.00012398430425596138,
      "loss": 0.1636,
      "step": 18893
    },
    {
      "epoch": 38.01609657947686,
      "grad_norm": 0.8683317303657532,
      "learning_rate": 0.00012398027970620786,
      "loss": 0.1539,
      "step": 18894
    },
    {
      "epoch": 38.01810865191147,
      "grad_norm": 0.9049487709999084,
      "learning_rate": 0.00012397625515645438,
      "loss": 0.1626,
      "step": 18895
    },
    {
      "epoch": 38.02012072434608,
      "grad_norm": 0.8889650702476501,
      "learning_rate": 0.0001239722306067009,
      "loss": 0.1496,
      "step": 18896
    },
    {
      "epoch": 38.02213279678068,
      "grad_norm": 0.8536062240600586,
      "learning_rate": 0.0001239682060569474,
      "loss": 0.1564,
      "step": 18897
    },
    {
      "epoch": 38.02414486921529,
      "grad_norm": 0.8965795040130615,
      "learning_rate": 0.00012396418150719389,
      "loss": 0.1763,
      "step": 18898
    },
    {
      "epoch": 38.0261569416499,
      "grad_norm": 0.8150070309638977,
      "learning_rate": 0.0001239601569574404,
      "loss": 0.1461,
      "step": 18899
    },
    {
      "epoch": 38.028169014084504,
      "grad_norm": 0.8522792458534241,
      "learning_rate": 0.00012395613240768688,
      "loss": 0.1557,
      "step": 18900
    },
    {
      "epoch": 38.030181086519114,
      "grad_norm": 0.9214935302734375,
      "learning_rate": 0.0001239521078579334,
      "loss": 0.1656,
      "step": 18901
    },
    {
      "epoch": 38.032193158953724,
      "grad_norm": 0.8839842677116394,
      "learning_rate": 0.0001239480833081799,
      "loss": 0.1517,
      "step": 18902
    },
    {
      "epoch": 38.03420523138833,
      "grad_norm": 0.881895899772644,
      "learning_rate": 0.00012394405875842642,
      "loss": 0.1631,
      "step": 18903
    },
    {
      "epoch": 38.03621730382294,
      "grad_norm": 0.9386904239654541,
      "learning_rate": 0.0001239400342086729,
      "loss": 0.1651,
      "step": 18904
    },
    {
      "epoch": 38.03822937625755,
      "grad_norm": 0.8725935816764832,
      "learning_rate": 0.00012393600965891942,
      "loss": 0.1529,
      "step": 18905
    },
    {
      "epoch": 38.04024144869215,
      "grad_norm": 0.8708465099334717,
      "learning_rate": 0.00012393198510916593,
      "loss": 0.1532,
      "step": 18906
    },
    {
      "epoch": 38.04225352112676,
      "grad_norm": 0.8911324739456177,
      "learning_rate": 0.00012392796055941241,
      "loss": 0.1543,
      "step": 18907
    },
    {
      "epoch": 38.04426559356137,
      "grad_norm": 0.8353311419487,
      "learning_rate": 0.00012392393600965893,
      "loss": 0.1562,
      "step": 18908
    },
    {
      "epoch": 38.04627766599597,
      "grad_norm": 0.9451267719268799,
      "learning_rate": 0.0001239199114599054,
      "loss": 0.1885,
      "step": 18909
    },
    {
      "epoch": 38.04828973843058,
      "grad_norm": 0.8768327236175537,
      "learning_rate": 0.00012391588691015192,
      "loss": 0.162,
      "step": 18910
    },
    {
      "epoch": 38.05030181086519,
      "grad_norm": 0.8893373012542725,
      "learning_rate": 0.00012391186236039844,
      "loss": 0.1566,
      "step": 18911
    },
    {
      "epoch": 38.052313883299796,
      "grad_norm": 0.8818461894989014,
      "learning_rate": 0.00012390783781064495,
      "loss": 0.1538,
      "step": 18912
    },
    {
      "epoch": 38.054325955734406,
      "grad_norm": 0.8832144737243652,
      "learning_rate": 0.00012390381326089143,
      "loss": 0.1608,
      "step": 18913
    },
    {
      "epoch": 38.056338028169016,
      "grad_norm": 0.8877715468406677,
      "learning_rate": 0.00012389978871113795,
      "loss": 0.1556,
      "step": 18914
    },
    {
      "epoch": 38.05835010060362,
      "grad_norm": 0.874521017074585,
      "learning_rate": 0.00012389576416138443,
      "loss": 0.167,
      "step": 18915
    },
    {
      "epoch": 38.06036217303823,
      "grad_norm": 0.8598503470420837,
      "learning_rate": 0.00012389173961163097,
      "loss": 0.1649,
      "step": 18916
    },
    {
      "epoch": 38.06237424547284,
      "grad_norm": 0.8627615571022034,
      "learning_rate": 0.00012388771506187746,
      "loss": 0.1527,
      "step": 18917
    },
    {
      "epoch": 38.06438631790744,
      "grad_norm": 0.8395593166351318,
      "learning_rate": 0.00012388369051212397,
      "loss": 0.1406,
      "step": 18918
    },
    {
      "epoch": 38.06639839034205,
      "grad_norm": 0.8278011083602905,
      "learning_rate": 0.00012387966596237045,
      "loss": 0.1452,
      "step": 18919
    },
    {
      "epoch": 38.06841046277666,
      "grad_norm": 0.9224121570587158,
      "learning_rate": 0.00012387564141261696,
      "loss": 0.1569,
      "step": 18920
    },
    {
      "epoch": 38.070422535211264,
      "grad_norm": 0.8712525963783264,
      "learning_rate": 0.00012387161686286348,
      "loss": 0.1504,
      "step": 18921
    },
    {
      "epoch": 38.072434607645874,
      "grad_norm": 0.8879249095916748,
      "learning_rate": 0.00012386759231311,
      "loss": 0.1602,
      "step": 18922
    },
    {
      "epoch": 38.074446680080484,
      "grad_norm": 0.930014431476593,
      "learning_rate": 0.00012386356776335647,
      "loss": 0.1682,
      "step": 18923
    },
    {
      "epoch": 38.07645875251509,
      "grad_norm": 0.8547399044036865,
      "learning_rate": 0.00012385954321360299,
      "loss": 0.1485,
      "step": 18924
    },
    {
      "epoch": 38.0784708249497,
      "grad_norm": 0.9703657627105713,
      "learning_rate": 0.00012385551866384947,
      "loss": 0.1847,
      "step": 18925
    },
    {
      "epoch": 38.08048289738431,
      "grad_norm": 0.8702252507209778,
      "learning_rate": 0.000123851494114096,
      "loss": 0.1611,
      "step": 18926
    },
    {
      "epoch": 38.08249496981891,
      "grad_norm": 0.875586986541748,
      "learning_rate": 0.0001238474695643425,
      "loss": 0.1518,
      "step": 18927
    },
    {
      "epoch": 38.08450704225352,
      "grad_norm": 0.8852984309196472,
      "learning_rate": 0.000123843445014589,
      "loss": 0.1554,
      "step": 18928
    },
    {
      "epoch": 38.08651911468813,
      "grad_norm": 0.8853213787078857,
      "learning_rate": 0.0001238394204648355,
      "loss": 0.1432,
      "step": 18929
    },
    {
      "epoch": 38.08853118712273,
      "grad_norm": 0.9227067232131958,
      "learning_rate": 0.000123835395915082,
      "loss": 0.1547,
      "step": 18930
    },
    {
      "epoch": 38.09054325955734,
      "grad_norm": 0.912645697593689,
      "learning_rate": 0.00012383137136532852,
      "loss": 0.1633,
      "step": 18931
    },
    {
      "epoch": 38.09255533199195,
      "grad_norm": 0.9222813248634338,
      "learning_rate": 0.00012382734681557503,
      "loss": 0.1559,
      "step": 18932
    },
    {
      "epoch": 38.094567404426556,
      "grad_norm": 0.9187544584274292,
      "learning_rate": 0.00012382332226582151,
      "loss": 0.1614,
      "step": 18933
    },
    {
      "epoch": 38.096579476861166,
      "grad_norm": 0.9058705568313599,
      "learning_rate": 0.00012381929771606803,
      "loss": 0.1523,
      "step": 18934
    },
    {
      "epoch": 38.098591549295776,
      "grad_norm": 1.0066598653793335,
      "learning_rate": 0.0001238152731663145,
      "loss": 0.1736,
      "step": 18935
    },
    {
      "epoch": 38.100603621730386,
      "grad_norm": 0.9113937020301819,
      "learning_rate": 0.00012381124861656102,
      "loss": 0.1558,
      "step": 18936
    },
    {
      "epoch": 38.10261569416499,
      "grad_norm": 0.8584718704223633,
      "learning_rate": 0.00012380722406680754,
      "loss": 0.1471,
      "step": 18937
    },
    {
      "epoch": 38.1046277665996,
      "grad_norm": 0.8818424344062805,
      "learning_rate": 0.00012380319951705405,
      "loss": 0.1552,
      "step": 18938
    },
    {
      "epoch": 38.10663983903421,
      "grad_norm": 0.8615391850471497,
      "learning_rate": 0.00012379917496730053,
      "loss": 0.1565,
      "step": 18939
    },
    {
      "epoch": 38.10865191146881,
      "grad_norm": 0.8923956751823425,
      "learning_rate": 0.00012379515041754705,
      "loss": 0.1565,
      "step": 18940
    },
    {
      "epoch": 38.11066398390342,
      "grad_norm": 0.9237781763076782,
      "learning_rate": 0.00012379112586779356,
      "loss": 0.156,
      "step": 18941
    },
    {
      "epoch": 38.11267605633803,
      "grad_norm": 0.9136436581611633,
      "learning_rate": 0.00012378710131804004,
      "loss": 0.1611,
      "step": 18942
    },
    {
      "epoch": 38.114688128772634,
      "grad_norm": 0.8706792593002319,
      "learning_rate": 0.00012378307676828656,
      "loss": 0.1636,
      "step": 18943
    },
    {
      "epoch": 38.116700201207244,
      "grad_norm": 0.901362955570221,
      "learning_rate": 0.00012377905221853304,
      "loss": 0.1545,
      "step": 18944
    },
    {
      "epoch": 38.118712273641854,
      "grad_norm": 0.8654941916465759,
      "learning_rate": 0.00012377502766877955,
      "loss": 0.1542,
      "step": 18945
    },
    {
      "epoch": 38.12072434607646,
      "grad_norm": 0.8849256634712219,
      "learning_rate": 0.00012377100311902607,
      "loss": 0.1576,
      "step": 18946
    },
    {
      "epoch": 38.12273641851107,
      "grad_norm": 0.8582028150558472,
      "learning_rate": 0.00012376697856927258,
      "loss": 0.1453,
      "step": 18947
    },
    {
      "epoch": 38.12474849094568,
      "grad_norm": 0.8861251473426819,
      "learning_rate": 0.00012376295401951906,
      "loss": 0.166,
      "step": 18948
    },
    {
      "epoch": 38.12676056338028,
      "grad_norm": 0.9523430466651917,
      "learning_rate": 0.00012375892946976557,
      "loss": 0.1531,
      "step": 18949
    },
    {
      "epoch": 38.12877263581489,
      "grad_norm": 0.8917123675346375,
      "learning_rate": 0.00012375490492001206,
      "loss": 0.1617,
      "step": 18950
    },
    {
      "epoch": 38.1307847082495,
      "grad_norm": 0.8335344195365906,
      "learning_rate": 0.0001237508803702586,
      "loss": 0.148,
      "step": 18951
    },
    {
      "epoch": 38.1327967806841,
      "grad_norm": 0.8897712230682373,
      "learning_rate": 0.00012374685582050508,
      "loss": 0.1549,
      "step": 18952
    },
    {
      "epoch": 38.13480885311871,
      "grad_norm": 0.9348442554473877,
      "learning_rate": 0.0001237428312707516,
      "loss": 0.1631,
      "step": 18953
    },
    {
      "epoch": 38.13682092555332,
      "grad_norm": 0.8588157296180725,
      "learning_rate": 0.00012373880672099808,
      "loss": 0.1468,
      "step": 18954
    },
    {
      "epoch": 38.138832997987926,
      "grad_norm": 0.9090600609779358,
      "learning_rate": 0.0001237347821712446,
      "loss": 0.1513,
      "step": 18955
    },
    {
      "epoch": 38.140845070422536,
      "grad_norm": 0.9184964895248413,
      "learning_rate": 0.0001237307576214911,
      "loss": 0.1492,
      "step": 18956
    },
    {
      "epoch": 38.142857142857146,
      "grad_norm": 1.0000964403152466,
      "learning_rate": 0.00012372673307173762,
      "loss": 0.1672,
      "step": 18957
    },
    {
      "epoch": 38.14486921529175,
      "grad_norm": 0.8744111657142639,
      "learning_rate": 0.0001237227085219841,
      "loss": 0.1572,
      "step": 18958
    },
    {
      "epoch": 38.14688128772636,
      "grad_norm": 0.8567209839820862,
      "learning_rate": 0.00012371868397223062,
      "loss": 0.154,
      "step": 18959
    },
    {
      "epoch": 38.14889336016097,
      "grad_norm": 0.8832258582115173,
      "learning_rate": 0.0001237146594224771,
      "loss": 0.1551,
      "step": 18960
    },
    {
      "epoch": 38.15090543259557,
      "grad_norm": 0.8762446641921997,
      "learning_rate": 0.00012371063487272364,
      "loss": 0.159,
      "step": 18961
    },
    {
      "epoch": 38.15291750503018,
      "grad_norm": 0.9685951471328735,
      "learning_rate": 0.00012370661032297013,
      "loss": 0.1643,
      "step": 18962
    },
    {
      "epoch": 38.15492957746479,
      "grad_norm": 0.8448221683502197,
      "learning_rate": 0.00012370258577321664,
      "loss": 0.1534,
      "step": 18963
    },
    {
      "epoch": 38.156941649899395,
      "grad_norm": 0.9618946313858032,
      "learning_rate": 0.00012369856122346312,
      "loss": 0.1756,
      "step": 18964
    },
    {
      "epoch": 38.158953722334005,
      "grad_norm": 0.8583900332450867,
      "learning_rate": 0.00012369453667370963,
      "loss": 0.149,
      "step": 18965
    },
    {
      "epoch": 38.160965794768615,
      "grad_norm": 0.8857767581939697,
      "learning_rate": 0.00012369051212395615,
      "loss": 0.1495,
      "step": 18966
    },
    {
      "epoch": 38.16297786720322,
      "grad_norm": 0.891246497631073,
      "learning_rate": 0.00012368648757420266,
      "loss": 0.1555,
      "step": 18967
    },
    {
      "epoch": 38.16498993963783,
      "grad_norm": 0.9457758069038391,
      "learning_rate": 0.00012368246302444914,
      "loss": 0.1618,
      "step": 18968
    },
    {
      "epoch": 38.16700201207244,
      "grad_norm": 0.9429978728294373,
      "learning_rate": 0.00012367843847469566,
      "loss": 0.152,
      "step": 18969
    },
    {
      "epoch": 38.16901408450704,
      "grad_norm": 0.9534206390380859,
      "learning_rate": 0.00012367441392494214,
      "loss": 0.1755,
      "step": 18970
    },
    {
      "epoch": 38.17102615694165,
      "grad_norm": 0.9234196543693542,
      "learning_rate": 0.00012367038937518865,
      "loss": 0.1665,
      "step": 18971
    },
    {
      "epoch": 38.17303822937626,
      "grad_norm": 0.8805460333824158,
      "learning_rate": 0.00012366636482543517,
      "loss": 0.1578,
      "step": 18972
    },
    {
      "epoch": 38.17505030181086,
      "grad_norm": 0.8790978193283081,
      "learning_rate": 0.00012366234027568168,
      "loss": 0.154,
      "step": 18973
    },
    {
      "epoch": 38.17706237424547,
      "grad_norm": 0.9163778424263,
      "learning_rate": 0.00012365831572592816,
      "loss": 0.1713,
      "step": 18974
    },
    {
      "epoch": 38.17907444668008,
      "grad_norm": 0.9027085900306702,
      "learning_rate": 0.00012365429117617468,
      "loss": 0.173,
      "step": 18975
    },
    {
      "epoch": 38.181086519114686,
      "grad_norm": 0.9620437026023865,
      "learning_rate": 0.0001236502666264212,
      "loss": 0.1621,
      "step": 18976
    },
    {
      "epoch": 38.183098591549296,
      "grad_norm": 0.9530149698257446,
      "learning_rate": 0.00012364624207666767,
      "loss": 0.1669,
      "step": 18977
    },
    {
      "epoch": 38.185110663983906,
      "grad_norm": 0.8730782866477966,
      "learning_rate": 0.00012364221752691419,
      "loss": 0.1611,
      "step": 18978
    },
    {
      "epoch": 38.18712273641851,
      "grad_norm": 0.8922951817512512,
      "learning_rate": 0.00012363819297716067,
      "loss": 0.1742,
      "step": 18979
    },
    {
      "epoch": 38.18913480885312,
      "grad_norm": 0.8491050601005554,
      "learning_rate": 0.00012363416842740718,
      "loss": 0.1586,
      "step": 18980
    },
    {
      "epoch": 38.19114688128773,
      "grad_norm": 0.8660168051719666,
      "learning_rate": 0.0001236301438776537,
      "loss": 0.1503,
      "step": 18981
    },
    {
      "epoch": 38.19315895372233,
      "grad_norm": 0.9341460466384888,
      "learning_rate": 0.0001236261193279002,
      "loss": 0.1606,
      "step": 18982
    },
    {
      "epoch": 38.19517102615694,
      "grad_norm": 0.9632737040519714,
      "learning_rate": 0.0001236220947781467,
      "loss": 0.1729,
      "step": 18983
    },
    {
      "epoch": 38.19718309859155,
      "grad_norm": 0.959377646446228,
      "learning_rate": 0.0001236180702283932,
      "loss": 0.1741,
      "step": 18984
    },
    {
      "epoch": 38.199195171026155,
      "grad_norm": 0.9293760657310486,
      "learning_rate": 0.0001236140456786397,
      "loss": 0.1562,
      "step": 18985
    },
    {
      "epoch": 38.201207243460765,
      "grad_norm": 0.9869129061698914,
      "learning_rate": 0.00012361002112888623,
      "loss": 0.1631,
      "step": 18986
    },
    {
      "epoch": 38.203219315895375,
      "grad_norm": 0.9488990902900696,
      "learning_rate": 0.00012360599657913271,
      "loss": 0.1692,
      "step": 18987
    },
    {
      "epoch": 38.20523138832998,
      "grad_norm": 0.9321979284286499,
      "learning_rate": 0.00012360197202937923,
      "loss": 0.1571,
      "step": 18988
    },
    {
      "epoch": 38.20724346076459,
      "grad_norm": 0.9197143912315369,
      "learning_rate": 0.0001235979474796257,
      "loss": 0.1604,
      "step": 18989
    },
    {
      "epoch": 38.2092555331992,
      "grad_norm": 0.9278119802474976,
      "learning_rate": 0.00012359392292987222,
      "loss": 0.1682,
      "step": 18990
    },
    {
      "epoch": 38.2112676056338,
      "grad_norm": 1.0094720125198364,
      "learning_rate": 0.00012358989838011874,
      "loss": 0.1774,
      "step": 18991
    },
    {
      "epoch": 38.21327967806841,
      "grad_norm": 0.9334585666656494,
      "learning_rate": 0.00012358587383036525,
      "loss": 0.1684,
      "step": 18992
    },
    {
      "epoch": 38.21529175050302,
      "grad_norm": 0.9010410308837891,
      "learning_rate": 0.00012358184928061173,
      "loss": 0.1495,
      "step": 18993
    },
    {
      "epoch": 38.21730382293762,
      "grad_norm": 0.9762001037597656,
      "learning_rate": 0.00012357782473085825,
      "loss": 0.1811,
      "step": 18994
    },
    {
      "epoch": 38.21931589537223,
      "grad_norm": 0.9381127953529358,
      "learning_rate": 0.00012357380018110473,
      "loss": 0.1654,
      "step": 18995
    },
    {
      "epoch": 38.22132796780684,
      "grad_norm": 0.9659090042114258,
      "learning_rate": 0.00012356977563135127,
      "loss": 0.1679,
      "step": 18996
    },
    {
      "epoch": 38.223340040241446,
      "grad_norm": 0.9521558880805969,
      "learning_rate": 0.00012356575108159775,
      "loss": 0.1802,
      "step": 18997
    },
    {
      "epoch": 38.225352112676056,
      "grad_norm": 0.8841896057128906,
      "learning_rate": 0.00012356172653184427,
      "loss": 0.1522,
      "step": 18998
    },
    {
      "epoch": 38.227364185110666,
      "grad_norm": 1.005793809890747,
      "learning_rate": 0.00012355770198209075,
      "loss": 0.1659,
      "step": 18999
    },
    {
      "epoch": 38.22937625754527,
      "grad_norm": 0.9021818041801453,
      "learning_rate": 0.00012355367743233726,
      "loss": 0.1785,
      "step": 19000
    },
    {
      "epoch": 38.23138832997988,
      "grad_norm": 0.9072760939598083,
      "learning_rate": 0.00012354965288258378,
      "loss": 0.1711,
      "step": 19001
    },
    {
      "epoch": 38.23340040241449,
      "grad_norm": 0.9278637170791626,
      "learning_rate": 0.0001235456283328303,
      "loss": 0.1629,
      "step": 19002
    },
    {
      "epoch": 38.23541247484909,
      "grad_norm": 0.922417938709259,
      "learning_rate": 0.00012354160378307677,
      "loss": 0.1605,
      "step": 19003
    },
    {
      "epoch": 38.2374245472837,
      "grad_norm": 0.8742804527282715,
      "learning_rate": 0.00012353757923332329,
      "loss": 0.159,
      "step": 19004
    },
    {
      "epoch": 38.23943661971831,
      "grad_norm": 1.0125550031661987,
      "learning_rate": 0.00012353355468356977,
      "loss": 0.1761,
      "step": 19005
    },
    {
      "epoch": 38.241448692152915,
      "grad_norm": 0.9412603974342346,
      "learning_rate": 0.00012352953013381628,
      "loss": 0.1698,
      "step": 19006
    },
    {
      "epoch": 38.243460764587525,
      "grad_norm": 1.0051676034927368,
      "learning_rate": 0.0001235255055840628,
      "loss": 0.1631,
      "step": 19007
    },
    {
      "epoch": 38.245472837022135,
      "grad_norm": 0.9073774814605713,
      "learning_rate": 0.0001235214810343093,
      "loss": 0.1632,
      "step": 19008
    },
    {
      "epoch": 38.24748490945674,
      "grad_norm": 0.922620415687561,
      "learning_rate": 0.0001235174564845558,
      "loss": 0.162,
      "step": 19009
    },
    {
      "epoch": 38.24949698189135,
      "grad_norm": 0.8959438800811768,
      "learning_rate": 0.0001235134319348023,
      "loss": 0.17,
      "step": 19010
    },
    {
      "epoch": 38.25150905432596,
      "grad_norm": 1.0172052383422852,
      "learning_rate": 0.00012350940738504882,
      "loss": 0.1823,
      "step": 19011
    },
    {
      "epoch": 38.25352112676056,
      "grad_norm": 1.0306506156921387,
      "learning_rate": 0.0001235053828352953,
      "loss": 0.1846,
      "step": 19012
    },
    {
      "epoch": 38.25553319919517,
      "grad_norm": 0.988964319229126,
      "learning_rate": 0.00012350135828554181,
      "loss": 0.1888,
      "step": 19013
    },
    {
      "epoch": 38.25754527162978,
      "grad_norm": 0.9486826062202454,
      "learning_rate": 0.0001234973337357883,
      "loss": 0.1654,
      "step": 19014
    },
    {
      "epoch": 38.25955734406438,
      "grad_norm": 0.9372172951698303,
      "learning_rate": 0.0001234933091860348,
      "loss": 0.1789,
      "step": 19015
    },
    {
      "epoch": 38.26156941649899,
      "grad_norm": 0.9580305218696594,
      "learning_rate": 0.00012348928463628132,
      "loss": 0.1733,
      "step": 19016
    },
    {
      "epoch": 38.2635814889336,
      "grad_norm": 0.9515354633331299,
      "learning_rate": 0.00012348526008652784,
      "loss": 0.1767,
      "step": 19017
    },
    {
      "epoch": 38.265593561368206,
      "grad_norm": 0.9412007331848145,
      "learning_rate": 0.00012348123553677432,
      "loss": 0.1603,
      "step": 19018
    },
    {
      "epoch": 38.267605633802816,
      "grad_norm": 0.9231723546981812,
      "learning_rate": 0.00012347721098702083,
      "loss": 0.1553,
      "step": 19019
    },
    {
      "epoch": 38.269617706237426,
      "grad_norm": 0.9540201425552368,
      "learning_rate": 0.00012347318643726732,
      "loss": 0.1733,
      "step": 19020
    },
    {
      "epoch": 38.27162977867203,
      "grad_norm": 0.9396102428436279,
      "learning_rate": 0.00012346916188751386,
      "loss": 0.1713,
      "step": 19021
    },
    {
      "epoch": 38.27364185110664,
      "grad_norm": 0.9935122728347778,
      "learning_rate": 0.00012346513733776034,
      "loss": 0.1701,
      "step": 19022
    },
    {
      "epoch": 38.27565392354125,
      "grad_norm": 0.9313750863075256,
      "learning_rate": 0.00012346111278800686,
      "loss": 0.1689,
      "step": 19023
    },
    {
      "epoch": 38.27766599597585,
      "grad_norm": 0.9365981221199036,
      "learning_rate": 0.00012345708823825334,
      "loss": 0.1773,
      "step": 19024
    },
    {
      "epoch": 38.27967806841046,
      "grad_norm": 0.9226269125938416,
      "learning_rate": 0.00012345306368849985,
      "loss": 0.1703,
      "step": 19025
    },
    {
      "epoch": 38.28169014084507,
      "grad_norm": 0.9478782415390015,
      "learning_rate": 0.00012344903913874637,
      "loss": 0.1656,
      "step": 19026
    },
    {
      "epoch": 38.283702213279675,
      "grad_norm": 0.9637779593467712,
      "learning_rate": 0.00012344501458899288,
      "loss": 0.1734,
      "step": 19027
    },
    {
      "epoch": 38.285714285714285,
      "grad_norm": 0.9403874278068542,
      "learning_rate": 0.00012344099003923936,
      "loss": 0.1688,
      "step": 19028
    },
    {
      "epoch": 38.287726358148895,
      "grad_norm": 0.932633638381958,
      "learning_rate": 0.00012343696548948587,
      "loss": 0.1694,
      "step": 19029
    },
    {
      "epoch": 38.2897384305835,
      "grad_norm": 0.9854529500007629,
      "learning_rate": 0.00012343294093973236,
      "loss": 0.1857,
      "step": 19030
    },
    {
      "epoch": 38.29175050301811,
      "grad_norm": 0.9082276821136475,
      "learning_rate": 0.0001234289163899789,
      "loss": 0.1661,
      "step": 19031
    },
    {
      "epoch": 38.29376257545272,
      "grad_norm": 0.9751853942871094,
      "learning_rate": 0.00012342489184022538,
      "loss": 0.1714,
      "step": 19032
    },
    {
      "epoch": 38.29577464788732,
      "grad_norm": 0.9861719012260437,
      "learning_rate": 0.0001234208672904719,
      "loss": 0.173,
      "step": 19033
    },
    {
      "epoch": 38.29778672032193,
      "grad_norm": 0.9317805767059326,
      "learning_rate": 0.00012341684274071838,
      "loss": 0.1768,
      "step": 19034
    },
    {
      "epoch": 38.29979879275654,
      "grad_norm": 0.9525514841079712,
      "learning_rate": 0.0001234128181909649,
      "loss": 0.1782,
      "step": 19035
    },
    {
      "epoch": 38.30181086519114,
      "grad_norm": 0.9146715998649597,
      "learning_rate": 0.00012340879364121138,
      "loss": 0.1766,
      "step": 19036
    },
    {
      "epoch": 38.30382293762575,
      "grad_norm": 1.0124702453613281,
      "learning_rate": 0.00012340476909145792,
      "loss": 0.1724,
      "step": 19037
    },
    {
      "epoch": 38.30583501006036,
      "grad_norm": 0.883979320526123,
      "learning_rate": 0.0001234007445417044,
      "loss": 0.1526,
      "step": 19038
    },
    {
      "epoch": 38.30784708249497,
      "grad_norm": 0.9549257755279541,
      "learning_rate": 0.00012339671999195092,
      "loss": 0.1697,
      "step": 19039
    },
    {
      "epoch": 38.309859154929576,
      "grad_norm": 0.9164266586303711,
      "learning_rate": 0.0001233926954421974,
      "loss": 0.1686,
      "step": 19040
    },
    {
      "epoch": 38.311871227364186,
      "grad_norm": 0.9363642930984497,
      "learning_rate": 0.0001233886708924439,
      "loss": 0.1783,
      "step": 19041
    },
    {
      "epoch": 38.313883299798796,
      "grad_norm": 0.9588695764541626,
      "learning_rate": 0.00012338464634269043,
      "loss": 0.1679,
      "step": 19042
    },
    {
      "epoch": 38.3158953722334,
      "grad_norm": 0.9576873183250427,
      "learning_rate": 0.0001233806217929369,
      "loss": 0.1887,
      "step": 19043
    },
    {
      "epoch": 38.31790744466801,
      "grad_norm": 1.0309090614318848,
      "learning_rate": 0.00012337659724318342,
      "loss": 0.1739,
      "step": 19044
    },
    {
      "epoch": 38.31991951710262,
      "grad_norm": 0.9577049612998962,
      "learning_rate": 0.00012337257269342993,
      "loss": 0.1735,
      "step": 19045
    },
    {
      "epoch": 38.32193158953722,
      "grad_norm": 0.9668408036231995,
      "learning_rate": 0.00012336854814367642,
      "loss": 0.1938,
      "step": 19046
    },
    {
      "epoch": 38.32394366197183,
      "grad_norm": 1.0031356811523438,
      "learning_rate": 0.00012336452359392293,
      "loss": 0.1712,
      "step": 19047
    },
    {
      "epoch": 38.32595573440644,
      "grad_norm": 0.9832841753959656,
      "learning_rate": 0.00012336049904416944,
      "loss": 0.1625,
      "step": 19048
    },
    {
      "epoch": 38.327967806841045,
      "grad_norm": 0.9815142750740051,
      "learning_rate": 0.00012335647449441593,
      "loss": 0.1877,
      "step": 19049
    },
    {
      "epoch": 38.329979879275655,
      "grad_norm": 0.9120591282844543,
      "learning_rate": 0.00012335244994466244,
      "loss": 0.1541,
      "step": 19050
    },
    {
      "epoch": 38.331991951710265,
      "grad_norm": 0.9524604082107544,
      "learning_rate": 0.00012334842539490893,
      "loss": 0.1806,
      "step": 19051
    },
    {
      "epoch": 38.33400402414487,
      "grad_norm": 0.9137952923774719,
      "learning_rate": 0.00012334440084515547,
      "loss": 0.1637,
      "step": 19052
    },
    {
      "epoch": 38.33601609657948,
      "grad_norm": 1.0160725116729736,
      "learning_rate": 0.00012334037629540195,
      "loss": 0.1974,
      "step": 19053
    },
    {
      "epoch": 38.33802816901409,
      "grad_norm": 1.0017571449279785,
      "learning_rate": 0.00012333635174564846,
      "loss": 0.1702,
      "step": 19054
    },
    {
      "epoch": 38.34004024144869,
      "grad_norm": 0.9435560703277588,
      "learning_rate": 0.00012333232719589495,
      "loss": 0.1637,
      "step": 19055
    },
    {
      "epoch": 38.3420523138833,
      "grad_norm": 1.011593222618103,
      "learning_rate": 0.00012332830264614146,
      "loss": 0.1834,
      "step": 19056
    },
    {
      "epoch": 38.34406438631791,
      "grad_norm": 0.9222779870033264,
      "learning_rate": 0.00012332427809638797,
      "loss": 0.1606,
      "step": 19057
    },
    {
      "epoch": 38.34607645875251,
      "grad_norm": 0.9694690108299255,
      "learning_rate": 0.00012332025354663448,
      "loss": 0.1863,
      "step": 19058
    },
    {
      "epoch": 38.34808853118712,
      "grad_norm": 0.9785642623901367,
      "learning_rate": 0.00012331622899688097,
      "loss": 0.1782,
      "step": 19059
    },
    {
      "epoch": 38.35010060362173,
      "grad_norm": 0.9405264258384705,
      "learning_rate": 0.00012331220444712748,
      "loss": 0.1832,
      "step": 19060
    },
    {
      "epoch": 38.352112676056336,
      "grad_norm": 0.9267129898071289,
      "learning_rate": 0.00012330817989737397,
      "loss": 0.1692,
      "step": 19061
    },
    {
      "epoch": 38.354124748490946,
      "grad_norm": 0.9337968826293945,
      "learning_rate": 0.0001233041553476205,
      "loss": 0.17,
      "step": 19062
    },
    {
      "epoch": 38.356136820925556,
      "grad_norm": 0.9666345715522766,
      "learning_rate": 0.000123300130797867,
      "loss": 0.1807,
      "step": 19063
    },
    {
      "epoch": 38.35814889336016,
      "grad_norm": 1.0248422622680664,
      "learning_rate": 0.0001232961062481135,
      "loss": 0.1712,
      "step": 19064
    },
    {
      "epoch": 38.36016096579477,
      "grad_norm": 1.0028623342514038,
      "learning_rate": 0.00012329208169836,
      "loss": 0.1793,
      "step": 19065
    },
    {
      "epoch": 38.36217303822938,
      "grad_norm": 0.948821485042572,
      "learning_rate": 0.0001232880571486065,
      "loss": 0.1787,
      "step": 19066
    },
    {
      "epoch": 38.36418511066398,
      "grad_norm": 0.9396036863327026,
      "learning_rate": 0.00012328403259885301,
      "loss": 0.1789,
      "step": 19067
    },
    {
      "epoch": 38.36619718309859,
      "grad_norm": 0.9718916416168213,
      "learning_rate": 0.00012328000804909953,
      "loss": 0.1463,
      "step": 19068
    },
    {
      "epoch": 38.3682092555332,
      "grad_norm": 0.9653444290161133,
      "learning_rate": 0.000123275983499346,
      "loss": 0.1901,
      "step": 19069
    },
    {
      "epoch": 38.370221327967805,
      "grad_norm": 0.9251576066017151,
      "learning_rate": 0.00012327195894959252,
      "loss": 0.1699,
      "step": 19070
    },
    {
      "epoch": 38.372233400402415,
      "grad_norm": 0.9619722366333008,
      "learning_rate": 0.000123267934399839,
      "loss": 0.1736,
      "step": 19071
    },
    {
      "epoch": 38.374245472837025,
      "grad_norm": 0.9278452396392822,
      "learning_rate": 0.00012326390985008555,
      "loss": 0.1507,
      "step": 19072
    },
    {
      "epoch": 38.37625754527163,
      "grad_norm": 0.9640690684318542,
      "learning_rate": 0.00012325988530033203,
      "loss": 0.1854,
      "step": 19073
    },
    {
      "epoch": 38.37826961770624,
      "grad_norm": 0.9338301420211792,
      "learning_rate": 0.00012325586075057854,
      "loss": 0.1697,
      "step": 19074
    },
    {
      "epoch": 38.38028169014085,
      "grad_norm": 1.0350475311279297,
      "learning_rate": 0.00012325183620082503,
      "loss": 0.2037,
      "step": 19075
    },
    {
      "epoch": 38.38229376257545,
      "grad_norm": 1.031950831413269,
      "learning_rate": 0.00012324781165107154,
      "loss": 0.1662,
      "step": 19076
    },
    {
      "epoch": 38.38430583501006,
      "grad_norm": 0.9477942585945129,
      "learning_rate": 0.00012324378710131805,
      "loss": 0.1785,
      "step": 19077
    },
    {
      "epoch": 38.38631790744467,
      "grad_norm": 0.9197182059288025,
      "learning_rate": 0.00012323976255156454,
      "loss": 0.1592,
      "step": 19078
    },
    {
      "epoch": 38.38832997987927,
      "grad_norm": 0.9272341132164001,
      "learning_rate": 0.00012323573800181105,
      "loss": 0.1647,
      "step": 19079
    },
    {
      "epoch": 38.39034205231388,
      "grad_norm": 0.9572968482971191,
      "learning_rate": 0.00012323171345205756,
      "loss": 0.1672,
      "step": 19080
    },
    {
      "epoch": 38.39235412474849,
      "grad_norm": 0.9969487190246582,
      "learning_rate": 0.00012322768890230405,
      "loss": 0.188,
      "step": 19081
    },
    {
      "epoch": 38.394366197183096,
      "grad_norm": 0.969860851764679,
      "learning_rate": 0.00012322366435255056,
      "loss": 0.1699,
      "step": 19082
    },
    {
      "epoch": 38.396378269617706,
      "grad_norm": 0.9791078567504883,
      "learning_rate": 0.00012321963980279707,
      "loss": 0.1744,
      "step": 19083
    },
    {
      "epoch": 38.398390342052316,
      "grad_norm": 0.9437174201011658,
      "learning_rate": 0.00012321561525304356,
      "loss": 0.1815,
      "step": 19084
    },
    {
      "epoch": 38.40040241448692,
      "grad_norm": 0.9846151471138,
      "learning_rate": 0.00012321159070329007,
      "loss": 0.1854,
      "step": 19085
    },
    {
      "epoch": 38.40241448692153,
      "grad_norm": 0.965054988861084,
      "learning_rate": 0.00012320756615353656,
      "loss": 0.1739,
      "step": 19086
    },
    {
      "epoch": 38.40442655935614,
      "grad_norm": 0.9266421794891357,
      "learning_rate": 0.0001232035416037831,
      "loss": 0.166,
      "step": 19087
    },
    {
      "epoch": 38.40643863179074,
      "grad_norm": 0.9825114011764526,
      "learning_rate": 0.00012319951705402958,
      "loss": 0.1742,
      "step": 19088
    },
    {
      "epoch": 38.40845070422535,
      "grad_norm": 1.0046173334121704,
      "learning_rate": 0.0001231954925042761,
      "loss": 0.1812,
      "step": 19089
    },
    {
      "epoch": 38.41046277665996,
      "grad_norm": 0.9506409764289856,
      "learning_rate": 0.00012319146795452258,
      "loss": 0.1766,
      "step": 19090
    },
    {
      "epoch": 38.412474849094565,
      "grad_norm": 0.965661346912384,
      "learning_rate": 0.0001231874434047691,
      "loss": 0.1655,
      "step": 19091
    },
    {
      "epoch": 38.414486921529175,
      "grad_norm": 0.9195318222045898,
      "learning_rate": 0.0001231834188550156,
      "loss": 0.1625,
      "step": 19092
    },
    {
      "epoch": 38.416498993963785,
      "grad_norm": 0.9458993077278137,
      "learning_rate": 0.00012317939430526211,
      "loss": 0.1751,
      "step": 19093
    },
    {
      "epoch": 38.41851106639839,
      "grad_norm": 0.9716826677322388,
      "learning_rate": 0.0001231753697555086,
      "loss": 0.1784,
      "step": 19094
    },
    {
      "epoch": 38.420523138833,
      "grad_norm": 0.9831830263137817,
      "learning_rate": 0.0001231713452057551,
      "loss": 0.1797,
      "step": 19095
    },
    {
      "epoch": 38.42253521126761,
      "grad_norm": 1.002962589263916,
      "learning_rate": 0.0001231673206560016,
      "loss": 0.176,
      "step": 19096
    },
    {
      "epoch": 38.42454728370221,
      "grad_norm": 0.9703023433685303,
      "learning_rate": 0.00012316329610624814,
      "loss": 0.1631,
      "step": 19097
    },
    {
      "epoch": 38.42655935613682,
      "grad_norm": 0.9808884859085083,
      "learning_rate": 0.00012315927155649462,
      "loss": 0.185,
      "step": 19098
    },
    {
      "epoch": 38.42857142857143,
      "grad_norm": 0.9554722905158997,
      "learning_rate": 0.00012315524700674113,
      "loss": 0.178,
      "step": 19099
    },
    {
      "epoch": 38.43058350100603,
      "grad_norm": 1.0055603981018066,
      "learning_rate": 0.00012315122245698762,
      "loss": 0.1897,
      "step": 19100
    },
    {
      "epoch": 38.43259557344064,
      "grad_norm": 1.0407112836837769,
      "learning_rate": 0.00012314719790723413,
      "loss": 0.1821,
      "step": 19101
    },
    {
      "epoch": 38.43460764587525,
      "grad_norm": 0.9293406009674072,
      "learning_rate": 0.00012314317335748064,
      "loss": 0.1692,
      "step": 19102
    },
    {
      "epoch": 38.436619718309856,
      "grad_norm": 0.9609190225601196,
      "learning_rate": 0.00012313914880772716,
      "loss": 0.1666,
      "step": 19103
    },
    {
      "epoch": 38.438631790744466,
      "grad_norm": 0.9439002871513367,
      "learning_rate": 0.00012313512425797364,
      "loss": 0.1735,
      "step": 19104
    },
    {
      "epoch": 38.440643863179076,
      "grad_norm": 0.9842615127563477,
      "learning_rate": 0.00012313109970822015,
      "loss": 0.1696,
      "step": 19105
    },
    {
      "epoch": 38.44265593561368,
      "grad_norm": 0.9266458749771118,
      "learning_rate": 0.00012312707515846664,
      "loss": 0.1681,
      "step": 19106
    },
    {
      "epoch": 38.44466800804829,
      "grad_norm": 0.9305209517478943,
      "learning_rate": 0.00012312305060871318,
      "loss": 0.1657,
      "step": 19107
    },
    {
      "epoch": 38.4466800804829,
      "grad_norm": 1.0212368965148926,
      "learning_rate": 0.00012311902605895966,
      "loss": 0.2111,
      "step": 19108
    },
    {
      "epoch": 38.4486921529175,
      "grad_norm": 0.9618115425109863,
      "learning_rate": 0.00012311500150920617,
      "loss": 0.1758,
      "step": 19109
    },
    {
      "epoch": 38.45070422535211,
      "grad_norm": 1.0723768472671509,
      "learning_rate": 0.00012311097695945266,
      "loss": 0.1716,
      "step": 19110
    },
    {
      "epoch": 38.45271629778672,
      "grad_norm": 1.0624083280563354,
      "learning_rate": 0.00012310695240969917,
      "loss": 0.1833,
      "step": 19111
    },
    {
      "epoch": 38.454728370221325,
      "grad_norm": 1.0082556009292603,
      "learning_rate": 0.00012310292785994568,
      "loss": 0.1907,
      "step": 19112
    },
    {
      "epoch": 38.456740442655935,
      "grad_norm": 0.9919114708900452,
      "learning_rate": 0.00012309890331019217,
      "loss": 0.1738,
      "step": 19113
    },
    {
      "epoch": 38.458752515090545,
      "grad_norm": 0.9761251211166382,
      "learning_rate": 0.00012309487876043868,
      "loss": 0.1846,
      "step": 19114
    },
    {
      "epoch": 38.46076458752515,
      "grad_norm": 0.9989450573921204,
      "learning_rate": 0.0001230908542106852,
      "loss": 0.189,
      "step": 19115
    },
    {
      "epoch": 38.46277665995976,
      "grad_norm": 1.0258677005767822,
      "learning_rate": 0.00012308682966093168,
      "loss": 0.2015,
      "step": 19116
    },
    {
      "epoch": 38.46478873239437,
      "grad_norm": 1.0277838706970215,
      "learning_rate": 0.0001230828051111782,
      "loss": 0.1734,
      "step": 19117
    },
    {
      "epoch": 38.46680080482897,
      "grad_norm": 1.0139529705047607,
      "learning_rate": 0.0001230787805614247,
      "loss": 0.1858,
      "step": 19118
    },
    {
      "epoch": 38.46881287726358,
      "grad_norm": 1.0086486339569092,
      "learning_rate": 0.0001230747560116712,
      "loss": 0.1704,
      "step": 19119
    },
    {
      "epoch": 38.47082494969819,
      "grad_norm": 0.9957894682884216,
      "learning_rate": 0.0001230707314619177,
      "loss": 0.1778,
      "step": 19120
    },
    {
      "epoch": 38.47283702213279,
      "grad_norm": 1.0062741041183472,
      "learning_rate": 0.00012306670691216419,
      "loss": 0.1884,
      "step": 19121
    },
    {
      "epoch": 38.4748490945674,
      "grad_norm": 1.0535873174667358,
      "learning_rate": 0.00012306268236241072,
      "loss": 0.1955,
      "step": 19122
    },
    {
      "epoch": 38.47686116700201,
      "grad_norm": 1.0372307300567627,
      "learning_rate": 0.0001230586578126572,
      "loss": 0.1866,
      "step": 19123
    },
    {
      "epoch": 38.478873239436616,
      "grad_norm": 0.9645249247550964,
      "learning_rate": 0.00012305463326290372,
      "loss": 0.1692,
      "step": 19124
    },
    {
      "epoch": 38.480885311871226,
      "grad_norm": 1.062196969985962,
      "learning_rate": 0.0001230506087131502,
      "loss": 0.1865,
      "step": 19125
    },
    {
      "epoch": 38.482897384305836,
      "grad_norm": 1.0057365894317627,
      "learning_rate": 0.00012304658416339672,
      "loss": 0.1996,
      "step": 19126
    },
    {
      "epoch": 38.48490945674044,
      "grad_norm": 1.0427436828613281,
      "learning_rate": 0.00012304255961364323,
      "loss": 0.1724,
      "step": 19127
    },
    {
      "epoch": 38.48692152917505,
      "grad_norm": 0.9561663866043091,
      "learning_rate": 0.00012303853506388974,
      "loss": 0.1831,
      "step": 19128
    },
    {
      "epoch": 38.48893360160966,
      "grad_norm": 1.0567625761032104,
      "learning_rate": 0.00012303451051413623,
      "loss": 0.1902,
      "step": 19129
    },
    {
      "epoch": 38.49094567404426,
      "grad_norm": 0.9739826917648315,
      "learning_rate": 0.00012303048596438274,
      "loss": 0.1906,
      "step": 19130
    },
    {
      "epoch": 38.49295774647887,
      "grad_norm": 0.9540469646453857,
      "learning_rate": 0.00012302646141462923,
      "loss": 0.1689,
      "step": 19131
    },
    {
      "epoch": 38.49496981891348,
      "grad_norm": 0.9597185850143433,
      "learning_rate": 0.00012302243686487577,
      "loss": 0.1884,
      "step": 19132
    },
    {
      "epoch": 38.496981891348085,
      "grad_norm": 0.9904765486717224,
      "learning_rate": 0.00012301841231512225,
      "loss": 0.1842,
      "step": 19133
    },
    {
      "epoch": 38.498993963782695,
      "grad_norm": 1.0018110275268555,
      "learning_rate": 0.00012301438776536876,
      "loss": 0.1941,
      "step": 19134
    },
    {
      "epoch": 38.501006036217305,
      "grad_norm": 0.9393438696861267,
      "learning_rate": 0.00012301036321561525,
      "loss": 0.1967,
      "step": 19135
    },
    {
      "epoch": 38.503018108651915,
      "grad_norm": 1.0294610261917114,
      "learning_rate": 0.00012300633866586176,
      "loss": 0.2016,
      "step": 19136
    },
    {
      "epoch": 38.50503018108652,
      "grad_norm": 1.1137691736221313,
      "learning_rate": 0.00012300231411610827,
      "loss": 0.189,
      "step": 19137
    },
    {
      "epoch": 38.50704225352113,
      "grad_norm": 0.9881224632263184,
      "learning_rate": 0.00012299828956635478,
      "loss": 0.1633,
      "step": 19138
    },
    {
      "epoch": 38.50905432595574,
      "grad_norm": 1.098149061203003,
      "learning_rate": 0.00012299426501660127,
      "loss": 0.2036,
      "step": 19139
    },
    {
      "epoch": 38.51106639839034,
      "grad_norm": 0.9660394787788391,
      "learning_rate": 0.00012299024046684778,
      "loss": 0.1681,
      "step": 19140
    },
    {
      "epoch": 38.51307847082495,
      "grad_norm": 0.9576752185821533,
      "learning_rate": 0.00012298621591709427,
      "loss": 0.1768,
      "step": 19141
    },
    {
      "epoch": 38.51509054325956,
      "grad_norm": 1.103808045387268,
      "learning_rate": 0.0001229821913673408,
      "loss": 0.1703,
      "step": 19142
    },
    {
      "epoch": 38.517102615694164,
      "grad_norm": 1.024101972579956,
      "learning_rate": 0.0001229781668175873,
      "loss": 0.1835,
      "step": 19143
    },
    {
      "epoch": 38.519114688128774,
      "grad_norm": 0.9536723494529724,
      "learning_rate": 0.0001229741422678338,
      "loss": 0.186,
      "step": 19144
    },
    {
      "epoch": 38.521126760563384,
      "grad_norm": 0.9649869203567505,
      "learning_rate": 0.0001229701177180803,
      "loss": 0.1934,
      "step": 19145
    },
    {
      "epoch": 38.52313883299799,
      "grad_norm": 1.0274677276611328,
      "learning_rate": 0.0001229660931683268,
      "loss": 0.1852,
      "step": 19146
    },
    {
      "epoch": 38.5251509054326,
      "grad_norm": 0.9809778928756714,
      "learning_rate": 0.0001229620686185733,
      "loss": 0.182,
      "step": 19147
    },
    {
      "epoch": 38.52716297786721,
      "grad_norm": 1.0435757637023926,
      "learning_rate": 0.0001229580440688198,
      "loss": 0.1843,
      "step": 19148
    },
    {
      "epoch": 38.52917505030181,
      "grad_norm": 1.0912381410598755,
      "learning_rate": 0.0001229540195190663,
      "loss": 0.1993,
      "step": 19149
    },
    {
      "epoch": 38.53118712273642,
      "grad_norm": 0.9458976984024048,
      "learning_rate": 0.00012294999496931282,
      "loss": 0.1776,
      "step": 19150
    },
    {
      "epoch": 38.53319919517103,
      "grad_norm": 1.0387941598892212,
      "learning_rate": 0.0001229459704195593,
      "loss": 0.1906,
      "step": 19151
    },
    {
      "epoch": 38.53521126760563,
      "grad_norm": 1.0509871244430542,
      "learning_rate": 0.00012294194586980582,
      "loss": 0.183,
      "step": 19152
    },
    {
      "epoch": 38.53722334004024,
      "grad_norm": 1.0569062232971191,
      "learning_rate": 0.00012293792132005233,
      "loss": 0.1918,
      "step": 19153
    },
    {
      "epoch": 38.53923541247485,
      "grad_norm": 1.0119407176971436,
      "learning_rate": 0.00012293389677029882,
      "loss": 0.1808,
      "step": 19154
    },
    {
      "epoch": 38.541247484909455,
      "grad_norm": 0.928900420665741,
      "learning_rate": 0.00012292987222054533,
      "loss": 0.1626,
      "step": 19155
    },
    {
      "epoch": 38.543259557344065,
      "grad_norm": 0.9280329346656799,
      "learning_rate": 0.00012292584767079181,
      "loss": 0.182,
      "step": 19156
    },
    {
      "epoch": 38.545271629778675,
      "grad_norm": 0.99632328748703,
      "learning_rate": 0.00012292182312103835,
      "loss": 0.178,
      "step": 19157
    },
    {
      "epoch": 38.54728370221328,
      "grad_norm": 0.9777097702026367,
      "learning_rate": 0.00012291779857128484,
      "loss": 0.187,
      "step": 19158
    },
    {
      "epoch": 38.54929577464789,
      "grad_norm": 1.0228296518325806,
      "learning_rate": 0.00012291377402153135,
      "loss": 0.2011,
      "step": 19159
    },
    {
      "epoch": 38.5513078470825,
      "grad_norm": 1.0296283960342407,
      "learning_rate": 0.00012290974947177784,
      "loss": 0.181,
      "step": 19160
    },
    {
      "epoch": 38.5533199195171,
      "grad_norm": 1.014033317565918,
      "learning_rate": 0.00012290572492202435,
      "loss": 0.1819,
      "step": 19161
    },
    {
      "epoch": 38.55533199195171,
      "grad_norm": 1.0163050889968872,
      "learning_rate": 0.00012290170037227086,
      "loss": 0.213,
      "step": 19162
    },
    {
      "epoch": 38.55734406438632,
      "grad_norm": 0.9785186648368835,
      "learning_rate": 0.00012289767582251737,
      "loss": 0.1806,
      "step": 19163
    },
    {
      "epoch": 38.559356136820924,
      "grad_norm": 0.9493659734725952,
      "learning_rate": 0.00012289365127276386,
      "loss": 0.1704,
      "step": 19164
    },
    {
      "epoch": 38.561368209255534,
      "grad_norm": 1.0355448722839355,
      "learning_rate": 0.00012288962672301037,
      "loss": 0.1844,
      "step": 19165
    },
    {
      "epoch": 38.563380281690144,
      "grad_norm": 1.000898838043213,
      "learning_rate": 0.00012288560217325686,
      "loss": 0.1781,
      "step": 19166
    },
    {
      "epoch": 38.56539235412475,
      "grad_norm": 1.0957496166229248,
      "learning_rate": 0.0001228815776235034,
      "loss": 0.1939,
      "step": 19167
    },
    {
      "epoch": 38.56740442655936,
      "grad_norm": 0.9747825860977173,
      "learning_rate": 0.00012287755307374988,
      "loss": 0.1872,
      "step": 19168
    },
    {
      "epoch": 38.56941649899397,
      "grad_norm": 0.9879132509231567,
      "learning_rate": 0.0001228735285239964,
      "loss": 0.1789,
      "step": 19169
    },
    {
      "epoch": 38.57142857142857,
      "grad_norm": 1.0502716302871704,
      "learning_rate": 0.00012286950397424288,
      "loss": 0.1864,
      "step": 19170
    },
    {
      "epoch": 38.57344064386318,
      "grad_norm": 0.9857876300811768,
      "learning_rate": 0.0001228654794244894,
      "loss": 0.1742,
      "step": 19171
    },
    {
      "epoch": 38.57545271629779,
      "grad_norm": 1.0126659870147705,
      "learning_rate": 0.0001228614548747359,
      "loss": 0.1798,
      "step": 19172
    },
    {
      "epoch": 38.57746478873239,
      "grad_norm": 1.004056453704834,
      "learning_rate": 0.00012285743032498241,
      "loss": 0.1819,
      "step": 19173
    },
    {
      "epoch": 38.579476861167,
      "grad_norm": 0.9384292364120483,
      "learning_rate": 0.0001228534057752289,
      "loss": 0.1784,
      "step": 19174
    },
    {
      "epoch": 38.58148893360161,
      "grad_norm": 0.9857586622238159,
      "learning_rate": 0.0001228493812254754,
      "loss": 0.1808,
      "step": 19175
    },
    {
      "epoch": 38.583501006036215,
      "grad_norm": 1.0340217351913452,
      "learning_rate": 0.0001228453566757219,
      "loss": 0.1916,
      "step": 19176
    },
    {
      "epoch": 38.585513078470825,
      "grad_norm": 0.9825915694236755,
      "learning_rate": 0.00012284133212596844,
      "loss": 0.1717,
      "step": 19177
    },
    {
      "epoch": 38.587525150905435,
      "grad_norm": 1.066651701927185,
      "learning_rate": 0.00012283730757621492,
      "loss": 0.2094,
      "step": 19178
    },
    {
      "epoch": 38.58953722334004,
      "grad_norm": 0.9983235001564026,
      "learning_rate": 0.00012283328302646143,
      "loss": 0.1815,
      "step": 19179
    },
    {
      "epoch": 38.59154929577465,
      "grad_norm": 1.0139975547790527,
      "learning_rate": 0.00012282925847670792,
      "loss": 0.1915,
      "step": 19180
    },
    {
      "epoch": 38.59356136820926,
      "grad_norm": 1.022918462753296,
      "learning_rate": 0.00012282523392695443,
      "loss": 0.1897,
      "step": 19181
    },
    {
      "epoch": 38.59557344064386,
      "grad_norm": 1.009735345840454,
      "learning_rate": 0.00012282120937720094,
      "loss": 0.1871,
      "step": 19182
    },
    {
      "epoch": 38.59758551307847,
      "grad_norm": 1.0284751653671265,
      "learning_rate": 0.00012281718482744743,
      "loss": 0.1975,
      "step": 19183
    },
    {
      "epoch": 38.59959758551308,
      "grad_norm": 0.9900974631309509,
      "learning_rate": 0.00012281316027769394,
      "loss": 0.1766,
      "step": 19184
    },
    {
      "epoch": 38.601609657947684,
      "grad_norm": 0.9739399552345276,
      "learning_rate": 0.00012280913572794043,
      "loss": 0.1963,
      "step": 19185
    },
    {
      "epoch": 38.603621730382294,
      "grad_norm": 1.0349723100662231,
      "learning_rate": 0.00012280511117818694,
      "loss": 0.1931,
      "step": 19186
    },
    {
      "epoch": 38.605633802816904,
      "grad_norm": 1.009948968887329,
      "learning_rate": 0.00012280108662843345,
      "loss": 0.2119,
      "step": 19187
    },
    {
      "epoch": 38.60764587525151,
      "grad_norm": 0.9871455430984497,
      "learning_rate": 0.00012279706207867996,
      "loss": 0.1825,
      "step": 19188
    },
    {
      "epoch": 38.60965794768612,
      "grad_norm": 0.9259877800941467,
      "learning_rate": 0.00012279303752892645,
      "loss": 0.1772,
      "step": 19189
    },
    {
      "epoch": 38.61167002012073,
      "grad_norm": 1.016372799873352,
      "learning_rate": 0.00012278901297917296,
      "loss": 0.1834,
      "step": 19190
    },
    {
      "epoch": 38.61368209255533,
      "grad_norm": 1.0803420543670654,
      "learning_rate": 0.00012278498842941944,
      "loss": 0.1961,
      "step": 19191
    },
    {
      "epoch": 38.61569416498994,
      "grad_norm": 0.9687610268592834,
      "learning_rate": 0.00012278096387966598,
      "loss": 0.1849,
      "step": 19192
    },
    {
      "epoch": 38.61770623742455,
      "grad_norm": 1.037363886833191,
      "learning_rate": 0.00012277693932991247,
      "loss": 0.2028,
      "step": 19193
    },
    {
      "epoch": 38.61971830985915,
      "grad_norm": 0.9846817255020142,
      "learning_rate": 0.00012277291478015898,
      "loss": 0.1915,
      "step": 19194
    },
    {
      "epoch": 38.62173038229376,
      "grad_norm": 1.06743323802948,
      "learning_rate": 0.00012276889023040547,
      "loss": 0.1977,
      "step": 19195
    },
    {
      "epoch": 38.62374245472837,
      "grad_norm": 1.0091105699539185,
      "learning_rate": 0.00012276486568065198,
      "loss": 0.1808,
      "step": 19196
    },
    {
      "epoch": 38.625754527162975,
      "grad_norm": 0.9921925663948059,
      "learning_rate": 0.0001227608411308985,
      "loss": 0.1811,
      "step": 19197
    },
    {
      "epoch": 38.627766599597585,
      "grad_norm": 0.9372546672821045,
      "learning_rate": 0.000122756816581145,
      "loss": 0.1767,
      "step": 19198
    },
    {
      "epoch": 38.629778672032195,
      "grad_norm": 0.9567952752113342,
      "learning_rate": 0.0001227527920313915,
      "loss": 0.1634,
      "step": 19199
    },
    {
      "epoch": 38.6317907444668,
      "grad_norm": 1.0063667297363281,
      "learning_rate": 0.000122748767481638,
      "loss": 0.1753,
      "step": 19200
    },
    {
      "epoch": 38.63380281690141,
      "grad_norm": 0.9996253252029419,
      "learning_rate": 0.00012274474293188449,
      "loss": 0.1748,
      "step": 19201
    },
    {
      "epoch": 38.63581488933602,
      "grad_norm": 0.9776473045349121,
      "learning_rate": 0.00012274071838213102,
      "loss": 0.1865,
      "step": 19202
    },
    {
      "epoch": 38.63782696177062,
      "grad_norm": 1.0971921682357788,
      "learning_rate": 0.0001227366938323775,
      "loss": 0.1993,
      "step": 19203
    },
    {
      "epoch": 38.63983903420523,
      "grad_norm": 1.025038719177246,
      "learning_rate": 0.00012273266928262402,
      "loss": 0.1952,
      "step": 19204
    },
    {
      "epoch": 38.64185110663984,
      "grad_norm": 0.9852489233016968,
      "learning_rate": 0.0001227286447328705,
      "loss": 0.2052,
      "step": 19205
    },
    {
      "epoch": 38.643863179074444,
      "grad_norm": 0.967033326625824,
      "learning_rate": 0.00012272462018311702,
      "loss": 0.199,
      "step": 19206
    },
    {
      "epoch": 38.645875251509054,
      "grad_norm": 1.0216896533966064,
      "learning_rate": 0.00012272059563336353,
      "loss": 0.1855,
      "step": 19207
    },
    {
      "epoch": 38.647887323943664,
      "grad_norm": 0.9833244681358337,
      "learning_rate": 0.00012271657108361004,
      "loss": 0.1833,
      "step": 19208
    },
    {
      "epoch": 38.64989939637827,
      "grad_norm": 1.053321361541748,
      "learning_rate": 0.00012271254653385653,
      "loss": 0.1935,
      "step": 19209
    },
    {
      "epoch": 38.65191146881288,
      "grad_norm": 1.0409139394760132,
      "learning_rate": 0.00012270852198410304,
      "loss": 0.1927,
      "step": 19210
    },
    {
      "epoch": 38.65392354124749,
      "grad_norm": 1.0231857299804688,
      "learning_rate": 0.00012270449743434953,
      "loss": 0.1891,
      "step": 19211
    },
    {
      "epoch": 38.65593561368209,
      "grad_norm": 1.0170413255691528,
      "learning_rate": 0.00012270047288459604,
      "loss": 0.1994,
      "step": 19212
    },
    {
      "epoch": 38.6579476861167,
      "grad_norm": 1.1013134717941284,
      "learning_rate": 0.00012269644833484255,
      "loss": 0.2065,
      "step": 19213
    },
    {
      "epoch": 38.65995975855131,
      "grad_norm": 1.008185863494873,
      "learning_rate": 0.00012269242378508906,
      "loss": 0.1927,
      "step": 19214
    },
    {
      "epoch": 38.66197183098591,
      "grad_norm": 1.0542126893997192,
      "learning_rate": 0.00012268839923533555,
      "loss": 0.1889,
      "step": 19215
    },
    {
      "epoch": 38.66398390342052,
      "grad_norm": 0.9701955318450928,
      "learning_rate": 0.00012268437468558206,
      "loss": 0.1846,
      "step": 19216
    },
    {
      "epoch": 38.66599597585513,
      "grad_norm": 0.9772871732711792,
      "learning_rate": 0.00012268035013582857,
      "loss": 0.1748,
      "step": 19217
    },
    {
      "epoch": 38.668008048289735,
      "grad_norm": 0.9325520396232605,
      "learning_rate": 0.00012267632558607506,
      "loss": 0.1859,
      "step": 19218
    },
    {
      "epoch": 38.670020120724345,
      "grad_norm": 1.0048965215682983,
      "learning_rate": 0.00012267230103632157,
      "loss": 0.196,
      "step": 19219
    },
    {
      "epoch": 38.672032193158955,
      "grad_norm": 1.0225452184677124,
      "learning_rate": 0.00012266827648656805,
      "loss": 0.2041,
      "step": 19220
    },
    {
      "epoch": 38.67404426559356,
      "grad_norm": 1.0084573030471802,
      "learning_rate": 0.00012266425193681457,
      "loss": 0.1931,
      "step": 19221
    },
    {
      "epoch": 38.67605633802817,
      "grad_norm": 1.1021604537963867,
      "learning_rate": 0.00012266022738706108,
      "loss": 0.2084,
      "step": 19222
    },
    {
      "epoch": 38.67806841046278,
      "grad_norm": 1.0340173244476318,
      "learning_rate": 0.0001226562028373076,
      "loss": 0.1974,
      "step": 19223
    },
    {
      "epoch": 38.68008048289738,
      "grad_norm": 1.0400387048721313,
      "learning_rate": 0.00012265217828755408,
      "loss": 0.2095,
      "step": 19224
    },
    {
      "epoch": 38.68209255533199,
      "grad_norm": 1.0103793144226074,
      "learning_rate": 0.0001226481537378006,
      "loss": 0.1896,
      "step": 19225
    },
    {
      "epoch": 38.6841046277666,
      "grad_norm": 1.0503206253051758,
      "learning_rate": 0.00012264412918804707,
      "loss": 0.202,
      "step": 19226
    },
    {
      "epoch": 38.686116700201204,
      "grad_norm": 0.9404228925704956,
      "learning_rate": 0.0001226401046382936,
      "loss": 0.1742,
      "step": 19227
    },
    {
      "epoch": 38.688128772635814,
      "grad_norm": 0.9870151281356812,
      "learning_rate": 0.0001226360800885401,
      "loss": 0.1916,
      "step": 19228
    },
    {
      "epoch": 38.690140845070424,
      "grad_norm": 1.0500906705856323,
      "learning_rate": 0.0001226320555387866,
      "loss": 0.1933,
      "step": 19229
    },
    {
      "epoch": 38.69215291750503,
      "grad_norm": 1.039169192314148,
      "learning_rate": 0.0001226280309890331,
      "loss": 0.1847,
      "step": 19230
    },
    {
      "epoch": 38.69416498993964,
      "grad_norm": 1.0301406383514404,
      "learning_rate": 0.0001226240064392796,
      "loss": 0.1871,
      "step": 19231
    },
    {
      "epoch": 38.69617706237425,
      "grad_norm": 1.0143696069717407,
      "learning_rate": 0.00012261998188952612,
      "loss": 0.1926,
      "step": 19232
    },
    {
      "epoch": 38.69818913480886,
      "grad_norm": 1.1210474967956543,
      "learning_rate": 0.00012261595733977263,
      "loss": 0.2035,
      "step": 19233
    },
    {
      "epoch": 38.70020120724346,
      "grad_norm": 1.0355585813522339,
      "learning_rate": 0.00012261193279001912,
      "loss": 0.1927,
      "step": 19234
    },
    {
      "epoch": 38.70221327967807,
      "grad_norm": 1.0754069089889526,
      "learning_rate": 0.00012260790824026563,
      "loss": 0.1882,
      "step": 19235
    },
    {
      "epoch": 38.70422535211267,
      "grad_norm": 1.0664376020431519,
      "learning_rate": 0.00012260388369051211,
      "loss": 0.2108,
      "step": 19236
    },
    {
      "epoch": 38.70623742454728,
      "grad_norm": 1.157827615737915,
      "learning_rate": 0.00012259985914075865,
      "loss": 0.2069,
      "step": 19237
    },
    {
      "epoch": 38.70824949698189,
      "grad_norm": 0.933107316493988,
      "learning_rate": 0.00012259583459100514,
      "loss": 0.1781,
      "step": 19238
    },
    {
      "epoch": 38.7102615694165,
      "grad_norm": 1.0447300672531128,
      "learning_rate": 0.00012259181004125165,
      "loss": 0.1922,
      "step": 19239
    },
    {
      "epoch": 38.712273641851105,
      "grad_norm": 0.9908482432365417,
      "learning_rate": 0.00012258778549149814,
      "loss": 0.2005,
      "step": 19240
    },
    {
      "epoch": 38.714285714285715,
      "grad_norm": 1.032383918762207,
      "learning_rate": 0.00012258376094174465,
      "loss": 0.2065,
      "step": 19241
    },
    {
      "epoch": 38.716297786720325,
      "grad_norm": 1.0672253370285034,
      "learning_rate": 0.00012257973639199116,
      "loss": 0.1951,
      "step": 19242
    },
    {
      "epoch": 38.71830985915493,
      "grad_norm": 1.0527912378311157,
      "learning_rate": 0.00012257571184223767,
      "loss": 0.2057,
      "step": 19243
    },
    {
      "epoch": 38.72032193158954,
      "grad_norm": 0.9624758362770081,
      "learning_rate": 0.00012257168729248416,
      "loss": 0.1843,
      "step": 19244
    },
    {
      "epoch": 38.72233400402415,
      "grad_norm": 1.0450048446655273,
      "learning_rate": 0.00012256766274273067,
      "loss": 0.2048,
      "step": 19245
    },
    {
      "epoch": 38.72434607645875,
      "grad_norm": 0.9587351083755493,
      "learning_rate": 0.00012256363819297716,
      "loss": 0.1667,
      "step": 19246
    },
    {
      "epoch": 38.72635814889336,
      "grad_norm": 0.9867609739303589,
      "learning_rate": 0.00012255961364322367,
      "loss": 0.174,
      "step": 19247
    },
    {
      "epoch": 38.72837022132797,
      "grad_norm": 0.9856508374214172,
      "learning_rate": 0.00012255558909347018,
      "loss": 0.1998,
      "step": 19248
    },
    {
      "epoch": 38.730382293762574,
      "grad_norm": 0.9794860482215881,
      "learning_rate": 0.0001225515645437167,
      "loss": 0.1828,
      "step": 19249
    },
    {
      "epoch": 38.732394366197184,
      "grad_norm": 1.0415756702423096,
      "learning_rate": 0.00012254753999396318,
      "loss": 0.1776,
      "step": 19250
    },
    {
      "epoch": 38.734406438631794,
      "grad_norm": 0.9679694771766663,
      "learning_rate": 0.0001225435154442097,
      "loss": 0.1785,
      "step": 19251
    },
    {
      "epoch": 38.7364185110664,
      "grad_norm": 1.079707384109497,
      "learning_rate": 0.0001225394908944562,
      "loss": 0.2017,
      "step": 19252
    },
    {
      "epoch": 38.73843058350101,
      "grad_norm": 0.9868770241737366,
      "learning_rate": 0.0001225354663447027,
      "loss": 0.1837,
      "step": 19253
    },
    {
      "epoch": 38.74044265593562,
      "grad_norm": 1.0172276496887207,
      "learning_rate": 0.0001225314417949492,
      "loss": 0.19,
      "step": 19254
    },
    {
      "epoch": 38.74245472837022,
      "grad_norm": 0.9847510457038879,
      "learning_rate": 0.00012252741724519568,
      "loss": 0.1746,
      "step": 19255
    },
    {
      "epoch": 38.74446680080483,
      "grad_norm": 1.0617247819900513,
      "learning_rate": 0.0001225233926954422,
      "loss": 0.221,
      "step": 19256
    },
    {
      "epoch": 38.74647887323944,
      "grad_norm": 1.0576434135437012,
      "learning_rate": 0.0001225193681456887,
      "loss": 0.1943,
      "step": 19257
    },
    {
      "epoch": 38.74849094567404,
      "grad_norm": 1.015159010887146,
      "learning_rate": 0.00012251534359593522,
      "loss": 0.2053,
      "step": 19258
    },
    {
      "epoch": 38.75050301810865,
      "grad_norm": 1.0214093923568726,
      "learning_rate": 0.0001225113190461817,
      "loss": 0.1977,
      "step": 19259
    },
    {
      "epoch": 38.75251509054326,
      "grad_norm": 1.0295809507369995,
      "learning_rate": 0.00012250729449642822,
      "loss": 0.1992,
      "step": 19260
    },
    {
      "epoch": 38.754527162977865,
      "grad_norm": 0.9992568492889404,
      "learning_rate": 0.0001225032699466747,
      "loss": 0.1854,
      "step": 19261
    },
    {
      "epoch": 38.756539235412475,
      "grad_norm": 0.9875772595405579,
      "learning_rate": 0.00012249924539692124,
      "loss": 0.1843,
      "step": 19262
    },
    {
      "epoch": 38.758551307847085,
      "grad_norm": 1.0815820693969727,
      "learning_rate": 0.00012249522084716773,
      "loss": 0.2129,
      "step": 19263
    },
    {
      "epoch": 38.76056338028169,
      "grad_norm": 0.988821268081665,
      "learning_rate": 0.00012249119629741424,
      "loss": 0.1844,
      "step": 19264
    },
    {
      "epoch": 38.7625754527163,
      "grad_norm": 1.027573585510254,
      "learning_rate": 0.00012248717174766072,
      "loss": 0.1906,
      "step": 19265
    },
    {
      "epoch": 38.76458752515091,
      "grad_norm": 1.0070782899856567,
      "learning_rate": 0.00012248314719790724,
      "loss": 0.191,
      "step": 19266
    },
    {
      "epoch": 38.76659959758551,
      "grad_norm": 0.9711970090866089,
      "learning_rate": 0.00012247912264815375,
      "loss": 0.1839,
      "step": 19267
    },
    {
      "epoch": 38.76861167002012,
      "grad_norm": 1.027878761291504,
      "learning_rate": 0.00012247509809840026,
      "loss": 0.1958,
      "step": 19268
    },
    {
      "epoch": 38.77062374245473,
      "grad_norm": 1.023128867149353,
      "learning_rate": 0.00012247107354864675,
      "loss": 0.1931,
      "step": 19269
    },
    {
      "epoch": 38.772635814889334,
      "grad_norm": 0.9848800301551819,
      "learning_rate": 0.00012246704899889326,
      "loss": 0.1784,
      "step": 19270
    },
    {
      "epoch": 38.774647887323944,
      "grad_norm": 1.0618518590927124,
      "learning_rate": 0.00012246302444913974,
      "loss": 0.1943,
      "step": 19271
    },
    {
      "epoch": 38.776659959758554,
      "grad_norm": 1.0269410610198975,
      "learning_rate": 0.00012245899989938628,
      "loss": 0.1896,
      "step": 19272
    },
    {
      "epoch": 38.77867203219316,
      "grad_norm": 1.0196393728256226,
      "learning_rate": 0.00012245497534963277,
      "loss": 0.1994,
      "step": 19273
    },
    {
      "epoch": 38.78068410462777,
      "grad_norm": 1.0674803256988525,
      "learning_rate": 0.00012245095079987928,
      "loss": 0.2051,
      "step": 19274
    },
    {
      "epoch": 38.78269617706238,
      "grad_norm": 1.0575947761535645,
      "learning_rate": 0.00012244692625012577,
      "loss": 0.1988,
      "step": 19275
    },
    {
      "epoch": 38.78470824949698,
      "grad_norm": 1.041919231414795,
      "learning_rate": 0.00012244290170037228,
      "loss": 0.199,
      "step": 19276
    },
    {
      "epoch": 38.78672032193159,
      "grad_norm": 0.9899861812591553,
      "learning_rate": 0.00012243887715061876,
      "loss": 0.1765,
      "step": 19277
    },
    {
      "epoch": 38.7887323943662,
      "grad_norm": 1.033758521080017,
      "learning_rate": 0.0001224348526008653,
      "loss": 0.1826,
      "step": 19278
    },
    {
      "epoch": 38.7907444668008,
      "grad_norm": 1.0287449359893799,
      "learning_rate": 0.0001224308280511118,
      "loss": 0.2011,
      "step": 19279
    },
    {
      "epoch": 38.79275653923541,
      "grad_norm": 1.053161382675171,
      "learning_rate": 0.0001224268035013583,
      "loss": 0.1952,
      "step": 19280
    },
    {
      "epoch": 38.79476861167002,
      "grad_norm": 1.017552375793457,
      "learning_rate": 0.00012242277895160478,
      "loss": 0.1966,
      "step": 19281
    },
    {
      "epoch": 38.796780684104625,
      "grad_norm": 1.0237560272216797,
      "learning_rate": 0.0001224187544018513,
      "loss": 0.1815,
      "step": 19282
    },
    {
      "epoch": 38.798792756539235,
      "grad_norm": 0.950305700302124,
      "learning_rate": 0.0001224147298520978,
      "loss": 0.1858,
      "step": 19283
    },
    {
      "epoch": 38.800804828973845,
      "grad_norm": 0.9989215731620789,
      "learning_rate": 0.00012241070530234432,
      "loss": 0.1962,
      "step": 19284
    },
    {
      "epoch": 38.80281690140845,
      "grad_norm": 0.9526196122169495,
      "learning_rate": 0.0001224066807525908,
      "loss": 0.1755,
      "step": 19285
    },
    {
      "epoch": 38.80482897384306,
      "grad_norm": 1.0152928829193115,
      "learning_rate": 0.00012240265620283732,
      "loss": 0.1987,
      "step": 19286
    },
    {
      "epoch": 38.80684104627767,
      "grad_norm": 0.986103355884552,
      "learning_rate": 0.0001223986316530838,
      "loss": 0.1882,
      "step": 19287
    },
    {
      "epoch": 38.80885311871227,
      "grad_norm": 0.9979425668716431,
      "learning_rate": 0.00012239460710333032,
      "loss": 0.1925,
      "step": 19288
    },
    {
      "epoch": 38.81086519114688,
      "grad_norm": 0.9996045827865601,
      "learning_rate": 0.00012239058255357683,
      "loss": 0.1913,
      "step": 19289
    },
    {
      "epoch": 38.81287726358149,
      "grad_norm": 1.1069588661193848,
      "learning_rate": 0.0001223865580038233,
      "loss": 0.2158,
      "step": 19290
    },
    {
      "epoch": 38.814889336016094,
      "grad_norm": 1.0049821138381958,
      "learning_rate": 0.00012238253345406983,
      "loss": 0.2028,
      "step": 19291
    },
    {
      "epoch": 38.816901408450704,
      "grad_norm": 1.0575133562088013,
      "learning_rate": 0.00012237850890431634,
      "loss": 0.2056,
      "step": 19292
    },
    {
      "epoch": 38.818913480885314,
      "grad_norm": 1.0294902324676514,
      "learning_rate": 0.00012237448435456285,
      "loss": 0.2077,
      "step": 19293
    },
    {
      "epoch": 38.82092555331992,
      "grad_norm": 1.0169181823730469,
      "learning_rate": 0.00012237045980480934,
      "loss": 0.1972,
      "step": 19294
    },
    {
      "epoch": 38.82293762575453,
      "grad_norm": 0.9969621300697327,
      "learning_rate": 0.00012236643525505585,
      "loss": 0.1973,
      "step": 19295
    },
    {
      "epoch": 38.82494969818914,
      "grad_norm": 0.9887537956237793,
      "learning_rate": 0.00012236241070530233,
      "loss": 0.1993,
      "step": 19296
    },
    {
      "epoch": 38.82696177062374,
      "grad_norm": 1.0933938026428223,
      "learning_rate": 0.00012235838615554884,
      "loss": 0.2078,
      "step": 19297
    },
    {
      "epoch": 38.82897384305835,
      "grad_norm": 1.0006108283996582,
      "learning_rate": 0.00012235436160579536,
      "loss": 0.1963,
      "step": 19298
    },
    {
      "epoch": 38.83098591549296,
      "grad_norm": 1.10090970993042,
      "learning_rate": 0.00012235033705604187,
      "loss": 0.1982,
      "step": 19299
    },
    {
      "epoch": 38.83299798792756,
      "grad_norm": 1.0473554134368896,
      "learning_rate": 0.00012234631250628835,
      "loss": 0.2088,
      "step": 19300
    },
    {
      "epoch": 38.83501006036217,
      "grad_norm": 1.023366928100586,
      "learning_rate": 0.00012234228795653487,
      "loss": 0.2093,
      "step": 19301
    },
    {
      "epoch": 38.83702213279678,
      "grad_norm": 0.9718654155731201,
      "learning_rate": 0.00012233826340678135,
      "loss": 0.1998,
      "step": 19302
    },
    {
      "epoch": 38.839034205231385,
      "grad_norm": 0.9713482856750488,
      "learning_rate": 0.0001223342388570279,
      "loss": 0.1812,
      "step": 19303
    },
    {
      "epoch": 38.841046277665995,
      "grad_norm": 1.0099908113479614,
      "learning_rate": 0.00012233021430727438,
      "loss": 0.1874,
      "step": 19304
    },
    {
      "epoch": 38.843058350100605,
      "grad_norm": 1.0515859127044678,
      "learning_rate": 0.0001223261897575209,
      "loss": 0.2026,
      "step": 19305
    },
    {
      "epoch": 38.84507042253521,
      "grad_norm": 1.003205418586731,
      "learning_rate": 0.00012232216520776737,
      "loss": 0.2007,
      "step": 19306
    },
    {
      "epoch": 38.84708249496982,
      "grad_norm": 1.0811868906021118,
      "learning_rate": 0.00012231814065801389,
      "loss": 0.1969,
      "step": 19307
    },
    {
      "epoch": 38.84909456740443,
      "grad_norm": 0.9935696125030518,
      "learning_rate": 0.0001223141161082604,
      "loss": 0.1919,
      "step": 19308
    },
    {
      "epoch": 38.85110663983903,
      "grad_norm": 1.0612568855285645,
      "learning_rate": 0.0001223100915585069,
      "loss": 0.206,
      "step": 19309
    },
    {
      "epoch": 38.85311871227364,
      "grad_norm": 1.0011088848114014,
      "learning_rate": 0.0001223060670087534,
      "loss": 0.2005,
      "step": 19310
    },
    {
      "epoch": 38.85513078470825,
      "grad_norm": 1.0151675939559937,
      "learning_rate": 0.0001223020424589999,
      "loss": 0.2036,
      "step": 19311
    },
    {
      "epoch": 38.857142857142854,
      "grad_norm": 0.9365987777709961,
      "learning_rate": 0.0001222980179092464,
      "loss": 0.1854,
      "step": 19312
    },
    {
      "epoch": 38.859154929577464,
      "grad_norm": 1.0547915697097778,
      "learning_rate": 0.00012229399335949293,
      "loss": 0.1963,
      "step": 19313
    },
    {
      "epoch": 38.861167002012074,
      "grad_norm": 1.0232611894607544,
      "learning_rate": 0.00012228996880973942,
      "loss": 0.2022,
      "step": 19314
    },
    {
      "epoch": 38.86317907444668,
      "grad_norm": 0.9586849212646484,
      "learning_rate": 0.00012228594425998593,
      "loss": 0.1887,
      "step": 19315
    },
    {
      "epoch": 38.86519114688129,
      "grad_norm": 0.9670507907867432,
      "learning_rate": 0.00012228191971023241,
      "loss": 0.193,
      "step": 19316
    },
    {
      "epoch": 38.8672032193159,
      "grad_norm": 1.0620496273040771,
      "learning_rate": 0.00012227789516047893,
      "loss": 0.2194,
      "step": 19317
    },
    {
      "epoch": 38.8692152917505,
      "grad_norm": 1.019644021987915,
      "learning_rate": 0.00012227387061072544,
      "loss": 0.1881,
      "step": 19318
    },
    {
      "epoch": 38.87122736418511,
      "grad_norm": 1.0172309875488281,
      "learning_rate": 0.00012226984606097195,
      "loss": 0.1868,
      "step": 19319
    },
    {
      "epoch": 38.87323943661972,
      "grad_norm": 1.0753719806671143,
      "learning_rate": 0.00012226582151121844,
      "loss": 0.1969,
      "step": 19320
    },
    {
      "epoch": 38.87525150905432,
      "grad_norm": 1.0489470958709717,
      "learning_rate": 0.00012226179696146495,
      "loss": 0.2004,
      "step": 19321
    },
    {
      "epoch": 38.87726358148893,
      "grad_norm": 1.040566325187683,
      "learning_rate": 0.00012225777241171143,
      "loss": 0.2026,
      "step": 19322
    },
    {
      "epoch": 38.87927565392354,
      "grad_norm": 1.050342321395874,
      "learning_rate": 0.00012225374786195795,
      "loss": 0.2068,
      "step": 19323
    },
    {
      "epoch": 38.881287726358146,
      "grad_norm": 1.0081392526626587,
      "learning_rate": 0.00012224972331220446,
      "loss": 0.1966,
      "step": 19324
    },
    {
      "epoch": 38.883299798792756,
      "grad_norm": 0.9744639992713928,
      "learning_rate": 0.00012224569876245094,
      "loss": 0.1736,
      "step": 19325
    },
    {
      "epoch": 38.885311871227366,
      "grad_norm": 0.9772273898124695,
      "learning_rate": 0.00012224167421269746,
      "loss": 0.1889,
      "step": 19326
    },
    {
      "epoch": 38.88732394366197,
      "grad_norm": 0.9927098155021667,
      "learning_rate": 0.00012223764966294394,
      "loss": 0.1945,
      "step": 19327
    },
    {
      "epoch": 38.88933601609658,
      "grad_norm": 1.059045672416687,
      "learning_rate": 0.00012223362511319048,
      "loss": 0.2139,
      "step": 19328
    },
    {
      "epoch": 38.89134808853119,
      "grad_norm": 1.0830447673797607,
      "learning_rate": 0.00012222960056343696,
      "loss": 0.2062,
      "step": 19329
    },
    {
      "epoch": 38.89336016096579,
      "grad_norm": 0.9781304001808167,
      "learning_rate": 0.00012222557601368348,
      "loss": 0.2077,
      "step": 19330
    },
    {
      "epoch": 38.8953722334004,
      "grad_norm": 1.052022933959961,
      "learning_rate": 0.00012222155146392996,
      "loss": 0.2057,
      "step": 19331
    },
    {
      "epoch": 38.89738430583501,
      "grad_norm": 1.0290229320526123,
      "learning_rate": 0.00012221752691417647,
      "loss": 0.1971,
      "step": 19332
    },
    {
      "epoch": 38.899396378269614,
      "grad_norm": 1.1229524612426758,
      "learning_rate": 0.000122213502364423,
      "loss": 0.2001,
      "step": 19333
    },
    {
      "epoch": 38.901408450704224,
      "grad_norm": 0.995908260345459,
      "learning_rate": 0.0001222094778146695,
      "loss": 0.1999,
      "step": 19334
    },
    {
      "epoch": 38.903420523138834,
      "grad_norm": 1.0681047439575195,
      "learning_rate": 0.00012220545326491598,
      "loss": 0.2152,
      "step": 19335
    },
    {
      "epoch": 38.905432595573444,
      "grad_norm": 1.0134634971618652,
      "learning_rate": 0.0001222014287151625,
      "loss": 0.1976,
      "step": 19336
    },
    {
      "epoch": 38.90744466800805,
      "grad_norm": 1.0649333000183105,
      "learning_rate": 0.00012219740416540898,
      "loss": 0.2029,
      "step": 19337
    },
    {
      "epoch": 38.90945674044266,
      "grad_norm": 0.9769490957260132,
      "learning_rate": 0.00012219337961565552,
      "loss": 0.1726,
      "step": 19338
    },
    {
      "epoch": 38.91146881287727,
      "grad_norm": 1.0113378763198853,
      "learning_rate": 0.000122189355065902,
      "loss": 0.1922,
      "step": 19339
    },
    {
      "epoch": 38.91348088531187,
      "grad_norm": 1.0351966619491577,
      "learning_rate": 0.00012218533051614852,
      "loss": 0.199,
      "step": 19340
    },
    {
      "epoch": 38.91549295774648,
      "grad_norm": 1.069370985031128,
      "learning_rate": 0.000122181305966395,
      "loss": 0.2205,
      "step": 19341
    },
    {
      "epoch": 38.91750503018109,
      "grad_norm": 1.0389785766601562,
      "learning_rate": 0.00012217728141664152,
      "loss": 0.2045,
      "step": 19342
    },
    {
      "epoch": 38.91951710261569,
      "grad_norm": 1.0637683868408203,
      "learning_rate": 0.00012217325686688803,
      "loss": 0.1915,
      "step": 19343
    },
    {
      "epoch": 38.9215291750503,
      "grad_norm": 1.0351051092147827,
      "learning_rate": 0.00012216923231713454,
      "loss": 0.2138,
      "step": 19344
    },
    {
      "epoch": 38.92354124748491,
      "grad_norm": 1.0343823432922363,
      "learning_rate": 0.00012216520776738102,
      "loss": 0.1943,
      "step": 19345
    },
    {
      "epoch": 38.925553319919516,
      "grad_norm": 1.0603708028793335,
      "learning_rate": 0.00012216118321762754,
      "loss": 0.1963,
      "step": 19346
    },
    {
      "epoch": 38.927565392354126,
      "grad_norm": 1.0433651208877563,
      "learning_rate": 0.00012215715866787402,
      "loss": 0.1952,
      "step": 19347
    },
    {
      "epoch": 38.929577464788736,
      "grad_norm": 1.0699355602264404,
      "learning_rate": 0.00012215313411812056,
      "loss": 0.2186,
      "step": 19348
    },
    {
      "epoch": 38.93158953722334,
      "grad_norm": 1.0542726516723633,
      "learning_rate": 0.00012214910956836705,
      "loss": 0.2155,
      "step": 19349
    },
    {
      "epoch": 38.93360160965795,
      "grad_norm": 1.1081624031066895,
      "learning_rate": 0.00012214508501861356,
      "loss": 0.212,
      "step": 19350
    },
    {
      "epoch": 38.93561368209256,
      "grad_norm": 1.0400605201721191,
      "learning_rate": 0.00012214106046886004,
      "loss": 0.1911,
      "step": 19351
    },
    {
      "epoch": 38.93762575452716,
      "grad_norm": 1.0577366352081299,
      "learning_rate": 0.00012213703591910656,
      "loss": 0.1888,
      "step": 19352
    },
    {
      "epoch": 38.93963782696177,
      "grad_norm": 1.0322519540786743,
      "learning_rate": 0.00012213301136935307,
      "loss": 0.2047,
      "step": 19353
    },
    {
      "epoch": 38.94164989939638,
      "grad_norm": 0.9977072477340698,
      "learning_rate": 0.00012212898681959955,
      "loss": 0.1936,
      "step": 19354
    },
    {
      "epoch": 38.943661971830984,
      "grad_norm": 1.0362646579742432,
      "learning_rate": 0.00012212496226984607,
      "loss": 0.1929,
      "step": 19355
    },
    {
      "epoch": 38.945674044265594,
      "grad_norm": 1.1458760499954224,
      "learning_rate": 0.00012212093772009258,
      "loss": 0.2173,
      "step": 19356
    },
    {
      "epoch": 38.947686116700204,
      "grad_norm": 1.1088978052139282,
      "learning_rate": 0.00012211691317033906,
      "loss": 0.2128,
      "step": 19357
    },
    {
      "epoch": 38.94969818913481,
      "grad_norm": 1.038494348526001,
      "learning_rate": 0.00012211288862058558,
      "loss": 0.2131,
      "step": 19358
    },
    {
      "epoch": 38.95171026156942,
      "grad_norm": 0.9676355719566345,
      "learning_rate": 0.0001221088640708321,
      "loss": 0.1834,
      "step": 19359
    },
    {
      "epoch": 38.95372233400403,
      "grad_norm": 1.0854368209838867,
      "learning_rate": 0.00012210483952107857,
      "loss": 0.2105,
      "step": 19360
    },
    {
      "epoch": 38.95573440643863,
      "grad_norm": 0.9980760812759399,
      "learning_rate": 0.00012210081497132508,
      "loss": 0.1989,
      "step": 19361
    },
    {
      "epoch": 38.95774647887324,
      "grad_norm": 0.9884381294250488,
      "learning_rate": 0.00012209679042157157,
      "loss": 0.1853,
      "step": 19362
    },
    {
      "epoch": 38.95975855130785,
      "grad_norm": 1.03759765625,
      "learning_rate": 0.0001220927658718181,
      "loss": 0.1921,
      "step": 19363
    },
    {
      "epoch": 38.96177062374245,
      "grad_norm": 1.0863243341445923,
      "learning_rate": 0.0001220887413220646,
      "loss": 0.2244,
      "step": 19364
    },
    {
      "epoch": 38.96378269617706,
      "grad_norm": 1.1726057529449463,
      "learning_rate": 0.0001220847167723111,
      "loss": 0.2103,
      "step": 19365
    },
    {
      "epoch": 38.96579476861167,
      "grad_norm": 1.0363967418670654,
      "learning_rate": 0.0001220806922225576,
      "loss": 0.1925,
      "step": 19366
    },
    {
      "epoch": 38.967806841046276,
      "grad_norm": 1.096561312675476,
      "learning_rate": 0.0001220766676728041,
      "loss": 0.2054,
      "step": 19367
    },
    {
      "epoch": 38.969818913480886,
      "grad_norm": 1.089158535003662,
      "learning_rate": 0.00012207264312305062,
      "loss": 0.2114,
      "step": 19368
    },
    {
      "epoch": 38.971830985915496,
      "grad_norm": 1.0248687267303467,
      "learning_rate": 0.00012206861857329713,
      "loss": 0.2016,
      "step": 19369
    },
    {
      "epoch": 38.9738430583501,
      "grad_norm": 1.0982571840286255,
      "learning_rate": 0.00012206459402354363,
      "loss": 0.2187,
      "step": 19370
    },
    {
      "epoch": 38.97585513078471,
      "grad_norm": 1.1641777753829956,
      "learning_rate": 0.00012206056947379013,
      "loss": 0.2264,
      "step": 19371
    },
    {
      "epoch": 38.97786720321932,
      "grad_norm": 0.9712082147598267,
      "learning_rate": 0.00012205654492403662,
      "loss": 0.1925,
      "step": 19372
    },
    {
      "epoch": 38.97987927565392,
      "grad_norm": 1.0098788738250732,
      "learning_rate": 0.00012205252037428314,
      "loss": 0.2024,
      "step": 19373
    },
    {
      "epoch": 38.98189134808853,
      "grad_norm": 1.0516479015350342,
      "learning_rate": 0.00012204849582452964,
      "loss": 0.2034,
      "step": 19374
    },
    {
      "epoch": 38.98390342052314,
      "grad_norm": 1.0407665967941284,
      "learning_rate": 0.00012204447127477613,
      "loss": 0.2151,
      "step": 19375
    },
    {
      "epoch": 38.985915492957744,
      "grad_norm": 1.0149364471435547,
      "learning_rate": 0.00012204044672502263,
      "loss": 0.2064,
      "step": 19376
    },
    {
      "epoch": 38.987927565392354,
      "grad_norm": 1.0407832860946655,
      "learning_rate": 0.00012203642217526914,
      "loss": 0.2092,
      "step": 19377
    },
    {
      "epoch": 38.989939637826964,
      "grad_norm": 1.013716220855713,
      "learning_rate": 0.00012203239762551566,
      "loss": 0.2037,
      "step": 19378
    },
    {
      "epoch": 38.99195171026157,
      "grad_norm": 1.0211334228515625,
      "learning_rate": 0.00012202837307576216,
      "loss": 0.2024,
      "step": 19379
    },
    {
      "epoch": 38.99396378269618,
      "grad_norm": 1.050673007965088,
      "learning_rate": 0.00012202434852600865,
      "loss": 0.1962,
      "step": 19380
    },
    {
      "epoch": 38.99597585513079,
      "grad_norm": 1.069024920463562,
      "learning_rate": 0.00012202032397625515,
      "loss": 0.1918,
      "step": 19381
    },
    {
      "epoch": 38.99798792756539,
      "grad_norm": 0.9968274831771851,
      "learning_rate": 0.00012201629942650165,
      "loss": 0.211,
      "step": 19382
    },
    {
      "epoch": 39.0,
      "grad_norm": 1.1753536462783813,
      "learning_rate": 0.00012201227487674818,
      "loss": 0.2,
      "step": 19383
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.6120237112045288,
      "eval_runtime": 49.8649,
      "eval_samples_per_second": 19.894,
      "eval_steps_per_second": 2.487,
      "step": 19383
    },
    {
      "epoch": 39.00201207243461,
      "grad_norm": 0.8118710517883301,
      "learning_rate": 0.00012200825032699468,
      "loss": 0.1522,
      "step": 19384
    },
    {
      "epoch": 39.00402414486921,
      "grad_norm": 0.8427455425262451,
      "learning_rate": 0.00012200422577724117,
      "loss": 0.1544,
      "step": 19385
    },
    {
      "epoch": 39.00603621730382,
      "grad_norm": 0.846548855304718,
      "learning_rate": 0.00012200020122748767,
      "loss": 0.1516,
      "step": 19386
    },
    {
      "epoch": 39.00804828973843,
      "grad_norm": 0.8202697038650513,
      "learning_rate": 0.00012199617667773417,
      "loss": 0.1518,
      "step": 19387
    },
    {
      "epoch": 39.010060362173036,
      "grad_norm": 0.8690692782402039,
      "learning_rate": 0.0001219921521279807,
      "loss": 0.1349,
      "step": 19388
    },
    {
      "epoch": 39.012072434607646,
      "grad_norm": 0.8777737617492676,
      "learning_rate": 0.0001219881275782272,
      "loss": 0.1313,
      "step": 19389
    },
    {
      "epoch": 39.014084507042256,
      "grad_norm": 0.9686263203620911,
      "learning_rate": 0.0001219841030284737,
      "loss": 0.1631,
      "step": 19390
    },
    {
      "epoch": 39.01609657947686,
      "grad_norm": 0.9680497050285339,
      "learning_rate": 0.0001219800784787202,
      "loss": 0.1572,
      "step": 19391
    },
    {
      "epoch": 39.01810865191147,
      "grad_norm": 0.8942095041275024,
      "learning_rate": 0.00012197605392896669,
      "loss": 0.1459,
      "step": 19392
    },
    {
      "epoch": 39.02012072434608,
      "grad_norm": 0.8498830199241638,
      "learning_rate": 0.00012197202937921322,
      "loss": 0.1484,
      "step": 19393
    },
    {
      "epoch": 39.02213279678068,
      "grad_norm": 0.9568175077438354,
      "learning_rate": 0.00012196800482945972,
      "loss": 0.1582,
      "step": 19394
    },
    {
      "epoch": 39.02414486921529,
      "grad_norm": 0.8241886496543884,
      "learning_rate": 0.00012196398027970622,
      "loss": 0.148,
      "step": 19395
    },
    {
      "epoch": 39.0261569416499,
      "grad_norm": 0.828545868396759,
      "learning_rate": 0.00012195995572995271,
      "loss": 0.1444,
      "step": 19396
    },
    {
      "epoch": 39.028169014084504,
      "grad_norm": 0.9202772378921509,
      "learning_rate": 0.00012195593118019921,
      "loss": 0.1508,
      "step": 19397
    },
    {
      "epoch": 39.030181086519114,
      "grad_norm": 0.8609819412231445,
      "learning_rate": 0.00012195190663044574,
      "loss": 0.1375,
      "step": 19398
    },
    {
      "epoch": 39.032193158953724,
      "grad_norm": 0.8993902206420898,
      "learning_rate": 0.00012194788208069224,
      "loss": 0.165,
      "step": 19399
    },
    {
      "epoch": 39.03420523138833,
      "grad_norm": 0.8558034300804138,
      "learning_rate": 0.00012194385753093874,
      "loss": 0.1339,
      "step": 19400
    },
    {
      "epoch": 39.03621730382294,
      "grad_norm": 0.9115811586380005,
      "learning_rate": 0.00012193983298118523,
      "loss": 0.1461,
      "step": 19401
    },
    {
      "epoch": 39.03822937625755,
      "grad_norm": 0.9568145275115967,
      "learning_rate": 0.00012193580843143173,
      "loss": 0.1518,
      "step": 19402
    },
    {
      "epoch": 39.04024144869215,
      "grad_norm": 0.9645949602127075,
      "learning_rate": 0.00012193178388167825,
      "loss": 0.1606,
      "step": 19403
    },
    {
      "epoch": 39.04225352112676,
      "grad_norm": 0.9022133946418762,
      "learning_rate": 0.00012192775933192476,
      "loss": 0.1577,
      "step": 19404
    },
    {
      "epoch": 39.04426559356137,
      "grad_norm": 0.887174665927887,
      "learning_rate": 0.00012192373478217126,
      "loss": 0.1619,
      "step": 19405
    },
    {
      "epoch": 39.04627766599597,
      "grad_norm": 0.8518109917640686,
      "learning_rate": 0.00012191971023241775,
      "loss": 0.1454,
      "step": 19406
    },
    {
      "epoch": 39.04828973843058,
      "grad_norm": 0.8392655849456787,
      "learning_rate": 0.00012191568568266425,
      "loss": 0.1307,
      "step": 19407
    },
    {
      "epoch": 39.05030181086519,
      "grad_norm": 0.893443763256073,
      "learning_rate": 0.00012191166113291077,
      "loss": 0.1439,
      "step": 19408
    },
    {
      "epoch": 39.052313883299796,
      "grad_norm": 0.8703325986862183,
      "learning_rate": 0.00012190763658315726,
      "loss": 0.1538,
      "step": 19409
    },
    {
      "epoch": 39.054325955734406,
      "grad_norm": 0.9134863615036011,
      "learning_rate": 0.00012190361203340376,
      "loss": 0.1542,
      "step": 19410
    },
    {
      "epoch": 39.056338028169016,
      "grad_norm": 0.8734728693962097,
      "learning_rate": 0.00012189958748365026,
      "loss": 0.1511,
      "step": 19411
    },
    {
      "epoch": 39.05835010060362,
      "grad_norm": 0.8141060471534729,
      "learning_rate": 0.00012189556293389677,
      "loss": 0.1523,
      "step": 19412
    },
    {
      "epoch": 39.06036217303823,
      "grad_norm": 0.8763968348503113,
      "learning_rate": 0.00012189153838414329,
      "loss": 0.1321,
      "step": 19413
    },
    {
      "epoch": 39.06237424547284,
      "grad_norm": 0.8794079422950745,
      "learning_rate": 0.00012188751383438978,
      "loss": 0.1549,
      "step": 19414
    },
    {
      "epoch": 39.06438631790744,
      "grad_norm": 0.9524703025817871,
      "learning_rate": 0.00012188348928463628,
      "loss": 0.1528,
      "step": 19415
    },
    {
      "epoch": 39.06639839034205,
      "grad_norm": 0.8540436029434204,
      "learning_rate": 0.00012187946473488278,
      "loss": 0.1491,
      "step": 19416
    },
    {
      "epoch": 39.06841046277666,
      "grad_norm": 0.8551546931266785,
      "learning_rate": 0.00012187544018512928,
      "loss": 0.1487,
      "step": 19417
    },
    {
      "epoch": 39.070422535211264,
      "grad_norm": 0.9199510216712952,
      "learning_rate": 0.0001218714156353758,
      "loss": 0.1462,
      "step": 19418
    },
    {
      "epoch": 39.072434607645874,
      "grad_norm": 0.9090136885643005,
      "learning_rate": 0.0001218673910856223,
      "loss": 0.1646,
      "step": 19419
    },
    {
      "epoch": 39.074446680080484,
      "grad_norm": 0.8691949248313904,
      "learning_rate": 0.0001218633665358688,
      "loss": 0.1563,
      "step": 19420
    },
    {
      "epoch": 39.07645875251509,
      "grad_norm": 0.8850576281547546,
      "learning_rate": 0.0001218593419861153,
      "loss": 0.1482,
      "step": 19421
    },
    {
      "epoch": 39.0784708249497,
      "grad_norm": 0.913143515586853,
      "learning_rate": 0.0001218553174363618,
      "loss": 0.1436,
      "step": 19422
    },
    {
      "epoch": 39.08048289738431,
      "grad_norm": 0.8771659731864929,
      "learning_rate": 0.00012185129288660833,
      "loss": 0.127,
      "step": 19423
    },
    {
      "epoch": 39.08249496981891,
      "grad_norm": 0.8397353291511536,
      "learning_rate": 0.00012184726833685483,
      "loss": 0.1482,
      "step": 19424
    },
    {
      "epoch": 39.08450704225352,
      "grad_norm": 0.9176324605941772,
      "learning_rate": 0.00012184324378710132,
      "loss": 0.153,
      "step": 19425
    },
    {
      "epoch": 39.08651911468813,
      "grad_norm": 0.8925610184669495,
      "learning_rate": 0.00012183921923734782,
      "loss": 0.1474,
      "step": 19426
    },
    {
      "epoch": 39.08853118712273,
      "grad_norm": 0.9566416144371033,
      "learning_rate": 0.00012183519468759432,
      "loss": 0.1583,
      "step": 19427
    },
    {
      "epoch": 39.09054325955734,
      "grad_norm": 0.9220134019851685,
      "learning_rate": 0.00012183117013784085,
      "loss": 0.1453,
      "step": 19428
    },
    {
      "epoch": 39.09255533199195,
      "grad_norm": 0.8805237412452698,
      "learning_rate": 0.00012182714558808735,
      "loss": 0.1466,
      "step": 19429
    },
    {
      "epoch": 39.094567404426556,
      "grad_norm": 0.9786991477012634,
      "learning_rate": 0.00012182312103833384,
      "loss": 0.156,
      "step": 19430
    },
    {
      "epoch": 39.096579476861166,
      "grad_norm": 0.9345687627792358,
      "learning_rate": 0.00012181909648858034,
      "loss": 0.1623,
      "step": 19431
    },
    {
      "epoch": 39.098591549295776,
      "grad_norm": 0.848872721195221,
      "learning_rate": 0.00012181507193882684,
      "loss": 0.1448,
      "step": 19432
    },
    {
      "epoch": 39.100603621730386,
      "grad_norm": 0.8939579725265503,
      "learning_rate": 0.00012181104738907337,
      "loss": 0.138,
      "step": 19433
    },
    {
      "epoch": 39.10261569416499,
      "grad_norm": 0.9266124963760376,
      "learning_rate": 0.00012180702283931987,
      "loss": 0.1459,
      "step": 19434
    },
    {
      "epoch": 39.1046277665996,
      "grad_norm": 0.8749862909317017,
      "learning_rate": 0.00012180299828956637,
      "loss": 0.1458,
      "step": 19435
    },
    {
      "epoch": 39.10663983903421,
      "grad_norm": 0.919661283493042,
      "learning_rate": 0.00012179897373981286,
      "loss": 0.1523,
      "step": 19436
    },
    {
      "epoch": 39.10865191146881,
      "grad_norm": 0.9027348160743713,
      "learning_rate": 0.00012179494919005936,
      "loss": 0.1605,
      "step": 19437
    },
    {
      "epoch": 39.11066398390342,
      "grad_norm": 0.8658977150917053,
      "learning_rate": 0.00012179092464030587,
      "loss": 0.1535,
      "step": 19438
    },
    {
      "epoch": 39.11267605633803,
      "grad_norm": 0.8654365539550781,
      "learning_rate": 0.00012178690009055239,
      "loss": 0.1391,
      "step": 19439
    },
    {
      "epoch": 39.114688128772634,
      "grad_norm": 1.0193268060684204,
      "learning_rate": 0.00012178287554079889,
      "loss": 0.157,
      "step": 19440
    },
    {
      "epoch": 39.116700201207244,
      "grad_norm": 0.886028528213501,
      "learning_rate": 0.00012177885099104538,
      "loss": 0.1526,
      "step": 19441
    },
    {
      "epoch": 39.118712273641854,
      "grad_norm": 0.8797205686569214,
      "learning_rate": 0.00012177482644129188,
      "loss": 0.1483,
      "step": 19442
    },
    {
      "epoch": 39.12072434607646,
      "grad_norm": 0.8985263705253601,
      "learning_rate": 0.0001217708018915384,
      "loss": 0.1522,
      "step": 19443
    },
    {
      "epoch": 39.12273641851107,
      "grad_norm": 0.9052159190177917,
      "learning_rate": 0.0001217667773417849,
      "loss": 0.146,
      "step": 19444
    },
    {
      "epoch": 39.12474849094568,
      "grad_norm": 0.9069432020187378,
      "learning_rate": 0.00012176275279203139,
      "loss": 0.1593,
      "step": 19445
    },
    {
      "epoch": 39.12676056338028,
      "grad_norm": 0.9123608469963074,
      "learning_rate": 0.00012175872824227789,
      "loss": 0.1539,
      "step": 19446
    },
    {
      "epoch": 39.12877263581489,
      "grad_norm": 0.9362623691558838,
      "learning_rate": 0.0001217547036925244,
      "loss": 0.1535,
      "step": 19447
    },
    {
      "epoch": 39.1307847082495,
      "grad_norm": 0.9071002006530762,
      "learning_rate": 0.00012175067914277092,
      "loss": 0.1406,
      "step": 19448
    },
    {
      "epoch": 39.1327967806841,
      "grad_norm": 0.8590543270111084,
      "learning_rate": 0.00012174665459301741,
      "loss": 0.1369,
      "step": 19449
    },
    {
      "epoch": 39.13480885311871,
      "grad_norm": 0.9421718716621399,
      "learning_rate": 0.00012174263004326391,
      "loss": 0.157,
      "step": 19450
    },
    {
      "epoch": 39.13682092555332,
      "grad_norm": 0.9954308271408081,
      "learning_rate": 0.00012173860549351041,
      "loss": 0.1887,
      "step": 19451
    },
    {
      "epoch": 39.138832997987926,
      "grad_norm": 0.9569764137268066,
      "learning_rate": 0.00012173458094375691,
      "loss": 0.1554,
      "step": 19452
    },
    {
      "epoch": 39.140845070422536,
      "grad_norm": 0.9183350205421448,
      "learning_rate": 0.00012173055639400344,
      "loss": 0.1495,
      "step": 19453
    },
    {
      "epoch": 39.142857142857146,
      "grad_norm": 0.9131845235824585,
      "learning_rate": 0.00012172653184424993,
      "loss": 0.1619,
      "step": 19454
    },
    {
      "epoch": 39.14486921529175,
      "grad_norm": 0.8670035004615784,
      "learning_rate": 0.00012172250729449643,
      "loss": 0.1459,
      "step": 19455
    },
    {
      "epoch": 39.14688128772636,
      "grad_norm": 0.9110186696052551,
      "learning_rate": 0.00012171848274474293,
      "loss": 0.1548,
      "step": 19456
    },
    {
      "epoch": 39.14889336016097,
      "grad_norm": 0.8976514339447021,
      "learning_rate": 0.00012171445819498943,
      "loss": 0.144,
      "step": 19457
    },
    {
      "epoch": 39.15090543259557,
      "grad_norm": 0.910702645778656,
      "learning_rate": 0.00012171043364523596,
      "loss": 0.1567,
      "step": 19458
    },
    {
      "epoch": 39.15291750503018,
      "grad_norm": 0.9983582496643066,
      "learning_rate": 0.00012170640909548246,
      "loss": 0.178,
      "step": 19459
    },
    {
      "epoch": 39.15492957746479,
      "grad_norm": 0.904176652431488,
      "learning_rate": 0.00012170238454572895,
      "loss": 0.1457,
      "step": 19460
    },
    {
      "epoch": 39.156941649899395,
      "grad_norm": 0.8953350782394409,
      "learning_rate": 0.00012169835999597545,
      "loss": 0.1459,
      "step": 19461
    },
    {
      "epoch": 39.158953722334005,
      "grad_norm": 0.918655514717102,
      "learning_rate": 0.00012169433544622195,
      "loss": 0.1425,
      "step": 19462
    },
    {
      "epoch": 39.160965794768615,
      "grad_norm": 0.9694523215293884,
      "learning_rate": 0.00012169031089646848,
      "loss": 0.1628,
      "step": 19463
    },
    {
      "epoch": 39.16297786720322,
      "grad_norm": 0.9826344847679138,
      "learning_rate": 0.00012168628634671498,
      "loss": 0.165,
      "step": 19464
    },
    {
      "epoch": 39.16498993963783,
      "grad_norm": 0.9412186145782471,
      "learning_rate": 0.00012168226179696147,
      "loss": 0.1561,
      "step": 19465
    },
    {
      "epoch": 39.16700201207244,
      "grad_norm": 0.9472585320472717,
      "learning_rate": 0.00012167823724720797,
      "loss": 0.1622,
      "step": 19466
    },
    {
      "epoch": 39.16901408450704,
      "grad_norm": 0.9542308449745178,
      "learning_rate": 0.00012167421269745447,
      "loss": 0.1706,
      "step": 19467
    },
    {
      "epoch": 39.17102615694165,
      "grad_norm": 0.9404928684234619,
      "learning_rate": 0.000121670188147701,
      "loss": 0.1504,
      "step": 19468
    },
    {
      "epoch": 39.17303822937626,
      "grad_norm": 0.965154230594635,
      "learning_rate": 0.0001216661635979475,
      "loss": 0.1667,
      "step": 19469
    },
    {
      "epoch": 39.17505030181086,
      "grad_norm": 0.9649452567100525,
      "learning_rate": 0.000121662139048194,
      "loss": 0.1654,
      "step": 19470
    },
    {
      "epoch": 39.17706237424547,
      "grad_norm": 0.8810777068138123,
      "learning_rate": 0.0001216581144984405,
      "loss": 0.1509,
      "step": 19471
    },
    {
      "epoch": 39.17907444668008,
      "grad_norm": 0.9417980313301086,
      "learning_rate": 0.00012165408994868699,
      "loss": 0.1451,
      "step": 19472
    },
    {
      "epoch": 39.181086519114686,
      "grad_norm": 0.994723379611969,
      "learning_rate": 0.0001216500653989335,
      "loss": 0.1681,
      "step": 19473
    },
    {
      "epoch": 39.183098591549296,
      "grad_norm": 0.883395254611969,
      "learning_rate": 0.00012164604084918,
      "loss": 0.141,
      "step": 19474
    },
    {
      "epoch": 39.185110663983906,
      "grad_norm": 0.9214369654655457,
      "learning_rate": 0.00012164201629942652,
      "loss": 0.1637,
      "step": 19475
    },
    {
      "epoch": 39.18712273641851,
      "grad_norm": 0.9441767930984497,
      "learning_rate": 0.00012163799174967301,
      "loss": 0.1658,
      "step": 19476
    },
    {
      "epoch": 39.18913480885312,
      "grad_norm": 0.918136477470398,
      "learning_rate": 0.00012163396719991951,
      "loss": 0.1402,
      "step": 19477
    },
    {
      "epoch": 39.19114688128773,
      "grad_norm": 0.9075019955635071,
      "learning_rate": 0.00012162994265016602,
      "loss": 0.1539,
      "step": 19478
    },
    {
      "epoch": 39.19315895372233,
      "grad_norm": 0.9105987548828125,
      "learning_rate": 0.00012162591810041252,
      "loss": 0.1481,
      "step": 19479
    },
    {
      "epoch": 39.19517102615694,
      "grad_norm": 0.9058669805526733,
      "learning_rate": 0.00012162189355065902,
      "loss": 0.1409,
      "step": 19480
    },
    {
      "epoch": 39.19718309859155,
      "grad_norm": 0.9263923764228821,
      "learning_rate": 0.00012161786900090552,
      "loss": 0.1555,
      "step": 19481
    },
    {
      "epoch": 39.199195171026155,
      "grad_norm": 0.913909912109375,
      "learning_rate": 0.00012161384445115202,
      "loss": 0.1574,
      "step": 19482
    },
    {
      "epoch": 39.201207243460765,
      "grad_norm": 0.9357840418815613,
      "learning_rate": 0.00012160981990139855,
      "loss": 0.1594,
      "step": 19483
    },
    {
      "epoch": 39.203219315895375,
      "grad_norm": 0.9206139445304871,
      "learning_rate": 0.00012160579535164504,
      "loss": 0.1463,
      "step": 19484
    },
    {
      "epoch": 39.20523138832998,
      "grad_norm": 0.8825691938400269,
      "learning_rate": 0.00012160177080189154,
      "loss": 0.1464,
      "step": 19485
    },
    {
      "epoch": 39.20724346076459,
      "grad_norm": 0.9707616567611694,
      "learning_rate": 0.00012159774625213804,
      "loss": 0.1683,
      "step": 19486
    },
    {
      "epoch": 39.2092555331992,
      "grad_norm": 0.9547857046127319,
      "learning_rate": 0.00012159372170238454,
      "loss": 0.1552,
      "step": 19487
    },
    {
      "epoch": 39.2112676056338,
      "grad_norm": 0.9949290752410889,
      "learning_rate": 0.00012158969715263107,
      "loss": 0.1512,
      "step": 19488
    },
    {
      "epoch": 39.21327967806841,
      "grad_norm": 0.9653371572494507,
      "learning_rate": 0.00012158567260287756,
      "loss": 0.155,
      "step": 19489
    },
    {
      "epoch": 39.21529175050302,
      "grad_norm": 0.9345272183418274,
      "learning_rate": 0.00012158164805312406,
      "loss": 0.1489,
      "step": 19490
    },
    {
      "epoch": 39.21730382293762,
      "grad_norm": 0.9818063974380493,
      "learning_rate": 0.00012157762350337056,
      "loss": 0.1725,
      "step": 19491
    },
    {
      "epoch": 39.21931589537223,
      "grad_norm": 0.9338552951812744,
      "learning_rate": 0.00012157359895361706,
      "loss": 0.1723,
      "step": 19492
    },
    {
      "epoch": 39.22132796780684,
      "grad_norm": 0.9060410261154175,
      "learning_rate": 0.00012156957440386359,
      "loss": 0.1486,
      "step": 19493
    },
    {
      "epoch": 39.223340040241446,
      "grad_norm": 0.9625350832939148,
      "learning_rate": 0.00012156554985411008,
      "loss": 0.1669,
      "step": 19494
    },
    {
      "epoch": 39.225352112676056,
      "grad_norm": 0.9679021239280701,
      "learning_rate": 0.00012156152530435658,
      "loss": 0.1575,
      "step": 19495
    },
    {
      "epoch": 39.227364185110666,
      "grad_norm": 0.9277054667472839,
      "learning_rate": 0.00012155750075460308,
      "loss": 0.1521,
      "step": 19496
    },
    {
      "epoch": 39.22937625754527,
      "grad_norm": 0.9146451950073242,
      "learning_rate": 0.00012155347620484958,
      "loss": 0.1526,
      "step": 19497
    },
    {
      "epoch": 39.23138832997988,
      "grad_norm": 0.9065735936164856,
      "learning_rate": 0.0001215494516550961,
      "loss": 0.156,
      "step": 19498
    },
    {
      "epoch": 39.23340040241449,
      "grad_norm": 0.9781424403190613,
      "learning_rate": 0.0001215454271053426,
      "loss": 0.17,
      "step": 19499
    },
    {
      "epoch": 39.23541247484909,
      "grad_norm": 0.8979436159133911,
      "learning_rate": 0.0001215414025555891,
      "loss": 0.1551,
      "step": 19500
    },
    {
      "epoch": 39.2374245472837,
      "grad_norm": 1.0289580821990967,
      "learning_rate": 0.0001215373780058356,
      "loss": 0.1704,
      "step": 19501
    },
    {
      "epoch": 39.23943661971831,
      "grad_norm": 0.9301424026489258,
      "learning_rate": 0.0001215333534560821,
      "loss": 0.1471,
      "step": 19502
    },
    {
      "epoch": 39.241448692152915,
      "grad_norm": 1.001579761505127,
      "learning_rate": 0.00012152932890632863,
      "loss": 0.1751,
      "step": 19503
    },
    {
      "epoch": 39.243460764587525,
      "grad_norm": 0.8769002556800842,
      "learning_rate": 0.00012152530435657513,
      "loss": 0.1523,
      "step": 19504
    },
    {
      "epoch": 39.245472837022135,
      "grad_norm": 0.988025426864624,
      "learning_rate": 0.00012152127980682162,
      "loss": 0.1822,
      "step": 19505
    },
    {
      "epoch": 39.24748490945674,
      "grad_norm": 0.9388073682785034,
      "learning_rate": 0.00012151725525706812,
      "loss": 0.1712,
      "step": 19506
    },
    {
      "epoch": 39.24949698189135,
      "grad_norm": 0.9471835494041443,
      "learning_rate": 0.00012151323070731462,
      "loss": 0.164,
      "step": 19507
    },
    {
      "epoch": 39.25150905432596,
      "grad_norm": 0.9755182862281799,
      "learning_rate": 0.00012150920615756113,
      "loss": 0.1746,
      "step": 19508
    },
    {
      "epoch": 39.25352112676056,
      "grad_norm": 0.9563297629356384,
      "learning_rate": 0.00012150518160780763,
      "loss": 0.1591,
      "step": 19509
    },
    {
      "epoch": 39.25553319919517,
      "grad_norm": 0.9054050445556641,
      "learning_rate": 0.00012150115705805414,
      "loss": 0.16,
      "step": 19510
    },
    {
      "epoch": 39.25754527162978,
      "grad_norm": 0.9206031560897827,
      "learning_rate": 0.00012149713250830064,
      "loss": 0.1672,
      "step": 19511
    },
    {
      "epoch": 39.25955734406438,
      "grad_norm": 0.9348293542861938,
      "learning_rate": 0.00012149310795854714,
      "loss": 0.1596,
      "step": 19512
    },
    {
      "epoch": 39.26156941649899,
      "grad_norm": 0.9162054061889648,
      "learning_rate": 0.00012148908340879365,
      "loss": 0.1679,
      "step": 19513
    },
    {
      "epoch": 39.2635814889336,
      "grad_norm": 0.9485262036323547,
      "learning_rate": 0.00012148505885904015,
      "loss": 0.1601,
      "step": 19514
    },
    {
      "epoch": 39.265593561368206,
      "grad_norm": 0.9205038547515869,
      "learning_rate": 0.00012148103430928665,
      "loss": 0.1621,
      "step": 19515
    },
    {
      "epoch": 39.267605633802816,
      "grad_norm": 0.9475769400596619,
      "learning_rate": 0.00012147700975953315,
      "loss": 0.1561,
      "step": 19516
    },
    {
      "epoch": 39.269617706237426,
      "grad_norm": 0.8551253080368042,
      "learning_rate": 0.00012147298520977965,
      "loss": 0.1374,
      "step": 19517
    },
    {
      "epoch": 39.27162977867203,
      "grad_norm": 0.9433031678199768,
      "learning_rate": 0.00012146896066002617,
      "loss": 0.1549,
      "step": 19518
    },
    {
      "epoch": 39.27364185110664,
      "grad_norm": 0.9294114112854004,
      "learning_rate": 0.00012146493611027267,
      "loss": 0.1641,
      "step": 19519
    },
    {
      "epoch": 39.27565392354125,
      "grad_norm": 0.930837869644165,
      "learning_rate": 0.00012146091156051917,
      "loss": 0.1516,
      "step": 19520
    },
    {
      "epoch": 39.27766599597585,
      "grad_norm": 0.9899060726165771,
      "learning_rate": 0.00012145688701076567,
      "loss": 0.1765,
      "step": 19521
    },
    {
      "epoch": 39.27967806841046,
      "grad_norm": 1.0018224716186523,
      "learning_rate": 0.00012145286246101217,
      "loss": 0.1598,
      "step": 19522
    },
    {
      "epoch": 39.28169014084507,
      "grad_norm": 0.8988104462623596,
      "learning_rate": 0.00012144883791125867,
      "loss": 0.1504,
      "step": 19523
    },
    {
      "epoch": 39.283702213279675,
      "grad_norm": 0.9835106134414673,
      "learning_rate": 0.0001214448133615052,
      "loss": 0.1651,
      "step": 19524
    },
    {
      "epoch": 39.285714285714285,
      "grad_norm": 0.924542248249054,
      "learning_rate": 0.00012144078881175169,
      "loss": 0.1641,
      "step": 19525
    },
    {
      "epoch": 39.287726358148895,
      "grad_norm": 0.9543266892433167,
      "learning_rate": 0.00012143676426199819,
      "loss": 0.1555,
      "step": 19526
    },
    {
      "epoch": 39.2897384305835,
      "grad_norm": 1.0061239004135132,
      "learning_rate": 0.00012143273971224469,
      "loss": 0.1717,
      "step": 19527
    },
    {
      "epoch": 39.29175050301811,
      "grad_norm": 1.0094653367996216,
      "learning_rate": 0.00012142871516249119,
      "loss": 0.1769,
      "step": 19528
    },
    {
      "epoch": 39.29376257545272,
      "grad_norm": 0.9781801700592041,
      "learning_rate": 0.00012142469061273771,
      "loss": 0.1642,
      "step": 19529
    },
    {
      "epoch": 39.29577464788732,
      "grad_norm": 0.9591629505157471,
      "learning_rate": 0.00012142066606298421,
      "loss": 0.1705,
      "step": 19530
    },
    {
      "epoch": 39.29778672032193,
      "grad_norm": 0.9491591453552246,
      "learning_rate": 0.00012141664151323071,
      "loss": 0.1807,
      "step": 19531
    },
    {
      "epoch": 39.29979879275654,
      "grad_norm": 0.9676780700683594,
      "learning_rate": 0.00012141261696347721,
      "loss": 0.1623,
      "step": 19532
    },
    {
      "epoch": 39.30181086519114,
      "grad_norm": 1.0144457817077637,
      "learning_rate": 0.00012140859241372371,
      "loss": 0.1678,
      "step": 19533
    },
    {
      "epoch": 39.30382293762575,
      "grad_norm": 0.9534104466438293,
      "learning_rate": 0.00012140456786397023,
      "loss": 0.1603,
      "step": 19534
    },
    {
      "epoch": 39.30583501006036,
      "grad_norm": 0.9337818026542664,
      "learning_rate": 0.00012140054331421673,
      "loss": 0.1474,
      "step": 19535
    },
    {
      "epoch": 39.30784708249497,
      "grad_norm": 1.0186753273010254,
      "learning_rate": 0.00012139651876446323,
      "loss": 0.1544,
      "step": 19536
    },
    {
      "epoch": 39.309859154929576,
      "grad_norm": 0.9755396842956543,
      "learning_rate": 0.00012139249421470973,
      "loss": 0.166,
      "step": 19537
    },
    {
      "epoch": 39.311871227364186,
      "grad_norm": 1.0476480722427368,
      "learning_rate": 0.00012138846966495623,
      "loss": 0.1688,
      "step": 19538
    },
    {
      "epoch": 39.313883299798796,
      "grad_norm": 0.9509550333023071,
      "learning_rate": 0.00012138444511520275,
      "loss": 0.1567,
      "step": 19539
    },
    {
      "epoch": 39.3158953722334,
      "grad_norm": 0.9230031967163086,
      "learning_rate": 0.00012138042056544925,
      "loss": 0.1578,
      "step": 19540
    },
    {
      "epoch": 39.31790744466801,
      "grad_norm": 0.9794882535934448,
      "learning_rate": 0.00012137639601569575,
      "loss": 0.1752,
      "step": 19541
    },
    {
      "epoch": 39.31991951710262,
      "grad_norm": 0.9556299448013306,
      "learning_rate": 0.00012137237146594225,
      "loss": 0.1564,
      "step": 19542
    },
    {
      "epoch": 39.32193158953722,
      "grad_norm": 0.9605109691619873,
      "learning_rate": 0.00012136834691618875,
      "loss": 0.1563,
      "step": 19543
    },
    {
      "epoch": 39.32394366197183,
      "grad_norm": 1.013038992881775,
      "learning_rate": 0.00012136432236643526,
      "loss": 0.1844,
      "step": 19544
    },
    {
      "epoch": 39.32595573440644,
      "grad_norm": 1.008188009262085,
      "learning_rate": 0.00012136029781668177,
      "loss": 0.1593,
      "step": 19545
    },
    {
      "epoch": 39.327967806841045,
      "grad_norm": 1.0017896890640259,
      "learning_rate": 0.00012135627326692827,
      "loss": 0.1823,
      "step": 19546
    },
    {
      "epoch": 39.329979879275655,
      "grad_norm": 0.9401037096977234,
      "learning_rate": 0.00012135224871717477,
      "loss": 0.1623,
      "step": 19547
    },
    {
      "epoch": 39.331991951710265,
      "grad_norm": 0.9755521416664124,
      "learning_rate": 0.00012134822416742127,
      "loss": 0.1625,
      "step": 19548
    },
    {
      "epoch": 39.33400402414487,
      "grad_norm": 1.0443979501724243,
      "learning_rate": 0.00012134419961766778,
      "loss": 0.1769,
      "step": 19549
    },
    {
      "epoch": 39.33601609657948,
      "grad_norm": 0.9717378616333008,
      "learning_rate": 0.00012134017506791428,
      "loss": 0.1691,
      "step": 19550
    },
    {
      "epoch": 39.33802816901409,
      "grad_norm": 0.9897200465202332,
      "learning_rate": 0.00012133615051816078,
      "loss": 0.1824,
      "step": 19551
    },
    {
      "epoch": 39.34004024144869,
      "grad_norm": 0.943225085735321,
      "learning_rate": 0.00012133212596840728,
      "loss": 0.1619,
      "step": 19552
    },
    {
      "epoch": 39.3420523138833,
      "grad_norm": 0.9978740215301514,
      "learning_rate": 0.00012132810141865378,
      "loss": 0.1648,
      "step": 19553
    },
    {
      "epoch": 39.34406438631791,
      "grad_norm": 0.9173649549484253,
      "learning_rate": 0.0001213240768689003,
      "loss": 0.1601,
      "step": 19554
    },
    {
      "epoch": 39.34607645875251,
      "grad_norm": 1.0063751935958862,
      "learning_rate": 0.0001213200523191468,
      "loss": 0.1727,
      "step": 19555
    },
    {
      "epoch": 39.34808853118712,
      "grad_norm": 0.9706952571868896,
      "learning_rate": 0.0001213160277693933,
      "loss": 0.1702,
      "step": 19556
    },
    {
      "epoch": 39.35010060362173,
      "grad_norm": 0.9891864061355591,
      "learning_rate": 0.0001213120032196398,
      "loss": 0.169,
      "step": 19557
    },
    {
      "epoch": 39.352112676056336,
      "grad_norm": 0.998170018196106,
      "learning_rate": 0.0001213079786698863,
      "loss": 0.1628,
      "step": 19558
    },
    {
      "epoch": 39.354124748490946,
      "grad_norm": 1.012930989265442,
      "learning_rate": 0.00012130395412013282,
      "loss": 0.1517,
      "step": 19559
    },
    {
      "epoch": 39.356136820925556,
      "grad_norm": 0.9705312252044678,
      "learning_rate": 0.00012129992957037932,
      "loss": 0.1625,
      "step": 19560
    },
    {
      "epoch": 39.35814889336016,
      "grad_norm": 0.9652106165885925,
      "learning_rate": 0.00012129590502062582,
      "loss": 0.1668,
      "step": 19561
    },
    {
      "epoch": 39.36016096579477,
      "grad_norm": 0.9726161956787109,
      "learning_rate": 0.00012129188047087232,
      "loss": 0.1692,
      "step": 19562
    },
    {
      "epoch": 39.36217303822938,
      "grad_norm": 0.986301839351654,
      "learning_rate": 0.00012128785592111882,
      "loss": 0.1786,
      "step": 19563
    },
    {
      "epoch": 39.36418511066398,
      "grad_norm": 1.0675052404403687,
      "learning_rate": 0.00012128383137136534,
      "loss": 0.1532,
      "step": 19564
    },
    {
      "epoch": 39.36619718309859,
      "grad_norm": 0.9232281446456909,
      "learning_rate": 0.00012127980682161184,
      "loss": 0.1636,
      "step": 19565
    },
    {
      "epoch": 39.3682092555332,
      "grad_norm": 0.9909054636955261,
      "learning_rate": 0.00012127578227185834,
      "loss": 0.1761,
      "step": 19566
    },
    {
      "epoch": 39.370221327967805,
      "grad_norm": 1.036259651184082,
      "learning_rate": 0.00012127175772210484,
      "loss": 0.1737,
      "step": 19567
    },
    {
      "epoch": 39.372233400402415,
      "grad_norm": 0.9955236315727234,
      "learning_rate": 0.00012126773317235134,
      "loss": 0.1709,
      "step": 19568
    },
    {
      "epoch": 39.374245472837025,
      "grad_norm": 1.0360227823257446,
      "learning_rate": 0.00012126370862259786,
      "loss": 0.1705,
      "step": 19569
    },
    {
      "epoch": 39.37625754527163,
      "grad_norm": 1.0140334367752075,
      "learning_rate": 0.00012125968407284436,
      "loss": 0.1674,
      "step": 19570
    },
    {
      "epoch": 39.37826961770624,
      "grad_norm": 0.9810565710067749,
      "learning_rate": 0.00012125565952309086,
      "loss": 0.1678,
      "step": 19571
    },
    {
      "epoch": 39.38028169014085,
      "grad_norm": 1.0682586431503296,
      "learning_rate": 0.00012125163497333736,
      "loss": 0.1774,
      "step": 19572
    },
    {
      "epoch": 39.38229376257545,
      "grad_norm": 0.9572087526321411,
      "learning_rate": 0.00012124761042358386,
      "loss": 0.1606,
      "step": 19573
    },
    {
      "epoch": 39.38430583501006,
      "grad_norm": 1.039707899093628,
      "learning_rate": 0.00012124358587383038,
      "loss": 0.1665,
      "step": 19574
    },
    {
      "epoch": 39.38631790744467,
      "grad_norm": 1.0018789768218994,
      "learning_rate": 0.00012123956132407688,
      "loss": 0.1703,
      "step": 19575
    },
    {
      "epoch": 39.38832997987927,
      "grad_norm": 0.9662685990333557,
      "learning_rate": 0.00012123553677432338,
      "loss": 0.1749,
      "step": 19576
    },
    {
      "epoch": 39.39034205231388,
      "grad_norm": 0.9172078371047974,
      "learning_rate": 0.00012123151222456988,
      "loss": 0.1675,
      "step": 19577
    },
    {
      "epoch": 39.39235412474849,
      "grad_norm": 0.9949496388435364,
      "learning_rate": 0.00012122748767481638,
      "loss": 0.1779,
      "step": 19578
    },
    {
      "epoch": 39.394366197183096,
      "grad_norm": 0.9237890839576721,
      "learning_rate": 0.00012122346312506289,
      "loss": 0.1624,
      "step": 19579
    },
    {
      "epoch": 39.396378269617706,
      "grad_norm": 0.9892812371253967,
      "learning_rate": 0.00012121943857530939,
      "loss": 0.1805,
      "step": 19580
    },
    {
      "epoch": 39.398390342052316,
      "grad_norm": 1.0269731283187866,
      "learning_rate": 0.0001212154140255559,
      "loss": 0.1756,
      "step": 19581
    },
    {
      "epoch": 39.40040241448692,
      "grad_norm": 0.9876618981361389,
      "learning_rate": 0.0001212113894758024,
      "loss": 0.1609,
      "step": 19582
    },
    {
      "epoch": 39.40241448692153,
      "grad_norm": 0.9453877210617065,
      "learning_rate": 0.0001212073649260489,
      "loss": 0.1592,
      "step": 19583
    },
    {
      "epoch": 39.40442655935614,
      "grad_norm": 1.0195797681808472,
      "learning_rate": 0.00012120334037629541,
      "loss": 0.1637,
      "step": 19584
    },
    {
      "epoch": 39.40643863179074,
      "grad_norm": 1.0757540464401245,
      "learning_rate": 0.00012119931582654191,
      "loss": 0.1684,
      "step": 19585
    },
    {
      "epoch": 39.40845070422535,
      "grad_norm": 1.0371408462524414,
      "learning_rate": 0.00012119529127678841,
      "loss": 0.1701,
      "step": 19586
    },
    {
      "epoch": 39.41046277665996,
      "grad_norm": 0.9512736201286316,
      "learning_rate": 0.00012119126672703491,
      "loss": 0.1642,
      "step": 19587
    },
    {
      "epoch": 39.412474849094565,
      "grad_norm": 1.0107064247131348,
      "learning_rate": 0.0001211872421772814,
      "loss": 0.1688,
      "step": 19588
    },
    {
      "epoch": 39.414486921529175,
      "grad_norm": 0.9631367921829224,
      "learning_rate": 0.00012118321762752793,
      "loss": 0.187,
      "step": 19589
    },
    {
      "epoch": 39.416498993963785,
      "grad_norm": 1.0653241872787476,
      "learning_rate": 0.00012117919307777443,
      "loss": 0.1792,
      "step": 19590
    },
    {
      "epoch": 39.41851106639839,
      "grad_norm": 1.0154516696929932,
      "learning_rate": 0.00012117516852802093,
      "loss": 0.1733,
      "step": 19591
    },
    {
      "epoch": 39.420523138833,
      "grad_norm": 0.9519726037979126,
      "learning_rate": 0.00012117114397826743,
      "loss": 0.1705,
      "step": 19592
    },
    {
      "epoch": 39.42253521126761,
      "grad_norm": 0.9380544424057007,
      "learning_rate": 0.00012116711942851393,
      "loss": 0.1601,
      "step": 19593
    },
    {
      "epoch": 39.42454728370221,
      "grad_norm": 0.9921844005584717,
      "learning_rate": 0.00012116309487876045,
      "loss": 0.1767,
      "step": 19594
    },
    {
      "epoch": 39.42655935613682,
      "grad_norm": 1.0410937070846558,
      "learning_rate": 0.00012115907032900695,
      "loss": 0.1835,
      "step": 19595
    },
    {
      "epoch": 39.42857142857143,
      "grad_norm": 0.9903391599655151,
      "learning_rate": 0.00012115504577925345,
      "loss": 0.1656,
      "step": 19596
    },
    {
      "epoch": 39.43058350100603,
      "grad_norm": 0.9967628121376038,
      "learning_rate": 0.00012115102122949995,
      "loss": 0.1754,
      "step": 19597
    },
    {
      "epoch": 39.43259557344064,
      "grad_norm": 1.0031702518463135,
      "learning_rate": 0.00012114699667974645,
      "loss": 0.1697,
      "step": 19598
    },
    {
      "epoch": 39.43460764587525,
      "grad_norm": 1.0248674154281616,
      "learning_rate": 0.00012114297212999297,
      "loss": 0.1834,
      "step": 19599
    },
    {
      "epoch": 39.436619718309856,
      "grad_norm": 1.034812092781067,
      "learning_rate": 0.00012113894758023947,
      "loss": 0.1651,
      "step": 19600
    },
    {
      "epoch": 39.438631790744466,
      "grad_norm": 1.0630688667297363,
      "learning_rate": 0.00012113492303048597,
      "loss": 0.1779,
      "step": 19601
    },
    {
      "epoch": 39.440643863179076,
      "grad_norm": 0.9643510580062866,
      "learning_rate": 0.00012113089848073247,
      "loss": 0.161,
      "step": 19602
    },
    {
      "epoch": 39.44265593561368,
      "grad_norm": 0.9386210441589355,
      "learning_rate": 0.00012112687393097897,
      "loss": 0.1663,
      "step": 19603
    },
    {
      "epoch": 39.44466800804829,
      "grad_norm": 0.9401106834411621,
      "learning_rate": 0.0001211228493812255,
      "loss": 0.174,
      "step": 19604
    },
    {
      "epoch": 39.4466800804829,
      "grad_norm": 1.0858350992202759,
      "learning_rate": 0.00012111882483147199,
      "loss": 0.1796,
      "step": 19605
    },
    {
      "epoch": 39.4486921529175,
      "grad_norm": 1.059051752090454,
      "learning_rate": 0.00012111480028171849,
      "loss": 0.1869,
      "step": 19606
    },
    {
      "epoch": 39.45070422535211,
      "grad_norm": 1.102848768234253,
      "learning_rate": 0.00012111077573196499,
      "loss": 0.1795,
      "step": 19607
    },
    {
      "epoch": 39.45271629778672,
      "grad_norm": 0.9976068735122681,
      "learning_rate": 0.00012110675118221149,
      "loss": 0.173,
      "step": 19608
    },
    {
      "epoch": 39.454728370221325,
      "grad_norm": 1.0068588256835938,
      "learning_rate": 0.00012110272663245801,
      "loss": 0.1878,
      "step": 19609
    },
    {
      "epoch": 39.456740442655935,
      "grad_norm": 1.0230159759521484,
      "learning_rate": 0.00012109870208270451,
      "loss": 0.1706,
      "step": 19610
    },
    {
      "epoch": 39.458752515090545,
      "grad_norm": 0.9894776940345764,
      "learning_rate": 0.00012109467753295101,
      "loss": 0.1966,
      "step": 19611
    },
    {
      "epoch": 39.46076458752515,
      "grad_norm": 1.0260392427444458,
      "learning_rate": 0.00012109065298319751,
      "loss": 0.1837,
      "step": 19612
    },
    {
      "epoch": 39.46277665995976,
      "grad_norm": 1.0000628232955933,
      "learning_rate": 0.00012108662843344401,
      "loss": 0.1725,
      "step": 19613
    },
    {
      "epoch": 39.46478873239437,
      "grad_norm": 0.9769287109375,
      "learning_rate": 0.00012108260388369052,
      "loss": 0.1698,
      "step": 19614
    },
    {
      "epoch": 39.46680080482897,
      "grad_norm": 0.9760200381278992,
      "learning_rate": 0.00012107857933393702,
      "loss": 0.1725,
      "step": 19615
    },
    {
      "epoch": 39.46881287726358,
      "grad_norm": 1.0215528011322021,
      "learning_rate": 0.00012107455478418353,
      "loss": 0.1769,
      "step": 19616
    },
    {
      "epoch": 39.47082494969819,
      "grad_norm": 0.984186589717865,
      "learning_rate": 0.00012107053023443003,
      "loss": 0.1706,
      "step": 19617
    },
    {
      "epoch": 39.47283702213279,
      "grad_norm": 1.0283969640731812,
      "learning_rate": 0.00012106650568467653,
      "loss": 0.1736,
      "step": 19618
    },
    {
      "epoch": 39.4748490945674,
      "grad_norm": 1.0375086069107056,
      "learning_rate": 0.00012106248113492304,
      "loss": 0.1698,
      "step": 19619
    },
    {
      "epoch": 39.47686116700201,
      "grad_norm": 1.022534728050232,
      "learning_rate": 0.00012105845658516954,
      "loss": 0.1817,
      "step": 19620
    },
    {
      "epoch": 39.478873239436616,
      "grad_norm": 1.0448369979858398,
      "learning_rate": 0.00012105443203541604,
      "loss": 0.1899,
      "step": 19621
    },
    {
      "epoch": 39.480885311871226,
      "grad_norm": 1.0035645961761475,
      "learning_rate": 0.00012105040748566254,
      "loss": 0.1824,
      "step": 19622
    },
    {
      "epoch": 39.482897384305836,
      "grad_norm": 1.0544732809066772,
      "learning_rate": 0.00012104638293590904,
      "loss": 0.1805,
      "step": 19623
    },
    {
      "epoch": 39.48490945674044,
      "grad_norm": 1.0048905611038208,
      "learning_rate": 0.00012104235838615556,
      "loss": 0.1624,
      "step": 19624
    },
    {
      "epoch": 39.48692152917505,
      "grad_norm": 0.9996845126152039,
      "learning_rate": 0.00012103833383640206,
      "loss": 0.1653,
      "step": 19625
    },
    {
      "epoch": 39.48893360160966,
      "grad_norm": 0.9866231083869934,
      "learning_rate": 0.00012103430928664856,
      "loss": 0.178,
      "step": 19626
    },
    {
      "epoch": 39.49094567404426,
      "grad_norm": 0.9726578593254089,
      "learning_rate": 0.00012103028473689506,
      "loss": 0.1648,
      "step": 19627
    },
    {
      "epoch": 39.49295774647887,
      "grad_norm": 0.9661504030227661,
      "learning_rate": 0.00012102626018714156,
      "loss": 0.1627,
      "step": 19628
    },
    {
      "epoch": 39.49496981891348,
      "grad_norm": 1.0553075075149536,
      "learning_rate": 0.00012102223563738808,
      "loss": 0.1816,
      "step": 19629
    },
    {
      "epoch": 39.496981891348085,
      "grad_norm": 0.972437858581543,
      "learning_rate": 0.00012101821108763458,
      "loss": 0.1538,
      "step": 19630
    },
    {
      "epoch": 39.498993963782695,
      "grad_norm": 0.9938730597496033,
      "learning_rate": 0.00012101418653788108,
      "loss": 0.1688,
      "step": 19631
    },
    {
      "epoch": 39.501006036217305,
      "grad_norm": 0.9887431859970093,
      "learning_rate": 0.00012101016198812758,
      "loss": 0.1617,
      "step": 19632
    },
    {
      "epoch": 39.503018108651915,
      "grad_norm": 0.9740713834762573,
      "learning_rate": 0.00012100613743837408,
      "loss": 0.157,
      "step": 19633
    },
    {
      "epoch": 39.50503018108652,
      "grad_norm": 1.032894492149353,
      "learning_rate": 0.0001210021128886206,
      "loss": 0.1794,
      "step": 19634
    },
    {
      "epoch": 39.50704225352113,
      "grad_norm": 1.0174336433410645,
      "learning_rate": 0.0001209980883388671,
      "loss": 0.1771,
      "step": 19635
    },
    {
      "epoch": 39.50905432595574,
      "grad_norm": 0.9708703756332397,
      "learning_rate": 0.0001209940637891136,
      "loss": 0.1683,
      "step": 19636
    },
    {
      "epoch": 39.51106639839034,
      "grad_norm": 0.994692325592041,
      "learning_rate": 0.0001209900392393601,
      "loss": 0.1648,
      "step": 19637
    },
    {
      "epoch": 39.51307847082495,
      "grad_norm": 1.0505657196044922,
      "learning_rate": 0.0001209860146896066,
      "loss": 0.1863,
      "step": 19638
    },
    {
      "epoch": 39.51509054325956,
      "grad_norm": 1.0409562587738037,
      "learning_rate": 0.00012098199013985312,
      "loss": 0.1744,
      "step": 19639
    },
    {
      "epoch": 39.517102615694164,
      "grad_norm": 1.023972988128662,
      "learning_rate": 0.00012097796559009962,
      "loss": 0.1827,
      "step": 19640
    },
    {
      "epoch": 39.519114688128774,
      "grad_norm": 0.9891141653060913,
      "learning_rate": 0.00012097394104034612,
      "loss": 0.1777,
      "step": 19641
    },
    {
      "epoch": 39.521126760563384,
      "grad_norm": 1.0510694980621338,
      "learning_rate": 0.00012096991649059262,
      "loss": 0.1871,
      "step": 19642
    },
    {
      "epoch": 39.52313883299799,
      "grad_norm": 0.9937376379966736,
      "learning_rate": 0.00012096589194083912,
      "loss": 0.1796,
      "step": 19643
    },
    {
      "epoch": 39.5251509054326,
      "grad_norm": 0.985919713973999,
      "learning_rate": 0.00012096186739108564,
      "loss": 0.1746,
      "step": 19644
    },
    {
      "epoch": 39.52716297786721,
      "grad_norm": 1.0141018629074097,
      "learning_rate": 0.00012095784284133214,
      "loss": 0.179,
      "step": 19645
    },
    {
      "epoch": 39.52917505030181,
      "grad_norm": 0.9938560128211975,
      "learning_rate": 0.00012095381829157864,
      "loss": 0.1837,
      "step": 19646
    },
    {
      "epoch": 39.53118712273642,
      "grad_norm": 1.0575101375579834,
      "learning_rate": 0.00012094979374182514,
      "loss": 0.1888,
      "step": 19647
    },
    {
      "epoch": 39.53319919517103,
      "grad_norm": 1.0848802328109741,
      "learning_rate": 0.00012094576919207164,
      "loss": 0.1843,
      "step": 19648
    },
    {
      "epoch": 39.53521126760563,
      "grad_norm": 1.049222707748413,
      "learning_rate": 0.00012094174464231815,
      "loss": 0.1752,
      "step": 19649
    },
    {
      "epoch": 39.53722334004024,
      "grad_norm": 1.0116157531738281,
      "learning_rate": 0.00012093772009256465,
      "loss": 0.1835,
      "step": 19650
    },
    {
      "epoch": 39.53923541247485,
      "grad_norm": 1.070820689201355,
      "learning_rate": 0.00012093369554281115,
      "loss": 0.1733,
      "step": 19651
    },
    {
      "epoch": 39.541247484909455,
      "grad_norm": 1.0629963874816895,
      "learning_rate": 0.00012092967099305766,
      "loss": 0.1915,
      "step": 19652
    },
    {
      "epoch": 39.543259557344065,
      "grad_norm": 1.0254192352294922,
      "learning_rate": 0.00012092564644330416,
      "loss": 0.1793,
      "step": 19653
    },
    {
      "epoch": 39.545271629778675,
      "grad_norm": 1.0655897855758667,
      "learning_rate": 0.00012092162189355067,
      "loss": 0.1759,
      "step": 19654
    },
    {
      "epoch": 39.54728370221328,
      "grad_norm": 0.9811663627624512,
      "learning_rate": 0.00012091759734379717,
      "loss": 0.1694,
      "step": 19655
    },
    {
      "epoch": 39.54929577464789,
      "grad_norm": 1.0171982049942017,
      "learning_rate": 0.00012091357279404367,
      "loss": 0.1861,
      "step": 19656
    },
    {
      "epoch": 39.5513078470825,
      "grad_norm": 0.9997124075889587,
      "learning_rate": 0.00012090954824429017,
      "loss": 0.1755,
      "step": 19657
    },
    {
      "epoch": 39.5533199195171,
      "grad_norm": 0.9995587468147278,
      "learning_rate": 0.00012090552369453667,
      "loss": 0.1728,
      "step": 19658
    },
    {
      "epoch": 39.55533199195171,
      "grad_norm": 1.0216214656829834,
      "learning_rate": 0.00012090149914478319,
      "loss": 0.1847,
      "step": 19659
    },
    {
      "epoch": 39.55734406438632,
      "grad_norm": 0.993923008441925,
      "learning_rate": 0.00012089747459502969,
      "loss": 0.1609,
      "step": 19660
    },
    {
      "epoch": 39.559356136820924,
      "grad_norm": 1.0030157566070557,
      "learning_rate": 0.00012089345004527619,
      "loss": 0.1893,
      "step": 19661
    },
    {
      "epoch": 39.561368209255534,
      "grad_norm": 1.0863169431686401,
      "learning_rate": 0.00012088942549552269,
      "loss": 0.1904,
      "step": 19662
    },
    {
      "epoch": 39.563380281690144,
      "grad_norm": 0.9866394400596619,
      "learning_rate": 0.00012088540094576919,
      "loss": 0.185,
      "step": 19663
    },
    {
      "epoch": 39.56539235412475,
      "grad_norm": 0.974724292755127,
      "learning_rate": 0.00012088137639601571,
      "loss": 0.1625,
      "step": 19664
    },
    {
      "epoch": 39.56740442655936,
      "grad_norm": 1.0146543979644775,
      "learning_rate": 0.00012087735184626221,
      "loss": 0.1887,
      "step": 19665
    },
    {
      "epoch": 39.56941649899397,
      "grad_norm": 0.9535077214241028,
      "learning_rate": 0.00012087332729650871,
      "loss": 0.1613,
      "step": 19666
    },
    {
      "epoch": 39.57142857142857,
      "grad_norm": 1.0382620096206665,
      "learning_rate": 0.00012086930274675521,
      "loss": 0.1799,
      "step": 19667
    },
    {
      "epoch": 39.57344064386318,
      "grad_norm": 0.9702085256576538,
      "learning_rate": 0.0001208652781970017,
      "loss": 0.1672,
      "step": 19668
    },
    {
      "epoch": 39.57545271629779,
      "grad_norm": 0.9930781126022339,
      "learning_rate": 0.00012086125364724823,
      "loss": 0.1862,
      "step": 19669
    },
    {
      "epoch": 39.57746478873239,
      "grad_norm": 0.9694193005561829,
      "learning_rate": 0.00012085722909749473,
      "loss": 0.1933,
      "step": 19670
    },
    {
      "epoch": 39.579476861167,
      "grad_norm": 1.0439814329147339,
      "learning_rate": 0.00012085320454774123,
      "loss": 0.1778,
      "step": 19671
    },
    {
      "epoch": 39.58148893360161,
      "grad_norm": 1.0517544746398926,
      "learning_rate": 0.00012084917999798773,
      "loss": 0.1918,
      "step": 19672
    },
    {
      "epoch": 39.583501006036215,
      "grad_norm": 1.0340327024459839,
      "learning_rate": 0.00012084515544823423,
      "loss": 0.1902,
      "step": 19673
    },
    {
      "epoch": 39.585513078470825,
      "grad_norm": 1.0296733379364014,
      "learning_rate": 0.00012084113089848075,
      "loss": 0.1758,
      "step": 19674
    },
    {
      "epoch": 39.587525150905435,
      "grad_norm": 0.9960857033729553,
      "learning_rate": 0.00012083710634872725,
      "loss": 0.1675,
      "step": 19675
    },
    {
      "epoch": 39.58953722334004,
      "grad_norm": 1.029461145401001,
      "learning_rate": 0.00012083308179897375,
      "loss": 0.1691,
      "step": 19676
    },
    {
      "epoch": 39.59154929577465,
      "grad_norm": 1.036048412322998,
      "learning_rate": 0.00012082905724922025,
      "loss": 0.1838,
      "step": 19677
    },
    {
      "epoch": 39.59356136820926,
      "grad_norm": 0.977064847946167,
      "learning_rate": 0.00012082503269946675,
      "loss": 0.1876,
      "step": 19678
    },
    {
      "epoch": 39.59557344064386,
      "grad_norm": 1.085003137588501,
      "learning_rate": 0.00012082100814971327,
      "loss": 0.1893,
      "step": 19679
    },
    {
      "epoch": 39.59758551307847,
      "grad_norm": 1.060836911201477,
      "learning_rate": 0.00012081698359995977,
      "loss": 0.1827,
      "step": 19680
    },
    {
      "epoch": 39.59959758551308,
      "grad_norm": 1.0013352632522583,
      "learning_rate": 0.00012081295905020627,
      "loss": 0.1782,
      "step": 19681
    },
    {
      "epoch": 39.601609657947684,
      "grad_norm": 0.9723350405693054,
      "learning_rate": 0.00012080893450045277,
      "loss": 0.1739,
      "step": 19682
    },
    {
      "epoch": 39.603621730382294,
      "grad_norm": 0.9966927170753479,
      "learning_rate": 0.00012080490995069927,
      "loss": 0.1817,
      "step": 19683
    },
    {
      "epoch": 39.605633802816904,
      "grad_norm": 1.0048495531082153,
      "learning_rate": 0.00012080088540094578,
      "loss": 0.1867,
      "step": 19684
    },
    {
      "epoch": 39.60764587525151,
      "grad_norm": 1.0423682928085327,
      "learning_rate": 0.00012079686085119228,
      "loss": 0.1787,
      "step": 19685
    },
    {
      "epoch": 39.60965794768612,
      "grad_norm": 1.00099515914917,
      "learning_rate": 0.00012079283630143878,
      "loss": 0.1729,
      "step": 19686
    },
    {
      "epoch": 39.61167002012073,
      "grad_norm": 1.057068943977356,
      "learning_rate": 0.00012078881175168529,
      "loss": 0.1932,
      "step": 19687
    },
    {
      "epoch": 39.61368209255533,
      "grad_norm": 1.0253465175628662,
      "learning_rate": 0.00012078478720193179,
      "loss": 0.1665,
      "step": 19688
    },
    {
      "epoch": 39.61569416498994,
      "grad_norm": 1.022559404373169,
      "learning_rate": 0.0001207807626521783,
      "loss": 0.1862,
      "step": 19689
    },
    {
      "epoch": 39.61770623742455,
      "grad_norm": 1.0230311155319214,
      "learning_rate": 0.0001207767381024248,
      "loss": 0.1852,
      "step": 19690
    },
    {
      "epoch": 39.61971830985915,
      "grad_norm": 0.9394669532775879,
      "learning_rate": 0.0001207727135526713,
      "loss": 0.1671,
      "step": 19691
    },
    {
      "epoch": 39.62173038229376,
      "grad_norm": 1.0747653245925903,
      "learning_rate": 0.0001207686890029178,
      "loss": 0.197,
      "step": 19692
    },
    {
      "epoch": 39.62374245472837,
      "grad_norm": 1.0235178470611572,
      "learning_rate": 0.0001207646644531643,
      "loss": 0.1921,
      "step": 19693
    },
    {
      "epoch": 39.625754527162975,
      "grad_norm": 0.9325572848320007,
      "learning_rate": 0.00012076063990341082,
      "loss": 0.1662,
      "step": 19694
    },
    {
      "epoch": 39.627766599597585,
      "grad_norm": 0.9986615777015686,
      "learning_rate": 0.00012075661535365732,
      "loss": 0.1684,
      "step": 19695
    },
    {
      "epoch": 39.629778672032195,
      "grad_norm": 0.9901912212371826,
      "learning_rate": 0.00012075259080390382,
      "loss": 0.1943,
      "step": 19696
    },
    {
      "epoch": 39.6317907444668,
      "grad_norm": 0.9886651635169983,
      "learning_rate": 0.00012074856625415032,
      "loss": 0.1806,
      "step": 19697
    },
    {
      "epoch": 39.63380281690141,
      "grad_norm": 1.0738633871078491,
      "learning_rate": 0.00012074454170439682,
      "loss": 0.1871,
      "step": 19698
    },
    {
      "epoch": 39.63581488933602,
      "grad_norm": 1.0707730054855347,
      "learning_rate": 0.00012074051715464334,
      "loss": 0.1786,
      "step": 19699
    },
    {
      "epoch": 39.63782696177062,
      "grad_norm": 1.0324862003326416,
      "learning_rate": 0.00012073649260488984,
      "loss": 0.1868,
      "step": 19700
    },
    {
      "epoch": 39.63983903420523,
      "grad_norm": 1.115942358970642,
      "learning_rate": 0.00012073246805513634,
      "loss": 0.1894,
      "step": 19701
    },
    {
      "epoch": 39.64185110663984,
      "grad_norm": 1.0773084163665771,
      "learning_rate": 0.00012072844350538284,
      "loss": 0.1886,
      "step": 19702
    },
    {
      "epoch": 39.643863179074444,
      "grad_norm": 1.0063401460647583,
      "learning_rate": 0.00012072441895562934,
      "loss": 0.1834,
      "step": 19703
    },
    {
      "epoch": 39.645875251509054,
      "grad_norm": 1.0737128257751465,
      "learning_rate": 0.00012072039440587586,
      "loss": 0.1897,
      "step": 19704
    },
    {
      "epoch": 39.647887323943664,
      "grad_norm": 1.091958999633789,
      "learning_rate": 0.00012071636985612236,
      "loss": 0.1895,
      "step": 19705
    },
    {
      "epoch": 39.64989939637827,
      "grad_norm": 1.0131205320358276,
      "learning_rate": 0.00012071234530636886,
      "loss": 0.18,
      "step": 19706
    },
    {
      "epoch": 39.65191146881288,
      "grad_norm": 1.030496597290039,
      "learning_rate": 0.00012070832075661536,
      "loss": 0.1919,
      "step": 19707
    },
    {
      "epoch": 39.65392354124749,
      "grad_norm": 1.007602572441101,
      "learning_rate": 0.00012070429620686186,
      "loss": 0.1677,
      "step": 19708
    },
    {
      "epoch": 39.65593561368209,
      "grad_norm": 0.9897007942199707,
      "learning_rate": 0.00012070027165710838,
      "loss": 0.1869,
      "step": 19709
    },
    {
      "epoch": 39.6579476861167,
      "grad_norm": 1.0840882062911987,
      "learning_rate": 0.00012069624710735488,
      "loss": 0.1787,
      "step": 19710
    },
    {
      "epoch": 39.65995975855131,
      "grad_norm": 0.9514938592910767,
      "learning_rate": 0.00012069222255760138,
      "loss": 0.1737,
      "step": 19711
    },
    {
      "epoch": 39.66197183098591,
      "grad_norm": 1.0225796699523926,
      "learning_rate": 0.00012068819800784788,
      "loss": 0.1602,
      "step": 19712
    },
    {
      "epoch": 39.66398390342052,
      "grad_norm": 1.042175054550171,
      "learning_rate": 0.00012068417345809438,
      "loss": 0.2008,
      "step": 19713
    },
    {
      "epoch": 39.66599597585513,
      "grad_norm": 1.01460862159729,
      "learning_rate": 0.0001206801489083409,
      "loss": 0.1876,
      "step": 19714
    },
    {
      "epoch": 39.668008048289735,
      "grad_norm": 1.0673699378967285,
      "learning_rate": 0.0001206761243585874,
      "loss": 0.1815,
      "step": 19715
    },
    {
      "epoch": 39.670020120724345,
      "grad_norm": 1.0079345703125,
      "learning_rate": 0.0001206720998088339,
      "loss": 0.1686,
      "step": 19716
    },
    {
      "epoch": 39.672032193158955,
      "grad_norm": 1.011159062385559,
      "learning_rate": 0.0001206680752590804,
      "loss": 0.171,
      "step": 19717
    },
    {
      "epoch": 39.67404426559356,
      "grad_norm": 0.9913216829299927,
      "learning_rate": 0.0001206640507093269,
      "loss": 0.1818,
      "step": 19718
    },
    {
      "epoch": 39.67605633802817,
      "grad_norm": 1.0661544799804688,
      "learning_rate": 0.00012066002615957341,
      "loss": 0.1922,
      "step": 19719
    },
    {
      "epoch": 39.67806841046278,
      "grad_norm": 0.9887371063232422,
      "learning_rate": 0.00012065600160981991,
      "loss": 0.1953,
      "step": 19720
    },
    {
      "epoch": 39.68008048289738,
      "grad_norm": 1.0111207962036133,
      "learning_rate": 0.0001206519770600664,
      "loss": 0.191,
      "step": 19721
    },
    {
      "epoch": 39.68209255533199,
      "grad_norm": 0.9537275433540344,
      "learning_rate": 0.0001206479525103129,
      "loss": 0.1764,
      "step": 19722
    },
    {
      "epoch": 39.6841046277666,
      "grad_norm": 1.0159754753112793,
      "learning_rate": 0.00012064392796055942,
      "loss": 0.1948,
      "step": 19723
    },
    {
      "epoch": 39.686116700201204,
      "grad_norm": 1.0733577013015747,
      "learning_rate": 0.00012063990341080593,
      "loss": 0.1856,
      "step": 19724
    },
    {
      "epoch": 39.688128772635814,
      "grad_norm": 0.9811882376670837,
      "learning_rate": 0.00012063587886105243,
      "loss": 0.1735,
      "step": 19725
    },
    {
      "epoch": 39.690140845070424,
      "grad_norm": 1.0768418312072754,
      "learning_rate": 0.00012063185431129893,
      "loss": 0.1796,
      "step": 19726
    },
    {
      "epoch": 39.69215291750503,
      "grad_norm": 0.9793239235877991,
      "learning_rate": 0.00012062782976154543,
      "loss": 0.1626,
      "step": 19727
    },
    {
      "epoch": 39.69416498993964,
      "grad_norm": 1.0177249908447266,
      "learning_rate": 0.00012062380521179192,
      "loss": 0.1863,
      "step": 19728
    },
    {
      "epoch": 39.69617706237425,
      "grad_norm": 1.03033447265625,
      "learning_rate": 0.00012061978066203845,
      "loss": 0.1744,
      "step": 19729
    },
    {
      "epoch": 39.69818913480886,
      "grad_norm": 0.9915809631347656,
      "learning_rate": 0.00012061575611228495,
      "loss": 0.1702,
      "step": 19730
    },
    {
      "epoch": 39.70020120724346,
      "grad_norm": 1.053026556968689,
      "learning_rate": 0.00012061173156253145,
      "loss": 0.1796,
      "step": 19731
    },
    {
      "epoch": 39.70221327967807,
      "grad_norm": 0.9872527718544006,
      "learning_rate": 0.00012060770701277795,
      "loss": 0.1786,
      "step": 19732
    },
    {
      "epoch": 39.70422535211267,
      "grad_norm": 1.0476998090744019,
      "learning_rate": 0.00012060368246302444,
      "loss": 0.1899,
      "step": 19733
    },
    {
      "epoch": 39.70623742454728,
      "grad_norm": 1.0092995166778564,
      "learning_rate": 0.00012059965791327097,
      "loss": 0.1781,
      "step": 19734
    },
    {
      "epoch": 39.70824949698189,
      "grad_norm": 1.0259504318237305,
      "learning_rate": 0.00012059563336351747,
      "loss": 0.1954,
      "step": 19735
    },
    {
      "epoch": 39.7102615694165,
      "grad_norm": 1.0502429008483887,
      "learning_rate": 0.00012059160881376397,
      "loss": 0.1948,
      "step": 19736
    },
    {
      "epoch": 39.712273641851105,
      "grad_norm": 1.0289217233657837,
      "learning_rate": 0.00012058758426401047,
      "loss": 0.1894,
      "step": 19737
    },
    {
      "epoch": 39.714285714285715,
      "grad_norm": 0.9686374068260193,
      "learning_rate": 0.00012058355971425696,
      "loss": 0.1684,
      "step": 19738
    },
    {
      "epoch": 39.716297786720325,
      "grad_norm": 1.0388342142105103,
      "learning_rate": 0.00012057953516450349,
      "loss": 0.195,
      "step": 19739
    },
    {
      "epoch": 39.71830985915493,
      "grad_norm": 1.0164716243743896,
      "learning_rate": 0.00012057551061474999,
      "loss": 0.1662,
      "step": 19740
    },
    {
      "epoch": 39.72032193158954,
      "grad_norm": 1.0449917316436768,
      "learning_rate": 0.00012057148606499649,
      "loss": 0.1876,
      "step": 19741
    },
    {
      "epoch": 39.72233400402415,
      "grad_norm": 1.0380687713623047,
      "learning_rate": 0.00012056746151524299,
      "loss": 0.193,
      "step": 19742
    },
    {
      "epoch": 39.72434607645875,
      "grad_norm": 1.0479971170425415,
      "learning_rate": 0.00012056343696548949,
      "loss": 0.2022,
      "step": 19743
    },
    {
      "epoch": 39.72635814889336,
      "grad_norm": 1.015509843826294,
      "learning_rate": 0.00012055941241573601,
      "loss": 0.1776,
      "step": 19744
    },
    {
      "epoch": 39.72837022132797,
      "grad_norm": 1.009077787399292,
      "learning_rate": 0.00012055538786598251,
      "loss": 0.1901,
      "step": 19745
    },
    {
      "epoch": 39.730382293762574,
      "grad_norm": 1.0317161083221436,
      "learning_rate": 0.00012055136331622901,
      "loss": 0.1907,
      "step": 19746
    },
    {
      "epoch": 39.732394366197184,
      "grad_norm": 1.0123831033706665,
      "learning_rate": 0.00012054733876647551,
      "loss": 0.1934,
      "step": 19747
    },
    {
      "epoch": 39.734406438631794,
      "grad_norm": 1.0232155323028564,
      "learning_rate": 0.000120543314216722,
      "loss": 0.1954,
      "step": 19748
    },
    {
      "epoch": 39.7364185110664,
      "grad_norm": 1.1093655824661255,
      "learning_rate": 0.00012053928966696852,
      "loss": 0.1839,
      "step": 19749
    },
    {
      "epoch": 39.73843058350101,
      "grad_norm": 1.113794207572937,
      "learning_rate": 0.00012053526511721503,
      "loss": 0.191,
      "step": 19750
    },
    {
      "epoch": 39.74044265593562,
      "grad_norm": 1.1735191345214844,
      "learning_rate": 0.00012053124056746153,
      "loss": 0.1935,
      "step": 19751
    },
    {
      "epoch": 39.74245472837022,
      "grad_norm": 1.0669279098510742,
      "learning_rate": 0.00012052721601770803,
      "loss": 0.1834,
      "step": 19752
    },
    {
      "epoch": 39.74446680080483,
      "grad_norm": 1.015960693359375,
      "learning_rate": 0.00012052319146795453,
      "loss": 0.181,
      "step": 19753
    },
    {
      "epoch": 39.74647887323944,
      "grad_norm": 1.0415457487106323,
      "learning_rate": 0.00012051916691820104,
      "loss": 0.1822,
      "step": 19754
    },
    {
      "epoch": 39.74849094567404,
      "grad_norm": 1.0265730619430542,
      "learning_rate": 0.00012051514236844754,
      "loss": 0.1995,
      "step": 19755
    },
    {
      "epoch": 39.75050301810865,
      "grad_norm": 1.0267142057418823,
      "learning_rate": 0.00012051111781869404,
      "loss": 0.1861,
      "step": 19756
    },
    {
      "epoch": 39.75251509054326,
      "grad_norm": 1.0722180604934692,
      "learning_rate": 0.00012050709326894053,
      "loss": 0.1927,
      "step": 19757
    },
    {
      "epoch": 39.754527162977865,
      "grad_norm": 1.0602327585220337,
      "learning_rate": 0.00012050306871918705,
      "loss": 0.1967,
      "step": 19758
    },
    {
      "epoch": 39.756539235412475,
      "grad_norm": 1.034653663635254,
      "learning_rate": 0.00012049904416943356,
      "loss": 0.1855,
      "step": 19759
    },
    {
      "epoch": 39.758551307847085,
      "grad_norm": 0.9909058213233948,
      "learning_rate": 0.00012049501961968006,
      "loss": 0.1677,
      "step": 19760
    },
    {
      "epoch": 39.76056338028169,
      "grad_norm": 1.048439621925354,
      "learning_rate": 0.00012049099506992656,
      "loss": 0.1881,
      "step": 19761
    },
    {
      "epoch": 39.7625754527163,
      "grad_norm": 1.0172079801559448,
      "learning_rate": 0.00012048697052017305,
      "loss": 0.1777,
      "step": 19762
    },
    {
      "epoch": 39.76458752515091,
      "grad_norm": 1.0035974979400635,
      "learning_rate": 0.00012048294597041955,
      "loss": 0.1935,
      "step": 19763
    },
    {
      "epoch": 39.76659959758551,
      "grad_norm": 1.0061911344528198,
      "learning_rate": 0.00012047892142066605,
      "loss": 0.1916,
      "step": 19764
    },
    {
      "epoch": 39.76861167002012,
      "grad_norm": 1.0264085531234741,
      "learning_rate": 0.00012047489687091258,
      "loss": 0.1874,
      "step": 19765
    },
    {
      "epoch": 39.77062374245473,
      "grad_norm": 1.0191733837127686,
      "learning_rate": 0.00012047087232115908,
      "loss": 0.1887,
      "step": 19766
    },
    {
      "epoch": 39.772635814889334,
      "grad_norm": 1.026059627532959,
      "learning_rate": 0.00012046684777140558,
      "loss": 0.2014,
      "step": 19767
    },
    {
      "epoch": 39.774647887323944,
      "grad_norm": 1.0074175596237183,
      "learning_rate": 0.00012046282322165207,
      "loss": 0.1803,
      "step": 19768
    },
    {
      "epoch": 39.776659959758554,
      "grad_norm": 0.9755232930183411,
      "learning_rate": 0.00012045879867189857,
      "loss": 0.1845,
      "step": 19769
    },
    {
      "epoch": 39.77867203219316,
      "grad_norm": 1.0233912467956543,
      "learning_rate": 0.0001204547741221451,
      "loss": 0.1887,
      "step": 19770
    },
    {
      "epoch": 39.78068410462777,
      "grad_norm": 1.039753794670105,
      "learning_rate": 0.0001204507495723916,
      "loss": 0.202,
      "step": 19771
    },
    {
      "epoch": 39.78269617706238,
      "grad_norm": 1.0847651958465576,
      "learning_rate": 0.0001204467250226381,
      "loss": 0.173,
      "step": 19772
    },
    {
      "epoch": 39.78470824949698,
      "grad_norm": 1.0961918830871582,
      "learning_rate": 0.0001204427004728846,
      "loss": 0.2073,
      "step": 19773
    },
    {
      "epoch": 39.78672032193159,
      "grad_norm": 1.0338026285171509,
      "learning_rate": 0.00012043867592313109,
      "loss": 0.1991,
      "step": 19774
    },
    {
      "epoch": 39.7887323943662,
      "grad_norm": 1.1063565015792847,
      "learning_rate": 0.00012043465137337762,
      "loss": 0.1831,
      "step": 19775
    },
    {
      "epoch": 39.7907444668008,
      "grad_norm": 1.0418548583984375,
      "learning_rate": 0.00012043062682362412,
      "loss": 0.1997,
      "step": 19776
    },
    {
      "epoch": 39.79275653923541,
      "grad_norm": 1.021560788154602,
      "learning_rate": 0.00012042660227387062,
      "loss": 0.1825,
      "step": 19777
    },
    {
      "epoch": 39.79476861167002,
      "grad_norm": 1.0033684968948364,
      "learning_rate": 0.00012042257772411711,
      "loss": 0.1741,
      "step": 19778
    },
    {
      "epoch": 39.796780684104625,
      "grad_norm": 1.112131118774414,
      "learning_rate": 0.00012041855317436361,
      "loss": 0.2038,
      "step": 19779
    },
    {
      "epoch": 39.798792756539235,
      "grad_norm": 1.015499472618103,
      "learning_rate": 0.00012041452862461014,
      "loss": 0.1894,
      "step": 19780
    },
    {
      "epoch": 39.800804828973845,
      "grad_norm": 0.9921025633811951,
      "learning_rate": 0.00012041050407485664,
      "loss": 0.1863,
      "step": 19781
    },
    {
      "epoch": 39.80281690140845,
      "grad_norm": 1.0652698278427124,
      "learning_rate": 0.00012040647952510314,
      "loss": 0.1997,
      "step": 19782
    },
    {
      "epoch": 39.80482897384306,
      "grad_norm": 1.0453301668167114,
      "learning_rate": 0.00012040245497534964,
      "loss": 0.1934,
      "step": 19783
    },
    {
      "epoch": 39.80684104627767,
      "grad_norm": 1.0982156991958618,
      "learning_rate": 0.00012039843042559613,
      "loss": 0.2003,
      "step": 19784
    },
    {
      "epoch": 39.80885311871227,
      "grad_norm": 1.0653362274169922,
      "learning_rate": 0.00012039440587584266,
      "loss": 0.1929,
      "step": 19785
    },
    {
      "epoch": 39.81086519114688,
      "grad_norm": 1.0363482236862183,
      "learning_rate": 0.00012039038132608916,
      "loss": 0.1886,
      "step": 19786
    },
    {
      "epoch": 39.81287726358149,
      "grad_norm": 1.0414464473724365,
      "learning_rate": 0.00012038635677633566,
      "loss": 0.182,
      "step": 19787
    },
    {
      "epoch": 39.814889336016094,
      "grad_norm": 1.0854334831237793,
      "learning_rate": 0.00012038233222658216,
      "loss": 0.1854,
      "step": 19788
    },
    {
      "epoch": 39.816901408450704,
      "grad_norm": 1.0153069496154785,
      "learning_rate": 0.00012037830767682865,
      "loss": 0.1888,
      "step": 19789
    },
    {
      "epoch": 39.818913480885314,
      "grad_norm": 1.047028660774231,
      "learning_rate": 0.00012037428312707517,
      "loss": 0.184,
      "step": 19790
    },
    {
      "epoch": 39.82092555331992,
      "grad_norm": 1.0655266046524048,
      "learning_rate": 0.00012037025857732167,
      "loss": 0.1915,
      "step": 19791
    },
    {
      "epoch": 39.82293762575453,
      "grad_norm": 1.0551307201385498,
      "learning_rate": 0.00012036623402756816,
      "loss": 0.21,
      "step": 19792
    },
    {
      "epoch": 39.82494969818914,
      "grad_norm": 1.0670255422592163,
      "learning_rate": 0.00012036220947781466,
      "loss": 0.1839,
      "step": 19793
    },
    {
      "epoch": 39.82696177062374,
      "grad_norm": 1.0482254028320312,
      "learning_rate": 0.00012035818492806117,
      "loss": 0.1808,
      "step": 19794
    },
    {
      "epoch": 39.82897384305835,
      "grad_norm": 1.0880999565124512,
      "learning_rate": 0.00012035416037830769,
      "loss": 0.1933,
      "step": 19795
    },
    {
      "epoch": 39.83098591549296,
      "grad_norm": 1.082807183265686,
      "learning_rate": 0.00012035013582855419,
      "loss": 0.2134,
      "step": 19796
    },
    {
      "epoch": 39.83299798792756,
      "grad_norm": 1.0786092281341553,
      "learning_rate": 0.00012034611127880068,
      "loss": 0.2104,
      "step": 19797
    },
    {
      "epoch": 39.83501006036217,
      "grad_norm": 1.0707964897155762,
      "learning_rate": 0.00012034208672904718,
      "loss": 0.1928,
      "step": 19798
    },
    {
      "epoch": 39.83702213279678,
      "grad_norm": 0.9679277539253235,
      "learning_rate": 0.00012033806217929368,
      "loss": 0.1734,
      "step": 19799
    },
    {
      "epoch": 39.839034205231385,
      "grad_norm": 1.0564570426940918,
      "learning_rate": 0.00012033403762954021,
      "loss": 0.1829,
      "step": 19800
    },
    {
      "epoch": 39.841046277665995,
      "grad_norm": 0.9726573824882507,
      "learning_rate": 0.0001203300130797867,
      "loss": 0.1865,
      "step": 19801
    },
    {
      "epoch": 39.843058350100605,
      "grad_norm": 0.9860323667526245,
      "learning_rate": 0.0001203259885300332,
      "loss": 0.1856,
      "step": 19802
    },
    {
      "epoch": 39.84507042253521,
      "grad_norm": 1.0835368633270264,
      "learning_rate": 0.0001203219639802797,
      "loss": 0.1786,
      "step": 19803
    },
    {
      "epoch": 39.84708249496982,
      "grad_norm": 1.0610859394073486,
      "learning_rate": 0.0001203179394305262,
      "loss": 0.2062,
      "step": 19804
    },
    {
      "epoch": 39.84909456740443,
      "grad_norm": 0.9933993220329285,
      "learning_rate": 0.00012031391488077273,
      "loss": 0.1796,
      "step": 19805
    },
    {
      "epoch": 39.85110663983903,
      "grad_norm": 1.1171244382858276,
      "learning_rate": 0.00012030989033101923,
      "loss": 0.1927,
      "step": 19806
    },
    {
      "epoch": 39.85311871227364,
      "grad_norm": 1.0382335186004639,
      "learning_rate": 0.00012030586578126573,
      "loss": 0.1928,
      "step": 19807
    },
    {
      "epoch": 39.85513078470825,
      "grad_norm": 1.10710871219635,
      "learning_rate": 0.00012030184123151222,
      "loss": 0.2036,
      "step": 19808
    },
    {
      "epoch": 39.857142857142854,
      "grad_norm": 1.1225571632385254,
      "learning_rate": 0.00012029781668175872,
      "loss": 0.1885,
      "step": 19809
    },
    {
      "epoch": 39.859154929577464,
      "grad_norm": 1.0577324628829956,
      "learning_rate": 0.00012029379213200525,
      "loss": 0.1828,
      "step": 19810
    },
    {
      "epoch": 39.861167002012074,
      "grad_norm": 1.0173676013946533,
      "learning_rate": 0.00012028976758225175,
      "loss": 0.163,
      "step": 19811
    },
    {
      "epoch": 39.86317907444668,
      "grad_norm": 1.0290117263793945,
      "learning_rate": 0.00012028574303249825,
      "loss": 0.1866,
      "step": 19812
    },
    {
      "epoch": 39.86519114688129,
      "grad_norm": 1.0379904508590698,
      "learning_rate": 0.00012028171848274474,
      "loss": 0.2048,
      "step": 19813
    },
    {
      "epoch": 39.8672032193159,
      "grad_norm": 1.0117961168289185,
      "learning_rate": 0.00012027769393299124,
      "loss": 0.1796,
      "step": 19814
    },
    {
      "epoch": 39.8692152917505,
      "grad_norm": 0.9837470650672913,
      "learning_rate": 0.00012027366938323777,
      "loss": 0.1815,
      "step": 19815
    },
    {
      "epoch": 39.87122736418511,
      "grad_norm": 1.035159707069397,
      "learning_rate": 0.00012026964483348427,
      "loss": 0.2082,
      "step": 19816
    },
    {
      "epoch": 39.87323943661972,
      "grad_norm": 1.0281808376312256,
      "learning_rate": 0.00012026562028373077,
      "loss": 0.1845,
      "step": 19817
    },
    {
      "epoch": 39.87525150905432,
      "grad_norm": 1.034240961074829,
      "learning_rate": 0.00012026159573397726,
      "loss": 0.1828,
      "step": 19818
    },
    {
      "epoch": 39.87726358148893,
      "grad_norm": 1.0425759553909302,
      "learning_rate": 0.00012025757118422376,
      "loss": 0.2101,
      "step": 19819
    },
    {
      "epoch": 39.87927565392354,
      "grad_norm": 1.0563973188400269,
      "learning_rate": 0.00012025354663447028,
      "loss": 0.185,
      "step": 19820
    },
    {
      "epoch": 39.881287726358146,
      "grad_norm": 1.018044352531433,
      "learning_rate": 0.00012024952208471679,
      "loss": 0.1774,
      "step": 19821
    },
    {
      "epoch": 39.883299798792756,
      "grad_norm": 1.040756106376648,
      "learning_rate": 0.00012024549753496329,
      "loss": 0.1847,
      "step": 19822
    },
    {
      "epoch": 39.885311871227366,
      "grad_norm": 1.0610170364379883,
      "learning_rate": 0.00012024147298520979,
      "loss": 0.1854,
      "step": 19823
    },
    {
      "epoch": 39.88732394366197,
      "grad_norm": 1.0822232961654663,
      "learning_rate": 0.00012023744843545628,
      "loss": 0.209,
      "step": 19824
    },
    {
      "epoch": 39.88933601609658,
      "grad_norm": 1.066864252090454,
      "learning_rate": 0.0001202334238857028,
      "loss": 0.2022,
      "step": 19825
    },
    {
      "epoch": 39.89134808853119,
      "grad_norm": 1.0677123069763184,
      "learning_rate": 0.0001202293993359493,
      "loss": 0.2005,
      "step": 19826
    },
    {
      "epoch": 39.89336016096579,
      "grad_norm": 1.058034896850586,
      "learning_rate": 0.0001202253747861958,
      "loss": 0.1968,
      "step": 19827
    },
    {
      "epoch": 39.8953722334004,
      "grad_norm": 1.097038984298706,
      "learning_rate": 0.00012022135023644229,
      "loss": 0.1989,
      "step": 19828
    },
    {
      "epoch": 39.89738430583501,
      "grad_norm": 1.0429048538208008,
      "learning_rate": 0.0001202173256866888,
      "loss": 0.1937,
      "step": 19829
    },
    {
      "epoch": 39.899396378269614,
      "grad_norm": 1.0902485847473145,
      "learning_rate": 0.00012021330113693532,
      "loss": 0.1902,
      "step": 19830
    },
    {
      "epoch": 39.901408450704224,
      "grad_norm": 1.0558711290359497,
      "learning_rate": 0.00012020927658718181,
      "loss": 0.1869,
      "step": 19831
    },
    {
      "epoch": 39.903420523138834,
      "grad_norm": 1.1160048246383667,
      "learning_rate": 0.00012020525203742831,
      "loss": 0.2027,
      "step": 19832
    },
    {
      "epoch": 39.905432595573444,
      "grad_norm": 1.0087473392486572,
      "learning_rate": 0.00012020122748767481,
      "loss": 0.1971,
      "step": 19833
    },
    {
      "epoch": 39.90744466800805,
      "grad_norm": 1.0101127624511719,
      "learning_rate": 0.00012019720293792131,
      "loss": 0.1852,
      "step": 19834
    },
    {
      "epoch": 39.90945674044266,
      "grad_norm": 1.0671786069869995,
      "learning_rate": 0.00012019317838816784,
      "loss": 0.1916,
      "step": 19835
    },
    {
      "epoch": 39.91146881287727,
      "grad_norm": 1.0224013328552246,
      "learning_rate": 0.00012018915383841434,
      "loss": 0.1785,
      "step": 19836
    },
    {
      "epoch": 39.91348088531187,
      "grad_norm": 1.0837273597717285,
      "learning_rate": 0.00012018512928866083,
      "loss": 0.2074,
      "step": 19837
    },
    {
      "epoch": 39.91549295774648,
      "grad_norm": 1.0002188682556152,
      "learning_rate": 0.00012018110473890733,
      "loss": 0.1861,
      "step": 19838
    },
    {
      "epoch": 39.91750503018109,
      "grad_norm": 1.0603152513504028,
      "learning_rate": 0.00012017708018915383,
      "loss": 0.1915,
      "step": 19839
    },
    {
      "epoch": 39.91951710261569,
      "grad_norm": 1.0706771612167358,
      "learning_rate": 0.00012017305563940036,
      "loss": 0.1975,
      "step": 19840
    },
    {
      "epoch": 39.9215291750503,
      "grad_norm": 1.0738285779953003,
      "learning_rate": 0.00012016903108964686,
      "loss": 0.2033,
      "step": 19841
    },
    {
      "epoch": 39.92354124748491,
      "grad_norm": 1.0403802394866943,
      "learning_rate": 0.00012016500653989335,
      "loss": 0.1937,
      "step": 19842
    },
    {
      "epoch": 39.925553319919516,
      "grad_norm": 1.080259919166565,
      "learning_rate": 0.00012016098199013985,
      "loss": 0.1948,
      "step": 19843
    },
    {
      "epoch": 39.927565392354126,
      "grad_norm": 1.0194097757339478,
      "learning_rate": 0.00012015695744038635,
      "loss": 0.1776,
      "step": 19844
    },
    {
      "epoch": 39.929577464788736,
      "grad_norm": 1.0495474338531494,
      "learning_rate": 0.00012015293289063288,
      "loss": 0.1994,
      "step": 19845
    },
    {
      "epoch": 39.93158953722334,
      "grad_norm": 0.9541261792182922,
      "learning_rate": 0.00012014890834087938,
      "loss": 0.1787,
      "step": 19846
    },
    {
      "epoch": 39.93360160965795,
      "grad_norm": 1.018233060836792,
      "learning_rate": 0.00012014488379112587,
      "loss": 0.1952,
      "step": 19847
    },
    {
      "epoch": 39.93561368209256,
      "grad_norm": 1.0664631128311157,
      "learning_rate": 0.00012014085924137237,
      "loss": 0.1989,
      "step": 19848
    },
    {
      "epoch": 39.93762575452716,
      "grad_norm": 1.0584125518798828,
      "learning_rate": 0.00012013683469161887,
      "loss": 0.1983,
      "step": 19849
    },
    {
      "epoch": 39.93963782696177,
      "grad_norm": 1.0283812284469604,
      "learning_rate": 0.0001201328101418654,
      "loss": 0.1781,
      "step": 19850
    },
    {
      "epoch": 39.94164989939638,
      "grad_norm": 1.1007474660873413,
      "learning_rate": 0.0001201287855921119,
      "loss": 0.2019,
      "step": 19851
    },
    {
      "epoch": 39.943661971830984,
      "grad_norm": 1.0370343923568726,
      "learning_rate": 0.0001201247610423584,
      "loss": 0.1988,
      "step": 19852
    },
    {
      "epoch": 39.945674044265594,
      "grad_norm": 0.9918121099472046,
      "learning_rate": 0.0001201207364926049,
      "loss": 0.1887,
      "step": 19853
    },
    {
      "epoch": 39.947686116700204,
      "grad_norm": 1.000645637512207,
      "learning_rate": 0.00012011671194285139,
      "loss": 0.1863,
      "step": 19854
    },
    {
      "epoch": 39.94969818913481,
      "grad_norm": 1.0495615005493164,
      "learning_rate": 0.0001201126873930979,
      "loss": 0.2094,
      "step": 19855
    },
    {
      "epoch": 39.95171026156942,
      "grad_norm": 1.0794905424118042,
      "learning_rate": 0.00012010866284334442,
      "loss": 0.1942,
      "step": 19856
    },
    {
      "epoch": 39.95372233400403,
      "grad_norm": 1.1434710025787354,
      "learning_rate": 0.00012010463829359092,
      "loss": 0.2003,
      "step": 19857
    },
    {
      "epoch": 39.95573440643863,
      "grad_norm": 1.030507206916809,
      "learning_rate": 0.00012010061374383741,
      "loss": 0.1822,
      "step": 19858
    },
    {
      "epoch": 39.95774647887324,
      "grad_norm": 1.0190449953079224,
      "learning_rate": 0.00012009658919408391,
      "loss": 0.1752,
      "step": 19859
    },
    {
      "epoch": 39.95975855130785,
      "grad_norm": 0.9798325300216675,
      "learning_rate": 0.00012009256464433043,
      "loss": 0.1812,
      "step": 19860
    },
    {
      "epoch": 39.96177062374245,
      "grad_norm": 1.0713568925857544,
      "learning_rate": 0.00012008854009457692,
      "loss": 0.1949,
      "step": 19861
    },
    {
      "epoch": 39.96378269617706,
      "grad_norm": 1.0462124347686768,
      "learning_rate": 0.00012008451554482342,
      "loss": 0.19,
      "step": 19862
    },
    {
      "epoch": 39.96579476861167,
      "grad_norm": 1.0431935787200928,
      "learning_rate": 0.00012008049099506992,
      "loss": 0.1847,
      "step": 19863
    },
    {
      "epoch": 39.967806841046276,
      "grad_norm": 1.0371065139770508,
      "learning_rate": 0.00012007646644531642,
      "loss": 0.1995,
      "step": 19864
    },
    {
      "epoch": 39.969818913480886,
      "grad_norm": 1.036553978919983,
      "learning_rate": 0.00012007244189556295,
      "loss": 0.1815,
      "step": 19865
    },
    {
      "epoch": 39.971830985915496,
      "grad_norm": 1.0517743825912476,
      "learning_rate": 0.00012006841734580944,
      "loss": 0.2042,
      "step": 19866
    },
    {
      "epoch": 39.9738430583501,
      "grad_norm": 1.0692404508590698,
      "learning_rate": 0.00012006439279605594,
      "loss": 0.1866,
      "step": 19867
    },
    {
      "epoch": 39.97585513078471,
      "grad_norm": 1.0287023782730103,
      "learning_rate": 0.00012006036824630244,
      "loss": 0.1828,
      "step": 19868
    },
    {
      "epoch": 39.97786720321932,
      "grad_norm": 1.0705221891403198,
      "learning_rate": 0.00012005634369654894,
      "loss": 0.1981,
      "step": 19869
    },
    {
      "epoch": 39.97987927565392,
      "grad_norm": 1.0251466035842896,
      "learning_rate": 0.00012005231914679547,
      "loss": 0.1886,
      "step": 19870
    },
    {
      "epoch": 39.98189134808853,
      "grad_norm": 1.0603737831115723,
      "learning_rate": 0.00012004829459704196,
      "loss": 0.2055,
      "step": 19871
    },
    {
      "epoch": 39.98390342052314,
      "grad_norm": 1.0586540699005127,
      "learning_rate": 0.00012004427004728846,
      "loss": 0.1915,
      "step": 19872
    },
    {
      "epoch": 39.985915492957744,
      "grad_norm": 1.0444337129592896,
      "learning_rate": 0.00012004024549753496,
      "loss": 0.1916,
      "step": 19873
    },
    {
      "epoch": 39.987927565392354,
      "grad_norm": 1.0549172163009644,
      "learning_rate": 0.00012003622094778146,
      "loss": 0.2003,
      "step": 19874
    },
    {
      "epoch": 39.989939637826964,
      "grad_norm": 1.0551937818527222,
      "learning_rate": 0.00012003219639802799,
      "loss": 0.1972,
      "step": 19875
    },
    {
      "epoch": 39.99195171026157,
      "grad_norm": 1.0486500263214111,
      "learning_rate": 0.00012002817184827449,
      "loss": 0.1959,
      "step": 19876
    },
    {
      "epoch": 39.99396378269618,
      "grad_norm": 1.014207363128662,
      "learning_rate": 0.00012002414729852098,
      "loss": 0.1946,
      "step": 19877
    },
    {
      "epoch": 39.99597585513079,
      "grad_norm": 1.0621229410171509,
      "learning_rate": 0.00012002012274876748,
      "loss": 0.1744,
      "step": 19878
    },
    {
      "epoch": 39.99798792756539,
      "grad_norm": 1.0744717121124268,
      "learning_rate": 0.00012001609819901398,
      "loss": 0.2116,
      "step": 19879
    },
    {
      "epoch": 40.0,
      "grad_norm": 1.1089998483657837,
      "learning_rate": 0.00012001207364926051,
      "loss": 0.2053,
      "step": 19880
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.6662410497665405,
      "eval_runtime": 49.9059,
      "eval_samples_per_second": 19.877,
      "eval_steps_per_second": 2.485,
      "step": 19880
    },
    {
      "epoch": 40.00201207243461,
      "grad_norm": 0.8476727604866028,
      "learning_rate": 0.000120008049099507,
      "loss": 0.1416,
      "step": 19881
    },
    {
      "epoch": 40.00402414486921,
      "grad_norm": 0.8272978663444519,
      "learning_rate": 0.0001200040245497535,
      "loss": 0.1377,
      "step": 19882
    },
    {
      "epoch": 40.00603621730382,
      "grad_norm": 0.9425609111785889,
      "learning_rate": 0.00012,
      "loss": 0.1481,
      "step": 19883
    },
    {
      "epoch": 40.00804828973843,
      "grad_norm": 0.8762916922569275,
      "learning_rate": 0.0001199959754502465,
      "loss": 0.1395,
      "step": 19884
    },
    {
      "epoch": 40.010060362173036,
      "grad_norm": 0.9698245525360107,
      "learning_rate": 0.00011999195090049303,
      "loss": 0.1495,
      "step": 19885
    },
    {
      "epoch": 40.012072434607646,
      "grad_norm": 0.8863254189491272,
      "learning_rate": 0.00011998792635073953,
      "loss": 0.1355,
      "step": 19886
    },
    {
      "epoch": 40.014084507042256,
      "grad_norm": 0.969272255897522,
      "learning_rate": 0.00011998390180098602,
      "loss": 0.1618,
      "step": 19887
    },
    {
      "epoch": 40.01609657947686,
      "grad_norm": 0.9460046291351318,
      "learning_rate": 0.00011997987725123252,
      "loss": 0.1421,
      "step": 19888
    },
    {
      "epoch": 40.01810865191147,
      "grad_norm": 0.8725882172584534,
      "learning_rate": 0.00011997585270147902,
      "loss": 0.1384,
      "step": 19889
    },
    {
      "epoch": 40.02012072434608,
      "grad_norm": 0.8079214096069336,
      "learning_rate": 0.00011997182815172553,
      "loss": 0.1329,
      "step": 19890
    },
    {
      "epoch": 40.02213279678068,
      "grad_norm": 0.8850317597389221,
      "learning_rate": 0.00011996780360197203,
      "loss": 0.1402,
      "step": 19891
    },
    {
      "epoch": 40.02414486921529,
      "grad_norm": 0.8489805459976196,
      "learning_rate": 0.00011996377905221855,
      "loss": 0.1387,
      "step": 19892
    },
    {
      "epoch": 40.0261569416499,
      "grad_norm": 0.8596316576004028,
      "learning_rate": 0.00011995975450246504,
      "loss": 0.1479,
      "step": 19893
    },
    {
      "epoch": 40.028169014084504,
      "grad_norm": 0.8439486026763916,
      "learning_rate": 0.00011995572995271154,
      "loss": 0.1393,
      "step": 19894
    },
    {
      "epoch": 40.030181086519114,
      "grad_norm": 0.8723027110099792,
      "learning_rate": 0.00011995170540295805,
      "loss": 0.1402,
      "step": 19895
    },
    {
      "epoch": 40.032193158953724,
      "grad_norm": 0.9928231835365295,
      "learning_rate": 0.00011994768085320455,
      "loss": 0.1598,
      "step": 19896
    },
    {
      "epoch": 40.03420523138833,
      "grad_norm": 0.914997935295105,
      "learning_rate": 0.00011994365630345105,
      "loss": 0.1477,
      "step": 19897
    },
    {
      "epoch": 40.03621730382294,
      "grad_norm": 0.8942468762397766,
      "learning_rate": 0.00011993963175369755,
      "loss": 0.1421,
      "step": 19898
    },
    {
      "epoch": 40.03822937625755,
      "grad_norm": 0.863298237323761,
      "learning_rate": 0.00011993560720394405,
      "loss": 0.1404,
      "step": 19899
    },
    {
      "epoch": 40.04024144869215,
      "grad_norm": 0.8806853890419006,
      "learning_rate": 0.00011993158265419058,
      "loss": 0.1414,
      "step": 19900
    },
    {
      "epoch": 40.04225352112676,
      "grad_norm": 0.9121798872947693,
      "learning_rate": 0.00011992755810443707,
      "loss": 0.1373,
      "step": 19901
    },
    {
      "epoch": 40.04426559356137,
      "grad_norm": 0.9058262705802917,
      "learning_rate": 0.00011992353355468357,
      "loss": 0.1452,
      "step": 19902
    },
    {
      "epoch": 40.04627766599597,
      "grad_norm": 0.92441725730896,
      "learning_rate": 0.00011991950900493007,
      "loss": 0.1561,
      "step": 19903
    },
    {
      "epoch": 40.04828973843058,
      "grad_norm": 0.8371920585632324,
      "learning_rate": 0.00011991548445517657,
      "loss": 0.1385,
      "step": 19904
    },
    {
      "epoch": 40.05030181086519,
      "grad_norm": 0.8735076189041138,
      "learning_rate": 0.0001199114599054231,
      "loss": 0.1365,
      "step": 19905
    },
    {
      "epoch": 40.052313883299796,
      "grad_norm": 0.9518005847930908,
      "learning_rate": 0.0001199074353556696,
      "loss": 0.1488,
      "step": 19906
    },
    {
      "epoch": 40.054325955734406,
      "grad_norm": 0.9977352619171143,
      "learning_rate": 0.00011990341080591609,
      "loss": 0.1467,
      "step": 19907
    },
    {
      "epoch": 40.056338028169016,
      "grad_norm": 0.9114513397216797,
      "learning_rate": 0.00011989938625616259,
      "loss": 0.1327,
      "step": 19908
    },
    {
      "epoch": 40.05835010060362,
      "grad_norm": 0.8702793121337891,
      "learning_rate": 0.00011989536170640909,
      "loss": 0.1425,
      "step": 19909
    },
    {
      "epoch": 40.06036217303823,
      "grad_norm": 0.9084246754646301,
      "learning_rate": 0.00011989133715665562,
      "loss": 0.1472,
      "step": 19910
    },
    {
      "epoch": 40.06237424547284,
      "grad_norm": 0.9498435854911804,
      "learning_rate": 0.00011988731260690211,
      "loss": 0.1545,
      "step": 19911
    },
    {
      "epoch": 40.06438631790744,
      "grad_norm": 0.8816633820533752,
      "learning_rate": 0.00011988328805714861,
      "loss": 0.1355,
      "step": 19912
    },
    {
      "epoch": 40.06639839034205,
      "grad_norm": 0.9197270274162292,
      "learning_rate": 0.00011987926350739511,
      "loss": 0.1564,
      "step": 19913
    },
    {
      "epoch": 40.06841046277666,
      "grad_norm": 0.8831272125244141,
      "learning_rate": 0.00011987523895764161,
      "loss": 0.1403,
      "step": 19914
    },
    {
      "epoch": 40.070422535211264,
      "grad_norm": 0.8956292867660522,
      "learning_rate": 0.00011987121440788814,
      "loss": 0.1429,
      "step": 19915
    },
    {
      "epoch": 40.072434607645874,
      "grad_norm": 0.8798077702522278,
      "learning_rate": 0.00011986718985813464,
      "loss": 0.148,
      "step": 19916
    },
    {
      "epoch": 40.074446680080484,
      "grad_norm": 0.8178616166114807,
      "learning_rate": 0.00011986316530838113,
      "loss": 0.1247,
      "step": 19917
    },
    {
      "epoch": 40.07645875251509,
      "grad_norm": 0.9114243984222412,
      "learning_rate": 0.00011985914075862763,
      "loss": 0.1528,
      "step": 19918
    },
    {
      "epoch": 40.0784708249497,
      "grad_norm": 0.9445026516914368,
      "learning_rate": 0.00011985511620887413,
      "loss": 0.1384,
      "step": 19919
    },
    {
      "epoch": 40.08048289738431,
      "grad_norm": 0.9615182876586914,
      "learning_rate": 0.00011985109165912066,
      "loss": 0.1396,
      "step": 19920
    },
    {
      "epoch": 40.08249496981891,
      "grad_norm": 0.9278621673583984,
      "learning_rate": 0.00011984706710936716,
      "loss": 0.1367,
      "step": 19921
    },
    {
      "epoch": 40.08450704225352,
      "grad_norm": 0.880879282951355,
      "learning_rate": 0.00011984304255961365,
      "loss": 0.1397,
      "step": 19922
    },
    {
      "epoch": 40.08651911468813,
      "grad_norm": 0.858040988445282,
      "learning_rate": 0.00011983901800986015,
      "loss": 0.1504,
      "step": 19923
    },
    {
      "epoch": 40.08853118712273,
      "grad_norm": 0.9186462759971619,
      "learning_rate": 0.00011983499346010665,
      "loss": 0.1455,
      "step": 19924
    },
    {
      "epoch": 40.09054325955734,
      "grad_norm": 0.9144986867904663,
      "learning_rate": 0.00011983096891035316,
      "loss": 0.1509,
      "step": 19925
    },
    {
      "epoch": 40.09255533199195,
      "grad_norm": 0.926648736000061,
      "learning_rate": 0.00011982694436059966,
      "loss": 0.1506,
      "step": 19926
    },
    {
      "epoch": 40.094567404426556,
      "grad_norm": 0.8358964920043945,
      "learning_rate": 0.00011982291981084617,
      "loss": 0.135,
      "step": 19927
    },
    {
      "epoch": 40.096579476861166,
      "grad_norm": 0.9670107364654541,
      "learning_rate": 0.00011981889526109267,
      "loss": 0.1537,
      "step": 19928
    },
    {
      "epoch": 40.098591549295776,
      "grad_norm": 0.9640198945999146,
      "learning_rate": 0.00011981487071133917,
      "loss": 0.1349,
      "step": 19929
    },
    {
      "epoch": 40.100603621730386,
      "grad_norm": 0.9428364038467407,
      "learning_rate": 0.00011981084616158568,
      "loss": 0.1518,
      "step": 19930
    },
    {
      "epoch": 40.10261569416499,
      "grad_norm": 0.9523539543151855,
      "learning_rate": 0.00011980682161183218,
      "loss": 0.1563,
      "step": 19931
    },
    {
      "epoch": 40.1046277665996,
      "grad_norm": 0.8676078915596008,
      "learning_rate": 0.00011980279706207868,
      "loss": 0.1355,
      "step": 19932
    },
    {
      "epoch": 40.10663983903421,
      "grad_norm": 0.9169543385505676,
      "learning_rate": 0.00011979877251232518,
      "loss": 0.1548,
      "step": 19933
    },
    {
      "epoch": 40.10865191146881,
      "grad_norm": 0.9428091645240784,
      "learning_rate": 0.00011979474796257168,
      "loss": 0.1558,
      "step": 19934
    },
    {
      "epoch": 40.11066398390342,
      "grad_norm": 0.9725867509841919,
      "learning_rate": 0.0001197907234128182,
      "loss": 0.1587,
      "step": 19935
    },
    {
      "epoch": 40.11267605633803,
      "grad_norm": 0.9134925007820129,
      "learning_rate": 0.0001197866988630647,
      "loss": 0.1527,
      "step": 19936
    },
    {
      "epoch": 40.114688128772634,
      "grad_norm": 1.0031431913375854,
      "learning_rate": 0.0001197826743133112,
      "loss": 0.1654,
      "step": 19937
    },
    {
      "epoch": 40.116700201207244,
      "grad_norm": 0.9095102548599243,
      "learning_rate": 0.0001197786497635577,
      "loss": 0.1418,
      "step": 19938
    },
    {
      "epoch": 40.118712273641854,
      "grad_norm": 0.9915596842765808,
      "learning_rate": 0.0001197746252138042,
      "loss": 0.1497,
      "step": 19939
    },
    {
      "epoch": 40.12072434607646,
      "grad_norm": 0.9331305027008057,
      "learning_rate": 0.00011977060066405073,
      "loss": 0.1451,
      "step": 19940
    },
    {
      "epoch": 40.12273641851107,
      "grad_norm": 0.9338520169258118,
      "learning_rate": 0.00011976657611429722,
      "loss": 0.1386,
      "step": 19941
    },
    {
      "epoch": 40.12474849094568,
      "grad_norm": 0.9070796966552734,
      "learning_rate": 0.00011976255156454372,
      "loss": 0.1437,
      "step": 19942
    },
    {
      "epoch": 40.12676056338028,
      "grad_norm": 0.9320315718650818,
      "learning_rate": 0.00011975852701479022,
      "loss": 0.1359,
      "step": 19943
    },
    {
      "epoch": 40.12877263581489,
      "grad_norm": 0.9745854735374451,
      "learning_rate": 0.00011975450246503672,
      "loss": 0.1589,
      "step": 19944
    },
    {
      "epoch": 40.1307847082495,
      "grad_norm": 0.9640057682991028,
      "learning_rate": 0.00011975047791528325,
      "loss": 0.1584,
      "step": 19945
    },
    {
      "epoch": 40.1327967806841,
      "grad_norm": 0.936876654624939,
      "learning_rate": 0.00011974645336552974,
      "loss": 0.153,
      "step": 19946
    },
    {
      "epoch": 40.13480885311871,
      "grad_norm": 0.937371551990509,
      "learning_rate": 0.00011974242881577624,
      "loss": 0.155,
      "step": 19947
    },
    {
      "epoch": 40.13682092555332,
      "grad_norm": 0.9482196569442749,
      "learning_rate": 0.00011973840426602274,
      "loss": 0.1554,
      "step": 19948
    },
    {
      "epoch": 40.138832997987926,
      "grad_norm": 0.975613534450531,
      "learning_rate": 0.00011973437971626924,
      "loss": 0.1493,
      "step": 19949
    },
    {
      "epoch": 40.140845070422536,
      "grad_norm": 0.9455550909042358,
      "learning_rate": 0.00011973035516651577,
      "loss": 0.1516,
      "step": 19950
    },
    {
      "epoch": 40.142857142857146,
      "grad_norm": 0.9147980809211731,
      "learning_rate": 0.00011972633061676226,
      "loss": 0.1418,
      "step": 19951
    },
    {
      "epoch": 40.14486921529175,
      "grad_norm": 0.9456382989883423,
      "learning_rate": 0.00011972230606700876,
      "loss": 0.1441,
      "step": 19952
    },
    {
      "epoch": 40.14688128772636,
      "grad_norm": 0.8601709604263306,
      "learning_rate": 0.00011971828151725526,
      "loss": 0.1388,
      "step": 19953
    },
    {
      "epoch": 40.14889336016097,
      "grad_norm": 0.9016171097755432,
      "learning_rate": 0.00011971425696750176,
      "loss": 0.1482,
      "step": 19954
    },
    {
      "epoch": 40.15090543259557,
      "grad_norm": 0.9189863801002502,
      "learning_rate": 0.00011971023241774829,
      "loss": 0.1421,
      "step": 19955
    },
    {
      "epoch": 40.15291750503018,
      "grad_norm": 0.8785513639450073,
      "learning_rate": 0.00011970620786799479,
      "loss": 0.1358,
      "step": 19956
    },
    {
      "epoch": 40.15492957746479,
      "grad_norm": 0.9885522723197937,
      "learning_rate": 0.00011970218331824128,
      "loss": 0.1495,
      "step": 19957
    },
    {
      "epoch": 40.156941649899395,
      "grad_norm": 0.8985264897346497,
      "learning_rate": 0.00011969815876848778,
      "loss": 0.1338,
      "step": 19958
    },
    {
      "epoch": 40.158953722334005,
      "grad_norm": 0.869232714176178,
      "learning_rate": 0.00011969413421873428,
      "loss": 0.1341,
      "step": 19959
    },
    {
      "epoch": 40.160965794768615,
      "grad_norm": 0.8983924984931946,
      "learning_rate": 0.00011969010966898079,
      "loss": 0.1399,
      "step": 19960
    },
    {
      "epoch": 40.16297786720322,
      "grad_norm": 0.9767581820487976,
      "learning_rate": 0.00011968608511922729,
      "loss": 0.1626,
      "step": 19961
    },
    {
      "epoch": 40.16498993963783,
      "grad_norm": 0.8706032037734985,
      "learning_rate": 0.00011968206056947379,
      "loss": 0.1328,
      "step": 19962
    },
    {
      "epoch": 40.16700201207244,
      "grad_norm": 0.9369900822639465,
      "learning_rate": 0.0001196780360197203,
      "loss": 0.1492,
      "step": 19963
    },
    {
      "epoch": 40.16901408450704,
      "grad_norm": 0.9271478652954102,
      "learning_rate": 0.0001196740114699668,
      "loss": 0.1524,
      "step": 19964
    },
    {
      "epoch": 40.17102615694165,
      "grad_norm": 0.888614296913147,
      "learning_rate": 0.00011966998692021331,
      "loss": 0.1449,
      "step": 19965
    },
    {
      "epoch": 40.17303822937626,
      "grad_norm": 0.9675681591033936,
      "learning_rate": 0.00011966596237045981,
      "loss": 0.1557,
      "step": 19966
    },
    {
      "epoch": 40.17505030181086,
      "grad_norm": 1.008184790611267,
      "learning_rate": 0.00011966193782070631,
      "loss": 0.1508,
      "step": 19967
    },
    {
      "epoch": 40.17706237424547,
      "grad_norm": 0.8981128334999084,
      "learning_rate": 0.00011965791327095281,
      "loss": 0.1421,
      "step": 19968
    },
    {
      "epoch": 40.17907444668008,
      "grad_norm": 0.9101775288581848,
      "learning_rate": 0.00011965388872119931,
      "loss": 0.1474,
      "step": 19969
    },
    {
      "epoch": 40.181086519114686,
      "grad_norm": 0.9233794808387756,
      "learning_rate": 0.00011964986417144583,
      "loss": 0.1443,
      "step": 19970
    },
    {
      "epoch": 40.183098591549296,
      "grad_norm": 0.9402973651885986,
      "learning_rate": 0.00011964583962169233,
      "loss": 0.1397,
      "step": 19971
    },
    {
      "epoch": 40.185110663983906,
      "grad_norm": 0.8955156803131104,
      "learning_rate": 0.00011964181507193883,
      "loss": 0.1529,
      "step": 19972
    },
    {
      "epoch": 40.18712273641851,
      "grad_norm": 0.896224319934845,
      "learning_rate": 0.00011963779052218533,
      "loss": 0.1418,
      "step": 19973
    },
    {
      "epoch": 40.18913480885312,
      "grad_norm": 0.9571007490158081,
      "learning_rate": 0.00011963376597243183,
      "loss": 0.1567,
      "step": 19974
    },
    {
      "epoch": 40.19114688128773,
      "grad_norm": 0.9102354645729065,
      "learning_rate": 0.00011962974142267835,
      "loss": 0.1486,
      "step": 19975
    },
    {
      "epoch": 40.19315895372233,
      "grad_norm": 0.8912198543548584,
      "learning_rate": 0.00011962571687292485,
      "loss": 0.1393,
      "step": 19976
    },
    {
      "epoch": 40.19517102615694,
      "grad_norm": 0.8564490675926208,
      "learning_rate": 0.00011962169232317135,
      "loss": 0.1382,
      "step": 19977
    },
    {
      "epoch": 40.19718309859155,
      "grad_norm": 0.9405035972595215,
      "learning_rate": 0.00011961766777341785,
      "loss": 0.1481,
      "step": 19978
    },
    {
      "epoch": 40.199195171026155,
      "grad_norm": 0.8521263003349304,
      "learning_rate": 0.00011961364322366435,
      "loss": 0.1486,
      "step": 19979
    },
    {
      "epoch": 40.201207243460765,
      "grad_norm": 0.9602308869361877,
      "learning_rate": 0.00011960961867391087,
      "loss": 0.1609,
      "step": 19980
    },
    {
      "epoch": 40.203219315895375,
      "grad_norm": 0.962181568145752,
      "learning_rate": 0.00011960559412415737,
      "loss": 0.1562,
      "step": 19981
    },
    {
      "epoch": 40.20523138832998,
      "grad_norm": 0.9691664576530457,
      "learning_rate": 0.00011960156957440387,
      "loss": 0.1646,
      "step": 19982
    },
    {
      "epoch": 40.20724346076459,
      "grad_norm": 0.9508830904960632,
      "learning_rate": 0.00011959754502465037,
      "loss": 0.1451,
      "step": 19983
    },
    {
      "epoch": 40.2092555331992,
      "grad_norm": 0.9144584536552429,
      "learning_rate": 0.00011959352047489687,
      "loss": 0.1443,
      "step": 19984
    },
    {
      "epoch": 40.2112676056338,
      "grad_norm": 0.9160924553871155,
      "learning_rate": 0.0001195894959251434,
      "loss": 0.1582,
      "step": 19985
    },
    {
      "epoch": 40.21327967806841,
      "grad_norm": 0.9367079138755798,
      "learning_rate": 0.0001195854713753899,
      "loss": 0.1508,
      "step": 19986
    },
    {
      "epoch": 40.21529175050302,
      "grad_norm": 1.0592069625854492,
      "learning_rate": 0.00011958144682563639,
      "loss": 0.1578,
      "step": 19987
    },
    {
      "epoch": 40.21730382293762,
      "grad_norm": 0.8692222237586975,
      "learning_rate": 0.00011957742227588289,
      "loss": 0.1408,
      "step": 19988
    },
    {
      "epoch": 40.21931589537223,
      "grad_norm": 0.9699718356132507,
      "learning_rate": 0.00011957339772612939,
      "loss": 0.1657,
      "step": 19989
    },
    {
      "epoch": 40.22132796780684,
      "grad_norm": 0.9814648032188416,
      "learning_rate": 0.00011956937317637592,
      "loss": 0.1601,
      "step": 19990
    },
    {
      "epoch": 40.223340040241446,
      "grad_norm": 0.9680872559547424,
      "learning_rate": 0.00011956534862662241,
      "loss": 0.1552,
      "step": 19991
    },
    {
      "epoch": 40.225352112676056,
      "grad_norm": 0.9724818468093872,
      "learning_rate": 0.00011956132407686891,
      "loss": 0.1496,
      "step": 19992
    },
    {
      "epoch": 40.227364185110666,
      "grad_norm": 0.9997161626815796,
      "learning_rate": 0.00011955729952711541,
      "loss": 0.166,
      "step": 19993
    },
    {
      "epoch": 40.22937625754527,
      "grad_norm": 0.9753178954124451,
      "learning_rate": 0.00011955327497736191,
      "loss": 0.16,
      "step": 19994
    },
    {
      "epoch": 40.23138832997988,
      "grad_norm": 0.9322443008422852,
      "learning_rate": 0.00011954925042760842,
      "loss": 0.1525,
      "step": 19995
    },
    {
      "epoch": 40.23340040241449,
      "grad_norm": 0.9080924391746521,
      "learning_rate": 0.00011954522587785492,
      "loss": 0.1453,
      "step": 19996
    },
    {
      "epoch": 40.23541247484909,
      "grad_norm": 0.9716349244117737,
      "learning_rate": 0.00011954120132810142,
      "loss": 0.1452,
      "step": 19997
    },
    {
      "epoch": 40.2374245472837,
      "grad_norm": 0.9779914021492004,
      "learning_rate": 0.00011953717677834793,
      "loss": 0.168,
      "step": 19998
    },
    {
      "epoch": 40.23943661971831,
      "grad_norm": 1.0241211652755737,
      "learning_rate": 0.00011953315222859443,
      "loss": 0.1568,
      "step": 19999
    },
    {
      "epoch": 40.241448692152915,
      "grad_norm": 0.8972574472427368,
      "learning_rate": 0.00011952912767884094,
      "loss": 0.1525,
      "step": 20000
    }
  ],
  "logging_steps": 1,
  "max_steps": 49700,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.870406416971858e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
